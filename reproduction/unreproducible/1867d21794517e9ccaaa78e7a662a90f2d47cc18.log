[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Kafka-In-Action                                                    [pom]
[INFO] chapter2                                                           [jar]
[INFO] chapter3                                                           [jar]
[INFO] chapter4                                                           [jar]
[INFO] chapter5                                                           [jar]
[INFO] chapter6-no-java                                                   [jar]
[INFO] chapter7                                                           [jar]
[INFO] chapter8-no-java                                                   [jar]
[INFO] chapter9-no-java                                                   [jar]
[INFO] chapter10-no-java                                                  [jar]
[INFO] chapter11                                                          [jar]
[INFO] chapter12                                                          [jar]
[INFO] appendixB-no-java                                                  [jar]
[INFO] 
[INFO] -----------------< org.kafkainaction:Kafka-In-Action >------------------
[INFO] Building Kafka-In-Action 1.0.0-SNAPSHOT                           [1/13]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ Kafka-In-Action ---
[INFO] 
[INFO] ---------------------< org.kafkainaction:chapter2 >---------------------
[INFO] Building chapter2 1.0.0-SNAPSHOT                                  [2/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter2 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter2/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter2 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 2 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter2/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter2 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter2/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter2 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter2 ---
[INFO] No tests to run.
[INFO] 
[INFO] ---------------------< org.kafkainaction:chapter3 >---------------------
[INFO] Building chapter3 1.0.0-SNAPSHOT                                  [3/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter3 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter3/target
[INFO] 
[INFO] --- avro-maven-plugin:1.10.2:schema (default) @ chapter3 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter3 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter3 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 4 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter3/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter3 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter3/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter3 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter3 ---
[INFO] No tests to run.
[INFO] 
[INFO] ---------------------< org.kafkainaction:chapter4 >---------------------
[INFO] Building chapter4 1.0.0-SNAPSHOT                                  [4/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter4 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter4/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter4 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter4 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 9 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter4/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter4 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter4/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter4 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter4 ---
[INFO] No tests to run.
[INFO] 
[INFO] ---------------------< org.kafkainaction:chapter5 >---------------------
[INFO] Building chapter5 1.0.0-SNAPSHOT                                  [5/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter5 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter5/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter5 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter5 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 10 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter5/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter5 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter5/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter5 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter5 ---
[INFO] No tests to run.
[INFO] 
[INFO] -----------------< org.kafkainaction:chapter6-no-java >-----------------
[INFO] Building chapter6-no-java 1.0.0-SNAPSHOT                          [6/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter6-no-java ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter6/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter6-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter6-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter6-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter6/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter6-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter6-no-java ---
[INFO] No tests to run.
[INFO] 
[INFO] ---------------------< org.kafkainaction:chapter7 >---------------------
[INFO] Building chapter7 1.0.0-SNAPSHOT                                  [7/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter7 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter7/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter7 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter7 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 5 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter7 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter7/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter7 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1 source file to /home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter7 ---
[INFO] Surefire report directory: /home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.kafkainaction.producer.EmbeddedKafkaClusterTest
[33m2023-01-13 11:23:20[0;39m [36mkafka.utils.Log4jControllerRegistration$[0;39m [34m[INFO][0;39m [32m(Logging.scala:31)[0;39m - Registered kafka:type=kafka.Log4jController MBean
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:host.name=repairnator
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.version=11.0.17
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.vendor=Ubuntu
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.class.path=/home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/test-classes:/home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/classes:/home/gabsko/.m2/repository/org/apache/kafka/kafka-streams/2.7.1/kafka-streams-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/connect-json/2.7.1/connect-json-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/connect-api/2.7.1/connect-api-2.7.1.jar:/home/gabsko/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/gabsko/.m2/repository/org/rocksdb/rocksdbjni/5.18.4/rocksdbjni-5.18.4.jar:/home/gabsko/.m2/repository/org/apache/commons/commons-lang3/3.0/commons-lang3-3.0.jar:/home/gabsko/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/home/gabsko/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-clients/2.7.1/kafka-clients-2.7.1-test.jar:/home/gabsko/.m2/repository/com/github/luben/zstd-jni/1.4.5-6/zstd-jni-1.4.5-6.jar:/home/gabsko/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/gabsko/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.7/snappy-java-1.1.7.7.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-streams/2.7.1/kafka-streams-2.7.1-test.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-streams-test-utils/2.7.1/kafka-streams-test-utils-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka_2.12/2.7.1/kafka_2.12-2.7.1-test.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-raft/2.7.1/kafka-raft-2.7.1.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.5.1/jackson-databind-2.10.5.1.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.10.5/jackson-annotations-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.5/jackson-core-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.10.5/jackson-module-scala_2.12-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.10.5/jackson-module-paranamer-2.10.5.jar:/home/gabsko/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.10.5/jackson-dataformat-csv-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.5/jackson-datatype-jdk8-2.10.5.jar:/home/gabsko/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/home/gabsko/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/gabsko/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.2.0/scala-collection-compat_2.12-2.2.0.jar:/home/gabsko/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/0.9.1/scala-java8-compat_2.12-0.9.1.jar:/home/gabsko/.m2/repository/org/scala-lang/scala-library/2.12.12/scala-library-2.12.12.jar:/home/gabsko/.m2/repository/org/scala-lang/scala-reflect/2.12.12/scala-reflect-2.12.12.jar:/home/gabsko/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.9.2/scala-logging_2.12-3.9.2.jar:/home/gabsko/.m2/repository/org/apache/zookeeper/zookeeper/3.5.9/zookeeper-3.5.9.jar:/home/gabsko/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.9/zookeeper-jute-3.5.9.jar:/home/gabsko/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/gabsko/.m2/repository/io/netty/netty-handler/4.1.50.Final/netty-handler-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-common/4.1.50.Final/netty-common-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-resolver/4.1.50.Final/netty-resolver-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-buffer/4.1.50.Final/netty-buffer-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-transport/4.1.50.Final/netty-transport-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-codec/4.1.50.Final/netty-codec-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-transport-native-epoll/4.1.50.Final/netty-transport-native-epoll-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.50.Final/netty-transport-native-unix-common-4.1.50.Final.jar:/home/gabsko/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka_2.12/2.7.1/kafka_2.12-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-clients/2.7.1/kafka-clients-2.7.1.jar:/home/gabsko/.m2/repository/ch/qos/logback/logback-classic/1.2.4-groovyless/logback-classic-1.2.4-groovyless.jar:/home/gabsko/.m2/repository/ch/qos/logback/logback-core/1.2.4-groovyless/logback-core-1.2.4-groovyless.jar:
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.io.tmpdir=/tmp
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:java.compiler=<NA>
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:os.name=Linux
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:os.arch=amd64
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:os.version=5.4.0-113-generic
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:user.name=gabsko
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:user.home=/home/gabsko
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:user.dir=/home/gabsko/breaking-updates/KafkaInAction_Chapter7
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:os.memory.free=1981MB
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:os.memory.max=30688MB
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Server environment:os.memory.total=2016MB
[33m2023-01-13 11:23:20[0;39m [36mo.a.zookeeper.server.persistence.FileTxnSnapLog[0;39m [34m[INFO][0;39m [32m(FileTxnSnapLog.java:115)[0;39m - zookeeper.snapshot.trust.empty : false
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZKDatabase[0;39m [34m[INFO][0;39m [32m(ZKDatabase.java:117)[0;39m - zookeeper.snapshotSizeFactor = 0.33
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(ZooKeeperServer.java:953)[0;39m - minSessionTimeout set to 1600
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(ZooKeeperServer.java:962)[0;39m - maxSessionTimeout set to 16000
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(ZooKeeperServer.java:181)[0;39m - Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 datadir /tmp/kafka-584512265064018710/version-2 snapdir /tmp/kafka-6958187614132405405/version-2
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:673)[0;39m - Configuring NIO connection handler with 10s sessionless connection timeout, 4 selector thread(s), 72 worker threads, and 64 kB direct buffers.
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:686)[0;39m - binding to port /127.0.0.1:0
[33m2023-01-13 11:23:20[0;39m [36mo.a.zookeeper.server.persistence.FileTxnSnapLog[0;39m [34m[INFO][0;39m [32m(FileTxnSnapLog.java:404)[0;39m - Snapshotting: 0x0 to /tmp/kafka-6958187614132405405/version-2/snapshot.0
[33m2023-01-13 11:23:20[0;39m [36mo.a.zookeeper.server.persistence.FileTxnSnapLog[0;39m [34m[INFO][0;39m [32m(FileTxnSnapLog.java:404)[0;39m - Snapshotting: 0x0 to /tmp/kafka-6958187614132405405/version-2/snapshot.0
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.PrepRequestProcessor[0;39m [34m[INFO][0;39m [32m(PrepRequestProcessor.java:132)[0;39m - PrepRequestProcessor (sid:0) started, reconfigEnabled=false
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit2873730066927929797/junit6137519814873368186
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.common.X509Util[0;39m [34m[INFO][0;39m [32m(X509Util.java:79)[0;39m - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - starting
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Connecting to zookeeper on 127.0.0.1:42881
[33m2023-01-13 11:23:20[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:42881.
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:host.name=repairnator
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.version=11.0.17
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.vendor=Ubuntu
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.home=/usr/lib/jvm/java-11-openjdk-amd64
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.class.path=/home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/test-classes:/home/gabsko/breaking-updates/KafkaInAction_Chapter7/target/classes:/home/gabsko/.m2/repository/org/apache/kafka/kafka-streams/2.7.1/kafka-streams-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/connect-json/2.7.1/connect-json-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/connect-api/2.7.1/connect-api-2.7.1.jar:/home/gabsko/.m2/repository/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar:/home/gabsko/.m2/repository/org/rocksdb/rocksdbjni/5.18.4/rocksdbjni-5.18.4.jar:/home/gabsko/.m2/repository/org/apache/commons/commons-lang3/3.0/commons-lang3-3.0.jar:/home/gabsko/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/home/gabsko/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-clients/2.7.1/kafka-clients-2.7.1-test.jar:/home/gabsko/.m2/repository/com/github/luben/zstd-jni/1.4.5-6/zstd-jni-1.4.5-6.jar:/home/gabsko/.m2/repository/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar:/home/gabsko/.m2/repository/org/xerial/snappy/snappy-java/1.1.7.7/snappy-java-1.1.7.7.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-streams/2.7.1/kafka-streams-2.7.1-test.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-streams-test-utils/2.7.1/kafka-streams-test-utils-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka_2.12/2.7.1/kafka_2.12-2.7.1-test.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-raft/2.7.1/kafka-raft-2.7.1.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.5.1/jackson-databind-2.10.5.1.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.10.5/jackson-annotations-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.5/jackson-core-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.10.5/jackson-module-scala_2.12-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.10.5/jackson-module-paranamer-2.10.5.jar:/home/gabsko/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.10.5/jackson-dataformat-csv-2.10.5.jar:/home/gabsko/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.5/jackson-datatype-jdk8-2.10.5.jar:/home/gabsko/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/home/gabsko/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/gabsko/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.12/2.2.0/scala-collection-compat_2.12-2.2.0.jar:/home/gabsko/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/0.9.1/scala-java8-compat_2.12-0.9.1.jar:/home/gabsko/.m2/repository/org/scala-lang/scala-library/2.12.12/scala-library-2.12.12.jar:/home/gabsko/.m2/repository/org/scala-lang/scala-reflect/2.12.12/scala-reflect-2.12.12.jar:/home/gabsko/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.9.2/scala-logging_2.12-3.9.2.jar:/home/gabsko/.m2/repository/org/apache/zookeeper/zookeeper/3.5.9/zookeeper-3.5.9.jar:/home/gabsko/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.9/zookeeper-jute-3.5.9.jar:/home/gabsko/.m2/repository/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar:/home/gabsko/.m2/repository/io/netty/netty-handler/4.1.50.Final/netty-handler-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-common/4.1.50.Final/netty-common-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-resolver/4.1.50.Final/netty-resolver-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-buffer/4.1.50.Final/netty-buffer-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-transport/4.1.50.Final/netty-transport-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-codec/4.1.50.Final/netty-codec-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-transport-native-epoll/4.1.50.Final/netty-transport-native-epoll-4.1.50.Final.jar:/home/gabsko/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.50.Final/netty-transport-native-unix-common-4.1.50.Final.jar:/home/gabsko/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka_2.12/2.7.1/kafka_2.12-2.7.1.jar:/home/gabsko/.m2/repository/org/apache/kafka/kafka-clients/2.7.1/kafka-clients-2.7.1.jar:/home/gabsko/.m2/repository/ch/qos/logback/logback-classic/1.2.4-groovyless/logback-classic-1.2.4-groovyless.jar:/home/gabsko/.m2/repository/ch/qos/logback/logback-core/1.2.4-groovyless/logback-core-1.2.4-groovyless.jar:
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.io.tmpdir=/tmp
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:java.compiler=<NA>
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:os.name=Linux
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:os.arch=amd64
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:os.version=5.4.0-113-generic
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:user.name=gabsko
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:user.home=/home/gabsko
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:user.dir=/home/gabsko/breaking-updates/KafkaInAction_Chapter7
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:os.memory.free=1947MB
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:os.memory.max=30688MB
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(Environment.java:109)[0;39m - Client environment:os.memory.total=2016MB
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(ZooKeeper.java:868)[0;39m - Initiating client connection, connectString=127.0.0.1:42881 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@6c5945a7
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ClientCnxnSocket[0;39m [34m[INFO][0;39m [32m(ClientCnxnSocket.java:237)[0;39m - jute.maxbuffer value is 4194304 Bytes
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1653)[0;39m - zookeeper.request.timeout value is 0. feature enabled=
[33m2023-01-13 11:23:20[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Waiting until connected.
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1112)[0;39m - Opening socket connection to server localhost/127.0.0.1:42881. Will not attempt to authenticate using SASL (unknown error)
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:959)[0;39m - Socket connection established, initiating session, client: /127.0.0.1:34732, server: localhost/127.0.0.1:42881
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.server.persistence.FileTxnLog[0;39m [34m[INFO][0;39m [32m(FileTxnLog.java:218)[0;39m - Creating new log file: log.1
[33m2023-01-13 11:23:20[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1394)[0;39m - Session establishment complete on server localhost/127.0.0.1:42881, sessionid = 0x1048ffc08700000, negotiated timeout = 10000
[33m2023-01-13 11:23:20[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Connected.
[33m2023-01-13 11:23:20[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Starting
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.FinalizedFeatureChangeListener[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Feature ZK node at path: /feature does not exist
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.FinalizedFeatureCache[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Cleared cache
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Cluster ID = i1v9B5-MSx6J4saqVDHZMQ
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.BrokerMetadataCheckpoint[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - No meta.properties file under dir /tmp/junit2873730066927929797/junit6137519814873368186/meta.properties
[33m2023-01-13 11:23:20[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit2873730066927929797/junit6137519814873368186
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:20[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit2873730066927929797/junit6137519814873368186
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:20[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Starting
[33m2023-01-13 11:23:20[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Starting
[33m2023-01-13 11:23:20[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Starting
[33m2023-01-13 11:23:20[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Loading logs from log dirs ArrayBuffer(/tmp/junit2873730066927929797/junit6137519814873368186)
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Attempting recovery for all logs in /tmp/junit2873730066927929797/junit6137519814873368186 since no clean shutdown file was found
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Loaded 0 logs in 0ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting log cleanup with a period of 300000 ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting log flusher with a default period of 9223372036854775807 ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting the log cleaner
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Updated PLAINTEXT max connection creation rate to 2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.Acceptor[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Awaiting socket connections on localhost:42285.
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Produce]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Fetch]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-DeleteRecords]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-ElectLeader]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-0-to-controller-send-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating /brokers/ids/0 (is it secure? false)
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Stat of the created znode at /brokers/ids/0 is: 24,24,1673609001391,1673609001391,1,0,0,73341806580858880,204,0,24

[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:42285, czxid (broker epoch): 24
[33m2023-01-13 11:23:21[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=0] Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-topic]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Heartbeat]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Rebalance]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Successfully created /controller_epoch with initial epoch 0
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Starting up.
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Startup complete.
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.transaction.ProducerIdManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{})
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.FinalizedFeatureChangeListener[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Feature ZK node created at path: /feature
[33m2023-01-13 11:23:21[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=0] Starting up.
[33m2023-01-13 11:23:21[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=0] Startup complete.
[33m2023-01-13 11:23:21[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 0]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-AlterAcls]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.FinalizedFeatureCache[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Registering handlers
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Deleting log dir event notifications
[33m2023-01-13 11:23:21[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Deleting isr change notifications
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Initializing controller context
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Starting socket server acceptors and processors
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Started socket server acceptors and processors
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Initialized broker epochs cache: Map(0 -> 24)
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673608999998
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=0] started
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit16582194824177009327/junit3538309892005085918
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Connecting to zookeeper on 127.0.0.1:42881
[33m2023-01-13 11:23:21[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:42881.
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(ZooKeeper.java:868)[0;39m - Initiating client connection, connectString=127.0.0.1:42881 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@10358c32
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxnSocket[0;39m [34m[INFO][0;39m [32m(ClientCnxnSocket.java:237)[0;39m - jute.maxbuffer value is 4194304 Bytes
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1653)[0;39m - zookeeper.request.timeout value is 0. feature enabled=
[33m2023-01-13 11:23:21[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Waiting until connected.
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1112)[0;39m - Opening socket connection to server localhost/127.0.0.1:42881. Will not attempt to authenticate using SASL (unknown error)
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:959)[0;39m - Socket connection established, initiating session, client: /127.0.0.1:34820, server: localhost/127.0.0.1:42881
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1394)[0;39m - Session establishment complete on server localhost/127.0.0.1:42881, sessionid = 0x1048ffc08700001, negotiated timeout = 10000
[33m2023-01-13 11:23:21[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Connected.
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Currently active brokers in the cluster: Set(0)
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Currently shutting brokers in the cluster: Set()
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Current list of topics in the cluster: Set()
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Fetching topic deletions in progress
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] List of topics to be deleted: 
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] List of topics ineligible for deletion: 
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Initializing topic deletion manager
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set()
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Sending update metadata request
[33m2023-01-13 11:23:21[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0) for 0 partitions
[33m2023-01-13 11:23:21[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.FinalizedFeatureCache[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Cluster ID = i1v9B5-MSx6J4saqVDHZMQ
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerMetadataCheckpoint[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - No meta.properties file under dir /tmp/junit16582194824177009327/junit3538309892005085918/meta.properties
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=0] Initializing replica state
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=0] Triggering online replica state changes
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=0] Initializing partition state
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=0] Triggering online partition state changes
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit16582194824177009327/junit3538309892005085918
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Ready to serve as the new controller with epoch 1
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Controller 0 connected to localhost:42285 (id: 0 rack: null) for sending state change requests
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Partitions undergoing preferred replica election: 
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Partitions that completed preferred replica election: 
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Resuming preferred replica election for partitions: 
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit16582194824177009327/junit3538309892005085918
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Starting the controller scheduler
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Loading logs from log dirs ArrayBuffer(/tmp/junit16582194824177009327/junit3538309892005085918)
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Attempting recovery for all logs in /tmp/junit16582194824177009327/junit3538309892005085918 since no clean shutdown file was found
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Loaded 0 logs in 0ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting log cleanup with a period of 300000 ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting log flusher with a default period of 9223372036854775807 ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting the log cleaner
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Updated PLAINTEXT max connection creation rate to 2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.Acceptor[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Awaiting socket connections on localhost:43117.
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-0-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Produce]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Fetch]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-DeleteRecords]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-ElectLeader]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-1-to-controller-send-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating /brokers/ids/1 (is it secure? false)
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Stat of the created znode at /brokers/ids/1 is: 44,44,1673609001795,1673609001795,1,0,0,73341806580858881,204,0,44

[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://localhost:43117, czxid (broker epoch): 44
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Newly added brokers: 1, deleted brokers: , bounced brokers: , all live brokers: 0,1
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Starting
[33m2023-01-13 11:23:21[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=1] Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New broker startup callback for 1
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-topic]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Heartbeat]: Starting
[33m2023-01-13 11:23:21[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0) for 0 partitions
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Rebalance]: Starting
[33m2023-01-13 11:23:21[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(1) for 0 partitions
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Controller 0 connected to localhost:43117 (id: 1 rack: null) for sending state change requests
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 1]: Starting up.
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 1]: Startup complete.
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Updated broker epochs cache: Map(1 -> 44, 0 -> 24)
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.transaction.ProducerIdManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:1000,blockEndProducerId:1999) by writing to Zk with path version 2
[33m2023-01-13 11:23:21[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=1] Starting up.
[33m2023-01-13 11:23:21[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=1] Startup complete.
[33m2023-01-13 11:23:21[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 1]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-AlterAcls]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Starting socket server acceptors and processors
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Started socket server acceptors and processors
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673608999998
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=1] started
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit1377642647049437730/junit847551368990794809
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Connecting to zookeeper on 127.0.0.1:42881
[33m2023-01-13 11:23:21[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:42881.
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(ZooKeeper.java:868)[0;39m - Initiating client connection, connectString=127.0.0.1:42881 sessionTimeout=10000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@468dda3e
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxnSocket[0;39m [34m[INFO][0;39m [32m(ClientCnxnSocket.java:237)[0;39m - jute.maxbuffer value is 4194304 Bytes
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1653)[0;39m - zookeeper.request.timeout value is 0. feature enabled=
[33m2023-01-13 11:23:21[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Waiting until connected.
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1112)[0;39m - Opening socket connection to server localhost/127.0.0.1:42881. Will not attempt to authenticate using SASL (unknown error)
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:959)[0;39m - Socket connection established, initiating session, client: /127.0.0.1:34844, server: localhost/127.0.0.1:42881
[33m2023-01-13 11:23:21[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:1394)[0;39m - Session establishment complete on server localhost/127.0.0.1:42881, sessionid = 0x1048ffc08700002, negotiated timeout = 10000
[33m2023-01-13 11:23:21[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Connected.
[33m2023-01-13 11:23:21[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.FinalizedFeatureCache[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Cluster ID = i1v9B5-MSx6J4saqVDHZMQ
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerMetadataCheckpoint[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - No meta.properties file under dir /tmp/junit1377642647049437730/junit847551368990794809/meta.properties
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit1377642647049437730/junit847551368990794809
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 0
	host.name = localhost
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.7-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/junit1377642647049437730/junit847551368990794809
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.7-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 0
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 5
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:42881
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 10000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-1-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Loading logs from log dirs ArrayBuffer(/tmp/junit1377642647049437730/junit847551368990794809)
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Attempting recovery for all logs in /tmp/junit1377642647049437730/junit847551368990794809 since no clean shutdown file was found
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Loaded 0 logs in 0ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting log cleanup with a period of 300000 ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting log flusher with a default period of 9223372036854775807 ms.
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Starting the log cleaner
[33m2023-01-13 11:23:21[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created ConnectionAcceptRate sensor, quotaLimit=2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created ConnectionAcceptRate-PLAINTEXT sensor, quotaLimit=2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.ConnectionQuotas[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Updated PLAINTEXT max connection creation rate to 2147483647
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.Acceptor[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Awaiting socket connections on localhost:35497.
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Produce]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Fetch]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-DeleteRecords]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-ElectLeader]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-2-to-controller-send-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating /brokers/ids/2 (is it secure? false)
[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Stat of the created znode at /brokers/ids/2 is: 60,60,1673609001942,1673609001942,1,0,0,73341806580858882,204,0,60

[33m2023-01-13 11:23:21[0;39m [36mkafka.zk.KafkaZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Registered broker 2 at path /brokers/ids/2 with addresses: PLAINTEXT://localhost:35497, czxid (broker epoch): 60
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Newly added brokers: 2, deleted brokers: , bounced brokers: , all live brokers: 0,1,2
[33m2023-01-13 11:23:21[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=2] Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-topic]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Heartbeat]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Rebalance]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New broker startup callback for 2
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 2]: Starting up.
[33m2023-01-13 11:23:21[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1) for 0 partitions
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 2]: Startup complete.
[33m2023-01-13 11:23:21[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(2) for 0 partitions
[33m2023-01-13 11:23:21[0;39m [36mkafka.coordinator.transaction.ProducerIdManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ProducerId Manager 2]: Acquired new producerId block (brokerId:2,blockStartProducerId:2000,blockEndProducerId:2999) by writing to Zk with path version 3
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Controller 0 connected to localhost:35497 (id: 2 rack: null) for sending state change requests
[33m2023-01-13 11:23:21[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Updated broker epochs cache: Map(2 -> 60, 1 -> 44, 0 -> 24)
[33m2023-01-13 11:23:21[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=2] Starting up.
[33m2023-01-13 11:23:21[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=2] Startup complete.
[33m2023-01-13 11:23:21[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 2]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-AlterAcls]: Starting
[33m2023-01-13 11:23:21[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Starting
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Starting socket server acceptors and processors
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
[33m2023-01-13 11:23:21[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Started socket server acceptors and processors
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673608999998
[33m2023-01-13 11:23:21[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=2] started
[33m2023-01-13 11:23:21[0;39m [36morg.apache.kafka.clients.admin.AdminClientConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - AdminClientConfig values: 
	bootstrap.servers = [localhost:42285]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673609002006
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-2-to-controller-send-thread]: Recorded new controller, from now on will use broker 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.zk.AdminZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating topic kinaction_alert with configuration {} and initial partition assignment Map(2 -> ArrayBuffer(1, 2, 0), 1 -> ArrayBuffer(2, 0, 1), 0 -> ArrayBuffer(0, 1, 2))
[33m2023-01-13 11:23:22[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New topics: [Set(kinaction_alert)], deleted topics: [Set()], new partition replica assignment [Map(kinaction_alert-2 -> ReplicaAssignment(replicas=1,2,0, addingReplicas=, removingReplicas=), kinaction_alert-1 -> ReplicaAssignment(replicas=2,0,1, addingReplicas=, removingReplicas=), kinaction_alert-0 -> ReplicaAssignment(replicas=0,1,2, addingReplicas=, removingReplicas=))]
[33m2023-01-13 11:23:22[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New partition creation callback for kinaction_alert-2,kinaction_alert-1,kinaction_alert-0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-2 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-1 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-0 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0)
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0)
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0)
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 2 with 1 become-leader and 2 become-follower partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 2 become-follower partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 2 become-follower partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1, 2) for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling LeaderAndIsr request correlationId 2 from controller 0 for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling LeaderAndIsr request correlationId 1 from controller 0 for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(kinaction_alert-0)
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-1)
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(kinaction_alert-2)
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 2 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-0, dir=/tmp/junit2873730066927929797/junit6137519814873368186] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-2, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-1, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-0 in /tmp/junit2873730066927929797/junit6137519814873368186/kinaction_alert-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-1 in /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-2 in /tmp/junit16582194824177009327/junit3538309892005085918/kinaction_alert-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=0] No checkpointed highwatermark is found for partition kinaction_alert-0
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-2 broker=1] No checkpointed highwatermark is found for partition kinaction_alert-2
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-1 broker=2] No checkpointed highwatermark is found for partition kinaction_alert-1
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=0] Log loaded for partition kinaction_alert-0 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-2 broker=1] Log loaded for partition kinaction_alert-2 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-1 broker=2] Log loaded for partition kinaction_alert-1 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Leader kinaction_alert-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0,1,2] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Leader kinaction_alert-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1,2,0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Leader kinaction_alert-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [2,0,1] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-1, dir=/tmp/junit2873730066927929797/junit6137519814873368186] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-1 in /tmp/junit2873730066927929797/junit6137519814873368186/kinaction_alert-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-1 broker=0] No checkpointed highwatermark is found for partition kinaction_alert-1
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-1 broker=0] Log loaded for partition kinaction_alert-1 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Follower kinaction_alert-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-0, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-0, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-0 in /tmp/junit16582194824177009327/junit3538309892005085918/kinaction_alert-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=1] No checkpointed highwatermark is found for partition kinaction_alert-0
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=1] Log loaded for partition kinaction_alert-0 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Follower kinaction_alert-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-0 in /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=2] No checkpointed highwatermark is found for partition kinaction_alert-0
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-2, dir=/tmp/junit2873730066927929797/junit6137519814873368186] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=2] Log loaded for partition kinaction_alert-0 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Follower kinaction_alert-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-2 in /tmp/junit2873730066927929797/junit6137519814873368186/kinaction_alert-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-2 broker=0] No checkpointed highwatermark is found for partition kinaction_alert-2
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-2 broker=0] Log loaded for partition kinaction_alert-2 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Follower kinaction_alert-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-1, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 3 for 2 partitions
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-1 in /tmp/junit16582194824177009327/junit3538309892005085918/kinaction_alert-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-1 broker=1] No checkpointed highwatermark is found for partition kinaction_alert-1
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-1 broker=1] Log loaded for partition kinaction_alert-1 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Follower kinaction_alert-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(kinaction_alert-1, kinaction_alert-0)
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 2 for 2 partitions
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-2, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-2 in /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-2 broker=2] No checkpointed highwatermark is found for partition kinaction_alert-2
[33m2023-01-13 11:23:22[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-2 broker=2] Log loaded for partition kinaction_alert-2 with initial high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Follower kinaction_alert-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-2, kinaction_alert-0)
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 1 for 2 partitions
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Starting
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Starting
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Starting
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Added fetcher to broker 2 for partitions Map(kinaction_alert-1 -> (offset=0, leaderEpoch=0))
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Added fetcher to broker 2 for partitions Map(kinaction_alert-1 -> (offset=0, leaderEpoch=0))
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Added fetcher to broker 0 for partitions Map(kinaction_alert-0 -> (offset=0, leaderEpoch=0))
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition kinaction_alert-1 to local high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Truncating partition kinaction_alert-1 to local high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition kinaction_alert-0 to local high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Added fetcher to broker 1 for partitions Map(kinaction_alert-2 -> (offset=0, leaderEpoch=0))
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Starting
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Starting
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Added fetcher to broker 0 for partitions Map(kinaction_alert-0 -> (offset=0, leaderEpoch=0))
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition kinaction_alert-2 to local high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition kinaction_alert-0 to local high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Added fetcher to broker 1 for partitions Map(kinaction_alert-2 -> (offset=0, leaderEpoch=0))
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Starting
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition kinaction_alert-2 to local high watermark 0
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-0, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Truncating to 0 has no effect as the largest offset in the log is -1
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-1, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Truncating to 0 has no effect as the largest offset in the log is -1
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-1, dir=/tmp/junit2873730066927929797/junit6137519814873368186] Truncating to 0 has no effect as the largest offset in the log is -1
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-2, dir=/tmp/junit1377642647049437730/junit847551368990794809] Truncating to 0 has no effect as the largest offset in the log is -1
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-0, dir=/tmp/junit1377642647049437730/junit847551368990794809] Truncating to 0 has no effect as the largest offset in the log is -1
[33m2023-01-13 11:23:22[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-2, dir=/tmp/junit2873730066927929797/junit6137519814873368186] Truncating to 0 has no effect as the largest offset in the log is -1
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 0 for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Finished LeaderAndIsr request in 0ms correlationId 2 from controller 0 for 3 partitions
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Add 3 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 3 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 3
[33m2023-01-13 11:23:22[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 3 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:83)[0;39m - App info kafka.admin.client for adminclient-1 unregistered
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:668)[0;39m - Metrics scheduler closed
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:672)[0;39m - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:678)[0;39m - Metrics reporters closed
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition kinaction_alert-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition kinaction_alert-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition kinaction_alert-1. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition kinaction_alert-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition kinaction_alert-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[33m2023-01-13 11:23:22[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:70)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition kinaction_alert-1. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.clients.producer.ProducerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:42285]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.kafkainaction.serde.AlertKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.kafkainaction.partitioner.AlertLevelPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673609002358
[33m2023-01-13 11:23:22[0;39m [36morg.apache.kafka.clients.Metadata[0;39m [34m[INFO][0;39m [32m(Metadata.java:279)[0;39m - [Producer clientId=producer-1] Cluster ID: i1v9B5-MSx6J4saqVDHZMQ
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.producer.KafkaProducer[0;39m [34m[INFO][0;39m [32m(KafkaProducer.java:1205)[0;39m - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:668)[0;39m - Metrics scheduler closed
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:672)[0;39m - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:678)[0;39m - Metrics reporters closed
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:83)[0;39m - App info kafka.producer for producer-1 unregistered
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.consumer.ConsumerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:42285]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dc12076a-d2fb-48d1-92eb-9f8114a01174
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.kafkainaction.serde.AlertKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673609003456
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.consumer.KafkaConsumer[0;39m [34m[INFO][0;39m [32m(KafkaConsumer.java:961)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Subscribed to topic(s): kinaction_alert
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.Metadata[0;39m [34m[INFO][0;39m [32m(Metadata.java:279)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Cluster ID: i1v9B5-MSx6J4saqVDHZMQ
[33m2023-01-13 11:23:23[0;39m [36mkafka.zk.AdminZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment Map(2 -> ArrayBuffer(0), 4 -> ArrayBuffer(2), 1 -> ArrayBuffer(2), 3 -> ArrayBuffer(1), 0 -> ArrayBuffer(1))
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaApis[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaApi-0] Auto creation of topic __consumer_offsets with 5 partitions and replication factor 1 is successful
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-4 -> ReplicaAssignment(replicas=2, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=2, addingReplicas=, removingReplicas=))]
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 2 with 2 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling LeaderAndIsr request correlationId 3 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 2 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-4, __consumer_offsets-1)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling LeaderAndIsr request correlationId 4 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-0, __consumer_offsets-3)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 4 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-4, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-4 in /tmp/junit1377642647049437730/junit847551368990794809/__consumer_offsets-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-0, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-4 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-4
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-4 broker=2] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [2] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-0 in /tmp/junit16582194824177009327/junit3538309892005085918/__consumer_offsets-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1, 2) for 5 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-2, dir=/tmp/junit2873730066927929797/junit6137519814873368186] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-1, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-2 in /tmp/junit2873730066927929797/junit6137519814873368186/__consumer_offsets-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-1 in /tmp/junit1377642647049437730/junit847551368990794809/__consumer_offsets-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-1 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-1
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-1 broker=2] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [2] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-3, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-3 in /tmp/junit16582194824177009327/junit3538309892005085918/__consumer_offsets-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Finished LeaderAndIsr request in 0ms correlationId 5 from controller 0 for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-4
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Finished LeaderAndIsr request in 0ms correlationId 4 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-1 in 4 milliseconds, of which 1 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds, of which 1 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds, of which 1 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds, of which 2 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:42285 (id: 2147483647 rack: null)
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:538)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] (Re-)joining group
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:538)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] (Re-)joining group
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Preparing to rebalance group dc12076a-d2fb-48d1-92eb-9f8114a01174 in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1-27815e02-8708-4fc7-a63a-50b708edf3d9 with group instance id None)
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Stabilized group dc12076a-d2fb-48d1-92eb-9f8114a01174 generation 1 (__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:594)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Successfully joined group with generation Generation{generationId=1, memberId='consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1-27815e02-8708-4fc7-a63a-50b708edf3d9', protocol='range'}
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.ConsumerCoordinator[0;39m [34m[INFO][0;39m [32m(ConsumerCoordinator.java:626)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Finished assignment for group at generation 1: {consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1-27815e02-8708-4fc7-a63a-50b708edf3d9=Assignment(partitions=[kinaction_alert-0, kinaction_alert-1, kinaction_alert-2])}
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Assignment received from leader for group dc12076a-d2fb-48d1-92eb-9f8114a01174 for generation 1
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:754)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Successfully synced group in generation Generation{generationId=1, memberId='consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1-27815e02-8708-4fc7-a63a-50b708edf3d9', protocol='range'}
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.ConsumerCoordinator[0;39m [34m[INFO][0;39m [32m(ConsumerCoordinator.java:276)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Notifying assignor about the new Assignment(partitions=[kinaction_alert-0, kinaction_alert-1, kinaction_alert-2])
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.ConsumerCoordinator[0;39m [34m[INFO][0;39m [32m(ConsumerCoordinator.java:288)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Adding newly assigned partitions: kinaction_alert-1, kinaction_alert-2, kinaction_alert-0
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.ConsumerCoordinator[0;39m [34m[INFO][0;39m [32m(ConsumerCoordinator.java:1354)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Found no committed offset for partition kinaction_alert-1
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.ConsumerCoordinator[0;39m [34m[INFO][0;39m [32m(ConsumerCoordinator.java:1354)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Found no committed offset for partition kinaction_alert-2
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.ConsumerCoordinator[0;39m [34m[INFO][0;39m [32m(ConsumerCoordinator.java:1354)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Found no committed offset for partition kinaction_alert-0
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.clients.consumer.internals.SubscriptionState[0;39m [34m[INFO][0;39m [32m(SubscriptionState.java:396)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Resetting offset for partition kinaction_alert-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35497 (id: 2 rack: null)], epoch=0}}.
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.clients.consumer.internals.SubscriptionState[0;39m [34m[INFO][0;39m [32m(SubscriptionState.java:396)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Resetting offset for partition kinaction_alert-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:42285 (id: 0 rack: null)], epoch=0}}.
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.clients.consumer.internals.SubscriptionState[0;39m [34m[INFO][0;39m [32m(SubscriptionState.java:396)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Resetting offset for partition kinaction_alert-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:43117 (id: 1 rack: null)], epoch=0}}.
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.admin.AdminClientConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - AdminClientConfig values: 
	bootstrap.servers = [localhost:42285]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 127000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:119)[0;39m - Kafka version: 2.7.1
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:120)[0;39m - Kafka commitId: 61dbce85d0d41457
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:121)[0;39m - Kafka startTimeMs: 1673609003721
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Starting topic deletion for topics kinaction_alert
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics kinaction_alert
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Deletion of topic kinaction_alert (re)started
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1, 2) for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Add 0 partitions and deleted 3 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 7
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 0 partitions and deleted 3 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 0 partitions and deleted 3 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 2]: Removed 0 offsets associated with deleted partitions: kinaction_alert-0, kinaction_alert-2, kinaction_alert-1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 1]: Removed 0 offsets associated with deleted partitions: kinaction_alert-0, kinaction_alert-2, kinaction_alert-1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: kinaction_alert-0, kinaction_alert-2, kinaction_alert-1.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-0 state changed to (Leader:0,ISR:0,1,LeaderEpoch:1,ControllerEpoch:1) after removing replica 2 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-2 state changed to (Leader:1,ISR:1,0,LeaderEpoch:1,ControllerEpoch:1) after removing replica 2 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-1 state changed to (Leader:-1,ISR:0,1,LeaderEpoch:1,ControllerEpoch:1) after removing replica 2 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-0 state changed to (Leader:0,ISR:0,LeaderEpoch:2,ControllerEpoch:1) after removing replica 1 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-2 state changed to (Leader:-1,ISR:0,LeaderEpoch:2,ControllerEpoch:1) after removing replica 1 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-1 state changed to (Leader:-1,ISR:0,LeaderEpoch:2,ControllerEpoch:1) after removing replica 1 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-0 state changed to (Leader:-1,ISR:0,LeaderEpoch:3,ControllerEpoch:1) after removing replica 0 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-2 state changed to (Leader:-1,ISR:0,LeaderEpoch:3,ControllerEpoch:1) after removing replica 0 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition kinaction_alert-1 state changed to (Leader:-1,ISR:0,LeaderEpoch:3,ControllerEpoch:1) after removing replica 0 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 3 replicas to broker 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 3 replicas to broker 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 3 replicas to broker 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling StopReplica request correlationId 6 from controller 0 for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 3 replicas to broker 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 3 replicas to broker 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling StopReplica request correlationId 7 from controller 0 for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Handling StopReplica request correlationId 8 from controller 0 for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 3 replicas to broker 0
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Starting topic deletion for topics kinaction_alert,__consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics kinaction_alert,__consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Deletion of topic __consumer_offsets (re)started
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1, 2) for 5 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Shutting down
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling StopReplica request correlationId 8 from controller 0 for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling StopReplica request correlationId 7 from controller 0 for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Handling StopReplica request correlationId 9 from controller 0 for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition __consumer_offsets-4 state changed to (Leader:-1,ISR:2,LeaderEpoch:1,ControllerEpoch:1) after removing replica 2 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition __consumer_offsets-1 state changed to (Leader:-1,ISR:2,LeaderEpoch:1,ControllerEpoch:1) after removing replica 2 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.FetchSessionHandler[0;39m [34m[INFO][0;39m [32m(FetchSessionHandler.java:481)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 1:
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.FetchSessionHandler[0;39m [34m[INFO][0;39m [32m(FetchSessionHandler.java:481)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 1:
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.FetchSessionHandler[0;39m [34m[INFO][0;39m [32m(FetchSessionHandler.java:481)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 2:
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Stopped
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Stopped
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Stopped
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Shutting down
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.FetchSessionHandler[0;39m [34m[INFO][0;39m [32m(FetchSessionHandler.java:481)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Error sending fetch request (sessionId=730510834, epoch=1) to node 0:
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Stopped
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.FetchSessionHandler[0;39m [34m[INFO][0;39m [32m(FetchSessionHandler.java:481)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=INVALID, epoch=INITIAL) to node 2:
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Stopped
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition __consumer_offsets-3 state changed to (Leader:-1,ISR:1,LeaderEpoch:1,ControllerEpoch:1) after removing replica 1 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.FetchSessionHandler[0;39m [34m[INFO][0;39m [32m(FetchSessionHandler.java:481)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Error sending fetch request (sessionId=417695201, epoch=1) to node 0:
java.io.IOException: Client was shutdown before response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition __consumer_offsets-0 state changed to (Leader:-1,ISR:1,LeaderEpoch:1,ControllerEpoch:1) after removing replica 1 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Stopped
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(kinaction_alert-0, kinaction_alert-2, kinaction_alert-1)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Partition __consumer_offsets-2 state changed to (Leader:-1,ISR:0,LeaderEpoch:1,ControllerEpoch:1) after removing replica 0 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 2 replicas to broker 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 2 replicas to broker 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 1 replicas to broker 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 2 replicas to broker 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 2 replicas to broker 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending StopReplica request for 1 replicas to broker 0
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-2 is renamed to /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-2.e66d43612fa24f47b2d6d2c61912f681-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-2 is renamed to /tmp/junit16582194824177009327/junit3538309892005085918/kinaction_alert-2.75eb8d052b014c23893691cc22d3a69c-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-2 is renamed to /tmp/junit2873730066927929797/junit6137519814873368186/kinaction_alert-2.f3ea77ddb3774534883cb388615ef430-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-1 is renamed to /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-1.ecc6ad36f14b453d8db0a61d578e55dd-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-1 is renamed to /tmp/junit16582194824177009327/junit3538309892005085918/kinaction_alert-1.ed9ea252a1e646149357fa5c6680d14d-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-1 is renamed to /tmp/junit2873730066927929797/junit6137519814873368186/kinaction_alert-1.2960268e581d42b2938201545cbdd8c4-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-0 is renamed to /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-0.1ad546276ffc4bb0b2b3acba071925b4-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-0 is renamed to /tmp/junit16582194824177009327/junit3538309892005085918/kinaction_alert-0.7c4a92862d2f4cacb5b14fef16a8132c-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition kinaction_alert-0 is renamed to /tmp/junit2873730066927929797/junit6137519814873368186/kinaction_alert-0.107a19083a1e46bb991eb6d173e39edd-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Add 0 partitions and deleted 5 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 10
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Removed 0 offsets associated with deleted partitions: __consumer_offsets-2, __consumer_offsets-4, __consumer_offsets-1, __consumer_offsets-3, __consumer_offsets-0.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 0 partitions and deleted 5 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 9
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 0 partitions and deleted 5 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 1]: Removed 0 offsets associated with deleted partitions: __consumer_offsets-2, __consumer_offsets-4, __consumer_offsets-1, __consumer_offsets-3, __consumer_offsets-0.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 2]: Removed 0 offsets associated with deleted partitions: __consumer_offsets-2, __consumer_offsets-4, __consumer_offsets-1, __consumer_offsets-3, __consumer_offsets-0.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling StopReplica request correlationId 10 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling StopReplica request correlationId 9 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:83)[0;39m - App info kafka.admin.client for adminclient-2 unregistered
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-4, __consumer_offsets-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-3, __consumer_offsets-0)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-4, __consumer_offsets-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-3, __consumer_offsets-0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Handling StopReplica request correlationId 11 from controller 0 for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:668)[0;39m - Metrics scheduler closed
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:672)[0;39m - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:678)[0;39m - Metrics reporters closed
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling StopReplica request correlationId 10 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling StopReplica request correlationId 11 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-4, __consumer_offsets-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-3, __consumer_offsets-0)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=0] shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics kinaction_alert,__consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-4, __consumer_offsets-1)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-3, __consumer_offsets-0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=0] Handling StopReplica request correlationId 12 from controller 0 for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=0] Starting controlled shutdown
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics kinaction_alert,__consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition __consumer_offsets-2 is renamed to /tmp/junit2873730066927929797/junit6137519814873368186/__consumer_offsets-2.e4e12ebe2481451ebaa4207af57a8fe0-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition __consumer_offsets-4 is renamed to /tmp/junit1377642647049437730/junit847551368990794809/__consumer_offsets-4.17f386fa89c8458d8d0a25f1a0a7cc66-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition __consumer_offsets-0 is renamed to /tmp/junit16582194824177009327/junit3538309892005085918/__consumer_offsets-0.6251050510a54b0fbf37384033f8d585-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition __consumer_offsets-1 is renamed to /tmp/junit1377642647049437730/junit847551368990794809/__consumer_offsets-1.86f87d1967c042689a8407f931764626-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Log for partition __consumer_offsets-3 is renamed to /tmp/junit16582194824177009327/junit3538309892005085918/__consumer_offsets-3.ed988986239a4fd7b029707101da860c-delete and is scheduled for deletion
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics kinaction_alert,__consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Deletion of topic kinaction_alert successfully completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Shutting down broker 0
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Unloading group metadata for dc12076a-d2fb-48d1-92eb-9f8114a01174 with generation 1
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=0] Controlled shutdown succeeded
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-3
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-4
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-0
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-1
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New topics: [Set()], deleted topics: [Set()], new partition replica assignment [Map()]
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-4. Removed 0 cached offsets and 0 cached groups.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-3. Removed 0 cached offsets and 0 cached groups.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-1. Removed 0 cached offsets and 0 cached groups.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-0. Removed 0 cached offsets and 0 cached groups.
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Starting topic deletion for topics __consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics __consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-2. Removed 0 cached offsets and 1 cached groups.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics __consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Stopped
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics __consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Stopping socket server request processors
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Handling deletion for topics __consumer_offsets
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Stopped socket server request processors
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaRequestHandlerPool[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [data-plane Kafka Request Handler on Broker 0], shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaRequestHandlerPool[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [data-plane Kafka Request Handler on Broker 0], shut down completely
[33m2023-01-13 11:23:23[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-AlterAcls]: Shutting down
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 0] Deletion of topic __consumer_offsets successfully completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New topics: [Set()], deleted topics: [Set()], new partition replica assignment [Map()]
[33m2023-01-13 11:23:23[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:42285 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:23[0;39m [36mkafka.zk.AdminZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment Map(2 -> ArrayBuffer(1), 4 -> ArrayBuffer(0), 1 -> ArrayBuffer(0), 3 -> ArrayBuffer(2), 0 -> ArrayBuffer(2))
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaApis[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaApi-1] Auto creation of topic __consumer_offsets with 5 partitions and replication factor 1 is successful
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=2, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=2, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))]
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=0 epoch=1] Controller 0 epoch 1 failed to change state for partition __consumer_offsets-4 from NewPartition to OnlinePartition
kafka.common.StateChangeFailedException: Controller 0 epoch 1 encountered error during state change of partition __consumer_offsets-4 from New to Online, assigned replicas are [], live brokers are [Set(2, 1)]. No assigned replica is alive.
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5(PartitionStateMachine.scala:291)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5$adapted(PartitionStateMachine.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.controller.ZkPartitionStateMachine.initializeLeaderAndIsrForPartitions(PartitionStateMachine.scala:286)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:227)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.KafkaController.onNewPartitionCreation(KafkaController.scala:650)
	at kafka.controller.KafkaController.processTopicChange(KafkaController.scala:1643)
	at kafka.controller.KafkaController.process(KafkaController.scala:2412)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=0 epoch=1] Controller 0 epoch 1 failed to change state for partition __consumer_offsets-1 from NewPartition to OnlinePartition
kafka.common.StateChangeFailedException: Controller 0 epoch 1 encountered error during state change of partition __consumer_offsets-1 from New to Online, assigned replicas are [], live brokers are [Set(2, 1)]. No assigned replica is alive.
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5(PartitionStateMachine.scala:291)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5$adapted(PartitionStateMachine.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.controller.ZkPartitionStateMachine.initializeLeaderAndIsrForPartitions(PartitionStateMachine.scala:286)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:227)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.KafkaController.onNewPartitionCreation(KafkaController.scala:650)
	at kafka.controller.KafkaController.processTopicChange(KafkaController.scala:1643)
	at kafka.controller.KafkaController.process(KafkaController.scala:2412)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-AlterAcls]: Stopped
[33m2023-01-13 11:23:23[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-AlterAcls]: Shutdown completed
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.KafkaApis[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaApi-0] Shutdown complete.
[33m2023-01-13 11:23:23[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-topic]: Shutting down
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2), zkVersion=0)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 2 with 2 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1, 2) for 3 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling LeaderAndIsr request correlationId 12 from controller 0 for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling LeaderAndIsr request correlationId 11 from controller 0 for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-2)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 12 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:23[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-0, __consumer_offsets-3)
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 11 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-2, dir=/tmp/junit16582194824177009327/junit3538309892005085918] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-0, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-2 in /tmp/junit16582194824177009327/junit3538309892005085918/__consumer_offsets-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-0 in /tmp/junit1377642647049437730/junit847551368990794809/__consumer_offsets-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-0 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-0
[33m2023-01-13 11:23:23[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-0 broker=2] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [2] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Finished LeaderAndIsr request in 0ms correlationId 12 from controller 0 for 1 partitions
[33m2023-01-13 11:23:23[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:23[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 3 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 13
[33m2023-01-13 11:23:23[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=__consumer_offsets-3, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:24[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition __consumer_offsets-3 in /tmp/junit1377642647049437730/junit847551368990794809/__consumer_offsets-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:24[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-3 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-3
[33m2023-01-13 11:23:24[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition __consumer_offsets-3 broker=2] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [2] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-3
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-0
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Finished LeaderAndIsr request in 0ms correlationId 11 from controller 0 for 2 partitions
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.group.GroupMetadataManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler.
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 3 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 12
[33m2023-01-13 11:23:24[0;39m [36mkafka.zk.AdminZkClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Creating topic kinaction_alert with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(2))
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.KafkaApis[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaApi-2] Auto creation of topic kinaction_alert with 1 partitions and replication factor 1 is successful
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-topic]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-topic]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New topics: [Set(kinaction_alert)], deleted topics: [Set()], new partition replica assignment [Map(kinaction_alert-0 -> ReplicaAssignment(replicas=2, addingReplicas=, removingReplicas=))]
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] New partition creation callback for kinaction_alert-0
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-0 state from NonExistentPartition to NewPartition with assigned replicas 2
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:24[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=0] Shutting down.
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.transaction.ProducerIdManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0
[33m2023-01-13 11:23:24[0;39m [36mk.coordinator.transaction.TransactionStateManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction State Manager 0]: Shutdown complete
[33m2023-01-13 11:23:24[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 0]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 0]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 0]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=0] Shutdown complete.
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Changed partition kinaction_alert-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2), zkVersion=0)
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 2 with 1 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set(0, 1, 2) for 1 partitions
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Shutting down.
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Heartbeat]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling LeaderAndIsr request correlationId 13 from controller 0 for 1 partitions
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 14
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(kinaction_alert-0)
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 13 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36mkafka.log.Log[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Log partition=kinaction_alert-0, dir=/tmp/junit1377642647049437730/junit847551368990794809] Loading producer state till offset 0 with message format version 2
[33m2023-01-13 11:23:24[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Created log for partition kinaction_alert-0 in /tmp/junit1377642647049437730/junit847551368990794809/kinaction_alert-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.7-IV2, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1000000, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}.
[33m2023-01-13 11:23:24[0;39m [36mkafka.cluster.Partition[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Partition kinaction_alert-0 broker=2] Log loaded for partition kinaction_alert-0 with initial high watermark 0
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Leader kinaction_alert-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [2] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Finished LeaderAndIsr request in 0ms correlationId 13 from controller 0 for 1 partitions
[33m2023-01-13 11:23:24[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 14
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:1119)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Error while fetching metadata with correlation id 21 : {kinaction_alert=LEADER_NOT_AVAILABLE}
[33m2023-01-13 11:23:24[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Heartbeat]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Heartbeat]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Rebalance]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Rebalance]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Rebalance]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 0]: Shutdown complete.
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaManager broker=0] Shutting down
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] shutting down
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 0] shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 0] shutting down
[33m2023-01-13 11:23:24[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 0] shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Fetch]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Fetch]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Fetch]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Produce]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Produce]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-Produce]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-DeleteRecords]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-DeleteRecords]: Stopped
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-DeleteRecords]: Shutdown completed
[33m2023-01-13 11:23:24[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-ElectLeader]: Shutting down
[33m2023-01-13 11:23:24[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:24[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:25[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:25[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-ElectLeader]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-0-ElectLeader]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.server.ReplicaManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaManager broker=0] Shut down completely
[33m2023-01-13 11:23:25[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-0-to-controller-send-thread]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-0-to-controller-send-thread]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-0-to-controller-send-thread]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-0-to-controller-send-thread]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutting down.
[33m2023-01-13 11:23:25[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutting down the log cleaner.
[33m2023-01-13 11:23:25[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutdown complete.
[33m2023-01-13 11:23:25[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=0] Shutting down
[33m2023-01-13 11:23:25[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=0] Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=0] Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=0] Stopped partition state machine
[33m2023-01-13 11:23:25[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Controller id=0, targetBrokerId=0] Connection to node 0 (localhost/127.0.0.1:42285) could not be established. Broker may not be available.
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0's connection to broker localhost:42285 (id: 0 rack: null) was unsuccessful
java.io.IOException: Connection to localhost:42285 (id: 0 rack: null) failed.
	at org.apache.kafka.clients.NetworkClientUtils.awaitReady(NetworkClientUtils.java:71)
	at kafka.controller.RequestSendThread.brokerReady(ControllerChannelManager.scala:292)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:246)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=0] Stopped replica state machine
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Shutting down
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Stopped
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Shutting down
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Stopped
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Shutting down
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [31m[WARN][0;39m [32m(Logging.scala:72)[0;39m - [RequestSendThread controllerId=0] Controller 0 epoch 1 fails to send request (type: UpdateMetadataRequest=, controllerId=0, controllerEpoch=1, brokerEpoch=24, partitionStates=[UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]), UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2], zkVersion=0, replicas=[2], offlineReplicas=[]), UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2], zkVersion=0, replicas=[2], offlineReplicas=[])], liveBrokers=UpdateMetadataBroker(id=0, v0Host='', v0Port=0, endpoints=[UpdateMetadataEndpoint(port=42285, host='localhost', listener='PLAINTEXT', securityProtocol=0)], rack=null), UpdateMetadataBroker(id=2, v0Host='', v0Port=0, endpoints=[UpdateMetadataEndpoint(port=35497, host='localhost', listener='PLAINTEXT', securityProtocol=0)], rack=null), UpdateMetadataBroker(id=1, v0Host='', v0Port=0, endpoints=[UpdateMetadataEndpoint(port=43117, host='localhost', listener='PLAINTEXT', securityProtocol=0)], rack=null)) to broker localhost:42285 (id: 0 rack: null). Reconnecting to broker.
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:278)
	at kafka.utils.ShutdownableThread.pause(ShutdownableThread.scala:82)
	at kafka.controller.RequestSendThread.backoff$1(ControllerChannelManager.scala:234)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:248)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Stopped
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=0] Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=0] Resigned
[33m2023-01-13 11:23:25[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Closing.
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] 2 successfully elected as the controller. Epoch incremented to 2 and epoch zk version is now 2
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Registering handlers
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Deleting log dir event notifications
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Deleting isr change notifications
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Initializing controller context
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Initialized broker epochs cache: Map(2 -> 60, 1 -> 44)
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Starting
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Currently active brokers in the cluster: Set(2, 1)
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Starting
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Currently shutting brokers in the cluster: Set()
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Current list of topics in the cluster: Set(kinaction_alert, __consumer_offsets)
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Fetching topic deletions in progress
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] List of topics to be deleted: 
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] List of topics ineligible for deletion: __consumer_offsets
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Initializing topic deletion manager
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.TopicDeletionManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Topic Deletion Manager 2] Initializing manager with initial deletions: Set(), initial ineligible deletions: Set(__consumer_offsets)
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Sending update metadata request
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set(1, 2) for 0 partitions
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=2] Initializing replica state
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Controller 2 connected to localhost:43117 (id: 1 rack: null) for sending state change requests
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Controller 2 connected to localhost:35497 (id: 2 rack: null) for sending state change requests
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=2] Triggering online replica state changes
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending LeaderAndIsr request to broker 2 with 3 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set(1, 2) for 4 partitions
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=2] Triggering offline replica state changes
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Handling LeaderAndIsr request correlationId 1 from controller 2 for 3 partitions
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 2 for 1 partitions
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ControllerBrokerRequestBatch[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Leader not yet assigned for partition __consumer_offsets-4. Skip sending UpdateMetadataRequest.
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ControllerBrokerRequestBatch[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Leader not yet assigned for partition __consumer_offsets-1. Skip sending UpdateMetadataRequest.
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set(1, 2) for 0 partitions
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=2] Initializing partition state
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=2] Triggering online partition state changes
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition __consumer_offsets-2 since its associated leader epoch 0 matches the current leader epoch
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition kinaction_alert-0 since its associated leader epoch 0 matches the current leader epoch
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition __consumer_offsets-3 since its associated leader epoch 0 matches the current leader epoch
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Ignoring LeaderAndIsr request from controller 2 with correlation id 1 epoch 2 for partition __consumer_offsets-0 since its associated leader epoch 0 matches the current leader epoch
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 2 for 1 partitions
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __consumer_offsets-4 from NewPartition to OnlinePartition
kafka.common.StateChangeFailedException: Controller 2 epoch 2 encountered error during state change of partition __consumer_offsets-4 from New to Online, assigned replicas are [], live brokers are [Set(2, 1)]. No assigned replica is alive.
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5(PartitionStateMachine.scala:291)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5$adapted(PartitionStateMachine.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.controller.ZkPartitionStateMachine.initializeLeaderAndIsrForPartitions(PartitionStateMachine.scala:286)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:227)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:74)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:59)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:42)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:261)
	at kafka.controller.KafkaController.elect(KafkaController.scala:1514)
	at kafka.controller.KafkaController.processReelect(KafkaController.scala:2361)
	at kafka.controller.KafkaController.process(KafkaController.scala:2406)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Finished LeaderAndIsr request in 0ms correlationId 1 from controller 2 for 3 partitions
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __consumer_offsets-1 from NewPartition to OnlinePartition
kafka.common.StateChangeFailedException: Controller 2 epoch 2 encountered error during state change of partition __consumer_offsets-1 from New to Online, assigned replicas are [], live brokers are [Set(2, 1)]. No assigned replica is alive.
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5(PartitionStateMachine.scala:291)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5$adapted(PartitionStateMachine.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.controller.ZkPartitionStateMachine.initializeLeaderAndIsrForPartitions(PartitionStateMachine.scala:286)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:227)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:74)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:59)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:42)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:261)
	at kafka.controller.KafkaController.elect(KafkaController.scala:1514)
	at kafka.controller.KafkaController.processReelect(KafkaController.scala:2361)
	at kafka.controller.KafkaController.process(KafkaController.scala:2406)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Ready to serve as the new controller with epoch 2
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=1] Add 4 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 2 with correlation id 2
[33m2023-01-13 11:23:25[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 4 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 2 with correlation id 2
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Partitions undergoing preferred replica election: 
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Partitions that completed preferred replica election: 
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Skipping preferred replica election for partitions due to topic deletion: 
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Resuming preferred replica election for partitions: 
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
[33m2023-01-13 11:23:25[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Starting the controller scheduler
[33m2023-01-13 11:23:25[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(ZooKeeper.java:1422)[0;39m - Session: 0x1048ffc08700000 closed
[33m2023-01-13 11:23:25[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:524)[0;39m - EventThread shut down for session: 0x1048ffc08700000
[33m2023-01-13 11:23:25[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Closed.
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Shutting down
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Stopped
[33m2023-01-13 11:23:25[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
[33m2023-01-13 11:23:25[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Shutting down socket server
[33m2023-01-13 11:23:26[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=0] Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:668)[0;39m - Metrics scheduler closed
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:672)[0;39m - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:678)[0;39m - Metrics reporters closed
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.BrokerTopicStats[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Broker and topic stats closed
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:83)[0;39m - App info kafka.server for 0 unregistered
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=0] shut down completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=1] shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=1] Starting controlled shutdown
[33m2023-01-13 11:23:26[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Shutting down broker 1
[33m2023-01-13 11:23:26[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=1] Controlled shutdown succeeded
[33m2023-01-13 11:23:26[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Stopping socket server request processors
[33m2023-01-13 11:23:26[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Stopped socket server request processors
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaRequestHandlerPool[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [data-plane Kafka Request Handler on Broker 1], shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaRequestHandlerPool[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [data-plane Kafka Request Handler on Broker 1], shut down completely
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-AlterAcls]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-AlterAcls]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-AlterAcls]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.KafkaApis[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaApi-1] Shutdown complete.
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-topic]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2147483646 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-topic]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-topic]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=1] Shutting down.
[33m2023-01-13 11:23:26[0;39m [36mkafka.coordinator.transaction.ProducerIdManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ProducerId Manager 1]: Shutdown complete: last producerId assigned 1000
[33m2023-01-13 11:23:26[0;39m [36mk.coordinator.transaction.TransactionStateManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction State Manager 1]: Shutdown complete
[33m2023-01-13 11:23:26[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 1]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 1]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 1]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=1] Shutdown complete.
[33m2023-01-13 11:23:26[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 1]: Shutting down.
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Heartbeat]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2147483646 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Heartbeat]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Heartbeat]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Rebalance]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Rebalance]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Rebalance]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 1]: Shutdown complete.
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaManager broker=1] Shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 1] shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 1] shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 1] shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Fetch]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2147483646 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: null.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Fetch]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Fetch]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Produce]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:26[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Produce]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-Produce]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-DeleteRecords]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-DeleteRecords]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-DeleteRecords]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-ElectLeader]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-ElectLeader]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-1-ElectLeader]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.ReplicaManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaManager broker=1] Shut down completely
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-1-to-controller-send-thread]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-1-to-controller-send-thread]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-1-to-controller-send-thread]: Stopped
[33m2023-01-13 11:23:26[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-1-to-controller-send-thread]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutting down.
[33m2023-01-13 11:23:26[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutting down the log cleaner.
[33m2023-01-13 11:23:26[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Shutting down
[33m2023-01-13 11:23:26[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Shutdown completed
[33m2023-01-13 11:23:26[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Stopped
[33m2023-01-13 11:23:27[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutdown complete.
[33m2023-01-13 11:23:27[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=1] Shutting down
[33m2023-01-13 11:23:27[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=1] Shutdown completed
[33m2023-01-13 11:23:27[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=1] Stopped
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=1] Stopped partition state machine
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=1] Stopped replica state machine
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=1] Resigned
[33m2023-01-13 11:23:27[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Shutting down
[33m2023-01-13 11:23:27[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Shutdown completed
[33m2023-01-13 11:23:27[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Stopped
[33m2023-01-13 11:23:27[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Closing.
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Newly added brokers: , deleted brokers: 1, bounced brokers: , all live brokers: 2
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Shutting down
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Stopped
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Shutdown completed
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Broker failure callback for 1
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Removed 1 from list of shutting down brokers.
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __consumer_offsets-4 from NewPartition to OnlinePartition
kafka.common.StateChangeFailedException: Controller 2 epoch 2 encountered error during state change of partition __consumer_offsets-4 from New to Online, assigned replicas are [], live brokers are [Set(2)]. No assigned replica is alive.
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5(PartitionStateMachine.scala:291)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5$adapted(PartitionStateMachine.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.controller.ZkPartitionStateMachine.initializeLeaderAndIsrForPartitions(PartitionStateMachine.scala:286)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:227)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:74)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:59)
	at kafka.controller.KafkaController.onReplicasBecomeOffline(KafkaController.scala:618)
	at kafka.controller.KafkaController.onBrokerFailure(KafkaController.scala:588)
	at kafka.controller.KafkaController.processBrokerChange(KafkaController.scala:1604)
	at kafka.controller.KafkaController.process(KafkaController.scala:2400)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __consumer_offsets-1 from NewPartition to OnlinePartition
kafka.common.StateChangeFailedException: Controller 2 epoch 2 encountered error during state change of partition __consumer_offsets-1 from New to Online, assigned replicas are [], live brokers are [Set(2)]. No assigned replica is alive.
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5(PartitionStateMachine.scala:291)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$initializeLeaderAndIsrForPartitions$5$adapted(PartitionStateMachine.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at kafka.controller.ZkPartitionStateMachine.initializeLeaderAndIsrForPartitions(PartitionStateMachine.scala:286)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:227)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:74)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:59)
	at kafka.controller.KafkaController.onReplicasBecomeOffline(KafkaController.scala:618)
	at kafka.controller.KafkaController.onBrokerFailure(KafkaController.scala:588)
	at kafka.controller.KafkaController.processBrokerChange(KafkaController.scala:1604)
	at kafka.controller.KafkaController.process(KafkaController.scala:2400)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [1;31m[ERROR][0;39m [32m(Logging.scala:76)[0;39m - [Controller id=2 epoch=2] Controller 2 epoch 2 failed to change state for partition __consumer_offsets-2 from OfflinePartition to OnlinePartition
kafka.common.StateChangeFailedException: Failed to elect leader for partition __consumer_offsets-2 under strategy OfflinePartitionLeaderElectionStrategy(false)
	at kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:431)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:428)
	at kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:339)
	at kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:238)
	at kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:158)
	at kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:74)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:59)
	at kafka.controller.KafkaController.onReplicasBecomeOffline(KafkaController.scala:618)
	at kafka.controller.KafkaController.onBrokerFailure(KafkaController.scala:588)
	at kafka.controller.KafkaController.processBrokerChange(KafkaController.scala:1604)
	at kafka.controller.KafkaController.process(KafkaController.scala:2400)
	at kafka.controller.QueuedEvent.process(ControllerEventManager.scala:52)
	at kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:130)
	at kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31)
	at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:133)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Partition __consumer_offsets-2 state changed to (Leader:-1,ISR:1,LeaderEpoch:1,ControllerEpoch:2) after removing replica 1 from the ISR as part of transition to OfflineReplica
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set(2) for 1 partitions
[33m2023-01-13 11:23:27[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Updated broker epochs cache: Map(2 -> 60)
[33m2023-01-13 11:23:27[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 2 with correlation id 4
[33m2023-01-13 11:23:27[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:844)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Discovered group coordinator localhost:43117 (id: 2147483646 rack: null)
[33m2023-01-13 11:23:27[0;39m [36mo.a.k.c.consumer.internals.AbstractCoordinator[0;39m [34m[INFO][0;39m [32m(AbstractCoordinator.java:907)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Group coordinator localhost:43117 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: true. Rediscovery will be attempted.
[33m2023-01-13 11:23:27[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:524)[0;39m - EventThread shut down for session: 0x1048ffc08700001
[33m2023-01-13 11:23:27[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(ZooKeeper.java:1422)[0;39m - Session: 0x1048ffc08700001 closed
[33m2023-01-13 11:23:27[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Closed.
[33m2023-01-13 11:23:27[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Shutting down
[33m2023-01-13 11:23:27[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Stopped
[33m2023-01-13 11:23:27[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Shutdown completed
[33m2023-01-13 11:23:27[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Shutting down
[33m2023-01-13 11:23:28[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Stopped
[33m2023-01-13 11:23:28[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Shutdown completed
[33m2023-01-13 11:23:28[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Shutting down
[33m2023-01-13 11:23:28[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Stopped
[33m2023-01-13 11:23:28[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Shutdown completed
[33m2023-01-13 11:23:28[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Shutting down
[33m2023-01-13 11:23:29[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Stopped
[33m2023-01-13 11:23:29[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
[33m2023-01-13 11:23:29[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Shutting down socket server
[33m2023-01-13 11:23:29[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=1] Shutdown completed
[33m2023-01-13 11:23:29[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:668)[0;39m - Metrics scheduler closed
[33m2023-01-13 11:23:29[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:672)[0;39m - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[33m2023-01-13 11:23:29[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:678)[0;39m - Metrics reporters closed
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.BrokerTopicStats[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Broker and topic stats closed
[33m2023-01-13 11:23:29[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:83)[0;39m - App info kafka.server for 1 unregistered
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=1] shut down completed
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=2] shutting down
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=2] Starting controlled shutdown
[33m2023-01-13 11:23:29[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Shutting down broker 2
[33m2023-01-13 11:23:29[0;39m [36mstate.change.logger[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2 epoch=2] Sending UpdateMetadata request to brokers Set() for 0 partitions
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=2] Controlled shutdown succeeded
[33m2023-01-13 11:23:29[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Shutting down
[33m2023-01-13 11:23:29[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Stopped
[33m2023-01-13 11:23:29[0;39m [36mk.c.ZkNodeChangeNotificationListener$ChangeEventProcessThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [/config/changes-event-process-thread]: Shutdown completed
[33m2023-01-13 11:23:29[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Stopping socket server request processors
[33m2023-01-13 11:23:29[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Stopped socket server request processors
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaRequestHandlerPool[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [data-plane Kafka Request Handler on Broker 2], shutting down
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaRequestHandlerPool[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [data-plane Kafka Request Handler on Broker 2], shut down completely
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-AlterAcls]: Shutting down
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-AlterAcls]: Stopped
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-AlterAcls]: Shutdown completed
[33m2023-01-13 11:23:29[0;39m [36mkafka.server.KafkaApis[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaApi-2] Shutdown complete.
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-topic]: Shutting down
[33m2023-01-13 11:23:29[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 1 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:29[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2 (localhost/127.0.0.1:35497) could not be established. Broker may not be available.
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-topic]: Stopped
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-topic]: Shutdown completed
[33m2023-01-13 11:23:29[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=2] Shutting down.
[33m2023-01-13 11:23:29[0;39m [36mkafka.coordinator.transaction.ProducerIdManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ProducerId Manager 2]: Shutdown complete: last producerId assigned 2000
[33m2023-01-13 11:23:29[0;39m [36mk.coordinator.transaction.TransactionStateManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction State Manager 2]: Shutdown complete
[33m2023-01-13 11:23:29[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 2]: Shutting down
[33m2023-01-13 11:23:29[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 2]: Shutdown completed
[33m2023-01-13 11:23:29[0;39m [36mk.c.transaction.TransactionMarkerChannelManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Transaction Marker Channel Manager 2]: Stopped
[33m2023-01-13 11:23:29[0;39m [36mk.coordinator.transaction.TransactionCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [TransactionCoordinator id=2] Shutdown complete.
[33m2023-01-13 11:23:29[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 2]: Shutting down.
[33m2023-01-13 11:23:29[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Heartbeat]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 1 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:30[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2 (localhost/127.0.0.1:35497) could not be established. Broker may not be available.
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Heartbeat]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Heartbeat]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Rebalance]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Rebalance]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Rebalance]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.coordinator.group.GroupCoordinator[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [GroupCoordinator 2]: Shutdown complete.
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaManager broker=2] Shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaManager$LogDirFailureHandler[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [LogDirFailureHandler]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaFetcherManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaFetcherManager on broker 2] shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 2] shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaAlterLogDirsManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaAlterLogDirsManager on broker 2] shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Fetch]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Fetch]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Fetch]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Produce]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 1 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Processing automatic preferred replica leader election
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Starting replica leader election (PREFERRED) for partitions  triggered by AutoTriggered
[33m2023-01-13 11:23:30[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2 (localhost/127.0.0.1:35497) could not be established. Broker may not be available.
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Produce]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-Produce]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-DeleteRecords]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-DeleteRecords]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-DeleteRecords]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-ElectLeader]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-ElectLeader]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.s.DelayedOperationPurgatory$ExpiredOperationReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ExpirationReaper-2-ElectLeader]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.ReplicaManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaManager broker=2] Shut down completely
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-2-to-controller-send-thread]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-2-to-controller-send-thread]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-2-to-controller-send-thread]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.server.BrokerToControllerRequestThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [broker-2-to-controller-send-thread]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutting down.
[33m2023-01-13 11:23:30[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutting down the log cleaner.
[33m2023-01-13 11:23:30[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.log.LogCleaner[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [kafka-log-cleaner-thread-0]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.log.LogManager[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Shutdown complete.
[33m2023-01-13 11:23:30[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=2] Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=2] Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.c.ControllerEventManager$ControllerEventThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ControllerEventThread controllerId=2] Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.ZkPartitionStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [PartitionStateMachine controllerId=2] Stopped partition state machine
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.ZkReplicaStateMachine[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ReplicaStateMachine controllerId=2] Stopped replica state machine
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Shutting down
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Stopped
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.RequestSendThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [RequestSendThread controllerId=2] Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.controller.KafkaController[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [Controller id=2] Resigned
[33m2023-01-13 11:23:30[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.s.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [feature-zk-node-event-process-thread]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Closing.
[33m2023-01-13 11:23:30[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 1 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:30[0;39m [36morg.apache.zookeeper.ClientCnxn[0;39m [34m[INFO][0;39m [32m(ClientCnxn.java:524)[0;39m - EventThread shut down for session: 0x1048ffc08700002
[33m2023-01-13 11:23:30[0;39m [36morg.apache.zookeeper.ZooKeeper[0;39m [34m[INFO][0;39m [32m(ZooKeeper.java:1422)[0;39m - Session: 0x1048ffc08700002 closed
[33m2023-01-13 11:23:30[0;39m [36mkafka.zookeeper.ZooKeeperClient[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ZooKeeperClient Kafka server] Closed.
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2 (localhost/127.0.0.1:35497) could not be established. Broker may not be available.
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Fetch]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Produce]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Shutting down
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Shutdown completed
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-Request]: Stopped
[33m2023-01-13 11:23:30[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Shutting down
[33m2023-01-13 11:23:31[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 1 (localhost/127.0.0.1:43117) could not be established. Broker may not be available.
[33m2023-01-13 11:23:31[0;39m [36morg.apache.kafka.clients.NetworkClient[0;39m [31m[WARN][0;39m [32m(NetworkClient.java:782)[0;39m - [Consumer clientId=consumer-dc12076a-d2fb-48d1-92eb-9f8114a01174-1, groupId=dc12076a-d2fb-48d1-92eb-9f8114a01174] Connection to node 2 (localhost/127.0.0.1:35497) could not be established. Broker may not be available.
[33m2023-01-13 11:23:31[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Stopped
[33m2023-01-13 11:23:31[0;39m [36mk.server.ClientQuotaManager$ThrottledChannelReaper[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
[33m2023-01-13 11:23:31[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Shutting down socket server
[33m2023-01-13 11:23:31[0;39m [36mkafka.network.SocketServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [SocketServer brokerId=2] Shutdown completed
[33m2023-01-13 11:23:31[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:668)[0;39m - Metrics scheduler closed
[33m2023-01-13 11:23:31[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:672)[0;39m - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[33m2023-01-13 11:23:31[0;39m [36morg.apache.kafka.common.metrics.Metrics[0;39m [34m[INFO][0;39m [32m(Metrics.java:678)[0;39m - Metrics reporters closed
[33m2023-01-13 11:23:31[0;39m [36mkafka.server.BrokerTopicStats[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - Broker and topic stats closed
[33m2023-01-13 11:23:31[0;39m [36morg.apache.kafka.common.utils.AppInfoParser[0;39m [34m[INFO][0;39m [32m(AppInfoParser.java:83)[0;39m - App info kafka.server for 2 unregistered
[33m2023-01-13 11:23:31[0;39m [36mkafka.server.KafkaServer[0;39m [34m[INFO][0;39m [32m(Logging.scala:66)[0;39m - [KafkaServer id=2] shut down completed
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:583)[0;39m - ConnnectionExpirerThread interrupted
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:420)[0;39m - selector thread exitted run method
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:420)[0;39m - selector thread exitted run method
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:420)[0;39m - selector thread exitted run method
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:219)[0;39m - accept thread exitted run method
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.NIOServerCnxnFactory[0;39m [34m[INFO][0;39m [32m(NIOServerCnxnFactory.java:420)[0;39m - selector thread exitted run method
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.ZooKeeperServer[0;39m [34m[INFO][0;39m [32m(ZooKeeperServer.java:573)[0;39m - shutting down
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.SessionTrackerImpl[0;39m [34m[INFO][0;39m [32m(SessionTrackerImpl.java:237)[0;39m - Shutting down
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.PrepRequestProcessor[0;39m [34m[INFO][0;39m [32m(PrepRequestProcessor.java:1008)[0;39m - Shutting down
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.PrepRequestProcessor[0;39m [34m[INFO][0;39m [32m(PrepRequestProcessor.java:156)[0;39m - PrepRequestProcessor exited loop!
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.SyncRequestProcessor[0;39m [34m[INFO][0;39m [32m(SyncRequestProcessor.java:191)[0;39m - Shutting down
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.SyncRequestProcessor[0;39m [34m[INFO][0;39m [32m(SyncRequestProcessor.java:169)[0;39m - SyncRequestProcessor exited!
[33m2023-01-13 11:23:31[0;39m [36morg.apache.zookeeper.server.FinalRequestProcessor[0;39m [34m[INFO][0;39m [32m(FinalRequestProcessor.java:514)[0;39m - shutdown of request processor complete
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.142 sec

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] -----------------< org.kafkainaction:chapter8-no-java >-----------------
[INFO] Building chapter8-no-java 1.0.0-SNAPSHOT                          [8/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter8-no-java ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter8/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter8-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter8-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter8-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter8/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter8-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter8-no-java ---
[INFO] No tests to run.
[INFO] 
[INFO] -----------------< org.kafkainaction:chapter9-no-java >-----------------
[INFO] Building chapter9-no-java 1.0.0-SNAPSHOT                          [9/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter9-no-java ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter9/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter9-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter9-no-java ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 7 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter9/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter9-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter9/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter9-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter9-no-java ---
[INFO] No tests to run.
[INFO] 
[INFO] ----------------< org.kafkainaction:chapter10-no-java >-----------------
[INFO] Building chapter10-no-java 1.0.0-SNAPSHOT                        [10/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter10-no-java ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter10/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter10-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter10-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter10-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter10/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter10-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter10-no-java ---
[INFO] No tests to run.
[INFO] 
[INFO] --------------------< org.kafkainaction:chapter11 >---------------------
[INFO] Building chapter11 1.0.0-SNAPSHOT                                [11/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter11 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter11/target
[INFO] 
[INFO] --- avro-maven-plugin:1.10.2:schema (default) @ chapter11 ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter11 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter11 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 4 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter11/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter11 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter11/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter11 ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter11 ---
[INFO] No tests to run.
[INFO] 
[INFO] --------------------< org.kafkainaction:chapter12 >---------------------
[INFO] Building chapter12 1.0.0-SNAPSHOT                                [12/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ chapter12 ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_Chapter12/target
[INFO] 
[INFO] --- avro-maven-plugin:1.10.2:schema (default) @ chapter12 ---
[INFO] Importing File: /home/gabsko/breaking-updates/KafkaInAction_Chapter12/src/main/avro/transaction.avsc
[INFO] Importing File: /home/gabsko/breaking-updates/KafkaInAction_Chapter12/src/main/avro/funds.avsc
[INFO] Importing File: /home/gabsko/breaking-updates/KafkaInAction_Chapter12/src/main/avro/transaction_result.avsc
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ chapter12 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ chapter12 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 21 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter12/target/classes
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ chapter12 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_Chapter12/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ chapter12 ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 4 source files to /home/gabsko/breaking-updates/KafkaInAction_Chapter12/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ chapter12 ---
[INFO] Surefire report directory: /home/gabsko/breaking-updates/KafkaInAction_Chapter12/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.kafkainaction.kstreams2.TransactionProcessorTest
[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [31m[WARN][0;39m [32m(StateDirectory.java:134)[0;39m - Using /tmp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS
[33m2023-01-13 11:23:33[0;39m [36mo.a.k.s.state.internals.RocksDBTimestampedStore[0;39m [34m[INFO][0;39m [32m(RocksDBTimestampedStore.java:100)[0;39m - Opening store latest-transactions in regular mode
[33m2023-01-13 11:23:33[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store latest-transactions did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-latest-transactions-changelog-0
[33m2023-01-13 11:23:33[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store funds-store did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-funds-store-changelog-0
[33m2023-01-13 11:23:33[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:219)[0;39m - stream-thread [main] task [0_0] Initialized
[33m2023-01-13 11:23:33[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:240)[0;39m - stream-thread [main] task [0_0] Restored and ready to run
[transactions logger]: 1, {"guid": "25b92085-12ac-4143-855a-2edcd30bdd20", "account": "1", "amount": 100.00, "type": "DEPOSIT", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:33[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 100.00. Current balance is 100.00.
[transactions logger]: 1, {"guid": "25b92085-12ac-4143-855a-2edcd30bdd20", "account": "1", "amount": 100.00, "type": "DEPOSIT", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:33[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 100.00. Current balance is 200.00.
[transactions logger]: 1, {"guid": "25b92085-12ac-4143-855a-2edcd30bdd20", "account": "1", "amount": 100.00, "type": "DEPOSIT", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:33[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 100.00. Current balance is 300.00.
[33m2023-01-13 11:23:33[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:279)[0;39m - stream-thread [main] task [0_0] Suspended running
[33m2023-01-13 11:23:33[0;39m [36mo.a.k.s.processor.internals.RecordCollectorImpl[0;39m [34m[INFO][0;39m [32m(RecordCollectorImpl.java:258)[0;39m - topology-test-driver Closing record collector clean
[33m2023-01-13 11:23:33[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:494)[0;39m - stream-thread [main] task [0_0] Closed clean
[33m2023-01-13 11:23:33[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [34m[INFO][0;39m [32m(StateDirectory.java:497)[0;39m - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:33[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [31m[WARN][0;39m [32m(StateDirectory.java:134)[0;39m - Using /tmp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.state.internals.RocksDBTimestampedStore[0;39m [34m[INFO][0;39m [32m(RocksDBTimestampedStore.java:100)[0;39m - Opening store latest-transactions in regular mode
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store latest-transactions did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-latest-transactions-changelog-0
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store funds-store did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-funds-store-changelog-0
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:219)[0;39m - stream-thread [main] task [0_0] Initialized
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:240)[0;39m - stream-thread [main] task [0_0] Restored and ready to run
[transactions logger]: 1, {"guid": "900805b7-f23a-474a-ad03-1509200cf6ac", "account": "1", "amount": 100.00, "type": "DEPOSIT", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 100.00. Current balance is 100.00.
[transactions logger]: 1, {"guid": "ba6834f6-6a95-40ea-8bb9-0221563b4341", "account": "1", "amount": 100.00, "type": "WITHDRAW", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with -100.00. Current balance is 0.00.
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:279)[0;39m - stream-thread [main] task [0_0] Suspended running
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.RecordCollectorImpl[0;39m [34m[INFO][0;39m [32m(RecordCollectorImpl.java:258)[0;39m - topology-test-driver Closing record collector clean
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:494)[0;39m - stream-thread [main] task [0_0] Closed clean
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [34m[INFO][0;39m [32m(StateDirectory.java:497)[0;39m - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [31m[WARN][0;39m [32m(StateDirectory.java:134)[0;39m - Using /tmp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.state.internals.RocksDBTimestampedStore[0;39m [34m[INFO][0;39m [32m(RocksDBTimestampedStore.java:100)[0;39m - Opening store latest-transactions in regular mode
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store latest-transactions did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-latest-transactions-changelog-0
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store funds-store did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-funds-store-changelog-0
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:219)[0;39m - stream-thread [main] task [0_0] Initialized
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:240)[0;39m - stream-thread [main] task [0_0] Restored and ready to run
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:279)[0;39m - stream-thread [main] task [0_0] Suspended running
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.RecordCollectorImpl[0;39m [34m[INFO][0;39m [32m(RecordCollectorImpl.java:258)[0;39m - topology-test-driver Closing record collector clean
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:494)[0;39m - stream-thread [main] task [0_0] Closed clean
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [34m[INFO][0;39m [32m(StateDirectory.java:497)[0;39m - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [31m[WARN][0;39m [32m(StateDirectory.java:134)[0;39m - Using /tmp directory in the state.dir property can cause failures with writing the checkpoint file due to the fact that this directory can be cleared by the OS
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.state.internals.RocksDBTimestampedStore[0;39m [34m[INFO][0;39m [32m(RocksDBTimestampedStore.java:100)[0;39m - Opening store latest-transactions in regular mode
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store latest-transactions did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-latest-transactions-changelog-0
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.ProcessorStateManager[0;39m [34m[INFO][0;39m [32m(ProcessorStateManager.java:256)[0;39m - topology-test-driver State store funds-store did not find checkpoint offset, hence would default to the starting offset at changelog dummy-topology-test-driver-app-id-funds-store-changelog-0
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:219)[0;39m - stream-thread [main] task [0_0] Initialized
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:240)[0;39m - stream-thread [main] task [0_0] Restored and ready to run
[transactions logger]: 1, {"guid": "1898004a-f0dc-4457-8191-ae8fec339fc5", "account": "1", "amount": 100.00, "type": "DEPOSIT", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 100.00. Current balance is 100.00.
[transactions logger]: 1, {"guid": "cb1db60a-ae9c-4361-a230-d617d92e83ba", "account": "1", "amount": 200.00, "type": "WITHDRAW", "currency": "USD", "country": "USA"}
[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:75)[0;39m - Not enough funds for account 1.
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:279)[0;39m - stream-thread [main] task [0_0] Suspended running
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.s.processor.internals.RecordCollectorImpl[0;39m [34m[INFO][0;39m [32m(RecordCollectorImpl.java:258)[0;39m - topology-test-driver Closing record collector clean
[33m2023-01-13 11:23:34[0;39m [36mo.a.kafka.streams.processor.internals.StreamTask[0;39m [34m[INFO][0;39m [32m(StreamTask.java:494)[0;39m - stream-thread [main] task [0_0] Closed clean
[33m2023-01-13 11:23:34[0;39m [36mo.a.k.streams.processor.internals.StateDirectory[0;39m [34m[INFO][0;39m [32m(StateDirectory.java:497)[0;39m - stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup.
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.518 sec
Running org.kafkainaction.kstreams2.TransactionTransformerTest
[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 300. Current balance is 300.
[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with -200. Current balance is 100.00.
[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:75)[0;39m - Not enough funds for account 1.
[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroSerializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.remove.java.properties = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36mi.c.kafka.serializers.KafkaAvroDeserializerConfig[0;39m [34m[INFO][0;39m [32m(AbstractConfig.java:361)[0;39m - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [mock://schema-registry.kafkainaction.org:8080]
	specific.avro.reader = true
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[33m2023-01-13 11:23:34[0;39m [36morg.kafkainaction.kstreams2.TransactionTransformer[0;39m [34m[INFO][0;39m [32m(TransactionTransformer.java:85)[0;39m - Updating funds for account 1 with 100. Current balance is 100.
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec

Results :

Tests run: 7, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] ----------------< org.kafkainaction:appendixB-no-java >-----------------
[INFO] Building appendixB-no-java 1.0.0-SNAPSHOT                        [13/13]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ appendixB-no-java ---
[INFO] Deleting /home/gabsko/breaking-updates/KafkaInAction_AppendixB/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ appendixB-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ appendixB-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ appendixB-no-java ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/KafkaInAction_AppendixB/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ appendixB-no-java ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ appendixB-no-java ---
[INFO] No tests to run.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Kafka-In-Action 1.0.0-SNAPSHOT:
[INFO] 
[INFO] Kafka-In-Action .................................... SUCCESS [  0.089 s]
[INFO] chapter2 ........................................... SUCCESS [  0.790 s]
[INFO] chapter3 ........................................... SUCCESS [  0.682 s]
[INFO] chapter4 ........................................... SUCCESS [  0.199 s]
[INFO] chapter5 ........................................... SUCCESS [  0.153 s]
[INFO] chapter6-no-java ................................... SUCCESS [  0.010 s]
[INFO] chapter7 ........................................... SUCCESS [ 12.708 s]
[INFO] chapter8-no-java ................................... SUCCESS [  0.010 s]
[INFO] chapter9-no-java ................................... SUCCESS [  0.070 s]
[INFO] chapter10-no-java .................................. SUCCESS [  0.006 s]
[INFO] chapter11 .......................................... SUCCESS [  0.161 s]
[INFO] chapter12 .......................................... SUCCESS [  2.331 s]
[INFO] appendixB-no-java .................................. SUCCESS [  0.013 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  17.307 s
[INFO] Finished at: 2023-01-13T11:23:34Z
[INFO] ------------------------------------------------------------------------
