[INFO] Scanning for projects...
[INFO] Inspecting build with total of 1 modules...
[INFO] Installing Nexus Staging features:
[INFO]   ... total of 1 executions of maven-deploy-plugin replaced with nexus-staging-maven-plugin
[INFO] 
[INFO] --------------< com.snowflake:snowflake-kafka-connector >---------------
[INFO] Building Snowflake Kafka Connector 1.8.1
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ snowflake-kafka-connector ---
[INFO] Deleting /home/gabsko/breaking-updates/target
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.2:prepare-agent (pre-unit-test) @ snowflake-kafka-connector ---
[INFO] argLine set to -javaagent:/home/gabsko/.m2/repository/org/jacoco/org.jacoco.agent/0.8.2/org.jacoco.agent-0.8.2-runtime.jar=destfile=/home/gabsko/breaking-updates/target/jacoco-ut.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ snowflake-kafka-connector ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /home/gabsko/breaking-updates/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ snowflake-kafka-connector ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 51 source files to /home/gabsko/breaking-updates/target/classes
[WARNING] /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/records/AvroConverterConfig.java: Some input files use or override a deprecated API.
[WARNING] /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/records/AvroConverterConfig.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/internal/streaming/TopicPartitionChannel.java: /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/internal/streaming/TopicPartitionChannel.java uses unchecked or unsafe operations.
[WARNING] /home/gabsko/breaking-updates/src/main/java/com/snowflake/kafka/connector/internal/streaming/TopicPartitionChannel.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ snowflake-kafka-connector ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 4 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ snowflake-kafka-connector ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 37 source files to /home/gabsko/breaking-updates/target/test-classes
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/InternalStageIT.java: Some input files use or override a deprecated API.
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/InternalStageIT.java: Recompile with -Xlint:deprecation for details.
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/ConnectionServiceIT.java: Some input files use unchecked or unsafe operations.
[WARNING] /home/gabsko/breaking-updates/src/test/java/com/snowflake/kafka/connector/internal/ConnectionServiceIT.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.0:test (default-test) @ snowflake-kafka-connector ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.snowflake.kafka.connector.ConnectorConfigTest
02-02-2023 01:21:41 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.Utils' without a Kafka Connect global instance id.
02-02-2023 01:21:41 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.SnowflakeSinkConnectorConfig' without a Kafka Connect global instance id.
02-02-2023 01:21:41 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.BufferThreshold' without a Kafka Connect global instance id.
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] buffer.flush.time is 0, it should be greater than 1
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Invalid snowflake.topic2table.map config format: $@#$#@%^$12312
02-02-2023 01:21:41 main ERROR StreamingUtils:213 - Config:key.converter has provided value:com.snowflake.kafka.connector.records.SnowflakeJsonConverter. If ingestionMethod is:snowpipe_streaming, Snowflake Custom Converters are not allowed. 
Invalid Converters:[com.snowflake.kafka.connector.records.SnowflakeJsonConverter, com.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry, com.snowflake.kafka.connector.records.SnowflakeAvroConverter]
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Schematization is only available with snowpipe_streaming.
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] snowflake.url.name cannot be empty.
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
02-02-2023 01:21:41 main ERROR StreamingUtils:196 - Kafka config:snowflake.ingestion.method error:Invalid value invalid for configuration errors.tolerance: String must be one of: none, all
02-02-2023 01:21:41 main ERROR StreamingUtils:158 - Config:snowflake.role.name should be present if ingestionMethod is:snowpipe_streaming
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Kafka provider config error:Unsupported provider name: Something_which_is_not_supported. Supported are: unknown,self_hosted,confluent
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records should be a positive integer. Provided:adssadsa
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] buffer.size.bytes is too low at 0. It must be 1 or greater.
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.size.bytes should be an integer. Provided:afdsa
02-02-2023 01:21:41 main ERROR StreamingUtils:196 - Kafka config:snowflake.ingestion.method error:Invalid value invalid_value for configuration snowflake.ingestion.method: String must be one of: snowpipe, snowpipe_streaming
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] snowflake.private.key cannot be empty.
02-02-2023 01:21:41 main ERROR StreamingUtils:213 - Config:value.converter has provided value:com.snowflake.kafka.connector.records.SnowflakeJsonConverter. If ingestionMethod is:snowpipe_streaming, Snowflake Custom Converters are not allowed. 
Invalid Converters:[com.snowflake.kafka.connector.records.SnowflakeJsonConverter, com.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry, com.snowflake.kafka.connector.records.SnowflakeAvroConverter]
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Proxy settings error: 
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] name is empty or invalid. It should match Snowflake object identifier syntax. Please see the documentation.
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Proxy settings error: 
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] snowflake.user.name cannot be empty.
02-02-2023 01:21:41 main ERROR StreamingUtils:196 - Kafka config:snowflake.ingestion.method error:Invalid value invalid for configuration errors.log.enable: String must be one of: true, false
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records is empty
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] buffer.flush.time should be an integer. Invalid integer was provided:fdas
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records should be a positive integer. Provided:adssadsa
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records is -1, it should at least 1
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.flush.time is empty
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] buffer.flush.time should be an integer. Invalid integer was provided:fdas
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Delivery Guarantee config:delivery.guarantee error:Unsupported Delivery Guarantee Type: INVALID. Supported are: at_least_once,exactly_once
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Kafka config:behavior.on.null.values error:Invalid value invalid for configuration behavior.on.null.values: String must be one of: default, ignore
02-02-2023 01:21:41 main ERROR StreamingUtils:174 - Config:delivery.guarantee should be:exactly_once if ingestion method is:snowpipe_streaming
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.count.records is -1, it should at least 1
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] snowflake.schema.name cannot be empty.
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] Config buffer.flush.time is empty
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] table name !@#@!#!@ should have at least 2 characters, start with _a-zA-Z, and only contains _$a-zA-z0-9
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] snowflake.database.name cannot be empty.
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] topic name topic1 is duplicated
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Kafka config:jmx should either be true or false
02-02-2023 01:21:41 main ERROR BufferThreshold:155 - [SF_KAFKA_CONNECTOR] buffer.flush.time is 9, it should be greater than 10
[INFO] Tests run: 58, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.618 s - in com.snowflake.kafka.connector.ConnectorConfigTest
[INFO] Running com.snowflake.kafka.connector.UtilsTest
02-02-2023 01:21:41 main INFO  Utils:107 - [SF_KAFKA_CONNECTOR] invalid JDBC_LOG_DIR /dummy_dir_not_exist defaulting to /tmp
02-02-2023 01:21:41 main INFO  Utils:107 - [SF_KAFKA_CONNECTOR] jdbc tracing directory = /usr
02-02-2023 01:21:41 main INFO  Utils:107 - [SF_KAFKA_CONNECTOR] jdbc tracing directory = /tmp
[SF_KAFKA_CONNECTOR] 1 test message
2 test message
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Invalid snowflake.topic2table.map config format: adsadas
02-02-2023 01:21:41 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] table name @123 should have at least 2 characters, start with _a-zA-Z, and only contains _$a-zA-z0-9
02-02-2023 01:21:41 main INFO  Utils:107 - [SF_KAFKA_CONNECTOR] Current Snowflake Kafka Connector Version: 1.8.1
02-02-2023 01:21:42 main WARN  Utils:143 - [SF_KAFKA_CONNECTOR] Connector update is available, please upgrade Snowflake Kafka Connector (1.8.1 -> 1.8.2) 
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.SnowflakeSinkTask_STATIC' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main ERROR Utils:155 - [SF_KAFKA_CONNECTOR] Invalid snowflake.topic2table.map config format: 12321
02-02-2023 01:21:42 main ERROR SnowflakeSinkTask_STATIC:155 - [SF_KAFKA_CONNECTOR] Invalid Input, Topic2Table Map disabled
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.596 s - in com.snowflake.kafka.connector.UtilsTest
[INFO] Running com.snowflake.kafka.connector.records.RecordContentTest
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.SnowflakeConverter' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
[INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.105 s - in com.snowflake.kafka.connector.records.RecordContentTest
[INFO] Running com.snowflake.kafka.connector.records.MetaColumnTest
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
{"content":{"name":"test"},"meta":{"topic":"test","offset":0,"partition":0,"key":"test"}}
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] Failed to parse JSON record
net.snowflake.client.jdbc.internal.fasterxml.jackson.core.JsonParseException: Unexpected character ('a' (code 97)): Expected space separating root-level values
 at [Source: (byte[])"123adsada"; line: 1, column: 5]
02-02-2023 01:21:42 main DEBUG CodecFactory:71 - Snappy was not available
java.lang.NoClassDefFoundError: org/xerial/snappy/Snappy
	at org.apache.avro.file.SnappyCodec$Option.<clinit>(SnappyCodec.java:35)
	at org.apache.avro.file.CodecFactory.snappyCodec(CodecFactory.java:69)
	at org.apache.avro.file.CodecFactory.<clinit>(CodecFactory.java:140)
	at org.apache.avro.file.DataFileStream.resolveCodec(DataFileStream.java:160)
	at org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:144)
	at org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:143)
	at org.apache.avro.file.DataFileReader.<init>(DataFileReader.java:130)
	at com.snowflake.kafka.connector.records.SnowflakeAvroConverterWithoutSchemaRegistry.toConnectData(SnowflakeAvroConverterWithoutSchemaRegistry.java:49)
	at com.snowflake.kafka.connector.records.MetaColumnTest.testSchemaID(MetaColumnTest.java:242)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)
Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.Snappy
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	... 37 more
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
Config test success
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.075 s - in com.snowflake.kafka.connector.records.MetaColumnTest
[INFO] Running com.snowflake.kafka.connector.records.ValueSchemaTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.snowflake.kafka.connector.records.ValueSchemaTest
[INFO] Running com.snowflake.kafka.connector.records.HeaderTest
02-02-2023 01:21:42 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 s - in com.snowflake.kafka.connector.records.HeaderTest
[INFO] Running com.snowflake.kafka.connector.records.ConverterTest
{
  "type" : "record",
  "name" : "MyRecord",
  "fields" : [ {
    "name" : "bytesDecimal",
    "type" : {
      "type" : "bytes",
      "logicalType" : "decimal",
      "precision" : 20,
      "scale" : 4
    }
  } ]
}
02-02-2023 01:21:42 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	generalized.sum.type.support = false
	schemas.cache.config = 100
	scrub.invalid.names = false

02-02-2023 01:21:42 main INFO  AvroConverterConfig:376 - AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main INFO  KafkaAvroSerializerConfig:376 - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main INFO  KafkaAvroDeserializerConfig:376 - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	specific.avro.reader = false
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	generalized.sum.type.support = false
	schemas.cache.config = 1000
	scrub.invalid.names = false

0000000001060dbba0
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] Failed to parse JSON record
net.snowflake.client.jdbc.internal.fasterxml.jackson.core.JsonParseException: Unrecognized token 'fasfas': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (byte[])"fasfas"; line: 1, column: 7]
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] failed to parse AVRO record
[SF_KAFKA_CONNECTOR] Exception: Invalid input record
Error Code: 0010
Detail: Input record value can't be parsed
Message: unknown bytes
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] failed to parse AVRO record
null
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] failed to parse AVRO record
null
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] Failed to parse AVRO record
[SF_KAFKA_CONNECTOR] Exception: Invalid input record
Error Code: 0010
Detail: Input record value can't be parsed
Message: Failed to parse AVRO record
Not an Avro data file.
02-02-2023 01:21:42 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] the string provided for reader.schema is no valid Avro schema: com.fasterxml.jackson.core.JsonParseException: Unexpected character (':' (code 58)): expected a valid value (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (String)"{"name":"test_avro","type":"record","fields":[{"name":"int","type":"int"},{"name":"newfield","type":"int","default": 1},{"name":"missingfield","type"::"int"}]}"; line: 1, column: 152]
02-02-2023 01:21:42 main INFO  AvroConverterConfig:376 - AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main DEBUG SslFactory:228 - Created SSL context with keystore null, truststore null, provider SunJSSE.
02-02-2023 01:21:42 main INFO  AvroConverterConfig:376 - AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main DEBUG SslFactory:228 - Created SSL context with keystore null, truststore null, provider SunJSSE.
02-02-2023 01:21:42 main INFO  AvroConverterConfig:376 - AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main INFO  KafkaAvroSerializerConfig:376 - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main INFO  KafkaAvroDeserializerConfig:376 - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	specific.avro.reader = false
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:42 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	generalized.sum.type.support = false
	schemas.cache.config = 1000
	scrub.invalid.names = false

Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-2,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-4,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-1,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-5,5,main]
Record Service result:"1997-05-19T00:00:00.000Z" Thread :Thread[pool-1-thread-3,5,main]
02-02-2023 01:21:43 main INFO  AvroConverterConfig:376 - AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:43 main INFO  KafkaAvroSerializerConfig:376 - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:43 main INFO  KafkaAvroDeserializerConfig:376 - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	specific.avro.reader = false
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:43 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	generalized.sum.type.support = false
	schemas.cache.config = 1000
	scrub.invalid.names = false

02-02-2023 01:21:43 main ERROR SnowflakeConverter:155 - [SF_KAFKA_CONNECTOR] failed to parse AVRO record
null
[INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.25 s - in com.snowflake.kafka.connector.records.ConverterTest
[INFO] Running com.snowflake.kafka.connector.records.ProcessRecordTest
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.038 s - in com.snowflake.kafka.connector.records.ProcessRecordTest
[INFO] Running com.snowflake.kafka.connector.SecurityTest
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.003 s <<< FAILURE! - in com.snowflake.kafka.connector.SecurityTest
[ERROR] testRSAPasswordOutput(com.snowflake.kafka.connector.SecurityTest)  Time elapsed: 0.003 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.SecurityTest.testRSAPasswordOutput(SecurityTest.java:21)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.SecurityTest.testRSAPasswordOutput(SecurityTest.java:21)

[INFO] Running com.snowflake.kafka.connector.internal.FIPSTest
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 s <<< FAILURE! - in com.snowflake.kafka.connector.internal.FIPSTest
[ERROR] testFips(com.snowflake.kafka.connector.internal.FIPSTest)  Time elapsed: 0 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.FIPSTest.testFips(FIPSTest.java:20)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.FIPSTest.testFips(FIPSTest.java:20)

[INFO] Running com.snowflake.kafka.connector.internal.InternalUtilsTest
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.InternalUtils' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main DEBUG InternalUtils:131 - [SF_KAFKA_CONNECTOR] converted date: 2019-07-18T23:32:38Z
[ERROR] Tests run: 6, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.129 s <<< FAILURE! - in com.snowflake.kafka.connector.internal.InternalUtilsTest
[ERROR] testCreateProperties(com.snowflake.kafka.connector.internal.InternalUtilsTest)  Time elapsed: 0 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testCreateProperties(InternalUtilsTest.java:79)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testCreateProperties(InternalUtilsTest.java:79)

[ERROR] testPrivateKey(com.snowflake.kafka.connector.internal.InternalUtilsTest)  Time elapsed: 0.123 s  <<< ERROR!
java.lang.RuntimeException: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testPrivateKey(InternalUtilsTest.java:19)
Caused by: java.io.FileNotFoundException: profile.json (No such file or directory)
	at com.snowflake.kafka.connector.internal.InternalUtilsTest.testPrivateKey(InternalUtilsTest.java:19)

[INFO] Running com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.streaming.SnowflakeSinkServiceV2' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannel' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:43 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:43 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:44 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:1, message:Channel INVALID_CHANNEL is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
02-02-2023 01:21:44 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:45 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:2, message:Channel INVALID_CHANNEL is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:45 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [GET_OFFSET_TOKEN_FALLBACK] Re-opening channel:TEST_0
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:45 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [GET_OFFSET_TOKEN_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:45 main ERROR TopicPartitionChannel:155 - [SF_KAFKA_CONNECTOR] [OFFSET_TOKEN_FALLBACK] Failed to open Channel/fetch offsetToken for channel:TEST_0
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:45 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}, insertResponseHasErrors:true
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:45 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:45 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Received offset:0 for topic:test as the first offset for this partition:0 after start/restart/rebalance
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:1076, currentBufferedRecordCount:4, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=5, bufferSizeThresholdBytes=1000, bufferKafkaRecordCountThreshold=10000000}
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=4, bufferSizeBytes=1076, firstOffset=0, lastOffset=3}
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 4 records, 1076 bytes, offset 0 - 3
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=4, bufferSizeBytes=1076, firstOffset=0, lastOffset=3}, insertResponseHasErrors:false
02-02-2023 01:21:45 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:45 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Fetched offsetToken:0 for channelName:TEST_0
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Time based flush for channel:TEST_0, CurrentTimeMs:1675300910853, previousFlushTimeMs:1675300905841, bufferThresholdSeconds:5
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=269, firstOffset=4, lastOffset=4}
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 269 bytes, offset 4 - 4
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=269, firstOffset=4, lastOffset=4}, insertResponseHasErrors:false
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:50 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Fetched offsetToken:100 for channelName:TEST_0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:50 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Received offset:0 for topic:TEST as the first offset for this partition:0 after start/restart/rebalance
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:50 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:50 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:50 main INFO  AvroConverterConfig:376 - AvroConverterConfig values: 
	auto.register.schemas = true
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:50 main INFO  KafkaAvroSerializerConfig:376 - KafkaAvroSerializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.remove.java.properties = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:50 main INFO  KafkaAvroDeserializerConfig:376 - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://fake-url]
	specific.avro.reader = false
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

02-02-2023 01:21:50 main INFO  AvroDataConfig:376 - AvroDataConfig values: 
	connect.meta.data = true
	discard.type.doc.default = false
	enhanced.avro.schema.support = false
	generalized.sum.type.support = false
	schemas.cache.config = 1000
	scrub.invalid.names = false

02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Received offset:0 for topic:test as the first offset for this partition:0 after start/restart/rebalance
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:12098, currentBufferedRecordCount:2, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=5, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=10000000}
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=2, bufferSizeBytes=12098, firstOffset=0, lastOffset=1}
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 2 records, 12098 bytes, offset 0 - 1
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=2, bufferSizeBytes=12098, firstOffset=0, lastOffset=1}, insertResponseHasErrors:false
02-02-2023 01:21:50 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:50 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Fetched offsetToken:1 for channelName:TEST_0
02-02-2023 01:21:55 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Time based flush for channel:TEST_0, CurrentTimeMs:1675300915939, previousFlushTimeMs:1675300910928, bufferThresholdSeconds:5
02-02-2023 01:21:55 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=6049, firstOffset=2, lastOffset=2}
02-02-2023 01:21:55 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 6049 bytes, offset 2 - 2
02-02-2023 01:21:55 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=6049, firstOffset=2, lastOffset=2}, insertResponseHasErrors:false
02-02-2023 01:21:55 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:55 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Fetched offsetToken:2 for channelName:TEST_0
02-02-2023 01:21:55 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:55 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:55 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:55 main ERROR TopicPartitionChannel:155 - [SF_KAFKA_CONNECTOR] The offsetToken string does not contain a parsable long:invalidNo for channel:TEST_0
02-02-2023 01:21:55 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:55 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:55 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:56 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:1, message:Channel INVALID_CHANNEL is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
02-02-2023 01:21:56 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [OFFSET_TOKEN_RETRY_POLICY] retry for getLatestCommittedOffsetToken. Retry no:2, message:Channel INVALID_CHANNEL is invalid and might contain uncommitted rows, please consider reopening the channel to restart.
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [GET_OFFSET_TOKEN_FALLBACK] Re-opening channel:TEST_0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [GET_OFFSET_TOKEN_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Fetched offsetToken:0 for channelName:TEST_0
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [RESET_PARTITION] Emptying current buffer:StreamingBuffer{numOfRecords=0, bufferSizeBytes=0, firstOffset=-1, lastOffset=-1} for Channel:TEST_0 due to reset of offsets in kafka
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [GET_OFFSET_TOKEN_FALLBACK] Channel:TEST_0, OffsetRecoveredFromSnowflake:0, Reset kafka offset to:1
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:57 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] Failed Attempt to invoke the insertRows API for buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [INSERT_ROWS_FALLBACK] Re-opening channel:TEST_0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:57 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [INSERT_ROWS_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:57 main ERROR TopicPartitionChannel:155 - [SF_KAFKA_CONNECTOR] [INSERT_ROWS_FALLBACK] Failed to open Channel or fetching offsetToken for channel:TEST_0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:57 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}, insertResponseHasErrors:true
02-02-2023 01:21:57 main ERROR TopicPartitionChannel:155 - [SF_KAFKA_CONNECTOR] Insert Row Error message
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:57 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Received offset:0 for topic:TEST as the first offset for this partition:0 after start/restart/rebalance
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}, insertResponseHasErrors:true
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:57 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:57 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:57 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main ERROR TopicPartitionChannel:155 - [SF_KAFKA_CONNECTOR] Failure closing Streaming Channel name:TEST_0 msg:Interrupted Exception
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Received offset:0 for topic:test as the first offset for this partition:0 after start/restart/rebalance
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:227, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=227, firstOffset=0, lastOffset=0}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 227 bytes, offset 0 - 0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=227, firstOffset=0, lastOffset=0}, insertResponseHasErrors:false
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.records.RecordService' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Received offset:0 for topic:TEST as the first offset for this partition:0 after start/restart/rebalance
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:58 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] Failed Attempt to invoke the insertRows API for buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:58 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [INSERT_ROWS_FALLBACK] Re-opening channel:TEST_0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Opening a channel with name:TEST_0 for table name:TEST_TABLE
02-02-2023 01:21:58 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [INSERT_ROWS_FALLBACK] Fetching offsetToken after re-opening the channel:TEST_0
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] OffsetToken not present for channelName:TEST_0
02-02-2023 01:21:58 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [RESET_PARTITION] Emptying current buffer:StreamingBuffer{numOfRecords=0, bufferSizeBytes=0, firstOffset=-1, lastOffset=-1} for Channel:TEST_0 due to reset of offsets in kafka
02-02-2023 01:21:58 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [INSERT_ROWS_FALLBACK] Channel:TEST_0, OffsetRecoveredFromSnowflake:-1, Reset kafka offset to:0
02-02-2023 01:21:58 main WARN  TopicPartitionChannel:143 - [SF_KAFKA_CONNECTOR] [INSERT_BUFFERED_RECORDS] Failure inserting buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0} for channel:TEST_0
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Ignore adding offset:1 to buffer for channel:TEST_0 because we recently encountered error and reset offset in Kafka. offsetPersistedInSnowflake:-1
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Ignore adding offset:2 to buffer for channel:TEST_0 because we recently encountered error and reset offset in Kafka. offsetPersistedInSnowflake:-1
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Ignore adding offset:3 to buffer for channel:TEST_0 because we recently encountered error and reset offset in Kafka. offsetPersistedInSnowflake:-1
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Ignore adding offset:4 to buffer for channel:TEST_0 because we recently encountered error and reset offset in Kafka. offsetPersistedInSnowflake:-1
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Got the desired offset:0 from Kafka, we can add this offset to buffer for channel:TEST_0
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 0 - 0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=0, lastOffset=0}, insertResponseHasErrors:false
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=1, lastOffset=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 1 - 1
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=1, lastOffset=1}, insertResponseHasErrors:false
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=2, lastOffset=2}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 2 - 2
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=2, lastOffset=2}, insertResponseHasErrors:false
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=3, lastOffset=3}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 3 - 3
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=3, lastOffset=3}, insertResponseHasErrors:false
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Flush based on buffered bytes or buffered number of records for channel:TEST_0,currentBufferSizeInBytes:273, currentBufferedRecordCount:1, connectorBufferThresholds:StreamingBufferThreshold{flushTimeThresholdSeconds=10, bufferSizeThresholdBytes=10000, bufferKafkaRecordCountThreshold=1}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Invoking insertRows API for Channel:TEST_0, streamingBuffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=4, lastOffset=4}
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Get rows for streaming ingest. 1 records, 273 bytes, offset 4 - 4
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Successfully called insertRows for channel:TEST_0, buffer:StreamingBuffer{numOfRecords=1, bufferSizeBytes=273, firstOffset=4, lastOffset=4}, insertResponseHasErrors:false
02-02-2023 01:21:58 main DEBUG TopicPartitionChannel:131 - [SF_KAFKA_CONNECTOR] Fetching last committed offset for partition channel:TEST_0
02-02-2023 01:21:58 main INFO  TopicPartitionChannel:107 - [SF_KAFKA_CONNECTOR] Fetched offsetToken:4 for channelName:TEST_0
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.949 s - in com.snowflake.kafka.connector.internal.streaming.TopicPartitionChannelTest
[INFO] Running com.snowflake.kafka.connector.internal.streaming.StreamingBufferThresholdTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.snowflake.kafka.connector.internal.streaming.StreamingBufferThresholdTest
[INFO] Running com.snowflake.kafka.connector.internal.FileNameUtilsTest
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.FileNameUtils' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG FileNameUtils:131 - [SF_KAFKA_CONNECTOR] generated file name: TEST_CONNECTOR/test_topic/123/456_789_1675300918167.json.gz
02-02-2023 01:21:58 main DEBUG FileNameUtils:131 - [SF_KAFKA_CONNECTOR] generated broken data file name: TEST_CONNECTOR/test_topic/123/456_key_1675300918173.gz
02-02-2023 01:21:58 main DEBUG FileNameUtils:131 - [SF_KAFKA_CONNECTOR] generated broken data file name: TEST_CONNECTOR/test_topic/123/456_value_1675300918173.gz
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 s - in com.snowflake.kafka.connector.internal.FileNameUtilsTest
[INFO] Running com.snowflake.kafka.connector.internal.telemetry.TelemetryUnitTest
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.metrics.MetricsJmxReporter' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.telemetry.SnowflakeTelemetryBasicInfo' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeTelemetryBasicInfo:131 - [SF_KAFKA_CONNECTOR] Registering metrics for pipe:pipe, existing:[]
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.072 s - in com.snowflake.kafka.connector.internal.telemetry.TelemetryUnitTest
[INFO] Running com.snowflake.kafka.connector.internal.telemetry.SnowflakeTelemetryServiceV1Test
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.telemetry.SnowflakeTelemetryServiceV1' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeTelemetryServiceV1:131 - [SF_KAFKA_CONNECTOR] sending telemetry data: {"app_name":"TEST_APP","task_id":"1","start_time":1675300918251,"kafka_version":"3.2.3","max_tasks":null,"buffer.size.bytes":"5000000","buffer.count.records":"10000","buffer.flush.time":"120","snowflake.ingestion.method":"snowpipe","delivery.guarantee":"EXACTLY_ONCE","key.converter":"org.apache.kafka.connect.storage.StringConverter","value.converter":"io.confluent.connect.avro.AvroConverter"} of type:kafka_start
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.telemetry.SnowflakeTelemetryServiceV1' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeTelemetryServiceV1:131 - [SF_KAFKA_CONNECTOR] sending telemetry data: {"app_name":"TEST_APP","task_id":"1","start_time":1675300918265,"kafka_version":"3.2.3","max_tasks":null,"buffer.size.bytes":"5000000","buffer.count.records":"10000","buffer.flush.time":"120","snowflake.ingestion.method":"snowpipe","delivery.guarantee":"at_least_once","key.converter":"org.apache.kafka.connect.storage.StringConverter","value.converter":"io.confluent.connect.avro.AvroConverter"} of type:kafka_start
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.telemetry.SnowflakeTelemetryServiceV1' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeTelemetryServiceV1:131 - [SF_KAFKA_CONNECTOR] sending telemetry data: {"app_name":"TEST_APP","task_id":"1","start_time":1675300918266,"kafka_version":"3.2.3","max_tasks":null,"buffer.size.bytes":"5000000","buffer.count.records":"10000","buffer.flush.time":"120","snowflake.ingestion.method":"snowpipe_streaming","key.converter":"org.apache.kafka.connect.storage.StringConverter","value.converter":"io.confluent.connect.avro.AvroConverter"} of type:kafka_start
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 s - in com.snowflake.kafka.connector.internal.telemetry.SnowflakeTelemetryServiceV1Test
[INFO] Running com.snowflake.kafka.connector.internal.LoggerHandlerTest
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:43 - Set Kafka Connect global instance id tag for logging: 'e499b8c3-623e-4476-baf8-0cf6531232e3'
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  name:91 - Given logger tag set to: 'TEST'
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main WARN  name:84 - Given logger tag 'null' is invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:43 - Set Kafka Connect global instance id tag for logging: '26d8a69f-aa1e-4a8c-aaa9-d55e66378218'
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' with Kafka Connect global instance id: '26d8a69f-aa1e-4a8c-aaa9-d55e66378218'
02-02-2023 01:21:58 main INFO  name:91 - Given logger tag set to: 'TEST'
02-02-2023 01:21:58 main INFO  LoggerHandler:46 - Given Kafka Connect global instance id was invalid (null or empty), continuing to log without it
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'test.logger.name' without a Kafka Connect global instance id.
[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.07 s - in com.snowflake.kafka.connector.internal.LoggerHandlerTest
[INFO] Running com.snowflake.kafka.connector.internal.SnowflakeURLTest
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: http://account.snowflake.com:80
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.snowflake.com:443
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL:  account.snowflake.com:80
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: account.snowflake.com
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: http://account.snowflake.com 
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.snowflake.com
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.region.aws.privatelink.snowflake.com:443
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: http://org-account.snowflake.com:80
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main INFO  SnowflakeURL:107 - [SF_KAFKA_CONNECTOR] enabling JDBC tracing
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://account.region.aws.privatelink.snowflake.com:443
02-02-2023 01:21:58 main INFO  LoggerHandler:64 - [SF_KAFKA_CONNECTOR] Created loggerHandler for class: 'com.snowflake.kafka.connector.internal.SnowflakeURL' without a Kafka Connect global instance id.
02-02-2023 01:21:58 main DEBUG SnowflakeURL:131 - [SF_KAFKA_CONNECTOR] parsed Snowflake URL: https://org-account.privatelink.snowflake.com:80
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.snowflake.kafka.connector.internal.SnowflakeURLTest
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   SecurityTest.testRSAPasswordOutput:21 » Runtime java.io.FileNotFoundException:...
[ERROR]   FIPSTest.testFips:20 » Runtime java.io.FileNotFoundException: profile.json (No...
[ERROR]   InternalUtilsTest.testCreateProperties:79 » Runtime java.io.FileNotFoundExcept...
[ERROR]   InternalUtilsTest.testPrivateKey:19 » Runtime java.io.FileNotFoundException: p...
[INFO] 
[ERROR] Tests run: 166, Failures: 0, Errors: 4, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  22.161 s
[INFO] Finished at: 2023-02-02T01:21:58Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.0:test (default-test) on project snowflake-kafka-connector: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/gabsko/breaking-updates/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
