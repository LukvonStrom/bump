[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Conquery Parent                                                    [pom]
[INFO] backend                                                            [jar]
[INFO] executable                                                         [jar]
[INFO] autodoc                                                            [jar]
[INFO] 
[INFO] --------------------< com.bakdata.conquery:parent >---------------------
[INFO] Building Conquery Parent 0.0.0-SNAPSHOT                            [1/4]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ parent ---
[INFO] 
[INFO] --------------------< com.bakdata.conquery:backend >--------------------
[INFO] Building backend 0.0.0-SNAPSHOT                                    [2/4]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ backend ---
[INFO] Deleting /home/gabsko/breaking-updates/backend/target
[INFO] 
[INFO] --- maven-resources-plugin:3.2.0:resources (default-resources) @ backend ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Using 'UTF-8' encoding to copy filtered properties files.
[INFO] Copying 2 resources
[INFO] Copying 32 resources
[INFO] The encoding used to copy filtered properties files have not been set. This means that the same encoding will be used to copy filtered properties files as when copying other filtered resources. This might not be what you want! Run your build with --debug to see which files might be affected. Read more at https://maven.apache.org/plugins/maven-resources-plugin/examples/filtering-properties-files.html
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ backend ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 822 source files to /home/gabsko/breaking-updates/backend/target/classes
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Some input files use or override a deprecated API.
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Some input files use unchecked or unsafe operations.
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:3.2.0:testResources (default-testResources) @ backend ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Using 'UTF-8' encoding to copy filtered properties files.
[INFO] Copying 467 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ backend ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 128 source files to /home/gabsko/breaking-updates/backend/target/test-classes
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/tasks/PermissionCleanupTaskTest.java: Some input files use or override a deprecated API.
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/tasks/PermissionCleanupTaskTest.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/integration/tests/ReusedQueryTest.java: Some input files use unchecked or unsafe operations.
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/integration/tests/ReusedQueryTest.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ backend ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.bakdata.conquery.models.forms.DateContextTest
[INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.079 s - in com.bakdata.conquery.models.forms.DateContextTest
[INFO] Running com.bakdata.conquery.models.execution.DefaultLabelTest
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver		Scanning Classpath
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver		Scanned: 1096 classes in classpath
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class CredentialType
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				PASSWORD	->	PasswordCredential
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class Mode
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ABSOLUTE	->	AbsoluteMode
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				RELATIVE	->	RelativeMode
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ENTITY_DATE	->	EntityDateMode
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class CQElement
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SAVED_QUERY	->	CQReusedQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				AND	->	CQAnd
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				NEGATION	->	CQNegation
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DAYS_BEFORE	->	CQDaysBeforeTemporalQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BEFORE_OR_SAME	->	CQBeforeOrSameTemporalQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				RESULT_INFO_DECORATOR	->	ResultInfoDecorator
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EXTERNAL	->	CQExternal
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				OR	->	CQOr
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DAYS_OR_NO_EVENT_BEFORE	->	CQDaysBeforeOrNeverTemporalQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BEFORE	->	CQBeforeTemporalQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CONCEPT	->	CQConcept
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SAME	->	CQSameTemporalQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_RESTRICTION	->	CQDateRestriction
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class QueryDescription
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SECONDARY_ID_QUERY	->	SecondaryIdQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ENTITY_DATE_QUERY	->	EntityDateQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				TABLE_EXPORT	->	TableExportQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CONCEPT_QUERY	->	ConceptQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FULL_EXPORT_FORM	->	FullExportForm
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EXPORT_FORM	->	ExportForm
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ABSOLUTE_FORM_QUERY	->	AbsoluteFormQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				RELATIVE_FORM_QUERY	->	RelativeFormQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ARRAY_CONCEPT_QUERY	->	ArrayConceptQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class FilterValue
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REAL_RANGE	->	CQRealRangeFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING	->	CQStringFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				INTEGER_RANGE	->	CQIntegerRangeFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MONEY_RANGE	->	CQMoneyRangeFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BIG_MULTI_SELECT	->	CQBigMultiSelectFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SELECT	->	CQSelectFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MULTI_SELECT	->	CQMultiSelectFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ConqueryTestSpec
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FILTER_TEST	->	FilterTest
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				QUERY_TEST	->	QueryTest
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORM_TEST	->	FormTest
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ResultRendererProvider
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				XLSX	->	XlsxResultProvider
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ARROW_STREAM	->	ArrowStreamResultProvider
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ARROW_FILE	->	ArrowFileResultProvider
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CSV	->	CsvResultRendererProvider
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ConqueryPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				WILDCARD_PERMISSION	->	WildcardPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class StringPermissionBuilder
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORM_CONFIG	->	FormConfigPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ADMIN	->	AdminPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORM_TYPE	->	FormPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATASET	->	DatasetPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SUPER	->	SuperPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CONCEPT	->	ConceptPermission
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EXECUTION	->	ExecutionPermission
[WARN] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class interface com.bakdata.conquery.models.config.PluginConfig:	No registered types
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class StoreFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				NON_PERSISTENT	->	NonPersistentStoreFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				XODUS	->	XodusStoreFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class AuthenticationRealmFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DEVELOPMENT	->	DevAuthConfig
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				API_TOKEN	->	ApiTokenRealmFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				JWT_PKCE_REALM	->	JwtPkceVerifyingRealmFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				LOCAL_AUTHENTICATION	->	LocalAuthenticationConfig
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				OIDC_AUTHORIZATION_CODE_FLOW	->	OIDCAuthorizationCodeFlowRealmFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				OIDC_RESOURCE_OWNER_PASSWORD_CREDENTIAL_AUTHENTICATION	->	OIDCResourceOwnerPasswordCredentialRealmFactory
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class AuthorizationConfig
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DEVELOPMENT	->	DevelopmentAuthorizationConfig
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DEFAULT	->	DefaultAuthorizationConfig
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class Concept
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				TREE	->	TreeConcept
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class CTCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EQUAL	->	EqualCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				OR	->	OrCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				GROOVY	->	GroovyCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				PREFIX_LIST	->	PrefixCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				AND	->	AndCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				PREFIX_RANGE	->	PrefixRangeCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				PRESENT	->	IsPresentCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COLUMN_EQUAL	->	ColumnEqualCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				NOT	->	NotCondition
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class Filter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COUNT	->	CountFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COUNT_QUARTERS	->	CountQuartersFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				PREFIX_TEXT	->	PrefixTextFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				NUMBER	->	NumberFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BIG_MULTI_SELECT	->	BigMultiSelectFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_DISTANCE	->	DateDistanceFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SELECT	->	MultiSelectFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DURATION_SUM	->	DurationSumFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				QUARTERS_IN_YEAR	->	QuartersInYearFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SINGLE_SELECT	->	SelectFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SUM	->	SumFilter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class Select
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				QUARTER	->	QuarterSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COUNT_QUARTERS	->	CountQuartersSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EXISTS	->	ExistsSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				QUARTERS_IN_YEAR	->	QuartersInYearSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FIRST	->	FirstValueSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_UNION	->	DateUnionSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DURATION_SUM	->	DurationSumSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COUNT	->	CountSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EVENT_DURATION_SUM	->	EventDurationSumSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				PREFIX	->	PrefixSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DISTINCT	->	DistinctSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				RANDOM	->	RandomValueSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COUNT_OCCURENCES	->	CountOccurencesSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				LAST	->	LastValueSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EVENT_DATE_UNION	->	EventDateUnionSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_DISTANCE	->	DateDistanceSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SUM	->	SumSelect
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class Dictionary
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MAP_DICTIONARY	->	MapDictionary
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SUCCINCT_TRIE	->	SuccinctTrie
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ConqueryError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_PLAN	->	ExecutionCreationPlanError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_JOB	->	ExecutionJobErrorWrapper
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_NO_SECONDARY_ID	->	NoSecondaryIdSelectedError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_PROCESSING	->	ExecutionProcessingError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE_EXTERNAL	->	ExternalResolveError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE_EXTERNAL_FORMAT	->	ExternalResolveFormatError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION	->	ExecutionCreationErrorUnspecified
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_PROCESSING_TIMEOUT	->	ExecutionProcessingTimeoutError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_UNKNOWN_ERROR	->	UnknownError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE	->	ExecutionCreationResolveError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_CREATION_PLAN_DATECONTEXT_MISMATCH	->	ExecutionCreationPlanDateContextError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE_EXTERNAL_EMPTY	->	ExternalResolveEmptyError
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ColumnStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BOOLEANS	->	BitSetStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING_NUMBER	->	StringTypeNumber
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BYTES	->	ByteArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				LONGS	->	LongArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SHORTS	->	ShortArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING_PREFIX	->	StringTypePrefixSuffix
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EMPTY	->	EmptyStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				INTEGERS	->	IntArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATES	->	IntegerDateStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REBASE	->	RebasingStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MONEY_VARINT	->	MoneyIntStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE_COMPOUND	->	DateRangeTypeCompound
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING_DICTIONARY	->	StringTypeDictionary
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DECIMALS	->	DecimalArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE_DATE_RANGE	->	DateRangeTypeDateRange
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DOUBLES	->	DoubleArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FLOATS	->	FloatArrayStore
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING_SINGLETON	->	StringTypeSingleton
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DECIMAL_SCALED	->	DecimalTypeScaled
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE_QUARTER	->	DateRangeTypeQuarter
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING_ENCODED	->	StringTypeEncoded
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ManagedExecution
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MANAGED_QUERY	->	ManagedQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				INTERNAL_FORM	->	ManagedInternalForm
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MANAGED_FORM	->	ManagedForm
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ResultType
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				INTEGER	->	IntegerT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MONEY	->	MoneyT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				RESOLUTION	->	ResolutionT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE	->	DateT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				STRING	->	StringT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ID	->	IdT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE	->	DateRangeT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				BOOLEAN	->	BooleanT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				LIST	->	ListT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				NUMERIC	->	NumericT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CATEGORICAL	->	CategoricalT
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class NamespacedMessage
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_SECONDARYID	->	RemoveSecondaryId
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				IMPORT_BIT	->	ImportBucket
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_IMPORT	->	RemoveImportJob
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_TABLE	->	RemoveTable
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_CONCEPT	->	RemoveConcept
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SHUTDOWN_WORKER	->	ShutdownWorkerStorage
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_SHARD_WORKER_IDENTITY	->	UpdateWorkerBucket
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EXECUTE_FORM	->	ExecuteForm
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CANCEL_QUERY	->	CancelQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_METADATA	->	UpdateElementMatchingStats
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REQUEST_CONSISTENCY	->	RequestConsistency
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_DATASET	->	UpdateDataset
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_DICTIONARY	->	UpdateDictionary
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_CONCEPT	->	UpdateConcept
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_MATCHING_STATS	->	UpdateMatchingStatsMessage
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ADD_IMPORT	->	AddImport
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REPORT_CONSISTENCY	->	ReportConsistency
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_TABLE	->	UpdateTable
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COLLECT_QUERY_RESULT	->	CollectQueryResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EXECUTE_QUERY	->	ExecuteQuery
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_SECONDARYID	->	UpdateSecondaryId
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class NetworkMessage
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORWARD_TO_WORKER	->	ForwardToWorker
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_JOB_MANAGER_STATUS	->	UpdateJobManagerStatus
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				CANCEL_JOB	->	CancelJobMessage
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORWARD_TO_NAMESPACE	->	ForwardToNamespace
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_SHARD_NODE	->	RemoveShardNode
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_WORKER	->	RemoveWorker
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ADD_SHARD_NODE	->	AddShardNode
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				REGISTER_SHARD_WORKER_IDENTITY	->	RegisterWorker
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ADD_WORKER	->	AddWorker
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class OutputDescription
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COMPOUND_DATE_RANGE	->	CompoundDateRangeOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				COPY	->	CopyOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				LINE	->	LineOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE	->	DateRangeOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EPOCH_DATE_RANGE	->	EpochDateRangeOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				EPOCH	->	EpochOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				NULL	->	NullOutput
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class EntityResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SINGLE_LINE	->	SinglelineEntityResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				MULTI_LINE	->	MultilineEntityResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ShardResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORM_SHARD_RESULT	->	FormShardResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				SHARD_RESULT	->	ShardResult
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver			Base Class ResourcesProvider
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				ApiV1	->	ApiV1
[INFO] [TEST] [2023-01-06 20:43:17]	c.b.c.i.c.CPSTypeIdResolver				FORM_RESOURCES	->	FormResourceProvider
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.32 s - in com.bakdata.conquery.models.execution.DefaultLabelTest
[INFO] Running com.bakdata.conquery.models.events.stores.primitive.BooleanStoreTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.148 s - in com.bakdata.conquery.models.events.stores.primitive.BooleanStoreTest
[INFO] Running com.bakdata.conquery.models.events.stores.primitive.IntArrayStoreTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.events.stores.primitive.IntArrayStoreTest
[INFO] Running com.bakdata.conquery.models.events.stores.types.ColumnStoreSerializationTests
ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...
[INFO] Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.89 s - in com.bakdata.conquery.models.events.stores.types.ColumnStoreSerializationTests
[INFO] Running com.bakdata.conquery.models.events.stores.types.MajorTypesTest
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in com.bakdata.conquery.models.events.stores.types.MajorTypesTest
[INFO] Running com.bakdata.conquery.models.events.stores.types.StringEncodingTest
[INFO] Tests run: 101, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.05 s - in com.bakdata.conquery.models.events.stores.types.StringEncodingTest
[INFO] Running com.bakdata.conquery.models.events.CBlockTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 s - in com.bakdata.conquery.models.events.CBlockTest
[INFO] Running com.bakdata.conquery.models.error.ConqueryErrorTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 s - in com.bakdata.conquery.models.error.ConqueryErrorTest
[INFO] Running com.bakdata.conquery.models.query.UniqueNameTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.query.UniqueNameTest
[INFO] Running com.bakdata.conquery.models.query.DefaultColumnNameTest
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.113 s - in com.bakdata.conquery.models.query.DefaultColumnNameTest
[INFO] Running com.bakdata.conquery.models.SerializationTests
[INFO] [TEST] [2023-01-06 20:43:19]	c.b.c.m.SerializationTests		Beware, this test will print an ERROR message.
[ERROR] [TEST] [2023-01-06 20:43:19]	c.b.c.m.e.ConqueryError$UnknownError		Encountered unknown Error[fa016a20-e6cc-41fa-9e85-a6afef37dcaa]
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.795 s - in com.bakdata.conquery.models.SerializationTests
[INFO] Running com.bakdata.conquery.models.auth.ApiTokenTest
[INFO] [TEST] [2023-01-06 20:43:19]	c.b.c.m.a.ApiTokenTest		Testing token: cq_44TZxFytw2tnEbJTa0OpX9gmBZENIzOWE2l47
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.auth.ApiTokenTest
[INFO] Running com.bakdata.conquery.models.auth.LocalAuthRealmTest
[INFO] [TEST] [2023-01-06 20:43:20]	c.b.c.m.a.b.PasswordHasher		Using the following settings to generate password hashes:
	Algorithm: PBKDF2WithHmacSHA1
	Iterations: 10000
	Key length: 256
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.59 s - in com.bakdata.conquery.models.auth.LocalAuthRealmTest
[INFO] Running com.bakdata.conquery.models.auth.CopyUserTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.auth.CopyUserTest
[INFO] Running com.bakdata.conquery.models.auth.InstancePermissionImplificationTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.auth.InstancePermissionImplificationTest
[INFO] Running com.bakdata.conquery.models.auth.IdpDelegatingAccessTokenCreatorTest
Loading JavaScript to validate ECMA262 regular expression in JsonSchema because java.util.regex package in Java does not match ECMA262
Warning: Nashorn engine is planned to be removed from a future JDK release
[INFO] [TEST] [2023-01-06 20:43:21]	c.b.c.m.a.IdpDelegatingAccessTokenCreatorTest		This test will print an Error below.
[ERROR] [TEST] [2023-01-06 20:43:22]	c.b.c.m.a.o.p.IdpDelegatingAccessTokenCreator		Received the following error from the auth server while validating username and password:
	Path: http://localhost:1080/realms/test_relam/protocol/openid-connect/token
	Status code: 403
	Status message: null
	Content: {"error":null}
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.463 s - in com.bakdata.conquery.models.auth.IdpDelegatingAccessTokenCreatorTest
[INFO] Running com.bakdata.conquery.models.auth.PermissionCreationTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.auth.PermissionCreationTest
[INFO] Running com.bakdata.conquery.models.auth.oidc.JwtPkceVerifyingRealmTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.813 s - in com.bakdata.conquery.models.auth.oidc.JwtPkceVerifyingRealmTest
[INFO] Running com.bakdata.conquery.models.auth.IntrospectionDelegatingRealmTest
[INFO] [TEST] [2023-01-06 20:43:26]	c.b.c.m.a.o.IntrospectionDelegatingRealm		Created new group: Group[group.group2]
[INFO] [TEST] [2023-01-06 20:43:26]	c.b.c.m.a.o.IntrospectionDelegatingRealm		Created new user: User[user.test_name1]
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.389 s - in com.bakdata.conquery.models.auth.IntrospectionDelegatingRealmTest
[INFO] Running com.bakdata.conquery.models.common.CQuarterTest
[INFO] Tests run: 616, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.151 s - in com.bakdata.conquery.models.common.CQuarterTest
[INFO] Running com.bakdata.conquery.models.common.daterange.CDateRangeTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.common.daterange.CDateRangeTest
[INFO] Running com.bakdata.conquery.models.common.RangeTest
[INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.common.RangeTest
[INFO] Running com.bakdata.conquery.models.common.QuarterUtilsTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.common.QuarterUtilsTest
[INFO] Running com.bakdata.conquery.models.externalservice.ResultTypeTest
[INFO] Tests run: 62, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 s - in com.bakdata.conquery.models.externalservice.ResultTypeTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.frontend.FEValueTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.datasets.concepts.frontend.FEValueTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.temporal.TemporalSamplerTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.datasets.concepts.temporal.TemporalSamplerTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.tree.GroovyIndexedTest
[INFO] Tests run: 51, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.801 s - in com.bakdata.conquery.models.datasets.concepts.tree.GroovyIndexedTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.tree.MatchingStatsTests
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.datasets.concepts.tree.MatchingStatsTests
[INFO] Running com.bakdata.conquery.models.identifiable.ids.IdTests
[INFO] Tests run: 57, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.063 s - in com.bakdata.conquery.models.identifiable.ids.IdTests
[INFO] Running com.bakdata.conquery.models.identifiable.mapping.PseudomizationTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 s - in com.bakdata.conquery.models.identifiable.mapping.PseudomizationTest
[INFO] Running com.bakdata.conquery.models.identifiable.IdMapTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.models.identifiable.IdMapTest
[INFO] Running com.bakdata.conquery.models.preproc.PreprocessorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in com.bakdata.conquery.models.preproc.PreprocessorTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.IntegerParserTest
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 s - in com.bakdata.conquery.models.preproc.parser.specific.IntegerParserTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.DateRangeParserTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.parser.specific.DateRangeParserTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.DecimalParserTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.parser.specific.DecimalParserTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.RealParserTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.parser.specific.RealParserTest
[INFO] Running com.bakdata.conquery.models.dictionary.MapDictionaryTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 s - in com.bakdata.conquery.models.dictionary.MapDictionaryTest
[INFO] Running com.bakdata.conquery.api.StoredQueriesProcessorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 s - in com.bakdata.conquery.api.StoredQueriesProcessorTest
[INFO] Running com.bakdata.conquery.api.form.config.FormConfigTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.081 s - in com.bakdata.conquery.api.form.config.FormConfigTest
[INFO] Running com.bakdata.conquery.util.dict.SuccinctTrieTest
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		structure build
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		trie compressed
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		forward lookup done
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		reverse lookup done
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		structure build
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		trie compressed
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		forward lookup done
[INFO] [TEST] [2023-01-06 20:43:28]	c.b.c.u.d.SuccinctTrieTest		reverse lookup done
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.264 s - in com.bakdata.conquery.util.dict.SuccinctTrieTest
[INFO] Running com.bakdata.conquery.util.dict.TernaryTreeBalancerTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.178 s - in com.bakdata.conquery.util.dict.TernaryTreeBalancerTest
[INFO] Running com.bakdata.conquery.util.ConqueryEscapeTest
[INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 s - in com.bakdata.conquery.util.ConqueryEscapeTest
[INFO] Running com.bakdata.conquery.util.progressreporter.ProgressReporterTest
[INFO] [TEST] [2023-01-06 20:43:32]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 09s 
[INFO] [TEST] [2023-01-06 20:43:32]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 09s 
[INFO] [TEST] [2023-01-06 20:43:32]	c.b.c.u.p.ProgressReporterTest		
[INFO] [TEST] [2023-01-06 20:43:32]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 10s 
[INFO] [TEST] [2023-01-06 20:43:32]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 20s 
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.319 s - in com.bakdata.conquery.util.progressreporter.ProgressReporterTest
[INFO] Running com.bakdata.conquery.util.progressreporter.ProgressReporterUtilTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.util.progressreporter.ProgressReporterUtilTest
[INFO] Running com.bakdata.conquery.util.search.QuickSearchTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.util.search.QuickSearchTest
[INFO] Running com.bakdata.conquery.integration.common.CDateSetTest
[INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 s - in com.bakdata.conquery.integration.common.CDateSetTest
[INFO] Running com.bakdata.conquery.integration.ConqueryIntegrationTests
[INFO] [TEST] [2023-01-06 20:43:32]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:43:32]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[WARN] [2023-01-06 20:43:32]	c.b.c.m.c.XodusStoreFactory	ManagerNode	Had to create Storage Dir at `/tmp/conqueryIntegrationTest6175202265804509010/manager`
[INFO] [2023-01-06 20:43:32]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: []
[INFO] [2023-01-06 20:43:32]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:43:32]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 0 entries, 0 B within 6.411 ms
[DEBUG] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 452.2 μs
[DEBUG] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 322.8 μs
[DEBUG] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 0 entries, 0 B within 284.1 μs
[DEBUG] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:32]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 318.8 μs
[INFO] [2023-01-06 20:43:32]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@59a8d137
[DEBUG] [2023-01-06 20:43:32]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:43:32]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:43:32]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_8
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_9
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_10
[INFO] [2023-01-06 20:43:32]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:43:32]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:43:32]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:43:32]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[WARN] [2023-01-06 20:43:32]	c.b.c.m.c.XodusStoreFactory	shard-node1	Had to create Storage Dir at `/tmp/conqueryIntegrationTest6175202265804509010/shard-node1`
[INFO] [2023-01-06 20:43:32]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: []
[WARN] [2023-01-06 20:43:32]	c.b.c.m.c.XodusStoreFactory	shard-node0	Had to create Storage Dir at `/tmp/conqueryIntegrationTest6175202265804509010/shard-node0`
[INFO] [2023-01-06 20:43:32]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 0
[INFO] [2023-01-06 20:43:32]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: []
[INFO] [2023-01-06 20:43:32]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 0
[DEBUG] [2023-01-06 20:43:32]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:50698 connected, waiting for identity
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ShardNode	/127.0.0.1:50698	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:50700 connected, waiting for identity
[INFO] [2023-01-06 20:43:33]	c.b.c.c.ShardNode	/127.0.0.1:50700	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:43:33]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:50698` registered.
[INFO] [2023-01-06 20:43:33]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:50700` registered.
[WARN] [2023-01-06 20:43:33]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:33]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:43:33]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:33]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:43:33]	c.b.c.i.IntegrationTest$Wrapper	DownloadLinkGeneration	STARTING integration test DownloadLinkGeneration
[INFO] [2023-01-06 20:43:33]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Setting up dataset
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:DATASET}): 0 entries, 0 B within 258.5 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:SECONDARY_IDS}): 0 entries, 0 B within 156.2 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:TABLES}): 0 entries, 0 B within 146.4 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 691.5 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:IMPORTS}): 0 entries, 0 B within 128.0 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:CONCEPTS}): 0 entries, 0 B within 141.5 μs
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.NamespacedStorage	DownloadLinkGeneration	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 154.3 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:STRUCTURE}): 0 entries, 0 B within 123.8 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:WORKER_TO_BUCKETS}): 0 entries, 0 B within 128.5 μs
[DEBUG] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:33]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration:PRIMARY_DICTIONARY}): 0 entries, 0 B within 113.0 μs
[INFO] [2023-01-06 20:43:33]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:DATASET}): 0 entries, 0 B within 254.8 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:SECONDARY_IDS}): 0 entries, 0 B within 129.7 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:TABLES}): 0 entries, 0 B within 166.0 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 107.9 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:IMPORTS}): 0 entries, 0 B within 109.2 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:CONCEPTS}): 0 entries, 0 B within 117.3 μs
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:WORKER}): 0 entries, 0 B within 116.5 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:BUCKETS}): 0 entries, 0 B within 108.5 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1:C_BLOCKS}): 0 entries, 0 B within 138.5 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:DATASET}): 0 entries, 0 B within 224.6 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:SECONDARY_IDS}): 0 entries, 0 B within 160.0 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:TABLES}): 0 entries, 0 B within 138.8 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 126.5 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:IMPORTS}): 0 entries, 0 B within 123.9 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:CONCEPTS}): 0 entries, 0 B within 164.4 μs
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:WORKER}): 0 entries, 0 B within 116.8 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:BUCKETS}): 0 entries, 0 B within 113.2 μs
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:34]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160:C_BLOCKS}): 0 entries, 0 B within 128.7 μs
[INFO] [2023-01-06 20:43:34]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Imports of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Buckets of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Consistency check was successful
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Imports of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Buckets of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Consistency check was successful
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.a.AuthorizationController	DownloadLinkGeneration	Security manager registered
[INFO] [2023-01-06 20:43:34]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:34]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.UpdateTable	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Received update of Table DownloadLinkGeneration.test_table
[INFO] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.UpdateTable	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Received update of Table DownloadLinkGeneration.test_table
[INFO] [2023-01-06 20:43:34]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Updating Concept[DownloadLinkGeneration.test_tree]
[DEBUG] [2023-01-06 20:43:34]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Updating Concept[DownloadLinkGeneration.test_tree]
[INFO] [2023-01-06 20:43:34]	c.b.c.c.PreprocessorCommand	DownloadLinkGeneration	Preprocessing from command line config.
[INFO] [2023-01-06 20:43:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:43:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)	Required to preprocess 94 B in total
[INFO] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000530735s[INFO] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:43:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Chosen encoding is Base16UpperCase
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@701ce89e(est. 64 B)
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		test_column: StringParser(super=Parser(lines=4, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[INFO] [2023-01-06 20:43:34]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7ab7da9e)
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7ab7da9e) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Headers
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing header
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing data
[INFO] [2023-01-06 20:43:34]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:43:34]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 1 Jobs:
[INFO] [2023-01-06 20:43:34]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 4 Entities.
[INFO] [2023-01-06 20:43:34]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into DownloadLinkGeneration.test_table
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Mapped 4 new ids
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Updating bucket assignments.
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.w.Namespace	Job Manager slow DownloadLinkGeneration	Assigning Bucket[0] to Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160]
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.w.Namespace	Job Manager slow DownloadLinkGeneration	Assigning Bucket[1] to Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1]
[INFO] [2023-01-06 20:43:34]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Importing Dictionaries
[DEBUG] [2023-01-06 20:43:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Received new WorkerInformation(size = 1,dataset = DownloadLinkGeneration)
[DEBUG] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Received new WorkerInformation(size = 1,dataset = DownloadLinkGeneration)
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Remapping Dictionaries []
[WARN] [2023-01-06 20:43:35]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DownloadLinkGeneration	Max cannot be decreased.
[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Received Dictionary[DownloadLinkGeneration.test_table#DownloadLinkGeneration$2etest_table$2etest_column] of size 2.
127.0.0.1 - - [06/Jan/2023:20:43:35 +0000] "POST /admin/datasets/DownloadLinkGeneration/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_DownloadLinkGeneration%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 179
[DEBUG] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Received Dictionary[DownloadLinkGeneration.test_table#DownloadLinkGeneration$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Start sending 2 Buckets
[INFO] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.AddImport	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Received Import[DownloadLinkGeneration.test_table.test_table], containing 4 entries.
[INFO] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.AddImport	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Received Import[DownloadLinkGeneration.test_table.test_table], containing 4 entries.
[INFO] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.ImportBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Received DownloadLinkGeneration.test_table.test_table.1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1, /127.0.0.1:50700]	Adding Bucket[DownloadLinkGeneration.test_table.test_table.1]
[WARN] [2023-01-06 20:43:35]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DownloadLinkGeneration	One or more Children are not done yet
[INFO] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.ImportBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Received DownloadLinkGeneration.test_table.test_table.0
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160, /127.0.0.1:50698]	Adding Bucket[DownloadLinkGeneration.test_table.test_table.0]
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.WorkerStorage		Adding CBlock[DownloadLinkGeneration.test_table.test_table.1.DownloadLinkGeneration.test_tree.test_column]
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.WorkerStorage		Adding CBlock[DownloadLinkGeneration.test_table.test_table.0.DownloadLinkGeneration.test_tree.test_column]
[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:43:35 +0000] "GET /api/datasets/DownloadLinkGeneration/queries/DownloadLinkGeneration.ff628e34-4539-4546-8d33-1f0bf5f5b8af HTTP/1.1" 200 752 "-" "Conquery (test client)" 59
127.0.0.1 - - [06/Jan/2023:20:43:35 +0000] "GET /api/datasets/DownloadLinkGeneration/queries/DownloadLinkGeneration.ff628e34-4539-4546-8d33-1f0bf5f5b8af HTTP/1.1" 200 753 "-" "Conquery (test client)" 7
127.0.0.1 - - [06/Jan/2023:20:43:35 +0000] "GET /api/datasets/DownloadLinkGeneration/queries/DownloadLinkGeneration.ff628e34-4539-4546-8d33-1f0bf5f5b8af HTTP/1.1" 200 1017 "-" "Conquery (test client)" 7
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DownloadLinkGeneration
[INFO] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[INFO] [2023-01-06 20:43:35]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[INFO] [2023-01-06 20:43:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DownloadLinkGeneration
[INFO] [2023-01-06 20:43:35]	c.b.c.m.w.Namespace		Removing namespace storage of DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[INFO] [2023-01-06 20:43:35]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[INFO] [2023-01-06 20:43:35]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DownloadLinkGeneration_2a5f7fee-e25d-4bc9-8f69-520b3bfd3bc1
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[DEBUG] [2023-01-06 20:43:35]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[INFO] [2023-01-06 20:43:35]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DownloadLinkGeneration_6b2938ce-fc56-436c-aff9-2fdea076d160
[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	DownloadLinkGeneration	SUCCESS integration test DownloadLinkGeneration
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	AdminEndpointTest	STARTING integration test AdminEndpointTest
[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	AdminEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	AdminEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery	AdminEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	AdminEndpointTest	SUCCESS integration test AdminEndpointTest
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	AdminUIEndpointTest	STARTING integration test AdminUIEndpointTest
[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	AdminUIEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	AdminUIEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery	AdminUIEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	AdminUIEndpointTest	SUCCESS integration test AdminUIEndpointTest
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	ApiEndpointTest	STARTING integration test ApiEndpointTest
[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:35]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery	ApiEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:35]	c.b.c.i.IntegrationTest$Wrapper	ApiEndpointTest	SUCCESS integration test ApiEndpointTest
[INFO] [2023-01-06 20:43:35]	c.b.c.u.s.TestConquery	ApiEndpointTest	Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
INFO  [2023-01-06 20:43:35,795] io.dropwizard.server.DefaultServerFactory: Registering jersey handler with root path prefix: /
INFO  [2023-01-06 20:43:35,795] io.dropwizard.server.DefaultServerFactory: Registering admin handler with root path prefix: /
WARN  [2023-01-06 20:43:35,800] com.bakdata.conquery.models.config.XodusStoreFactory: Had to create Storage Dir at `/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager`
INFO  [2023-01-06 20:43:35,800] com.bakdata.conquery.models.config.XodusStoreFactory: All WorkerStores loaded: []
INFO  [2023-01-06 20:43:35,800] com.bakdata.conquery.commands.ManagerNode: Opening MetaStorage
INFO  [2023-01-06 20:43:35,821] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/users
INFO  [2023-01-06 20:43:35,844] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/roles
INFO  [2023-01-06 20:43:35,863] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/groups
INFO  [2023-01-06 20:43:35,887] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/executions
INFO  [2023-01-06 20:43:35,911] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/formConfigs
INFO  [2023-01-06 20:43:35,916] com.bakdata.conquery.commands.ManagerNode: Loading MetaStorage
INFO  [2023-01-06 20:43:35,916] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/users:AUTH_USER}): 0 entries, 0 B within 346.9 μs
INFO  [2023-01-06 20:43:35,917] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 273.5 μs
INFO  [2023-01-06 20:43:35,918] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 245.9 μs
INFO  [2023-01-06 20:43:35,918] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/executions:EXECUTIONS}): 0 entries, 0 B within 243.6 μs
INFO  [2023-01-06 20:43:35,919] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 235.8 μs
INFO  [2023-01-06 20:43:35,919] com.bakdata.conquery.commands.ManagerNode: MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@5553a7ac
WARN  [2023-01-06 20:43:35,924] com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm: 
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
INFO  [2023-01-06 20:43:35,928] com.bakdata.conquery.models.auth.AuthorizationController: Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_11
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_12
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_13
	com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm_14
INFO  [2023-01-06 20:43:35,928] com.bakdata.conquery.commands.ManagerNode: Registering ResourcesProvider
INFO  [2023-01-06 20:43:35,935] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
INFO  [2023-01-06 20:43:35,935] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
WARN  [2023-01-06 20:43:35,955] com.bakdata.conquery.models.config.XodusStoreFactory: Had to create Storage Dir at `/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1`
INFO  [2023-01-06 20:43:35,956] com.bakdata.conquery.models.config.XodusStoreFactory: All WorkerStores loaded: []
INFO  [2023-01-06 20:43:35,956] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
WARN  [2023-01-06 20:43:35,956] com.bakdata.conquery.models.config.XodusStoreFactory: Had to create Storage Dir at `/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0`
INFO  [2023-01-06 20:43:35,956] com.bakdata.conquery.models.config.XodusStoreFactory: All WorkerStores loaded: []
INFO  [2023-01-06 20:43:35,956] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-06 20:43:35,957] io.dropwizard.server.ServerFactory: Starting Conquery


 ▄████████  ▄██████▄  ███▄▄▄▄   ████████▄   ███    █▄     ▄████████    ▄████████ ▄██   ▄   
███    ███ ███    ███ ███▀▀▀██▄ ███    ███  ███    ███   ███    ███   ███    ███ ███   ██▄ 
███    █▀  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▀    ███    ███ ███▄▄▄███ 
███        ███    ███ ███   ███ ███    ███  ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀ ▀▀▀▀▀▀███ 
███        ███    ███ ███   ███ ███    ███  ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀   ▄██   ███ 
███    █▄  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▄  ▀███████████ ███   ███ 
███    ███ ███    ███ ███   ███ ███  ▀ ███  ███    ███   ███    ███   ███    ███ ███   ███ 
████████▀   ▀██████▀   ▀█   █▀   ▀██████▀▄█ ████████▀    ██████████   ███    ███  ▀█████▀  
                                                                      ███    ███           
Version: 0.0.0-SNAPSHOT(${git.commit.id.describe})

           
INFO  [2023-01-06 20:43:35,963] org.eclipse.jetty.setuid.SetUIDListener: Opened application@74367715{HTTP/1.1, (http/1.1)}{0.0.0.0:42569}
INFO  [2023-01-06 20:43:35,963] org.eclipse.jetty.setuid.SetUIDListener: Opened admin@710ee5b1{HTTP/1.1, (http/1.1)}{0.0.0.0:39367}
INFO  [2023-01-06 20:43:35,963] org.eclipse.jetty.server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.17+8-post-Ubuntu-1ubuntu218.04
INFO  [2023-01-06 20:43:35,967] com.bakdata.conquery.commands.ManagerNode: Started ManagerNode @ /0:0:0:0:0:0:0:0:38151
INFO  [2023-01-06 20:43:35,987] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/api-token
INFO  [2023-01-06 20:43:36,012] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:38151
INFO  [2023-01-06 20:43:36,013] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:35454 connected, waiting for identity
INFO  [2023-01-06 20:43:36,013] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:38151
INFO  [2023-01-06 20:43:36,015] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:38151
INFO  [2023-01-06 20:43:36,017] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:35456 connected, waiting for identity
INFO  [2023-01-06 20:43:36,017] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:38151
INFO  [2023-01-06 20:43:36,024] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:35454` registered.
INFO  [2023-01-06 20:43:36,024] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:35456` registered.
INFO  [2023-01-06 20:43:36,122] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /api/config/frontend (com.bakdata.conquery.resources.api.ConfigResource)
    GET     /api/datasets (com.bakdata.conquery.resources.api.DatasetsResource)
    GET     /api/datasets/{dataset}/concepts (com.bakdata.conquery.resources.api.DatasetResource)
    GET     /api/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/resolve (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/autocomplete (com.bakdata.conquery.resources.api.FilterResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/resolve (com.bakdata.conquery.resources.api.FilterResource)
    GET     /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    POST    /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    DELETE  /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    PATCH   /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-queries (com.bakdata.conquery.resources.api.FormResource)
    GET     /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/upload (com.bakdata.conquery.resources.api.QueryResource)
    DELETE  /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    PATCH   /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/cancel (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/reexecute (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/result/{query}.arrf (com.bakdata.conquery.resources.api.ResultArrowFileResource)
    GET     /api/datasets/{dataset}/result/{query}.arrs (com.bakdata.conquery.resources.api.ResultArrowStreamResource)
    GET     /api/datasets/{dataset}/result/{query}.csv (com.bakdata.conquery.resources.api.ResultCsvResource)
    GET     /api/datasets/{dataset}/result/{query}.xlsx (com.bakdata.conquery.resources.api.ResultExcelResource)
    GET     /api/me (com.bakdata.conquery.resources.api.MeResource)
    GET     /api/token (com.bakdata.conquery.resources.api.ApiTokenResource)
    POST    /api/token (com.bakdata.conquery.resources.api.ApiTokenResource)
    DELETE  /api/token/{token} (com.bakdata.conquery.resources.api.ApiTokenResource)

WARN  [2023-01-06 20:43:36,122] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

INFO  [2023-01-06 20:43:36,157] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-06 20:43:36,158] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@6d13d52e{/,null,AVAILABLE}
INFO  [2023-01-06 20:43:36,158] io.dropwizard.setup.AdminEnvironment: tasks = 

    POST    /tasks/permission-cleanup (com.bakdata.conquery.tasks.PermissionCleanupTask)
    POST    /tasks/report-consistency (com.bakdata.conquery.tasks.ReportConsistencyTask)
    POST    /tasks/log-level (io.dropwizard.servlets.tasks.LogConfigurationTask)
    POST    /tasks/form-scanner (com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner)
    POST    /tasks/clear-filter-source-search (com.bakdata.conquery.tasks.ClearFilterSourceSearch)
    POST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)
    POST    /tasks/query-cleanup (com.bakdata.conquery.tasks.QueryCleanupTask)
    POST    /tasks/shutdown (com.bakdata.conquery.resources.admin.ShutdownTask)

WARN  [2023-01-06 20:43:36,158] io.dropwizard.setup.AdminEnvironment: 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
INFO  [2023-01-06 20:43:36,245] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin/auth-overview/csv (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/auth-overview/csv/group/{groupId} (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    POST    /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    DELETE  /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    GET     /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    POST    /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/label (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/secondaryId (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/secondaryId/{secondaryId} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/structure (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    DELETE  /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    POST    /admin/datasets/{dataset}/update-matching-stats (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/weight (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/jobs (com.bakdata.conquery.resources.admin.rest.AdminResource)
    POST    /admin/jobs/{jobId}/cancel (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/logout (com.bakdata.conquery.resources.admin.rest.AdminResource)
    DELETE  /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    POST    /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    GET     /admin/queries (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    DELETE  /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    GET     /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/script (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/upload (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    GET     /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)

WARN  [2023-01-06 20:43:36,245] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

INFO  [2023-01-06 20:43:36,284] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin-ui/ (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/auth-overview (com.bakdata.conquery.resources.admin.ui.AuthOverviewUIResource)
    GET     /admin-ui/datasets (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset} (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.ui.ConceptsUIResource)
    GET     /admin-ui/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table}/import/{importId} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/groups (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/groups/{groupId} (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/jobs (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/queries (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/roles (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/roles/{roleId} (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/script (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/users (com.bakdata.conquery.resources.admin.ui.UserUIResource)
    GET     /admin-ui/users/{userId} (com.bakdata.conquery.resources.admin.ui.UserUIResource)

WARN  [2023-01-06 20:43:36,285] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

INFO  [2023-01-06 20:43:36,335] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-06 20:43:36,335] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@dd29815{/,null,AVAILABLE}
INFO  [2023-01-06 20:43:36,349] org.eclipse.jetty.server.AbstractConnector: Started application@74367715{HTTP/1.1, (http/1.1)}{0.0.0.0:42569}
INFO  [2023-01-06 20:43:36,351] org.eclipse.jetty.server.AbstractConnector: Started admin@710ee5b1{HTTP/1.1, (http/1.1)}{0.0.0.0:39367}
INFO  [2023-01-06 20:43:36,351] org.eclipse.jetty.server.Server: Started @20876ms
INFO  [2023-01-06 20:43:36,359] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ApiTokenRealmTest
INFO  [2023-01-06 20:43:36,361] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:36,382] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest
INFO  [2023-01-06 20:43:36,407] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:DATASET}): 0 entries, 0 B within 157.5 μs
INFO  [2023-01-06 20:43:36,407] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:SECONDARY_IDS}): 0 entries, 0 B within 165.1 μs
INFO  [2023-01-06 20:43:36,407] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:TABLES}): 0 entries, 0 B within 113.9 μs
INFO  [2023-01-06 20:43:36,408] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 114.8 μs
INFO  [2023-01-06 20:43:36,408] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:IMPORTS}): 0 entries, 0 B within 126.1 μs
INFO  [2023-01-06 20:43:36,409] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:CONCEPTS}): 0 entries, 0 B within 403.9 μs
INFO  [2023-01-06 20:43:36,409] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:36,409] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 148.9 μs
INFO  [2023-01-06 20:43:36,409] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:STRUCTURE}): 0 entries, 0 B within 102.9 μs
INFO  [2023-01-06 20:43:36,409] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 105.5 μs
INFO  [2023-01-06 20:43:36,410] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 102.6 μs
INFO  [2023-01-06 20:43:36,427] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-06 20:43:36,427] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-06 20:43:36,455] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45
INFO  [2023-01-06 20:43:36,460] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2
INFO  [2023-01-06 20:43:36,511] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:DATASET}): 0 entries, 0 B within 280.1 μs
INFO  [2023-01-06 20:43:36,512] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:SECONDARY_IDS}): 0 entries, 0 B within 172.7 μs
INFO  [2023-01-06 20:43:36,512] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:TABLES}): 0 entries, 0 B within 132.2 μs
INFO  [2023-01-06 20:43:36,512] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 135.6 μs
INFO  [2023-01-06 20:43:36,513] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:IMPORTS}): 0 entries, 0 B within 113.9 μs
INFO  [2023-01-06 20:43:36,513] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:CONCEPTS}): 0 entries, 0 B within 217.0 μs
INFO  [2023-01-06 20:43:36,513] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:36,513] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:WORKER}): 0 entries, 0 B within 126.9 μs
INFO  [2023-01-06 20:43:36,513] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:BUCKETS}): 0 entries, 0 B within 128.5 μs
INFO  [2023-01-06 20:43:36,514] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:DATASET}): 0 entries, 0 B within 107.9 μs
INFO  [2023-01-06 20:43:36,514] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45:C_BLOCKS}): 0 entries, 0 B within 216.2 μs
INFO  [2023-01-06 20:43:36,514] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:SECONDARY_IDS}): 0 entries, 0 B within 237.0 μs
INFO  [2023-01-06 20:43:36,514] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:TABLES}): 0 entries, 0 B within 114.5 μs
INFO  [2023-01-06 20:43:36,515] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 131.1 μs
INFO  [2023-01-06 20:43:36,515] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:IMPORTS}): 0 entries, 0 B within 114.7 μs
INFO  [2023-01-06 20:43:36,515] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:CONCEPTS}): 0 entries, 0 B within 109.5 μs
INFO  [2023-01-06 20:43:36,515] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:36,515] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:WORKER}): 0 entries, 0 B within 106.9 μs
INFO  [2023-01-06 20:43:36,515] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:BUCKETS}): 0 entries, 0 B within 74.60 μs
INFO  [2023-01-06 20:43:36,516] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2:C_BLOCKS}): 0 entries, 0 B within 65.71 μs
INFO  [2023-01-06 20:43:36,531] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:36,536] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:36,536] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:36,536] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:36,536] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:36,536] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:36,536] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:36,712] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Creating new api token data for user user.SUPERUSER@SUPERUSER with id: 5eed8543-150a-41b7-a6f8-31e07783be4e
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "POST /api/token HTTP/1.1" 200 96 "-" "Conquery (test client)" 65
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "GET /api/token HTTP/1.1" 200 178 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:43:36,795] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Creating new api token data for user user.SUPERUSER@SUPERUSER with id: 6f7b87ea-66d8-4235-a90d-ccbd2d017151
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "POST /api/token HTTP/1.1" 200 96 "-" "Conquery (test client)" 34
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "GET /api/token HTTP/1.1" 200 353 "-" "Conquery (test client)" 3
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "GET /api/datasets HTTP/1.1" 200 56 "-" "Conquery (test client)" 29
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "GET /api/datasets HTTP/1.1" 200 2 "-" "Conquery (test client)" 42
127.0.0.1 - - [06/Jan/2023:20:43:36 +0000] "GET /admin/datasets HTTP/1.1" 200 21 "-" "Conquery (test client)" 49
127.0.0.1 - - [06/Jan/2023:20:43:37 +0000] "DELETE /api/token/6f7b87ea-66d8-4235-a90d-ccbd2d017151 HTTP/1.1" 403 63 "-" "Conquery (test client)" 48
127.0.0.1 - - [06/Jan/2023:20:43:37 +0000] "DELETE /api/token/6f7b87ea-66d8-4235-a90d-ccbd2d017151 HTTP/1.1" 200 0 "-" "Conquery (test client)" 3
WARN  [2023-01-06 20:43:37,177] com.bakdata.conquery.models.auth.web.DefaultAuthFilter: Non of the configured realms was able to successfully authenticate the extracted token(s).
127.0.0.1 - - [06/Jan/2023:20:43:37 +0000] "GET /admin/datasets HTTP/1.1" 401 46 "-" "Conquery (test client)" 42
WARN  [2023-01-06 20:43:37,224] com.bakdata.conquery.models.auth.web.AuthorizationExceptionMapper: Shiro failed to authorize the request. Reason: Subject does not have permission [admin]
127.0.0.1 - - [06/Jan/2023:20:43:37 +0000] "DELETE /api/token/5eed8543-150a-41b7-a6f8-31e07783be4e HTTP/1.1" 403 86 "-" "Conquery (test client)" 6
127.0.0.1 - - [06/Jan/2023:20:43:37 +0000] "POST /api/token HTTP/1.1" 422 27 "-" "Conquery (test client)" 103
INFO  [2023-01-06 20:43:37,399] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Creating new api token data for user user.TestUser2 with id: de576cb1-3297-44c9-a903-62c8b81d3f9a
INFO  [2023-01-06 20:43:37,477] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Supplied token expired on: 2023-01-05
WARN  [2023-01-06 20:43:37,477] com.bakdata.conquery.models.auth.web.DefaultAuthFilter: Non of the configured realms was able to successfully authenticate the extracted token(s).
127.0.0.1 - - [06/Jan/2023:20:43:37 +0000] "GET /api/datasets HTTP/1.1" 401 46 "-" "Conquery (test client)" 43
INFO  [2023-01-06 20:43:37,483] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ApiTokenRealmTest
INFO  [2023-01-06 20:43:37,484] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-06 20:43:37,484] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-06 20:43:37,484] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2
INFO  [2023-01-06 20:43:37,484] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45
INFO  [2023-01-06 20:43:37,536] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2
INFO  [2023-01-06 20:43:37,536] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45
INFO  [2023-01-06 20:43:37,536] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ApiTokenRealmTest
INFO  [2023-01-06 20:43:37,541] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_3f1d7ca9-94fa-4da9-ad95-5a2b0cf0dd45
INFO  [2023-01-06 20:43:37,637] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ApiTokenRealmTest
INFO  [2023-01-06 20:43:37,640] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_e8203194-dfa6-4172-aca9-fdaf5be737b2
INFO  [2023-01-06 20:43:37,641] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest
INFO  [2023-01-06 20:43:37,654] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:37,757] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ApiTokenRealmTest
INFO  [2023-01-06 20:43:37,764] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ConceptPermissionTest
INFO  [2023-01-06 20:43:37,766] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:37,786] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest
INFO  [2023-01-06 20:43:37,817] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:DATASET}): 0 entries, 0 B within 215.3 μs
INFO  [2023-01-06 20:43:37,817] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:SECONDARY_IDS}): 0 entries, 0 B within 164.6 μs
INFO  [2023-01-06 20:43:37,817] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:TABLES}): 0 entries, 0 B within 162.0 μs
INFO  [2023-01-06 20:43:37,818] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 190.9 μs
INFO  [2023-01-06 20:43:37,818] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:IMPORTS}): 0 entries, 0 B within 137.6 μs
INFO  [2023-01-06 20:43:37,818] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:CONCEPTS}): 0 entries, 0 B within 123.9 μs
INFO  [2023-01-06 20:43:37,818] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:37,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 128.2 μs
INFO  [2023-01-06 20:43:37,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:STRUCTURE}): 0 entries, 0 B within 120.5 μs
INFO  [2023-01-06 20:43:37,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 121.9 μs
INFO  [2023-01-06 20:43:37,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 121.1 μs
INFO  [2023-01-06 20:43:37,828] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-06 20:43:37,828] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-06 20:43:37,848] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3
INFO  [2023-01-06 20:43:37,848] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0
INFO  [2023-01-06 20:43:37,882] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:DATASET}): 0 entries, 0 B within 149.9 μs
INFO  [2023-01-06 20:43:37,882] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:SECONDARY_IDS}): 0 entries, 0 B within 79.45 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:TABLES}): 0 entries, 0 B within 79.90 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 75.07 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:DATASET}): 0 entries, 0 B within 193.2 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:IMPORTS}): 0 entries, 0 B within 70.94 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:CONCEPTS}): 0 entries, 0 B within 80.31 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:SECONDARY_IDS}): 0 entries, 0 B within 88.81 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:TABLES}): 0 entries, 0 B within 81.62 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:WORKER}): 0 entries, 0 B within 133.7 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 76.57 μs
INFO  [2023-01-06 20:43:37,883] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:BUCKETS}): 0 entries, 0 B within 71.33 μs
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:IMPORTS}): 0 entries, 0 B within 69.66 μs
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0:C_BLOCKS}): 0 entries, 0 B within 66.93 μs
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:CONCEPTS}): 0 entries, 0 B within 69.04 μs
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:WORKER}): 0 entries, 0 B within 92.79 μs
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:BUCKETS}): 0 entries, 0 B within 73.93 μs
INFO  [2023-01-06 20:43:37,884] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3:C_BLOCKS}): 0 entries, 0 B within 83.86 μs
INFO  [2023-01-06 20:43:37,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptPermissionTest.worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:37,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptPermissionTest.worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:37,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:37,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptPermissionTest.worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:37,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptPermissionTest.worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:37,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:37,891] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:37,992] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:38,003] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:38,004] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptPermissionTest.test_table
INFO  [2023-01-06 20:43:38,004] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptPermissionTest.test_table
INFO  [2023-01-06 20:43:38,140] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:38,257] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:38,257] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:38,257] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-06 20:43:38,258] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptPermissionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000326353sINFO  [2023-01-06 20:43:38,291] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:38,291] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:38,291] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@66562a9)
INFO  [2023-01-06 20:43:38,295] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptPermissionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:38,296] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:38,296] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptPermissionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:38,327] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into ConceptPermissionTest.test_table
127.0.0.1 - - [06/Jan/2023:20:43:38 +0000] "POST /admin/datasets/ConceptPermissionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ConceptPermissionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:43:38,328] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:38,333] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:38,347] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:38,347] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:38,404] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptPermissionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:38,404] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptPermissionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:38,406] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-06 20:43:38,410] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:38,410] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptPermissionTest.test_table.test_table.0
INFO  [2023-01-06 20:43:38,411] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptPermissionTest.test_table.test_table.1
INFO  [2023-01-06 20:43:38,559] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ConceptPermissionTest] by User[{user.testUser].
WARN  [2023-01-06 20:43:38,565] com.bakdata.conquery.models.auth.web.AuthorizationExceptionMapper: Shiro failed to authorize the request. Reason: Subject does not have permission [concepts:read:conceptpermissiontest.test_tree]
INFO  [2023-01-06 20:43:38,567] com.bakdata.conquery.integration.tests.ConceptPermissionTest: Adding the Permission[concepts:read:conceptpermissiontest.test_tree] to User[User[user.testUser]]
127.0.0.1 - - [06/Jan/2023:20:43:38 +0000] "POST /api/datasets/ConceptPermissionTest/queries HTTP/1.1" 403 126 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:43:38,586] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ConceptPermissionTest] by User[{user.testUser].
INFO  [2023-01-06 20:43:38,590] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[67e9c9d7-9d9c-4eee-a2ca-15822459de1b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptPermissionTest))]]
127.0.0.1 - - [06/Jan/2023:20:43:38 +0000] "POST /api/datasets/ConceptPermissionTest/queries HTTP/1.1" 201 1119 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:43:38,606] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b
INFO  [2023-01-06 20:43:38,611] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b
INFO  [2023-01-06 20:43:38,630] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b] with 0 results within PT0.019295S
INFO  [2023-01-06 20:43:38,648] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b] with 2 results within PT0.042089S
INFO  [2023-01-06 20:43:38,670] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b, workerId=ConceptPermissionTest.worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0, startTime=2023-01-06T20:43:38.606850, finishTime=2023-01-06T20:43:38.648939) of size 2
INFO  [2023-01-06 20:43:38,671] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b, workerId=ConceptPermissionTest.worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3, startTime=2023-01-06T20:43:38.611286, finishTime=2023-01-06T20:43:38.630581) of size 0
INFO  [2023-01-06 20:43:38,673] com.bakdata.conquery.models.execution.ManagedExecution: DONE 67e9c9d7-9d9c-4eee-a2ca-15822459de1b ManagedQuery within PT0.081798S
127.0.0.1 - - [06/Jan/2023:20:43:38 +0000] "GET /api/datasets/ConceptPermissionTest/queries/ConceptPermissionTest.67e9c9d7-9d9c-4eee-a2ca-15822459de1b HTTP/1.1" 200 1135 "-" "Conquery (test client)" 53
INFO  [2023-01-06 20:43:38,677] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:38,779] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.testUser
INFO  [2023-01-06 20:43:38,780] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ConceptPermissionTest
INFO  [2023-01-06 20:43:38,781] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-06 20:43:38,781] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-06 20:43:38,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0
INFO  [2023-01-06 20:43:38,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3
INFO  [2023-01-06 20:43:38,803] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0
INFO  [2023-01-06 20:43:38,803] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3
INFO  [2023-01-06 20:43:38,817] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptPermissionTest_bfac5d8d-c18a-4043-b4b2-052cbb3fedc0
INFO  [2023-01-06 20:43:38,820] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptPermissionTest_1359704e-3d82-43a0-a584-0607a4541cc3
INFO  [2023-01-06 20:43:38,849] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ConceptPermissionTest
INFO  [2023-01-06 20:43:38,910] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ConceptPermissionTest
INFO  [2023-01-06 20:43:38,913] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptPermissionTest
INFO  [2023-01-06 20:43:38,923] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:38,934] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ConceptPermissionTest
INFO  [2023-01-06 20:43:38,939] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ConceptResolutionTest
INFO  [2023-01-06 20:43:38,940] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:38,959] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest
INFO  [2023-01-06 20:43:38,979] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:DATASET}): 0 entries, 0 B within 218.7 μs
INFO  [2023-01-06 20:43:38,980] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:SECONDARY_IDS}): 0 entries, 0 B within 128.9 μs
INFO  [2023-01-06 20:43:38,980] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:TABLES}): 0 entries, 0 B within 100.3 μs
INFO  [2023-01-06 20:43:38,980] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 163.7 μs
INFO  [2023-01-06 20:43:38,980] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:IMPORTS}): 0 entries, 0 B within 103.7 μs
INFO  [2023-01-06 20:43:38,980] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:CONCEPTS}): 0 entries, 0 B within 116.2 μs
INFO  [2023-01-06 20:43:38,980] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:38,981] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 98.73 μs
INFO  [2023-01-06 20:43:38,981] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:STRUCTURE}): 0 entries, 0 B within 95.38 μs
INFO  [2023-01-06 20:43:38,981] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 93.07 μs
INFO  [2023-01-06 20:43:38,981] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 103.9 μs
INFO  [2023-01-06 20:43:38,989] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-06 20:43:38,989] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-06 20:43:39,018] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c
INFO  [2023-01-06 20:43:39,022] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308
INFO  [2023-01-06 20:43:39,050] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:DATASET}): 0 entries, 0 B within 125.8 μs
INFO  [2023-01-06 20:43:39,050] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:DATASET}): 0 entries, 0 B within 97.42 μs
INFO  [2023-01-06 20:43:39,050] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:SECONDARY_IDS}): 0 entries, 0 B within 60.07 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:SECONDARY_IDS}): 0 entries, 0 B within 64.41 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:TABLES}): 0 entries, 0 B within 55.13 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:TABLES}): 0 entries, 0 B within 93.06 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 67.22 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 55.80 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:IMPORTS}): 0 entries, 0 B within 52.53 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:IMPORTS}): 0 entries, 0 B within 50.19 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:CONCEPTS}): 0 entries, 0 B within 57.14 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:CONCEPTS}): 0 entries, 0 B within 50.16 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:WORKER}): 0 entries, 0 B within 66.13 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:WORKER}): 0 entries, 0 B within 83.75 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:BUCKETS}): 0 entries, 0 B within 215.0 μs
INFO  [2023-01-06 20:43:39,051] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:BUCKETS}): 0 entries, 0 B within 216.7 μs
INFO  [2023-01-06 20:43:39,052] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c:C_BLOCKS}): 0 entries, 0 B within 61.24 μs
INFO  [2023-01-06 20:43:39,052] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308:C_BLOCKS}): 0 entries, 0 B within 84.54 μs
INFO  [2023-01-06 20:43:39,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptResolutionTest.worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:39,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptResolutionTest.worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:39,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:39,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptResolutionTest.worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:39,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptResolutionTest.worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:39,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:39,057] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,158] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,167] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,168] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptResolutionTest.test_table
INFO  [2023-01-06 20:43:39,168] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptResolutionTest.test_table
INFO  [2023-01-06 20:43:39,295] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,416] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:39,416] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:39,416] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-06 20:43:39,417] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptResolutionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000294312sINFO  [2023-01-06 20:43:39,447] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:39,447] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:39,447] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@15a61caa)
INFO  [2023-01-06 20:43:39,451] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptResolutionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:39,451] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:39,451] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptResolutionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:39,482] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into ConceptResolutionTest.test_table
127.0.0.1 - - [06/Jan/2023:20:43:39 +0000] "POST /admin/datasets/ConceptResolutionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ConceptResolutionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:43:39,484] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,488] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:39,503] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:39,503] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:39,542] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptResolutionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:39,542] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptResolutionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:39,542] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-06 20:43:39,546] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:39,546] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptResolutionTest.test_table.test_table.0
INFO  [2023-01-06 20:43:39,546] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptResolutionTest.test_table.test_table.1
INFO  [2023-01-06 20:43:39,654] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,659] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,679] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,686] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ConceptResolutionTest
INFO  [2023-01-06 20:43:39,687] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-06 20:43:39,687] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-06 20:43:39,687] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308
INFO  [2023-01-06 20:43:39,687] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c
INFO  [2023-01-06 20:43:39,698] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ConceptResolutionTest
INFO  [2023-01-06 20:43:39,746] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ConceptResolutionTest
INFO  [2023-01-06 20:43:39,749] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptResolutionTest
INFO  [2023-01-06 20:43:39,753] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c
INFO  [2023-01-06 20:43:39,753] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308
INFO  [2023-01-06 20:43:39,760] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:39,854] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptResolutionTest_58f86736-a43d-4f8e-bf23-71ed87374308
INFO  [2023-01-06 20:43:39,855] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptResolutionTest_32f782ea-3bf8-43e1-b927-1295fc07201c
INFO  [2023-01-06 20:43:39,971] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ConceptResolutionTest
INFO  [2023-01-06 20:43:39,976] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FilterResolutionContainsTest
INFO  [2023-01-06 20:43:39,978] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:40,000] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest
INFO  [2023-01-06 20:43:40,012] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:DATASET}): 0 entries, 0 B within 174.0 μs
INFO  [2023-01-06 20:43:40,012] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:SECONDARY_IDS}): 0 entries, 0 B within 112.0 μs
INFO  [2023-01-06 20:43:40,012] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:TABLES}): 0 entries, 0 B within 95.35 μs
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 80.85 μs
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:IMPORTS}): 0 entries, 0 B within 78.78 μs
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:CONCEPTS}): 0 entries, 0 B within 92.53 μs
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 89.45 μs
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:STRUCTURE}): 0 entries, 0 B within 66.36 μs
INFO  [2023-01-06 20:43:40,013] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 70.45 μs
INFO  [2023-01-06 20:43:40,014] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 65.84 μs
INFO  [2023-01-06 20:43:40,021] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-06 20:43:40,021] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-06 20:43:40,057] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e
INFO  [2023-01-06 20:43:40,058] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:DATASET}): 0 entries, 0 B within 114.4 μs
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:SECONDARY_IDS}): 0 entries, 0 B within 63.03 μs
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:TABLES}): 0 entries, 0 B within 66.89 μs
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 71.61 μs
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:IMPORTS}): 0 entries, 0 B within 49.26 μs
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:CONCEPTS}): 0 entries, 0 B within 44.83 μs
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:40,079] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:WORKER}): 0 entries, 0 B within 46.49 μs
INFO  [2023-01-06 20:43:40,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:BUCKETS}): 0 entries, 0 B within 44.38 μs
INFO  [2023-01-06 20:43:40,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e:C_BLOCKS}): 0 entries, 0 B within 43.51 μs
INFO  [2023-01-06 20:43:40,085] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:40,085] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:40,085] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:DATASET}): 0 entries, 0 B within 105.4 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:SECONDARY_IDS}): 0 entries, 0 B within 62.79 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:TABLES}): 0 entries, 0 B within 46.37 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 50.94 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:IMPORTS}): 0 entries, 0 B within 36.99 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:CONCEPTS}): 0 entries, 0 B within 35.65 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:WORKER}): 0 entries, 0 B within 38.58 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:BUCKETS}): 0 entries, 0 B within 37.52 μs
INFO  [2023-01-06 20:43:40,088] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3:C_BLOCKS}): 0 entries, 0 B within 43.98 μs
INFO  [2023-01-06 20:43:40,091] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,092] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:40,092] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:40,092] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:40,196] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,207] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,207] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionContainsTest.table1
INFO  [2023-01-06 20:43:40,208] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionContainsTest.table1
INFO  [2023-01-06 20:43:40,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,511] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:40,511] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:40,511] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:43:40,511] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionContainsTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00027982sINFO  [2023-01-06 20:43:40,540] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:40,540] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:40,540] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@ea4897e)
INFO  [2023-01-06 20:43:40,544] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionContainsTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:40,544] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:40,544] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionContainsTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:40,570] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into FilterResolutionContainsTest.table1
127.0.0.1 - - [06/Jan/2023:20:43:40 +0000] "POST /admin/datasets/FilterResolutionContainsTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_FilterResolutionContainsTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:43:40,573] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,576] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:40,594] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:40,594] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:40,601] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
WARN  [2023-01-06 20:43:40,605] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:40,605] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionContainsTest.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:43:40,608] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionContainsTest.table1.table1.0
INFO  [2023-01-06 20:43:40,610] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionContainsTest.table1.table1.2
INFO  [2023-01-06 20:43:40,635] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionContainsTest.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:43:40,639] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionContainsTest.table1.table1.1
INFO  [2023-01-06 20:43:40,747] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,752] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,779] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,788] com.bakdata.conquery.apiv1.FilterSearch: Processing reference list '/tmp/conquery_search7675736542380748751.csv' ...
INFO  [2023-01-06 20:43:40,810] com.bakdata.conquery.apiv1.FilterSearch: Processed reference list '/tmp/conquery_search7675736542380748751.csv' in 22 ms (5 Items in 6 Lines)
INFO  [2023-01-06 20:43:40,814] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FilterResolutionContainsTest
INFO  [2023-01-06 20:43:40,814] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-06 20:43:40,815] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e
INFO  [2023-01-06 20:43:40,815] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-06 20:43:40,815] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3
INFO  [2023-01-06 20:43:40,832] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FilterResolutionContainsTest
INFO  [2023-01-06 20:43:40,881] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e
INFO  [2023-01-06 20:43:40,905] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FilterResolutionContainsTest
INFO  [2023-01-06 20:43:40,906] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3
INFO  [2023-01-06 20:43:40,907] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionContainsTest
INFO  [2023-01-06 20:43:40,919] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:40,936] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionContainsTest_b61f0f4d-db2c-42d5-ab18-db6fdca237a3
INFO  [2023-01-06 20:43:40,945] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionContainsTest_f97f5ef5-2e28-4919-a38b-f0f22f84ab7e
INFO  [2023-01-06 20:43:41,057] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FilterResolutionContainsTest
INFO  [2023-01-06 20:43:41,061] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FilterResolutionExactTest
INFO  [2023-01-06 20:43:41,062] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:41,082] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest
INFO  [2023-01-06 20:43:41,100] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:DATASET}): 0 entries, 0 B within 178.6 μs
INFO  [2023-01-06 20:43:41,100] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:SECONDARY_IDS}): 0 entries, 0 B within 102.0 μs
INFO  [2023-01-06 20:43:41,101] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:TABLES}): 0 entries, 0 B within 83.96 μs
INFO  [2023-01-06 20:43:41,101] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 92.06 μs
INFO  [2023-01-06 20:43:41,101] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:IMPORTS}): 0 entries, 0 B within 77.38 μs
INFO  [2023-01-06 20:43:41,101] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:CONCEPTS}): 0 entries, 0 B within 88.75 μs
INFO  [2023-01-06 20:43:41,101] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:41,101] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 79.89 μs
INFO  [2023-01-06 20:43:41,102] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:STRUCTURE}): 0 entries, 0 B within 85.17 μs
INFO  [2023-01-06 20:43:41,102] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 89.88 μs
INFO  [2023-01-06 20:43:41,102] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 84.45 μs
INFO  [2023-01-06 20:43:41,110] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-06 20:43:41,110] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-06 20:43:41,135] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448
INFO  [2023-01-06 20:43:41,140] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980
INFO  [2023-01-06 20:43:41,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:DATASET}): 0 entries, 0 B within 150.2 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:SECONDARY_IDS}): 0 entries, 0 B within 59.74 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:TABLES}): 0 entries, 0 B within 47.74 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 46.16 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:IMPORTS}): 0 entries, 0 B within 43.81 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:CONCEPTS}): 0 entries, 0 B within 41.57 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:WORKER}): 0 entries, 0 B within 44.86 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:BUCKETS}): 0 entries, 0 B within 41.84 μs
INFO  [2023-01-06 20:43:41,165] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448:C_BLOCKS}): 0 entries, 0 B within 41.33 μs
INFO  [2023-01-06 20:43:41,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:DATASET}): 0 entries, 0 B within 141.1 μs
INFO  [2023-01-06 20:43:41,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:SECONDARY_IDS}): 0 entries, 0 B within 69.51 μs
INFO  [2023-01-06 20:43:41,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:TABLES}): 0 entries, 0 B within 64.33 μs
INFO  [2023-01-06 20:43:41,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 54.30 μs
INFO  [2023-01-06 20:43:41,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:IMPORTS}): 0 entries, 0 B within 59.16 μs
INFO  [2023-01-06 20:43:41,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:CONCEPTS}): 0 entries, 0 B within 48.10 μs
INFO  [2023-01-06 20:43:41,169] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:41,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:WORKER}): 0 entries, 0 B within 60.40 μs
INFO  [2023-01-06 20:43:41,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:BUCKETS}): 0 entries, 0 B within 54.70 μs
INFO  [2023-01-06 20:43:41,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:41,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:41,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:41,170] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980:C_BLOCKS}): 0 entries, 0 B within 63.33 μs
INFO  [2023-01-06 20:43:41,173] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:41,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:41,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:41,277] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,286] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,287] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionExactTest.table1
INFO  [2023-01-06 20:43:41,287] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionExactTest.table1
INFO  [2023-01-06 20:43:41,414] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,532] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:41,532] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:41,532] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:43:41,532] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionExactTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000262656sINFO  [2023-01-06 20:43:41,559] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:41,559] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:41,559] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4a03d11e)
INFO  [2023-01-06 20:43:41,563] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionExactTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:41,563] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:41,563] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionExactTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:41,591] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into FilterResolutionExactTest.table1
127.0.0.1 - - [06/Jan/2023:20:43:41 +0000] "POST /admin/datasets/FilterResolutionExactTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_FilterResolutionExactTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:43:41,593] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,595] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:41,608] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:41,608] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:41,611] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
WARN  [2023-01-06 20:43:41,613] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:41,619] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionExactTest.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:43:41,620] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionExactTest.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:43:41,622] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionExactTest.table1.table1.0
INFO  [2023-01-06 20:43:41,623] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionExactTest.table1.table1.1
INFO  [2023-01-06 20:43:41,624] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionExactTest.table1.table1.2
INFO  [2023-01-06 20:43:41,730] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,735] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,756] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,761] com.bakdata.conquery.apiv1.FilterSearch: Processing reference list '/tmp/conquery_search7937584124297395339csv' ...
INFO  [2023-01-06 20:43:41,787] com.bakdata.conquery.apiv1.FilterSearch: Processed reference list '/tmp/conquery_search7937584124297395339csv' in 26 ms (4 Items in 5 Lines)
INFO  [2023-01-06 20:43:41,789] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FilterResolutionExactTest
INFO  [2023-01-06 20:43:41,792] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-06 20:43:41,792] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-06 20:43:41,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448
INFO  [2023-01-06 20:43:41,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980
INFO  [2023-01-06 20:43:41,819] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FilterResolutionExactTest
INFO  [2023-01-06 20:43:41,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448
INFO  [2023-01-06 20:43:41,872] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980
INFO  [2023-01-06 20:43:41,914] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FilterResolutionExactTest
INFO  [2023-01-06 20:43:41,916] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionExactTest
INFO  [2023-01-06 20:43:41,924] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:41,929] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionExactTest_22286564-891f-4d8b-be7f-aa2ef268e980
INFO  [2023-01-06 20:43:41,929] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionExactTest_8861c406-d5b4-4d6e-b699-1f8842304448
INFO  [2023-01-06 20:43:42,046] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FilterResolutionExactTest
INFO  [2023-01-06 20:43:42,055] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:42,056] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:42,075] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:42,094] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:DATASET}): 0 entries, 0 B within 169.3 μs
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:SECONDARY_IDS}): 0 entries, 0 B within 94.35 μs
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:TABLES}): 0 entries, 0 B within 79.26 μs
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 82.08 μs
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:IMPORTS}): 0 entries, 0 B within 91.09 μs
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:CONCEPTS}): 0 entries, 0 B within 76.74 μs
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:42,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 76.83 μs
INFO  [2023-01-06 20:43:42,096] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:STRUCTURE}): 0 entries, 0 B within 72.32 μs
INFO  [2023-01-06 20:43:42,096] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 56.92 μs
INFO  [2023-01-06 20:43:42,096] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 49.70 μs
INFO  [2023-01-06 20:43:42,112] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-06 20:43:42,112] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-06 20:43:42,132] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df
INFO  [2023-01-06 20:43:42,136] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8
INFO  [2023-01-06 20:43:42,160] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:DATASET}): 0 entries, 0 B within 95.54 μs
INFO  [2023-01-06 20:43:42,160] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:SECONDARY_IDS}): 0 entries, 0 B within 51.70 μs
INFO  [2023-01-06 20:43:42,160] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:TABLES}): 0 entries, 0 B within 38.07 μs
INFO  [2023-01-06 20:43:42,160] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 42.19 μs
INFO  [2023-01-06 20:43:42,160] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:IMPORTS}): 0 entries, 0 B within 49.61 μs
INFO  [2023-01-06 20:43:42,161] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:CONCEPTS}): 0 entries, 0 B within 38.34 μs
INFO  [2023-01-06 20:43:42,161] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:42,161] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:WORKER}): 0 entries, 0 B within 37.00 μs
INFO  [2023-01-06 20:43:42,161] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:BUCKETS}): 0 entries, 0 B within 36.70 μs
INFO  [2023-01-06 20:43:42,161] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df:C_BLOCKS}): 0 entries, 0 B within 33.80 μs
INFO  [2023-01-06 20:43:42,162] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:DATASET}): 0 entries, 0 B within 111.8 μs
INFO  [2023-01-06 20:43:42,162] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:SECONDARY_IDS}): 0 entries, 0 B within 49.87 μs
INFO  [2023-01-06 20:43:42,162] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:TABLES}): 0 entries, 0 B within 52.47 μs
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.69 μs
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:IMPORTS}): 0 entries, 0 B within 49.02 μs
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:CONCEPTS}): 0 entries, 0 B within 41.82 μs
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:WORKER}): 0 entries, 0 B within 40.05 μs
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:BUCKETS}): 0 entries, 0 B within 48.30 μs
INFO  [2023-01-06 20:43:42,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8:C_BLOCKS}): 0 entries, 0 B within 56.10 μs
INFO  [2023-01-06 20:43:42,165] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:42,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:42,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:42,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:42,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:42,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:42,270] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,279] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,279] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionPrefixTest.table1
INFO  [2023-01-06 20:43:42,280] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionPrefixTest.table1
INFO  [2023-01-06 20:43:42,405] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,544] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:42,545] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:42,545] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:43:42,546] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionPrefixTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000318789sINFO  [2023-01-06 20:43:42,579] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:42,579] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:42,579] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@675e82dd)
INFO  [2023-01-06 20:43:42,586] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionPrefixTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:42,586] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:42,586] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_FilterResolutionPrefixTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:42,612] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into FilterResolutionPrefixTest.table1
127.0.0.1 - - [06/Jan/2023:20:43:42 +0000] "POST /admin/datasets/FilterResolutionPrefixTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_FilterResolutionPrefixTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:43:42,613] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,616] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:42,631] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:42,631] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:42,650] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionPrefixTest.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:43:42,650] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionPrefixTest.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:43:42,650] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
WARN  [2023-01-06 20:43:42,652] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:42,652] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionPrefixTest.table1.table1.0
INFO  [2023-01-06 20:43:42,653] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionPrefixTest.table1.table1.1
INFO  [2023-01-06 20:43:42,655] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionPrefixTest.table1.table1.2
INFO  [2023-01-06 20:43:42,762] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,767] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,787] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,793] com.bakdata.conquery.apiv1.FilterSearch: Processing reference list '/tmp/conquery_search16801298804650196810csv' ...
INFO  [2023-01-06 20:43:42,824] com.bakdata.conquery.apiv1.FilterSearch: Processed reference list '/tmp/conquery_search16801298804650196810csv' in 31 ms (4 Items in 5 Lines)
INFO  [2023-01-06 20:43:42,825] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:42,826] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-06 20:43:42,826] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-06 20:43:42,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df
INFO  [2023-01-06 20:43:42,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8
INFO  [2023-01-06 20:43:42,838] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:42,852] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:42,854] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:42,862] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df
INFO  [2023-01-06 20:43:42,864] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:42,864] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8
INFO  [2023-01-06 20:43:42,959] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_FilterResolutionPrefixTest_980fad45-95dc-481d-a48b-208ecb21d6df
INFO  [2023-01-06 20:43:42,960] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_FilterResolutionPrefixTest_47cf9e23-5fe6-4095-9807-cb68c339fff8
INFO  [2023-01-06 20:43:43,074] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FilterResolutionPrefixTest
INFO  [2023-01-06 20:43:43,078] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test GroupHandlingTest
INFO  [2023-01-06 20:43:43,079] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:43,092] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest
INFO  [2023-01-06 20:43:43,113] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:DATASET}): 0 entries, 0 B within 241.5 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 149.5 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:TABLES}): 0 entries, 0 B within 74.49 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 70.68 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:IMPORTS}): 0 entries, 0 B within 80.64 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:CONCEPTS}): 0 entries, 0 B within 66.16 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 67.69 μs
INFO  [2023-01-06 20:43:43,114] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:STRUCTURE}): 0 entries, 0 B within 75.65 μs
INFO  [2023-01-06 20:43:43,115] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 65.32 μs
INFO  [2023-01-06 20:43:43,115] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 84.36 μs
INFO  [2023-01-06 20:43:43,123] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-06 20:43:43,124] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-06 20:43:43,136] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a
INFO  [2023-01-06 20:43:43,136] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:DATASET}): 0 entries, 0 B within 103.6 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:SECONDARY_IDS}): 0 entries, 0 B within 56.66 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:TABLES}): 0 entries, 0 B within 42.17 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 48.93 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:IMPORTS}): 0 entries, 0 B within 36.52 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:CONCEPTS}): 0 entries, 0 B within 45.77 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:DATASET}): 0 entries, 0 B within 83.36 μs
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:43,163] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:SECONDARY_IDS}): 0 entries, 0 B within 53.22 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:WORKER}): 0 entries, 0 B within 66.65 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:TABLES}): 0 entries, 0 B within 51.58 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:BUCKETS}): 0 entries, 0 B within 61.03 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 39.90 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47:C_BLOCKS}): 0 entries, 0 B within 41.31 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:IMPORTS}): 0 entries, 0 B within 37.37 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:CONCEPTS}): 0 entries, 0 B within 38.52 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:WORKER}): 0 entries, 0 B within 38.00 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:BUCKETS}): 0 entries, 0 B within 34.90 μs
INFO  [2023-01-06 20:43:43,164] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a:C_BLOCKS}): 0 entries, 0 B within 34.07 μs
INFO  [2023-01-06 20:43:43,167] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:43,168] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker GroupHandlingTest.worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:43,168] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker GroupHandlingTest.worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:43,168] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:43,168] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker GroupHandlingTest.worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:43,168] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker GroupHandlingTest.worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:43,168] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:43,274] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user
INFO  [2023-01-06 20:43:43,274] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user2
INFO  [2023-01-06 20:43:43,275] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast GroupHandlingTest
INFO  [2023-01-06 20:43:43,275] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-06 20:43:43,275] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47
INFO  [2023-01-06 20:43:43,276] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-06 20:43:43,276] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a
INFO  [2023-01-06 20:43:43,323] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow GroupHandlingTest
INFO  [2023-01-06 20:43:43,365] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47
INFO  [2023-01-06 20:43:43,365] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a
INFO  [2023-01-06 20:43:43,432] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of GroupHandlingTest
INFO  [2023-01-06 20:43:43,433] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_GroupHandlingTest
INFO  [2023-01-06 20:43:43,437] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:43,466] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_GroupHandlingTest_dc480b2e-2fc1-44b9-a7de-01a861baac47
INFO  [2023-01-06 20:43:43,466] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_GroupHandlingTest_c410ae64-6724-480d-900b-3fd7a58d0f9a
INFO  [2023-01-06 20:43:43,575] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test GroupHandlingTest
INFO  [2023-01-06 20:43:43,580] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ImportUpdateTest
INFO  [2023-01-06 20:43:43,581] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:43,605] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest
INFO  [2023-01-06 20:43:43,622] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:DATASET}): 0 entries, 0 B within 151.3 μs
INFO  [2023-01-06 20:43:43,622] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:SECONDARY_IDS}): 0 entries, 0 B within 89.96 μs
INFO  [2023-01-06 20:43:43,622] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:TABLES}): 0 entries, 0 B within 70.97 μs
INFO  [2023-01-06 20:43:43,622] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 70.93 μs
INFO  [2023-01-06 20:43:43,622] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:IMPORTS}): 0 entries, 0 B within 91.24 μs
INFO  [2023-01-06 20:43:43,623] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:CONCEPTS}): 0 entries, 0 B within 68.27 μs
INFO  [2023-01-06 20:43:43,623] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:43,623] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 71.39 μs
INFO  [2023-01-06 20:43:43,623] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:STRUCTURE}): 0 entries, 0 B within 70.53 μs
INFO  [2023-01-06 20:43:43,623] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 236.4 μs
INFO  [2023-01-06 20:43:43,623] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 110.0 μs
INFO  [2023-01-06 20:43:43,632] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-06 20:43:43,632] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-06 20:43:43,652] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af
INFO  [2023-01-06 20:43:43,652] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4
INFO  [2023-01-06 20:43:43,680] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:DATASET}): 0 entries, 0 B within 141.9 μs
INFO  [2023-01-06 20:43:43,680] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:SECONDARY_IDS}): 0 entries, 0 B within 154.4 μs
INFO  [2023-01-06 20:43:43,680] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:TABLES}): 0 entries, 0 B within 46.23 μs
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.99 μs
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:IMPORTS}): 0 entries, 0 B within 40.52 μs
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:CONCEPTS}): 0 entries, 0 B within 39.00 μs
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:WORKER}): 0 entries, 0 B within 27.14 μs
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:BUCKETS}): 0 entries, 0 B within 27.24 μs
INFO  [2023-01-06 20:43:43,681] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4:C_BLOCKS}): 0 entries, 0 B within 25.32 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:DATASET}): 0 entries, 0 B within 86.40 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:SECONDARY_IDS}): 0 entries, 0 B within 126.6 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:TABLES}): 0 entries, 0 B within 60.50 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 49.08 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:IMPORTS}): 0 entries, 0 B within 42.22 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:CONCEPTS}): 0 entries, 0 B within 53.38 μs
INFO  [2023-01-06 20:43:43,684] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:43,685] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:WORKER}): 0 entries, 0 B within 42.10 μs
INFO  [2023-01-06 20:43:43,685] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:BUCKETS}): 0 entries, 0 B within 45.91 μs
INFO  [2023-01-06 20:43:43,685] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af:C_BLOCKS}): 0 entries, 0 B within 61.12 μs
INFO  [2023-01-06 20:43:43,686] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ImportUpdateTest.worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:43,686] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ImportUpdateTest.worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:43,686] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:43,689] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:43,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ImportUpdateTest.worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:43,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ImportUpdateTest.worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:43,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:43,793] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:43,802] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table1
INFO  [2023-01-06 20:43:43,803] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:43,803] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table1
INFO  [2023-01-06 20:43:43,804] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table2
INFO  [2023-01-06 20:43:43,805] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table2
INFO  [2023-01-06 20:43:43,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:44,043] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:44,044] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:44,044] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:44,044] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 151 B in total
INFO  [2023-01-06 20:43:44,044] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
███████████████████████████████                   ▌  62%	est. time remaining: 0.020736127sINFO  [2023-01-06 20:43:44,078] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:44,078] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:44,078] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=15655, maxValue=16021), dateReader=com.bakdata.conquery.util.DateReader@42b881d0)
INFO  [2023-01-06 20:43:44,082] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:44,082] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000532372sINFO  [2023-01-06 20:43:44,098] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:44,098] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:44,098] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@68fd0f80)
INFO  [2023-01-06 20:43:44,101] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:44,101] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:43:44,101] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:44,101] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:44,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:44,133] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into ImportUpdateTest.table1
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "POST /admin/datasets/ImportUpdateTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ImportUpdateTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:43:44,134] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:44,137] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:44,153] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:44,153] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:44,158] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-06 20:43:44,161] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:44,161] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 4 entries.
INFO  [2023-01-06 20:43:44,163] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.1
INFO  [2023-01-06 20:43:44,164] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 4 entries.
INFO  [2023-01-06 20:43:44,167] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.0
INFO  [2023-01-06 20:43:44,283] com.bakdata.conquery.integration.tests.ImportUpdateTest: Checking state before update
INFO  [2023-01-06 20:43:44,306] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ImportUpdateTest] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:43:44,314] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b67d7bee-9f69-4f92-ac8f-ec82b910bbcb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportUpdateTest))]]
INFO  [2023-01-06 20:43:44,319] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb
INFO  [2023-01-06 20:43:44,319] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb
INFO  [2023-01-06 20:43:44,320] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb] with 2 results within PT0.001185S
INFO  [2023-01-06 20:43:44,320] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb] with 0 results within PT0.00089S
INFO  [2023-01-06 20:43:44,321] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb, workerId=ImportUpdateTest.worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4, startTime=2023-01-06T20:43:44.319104, finishTime=2023-01-06T20:43:44.320289) of size 2
INFO  [2023-01-06 20:43:44,321] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb, workerId=ImportUpdateTest.worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af, startTime=2023-01-06T20:43:44.319480, finishTime=2023-01-06T20:43:44.320370) of size 0
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "POST /api/datasets/ImportUpdateTest/queries HTTP/1.1" 201 1163 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:43:44,323] com.bakdata.conquery.models.execution.ManagedExecution: DONE b67d7bee-9f69-4f92-ac8f-ec82b910bbcb ManagedQuery within PT0.007819S
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "GET /api/datasets/ImportUpdateTest/queries/ImportUpdateTest.b67d7bee-9f69-4f92-ac8f-ec82b910bbcb HTTP/1.1" 200 1418 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:43:44,411] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "PUT /admin/datasets/ImportUpdateTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ImportUpdateTest%2Ftable2.cqpp HTTP/1.1" 404 79 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:43:44,428] com.bakdata.conquery.integration.tests.ImportUpdateTest: Manually loading new data for import
INFO  [2023-01-06 20:43:44,430] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:44,430] com.bakdata.conquery.commands.PreprocessorCommand: EXISTS ALREADY
INFO  [2023-01-06 20:43:44,431] com.bakdata.conquery.commands.PreprocessorCommand: 	HASH OUTDATED
INFO  [2023-01-06 20:43:44,432] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 124 B in total
INFO  [2023-01-06 20:43:44,433] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00031304sINFO  [2023-01-06 20:43:44,464] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=5, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:44,464] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:44,464] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=15929), dateReader=com.bakdata.conquery.util.DateReader@3c22a8a0)
INFO  [2023-01-06 20:43:44,467] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:44,467] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:44,467] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:44,467] com.bakdata.conquery.integration.tests.ImportUpdateTest: updating import
INFO  [2023-01-06 20:43:44,487] com.bakdata.conquery.models.messages.namespaces.specific.RemoveImportJob: Deleting Import[NamedImpl(name=table1)]
INFO  [2023-01-06 20:43:44,488] com.bakdata.conquery.models.messages.namespaces.specific.RemoveImportJob: Deleting Import[NamedImpl(name=table1)]
INFO  [2023-01-06 20:43:44,490] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into ImportUpdateTest.table1
INFO  [2023-01-06 20:43:44,491] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "PUT /admin/datasets/ImportUpdateTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ImportUpdateTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:43:44,492] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:44,502] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:44,502] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:44,504] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-06 20:43:44,505] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:44,513] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 5 entries.
INFO  [2023-01-06 20:43:44,514] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.1
INFO  [2023-01-06 20:43:44,535] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 5 entries.
INFO  [2023-01-06 20:43:44,536] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.2
INFO  [2023-01-06 20:43:44,642] com.bakdata.conquery.integration.tests.ImportUpdateTest: Checking state after update
INFO  [2023-01-06 20:43:44,666] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ImportUpdateTest] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:43:44,667] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[fabd92b4-8774-4be9-9b25-ef64f06b93e5] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportUpdateTest))]]
INFO  [2023-01-06 20:43:44,669] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5
INFO  [2023-01-06 20:43:44,669] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5
INFO  [2023-01-06 20:43:44,671] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5] with 2 results within PT0.001973S
INFO  [2023-01-06 20:43:44,671] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5] with 2 results within PT0.002427S
INFO  [2023-01-06 20:43:44,673] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5, workerId=ImportUpdateTest.worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4, startTime=2023-01-06T20:43:44.669370, finishTime=2023-01-06T20:43:44.671797) of size 2
INFO  [2023-01-06 20:43:44,673] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5, workerId=ImportUpdateTest.worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af, startTime=2023-01-06T20:43:44.669530, finishTime=2023-01-06T20:43:44.671503) of size 2
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "POST /api/datasets/ImportUpdateTest/queries HTTP/1.1" 201 1164 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:43:44,674] com.bakdata.conquery.models.execution.ManagedExecution: DONE fabd92b4-8774-4be9-9b25-ef64f06b93e5 ManagedQuery within PT0.006136S
127.0.0.1 - - [06/Jan/2023:20:43:44 +0000] "GET /api/datasets/ImportUpdateTest/queries/ImportUpdateTest.fabd92b4-8774-4be9-9b25-ef64f06b93e5 HTTP/1.1" 200 1419 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:43:44,698] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ImportUpdateTest
INFO  [2023-01-06 20:43:44,698] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-06 20:43:44,698] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-06 20:43:44,699] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af
INFO  [2023-01-06 20:43:44,699] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4
INFO  [2023-01-06 20:43:44,732] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ImportUpdateTest
INFO  [2023-01-06 20:43:44,783] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4
INFO  [2023-01-06 20:43:44,788] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af
INFO  [2023-01-06 20:43:44,805] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ImportUpdateTest
INFO  [2023-01-06 20:43:44,840] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportUpdateTest
INFO  [2023-01-06 20:43:44,842] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportUpdateTest_dfde2fa6-a158-4361-bb59-a58e7d3f08af
INFO  [2023-01-06 20:43:44,842] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportUpdateTest_14c8e9d4-72d6-4dcf-8938-6dc8dbfb1cf4
INFO  [2023-01-06 20:43:44,848] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:44,958] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ImportUpdateTest
INFO  [2023-01-06 20:43:44,962] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MetadataCollectionTest
INFO  [2023-01-06 20:43:44,964] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:44,987] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest
INFO  [2023-01-06 20:43:45,004] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:DATASET}): 0 entries, 0 B within 200.1 μs
INFO  [2023-01-06 20:43:45,004] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:SECONDARY_IDS}): 0 entries, 0 B within 64.90 μs
INFO  [2023-01-06 20:43:45,004] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:TABLES}): 0 entries, 0 B within 52.89 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 63.69 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:IMPORTS}): 0 entries, 0 B within 50.60 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:CONCEPTS}): 0 entries, 0 B within 48.88 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 51.70 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:STRUCTURE}): 0 entries, 0 B within 47.95 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 59.65 μs
INFO  [2023-01-06 20:43:45,005] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 47.96 μs
INFO  [2023-01-06 20:43:45,014] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-06 20:43:45,014] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-06 20:43:45,043] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb
INFO  [2023-01-06 20:43:45,049] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:DATASET}): 0 entries, 0 B within 88.05 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:SECONDARY_IDS}): 0 entries, 0 B within 54.05 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:TABLES}): 0 entries, 0 B within 38.24 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.29 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:IMPORTS}): 0 entries, 0 B within 34.81 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:CONCEPTS}): 0 entries, 0 B within 34.35 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:WORKER}): 0 entries, 0 B within 34.76 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:BUCKETS}): 0 entries, 0 B within 39.40 μs
INFO  [2023-01-06 20:43:45,072] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb:C_BLOCKS}): 0 entries, 0 B within 35.51 μs
INFO  [2023-01-06 20:43:45,077] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MetadataCollectionTest.worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:45,077] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MetadataCollectionTest.worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:45,077] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:DATASET}): 0 entries, 0 B within 61.63 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:SECONDARY_IDS}): 0 entries, 0 B within 32.34 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:TABLES}): 0 entries, 0 B within 26.02 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 28.00 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:IMPORTS}): 0 entries, 0 B within 23.83 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:CONCEPTS}): 0 entries, 0 B within 23.23 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:WORKER}): 0 entries, 0 B within 27.29 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:BUCKETS}): 0 entries, 0 B within 23.39 μs
INFO  [2023-01-06 20:43:45,078] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204:C_BLOCKS}): 0 entries, 0 B within 22.23 μs
INFO  [2023-01-06 20:43:45,081] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MetadataCollectionTest.worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:45,081] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MetadataCollectionTest.worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:45,081] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:45,082] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,185] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,194] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MetadataCollectionTest.test_table
INFO  [2023-01-06 20:43:45,194] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MetadataCollectionTest.test_table
INFO  [2023-01-06 20:43:45,312] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,422] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:45,423] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:45,423] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-06 20:43:45,423] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_MetadataCollectionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000305864sINFO  [2023-01-06 20:43:45,454] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:45,454] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:45,454] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2ff98d93)
INFO  [2023-01-06 20:43:45,457] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_MetadataCollectionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:45,458] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:45,458] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_MetadataCollectionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:45,481] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into MetadataCollectionTest.test_table
127.0.0.1 - - [06/Jan/2023:20:43:45 +0000] "POST /admin/datasets/MetadataCollectionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_MetadataCollectionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:43:45,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,485] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:45,495] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:45,495] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:45,501] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-06 20:43:45,504] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:45,506] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MetadataCollectionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:45,506] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MetadataCollectionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:45,508] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MetadataCollectionTest.test_table.test_table.1
INFO  [2023-01-06 20:43:45,509] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MetadataCollectionTest.test_table.test_table.0
INFO  [2023-01-06 20:43:45,614] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,620] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,632] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:43:45,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:43:45,773] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MetadataCollectionTest
INFO  [2023-01-06 20:43:45,774] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-06 20:43:45,774] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-06 20:43:45,774] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204
INFO  [2023-01-06 20:43:45,774] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb
INFO  [2023-01-06 20:43:45,800] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204
INFO  [2023-01-06 20:43:45,800] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb
INFO  [2023-01-06 20:43:45,814] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MetadataCollectionTest
INFO  [2023-01-06 20:43:45,869] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_MetadataCollectionTest_fcb109ce-9eff-40e8-9a8b-338a3c97f204
INFO  [2023-01-06 20:43:45,869] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_MetadataCollectionTest_28c344c3-ebb4-4963-b03d-735e821d6cdb
INFO  [2023-01-06 20:43:45,905] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MetadataCollectionTest
INFO  [2023-01-06 20:43:45,907] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_MetadataCollectionTest
INFO  [2023-01-06 20:43:45,916] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:45,981] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MetadataCollectionTest
INFO  [2023-01-06 20:43:45,985] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:45,986] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:46,006] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:46,024] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:DATASET}): 0 entries, 0 B within 105.0 μs
INFO  [2023-01-06 20:43:46,024] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 66.46 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:TABLES}): 0 entries, 0 B within 55.81 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 57.51 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:IMPORTS}): 0 entries, 0 B within 51.28 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:CONCEPTS}): 0 entries, 0 B within 62.83 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 65.66 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:STRUCTURE}): 0 entries, 0 B within 51.73 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 51.87 μs
INFO  [2023-01-06 20:43:46,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 50.89 μs
INFO  [2023-01-06 20:43:46,034] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-06 20:43:46,034] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-06 20:43:46,063] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4
INFO  [2023-01-06 20:43:46,068] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:DATASET}): 0 entries, 0 B within 80.62 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:SECONDARY_IDS}): 0 entries, 0 B within 42.68 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:TABLES}): 0 entries, 0 B within 31.30 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 33.90 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:IMPORTS}): 0 entries, 0 B within 29.17 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:CONCEPTS}): 0 entries, 0 B within 28.37 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:WORKER}): 0 entries, 0 B within 29.29 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:BUCKETS}): 0 entries, 0 B within 27.80 μs
INFO  [2023-01-06 20:43:46,090] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4:C_BLOCKS}): 0 entries, 0 B within 27.18 μs
INFO  [2023-01-06 20:43:46,095] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:46,095] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:46,095] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:46,096] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:DATASET}): 0 entries, 0 B within 61.66 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:SECONDARY_IDS}): 0 entries, 0 B within 35.80 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:TABLES}): 0 entries, 0 B within 29.66 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 31.41 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:IMPORTS}): 0 entries, 0 B within 25.07 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:CONCEPTS}): 0 entries, 0 B within 25.33 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:WORKER}): 0 entries, 0 B within 25.45 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:BUCKETS}): 0 entries, 0 B within 24.68 μs
INFO  [2023-01-06 20:43:46,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e:C_BLOCKS}): 0 entries, 0 B within 34.38 μs
INFO  [2023-01-06 20:43:46,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:46,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:46,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:46,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:46,223] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user
INFO  [2023-01-06 20:43:46,223] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:46,224] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-06 20:43:46,224] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e
INFO  [2023-01-06 20:43:46,224] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-06 20:43:46,224] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4
INFO  [2023-01-06 20:43:46,233] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:46,311] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e
INFO  [2023-01-06 20:43:46,311] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4
INFO  [2023-01-06 20:43:46,333] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:46,335] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:46,343] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:46,413] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionGroupHandlingTest_6d45e120-3952-4c10-a039-03c9d7257bb4
INFO  [2023-01-06 20:43:46,413] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionGroupHandlingTest_9fb4739e-2c89-4fdb-868c-ea3b870e158e
INFO  [2023-01-06 20:43:46,526] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test PermissionGroupHandlingTest
INFO  [2023-01-06 20:43:46,531] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:46,532] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:46,552] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:46,569] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:DATASET}): 0 entries, 0 B within 123.8 μs
INFO  [2023-01-06 20:43:46,569] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 64.87 μs
INFO  [2023-01-06 20:43:46,569] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:TABLES}): 0 entries, 0 B within 51.50 μs
INFO  [2023-01-06 20:43:46,569] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 54.45 μs
INFO  [2023-01-06 20:43:46,569] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:IMPORTS}): 0 entries, 0 B within 55.50 μs
INFO  [2023-01-06 20:43:46,570] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:CONCEPTS}): 0 entries, 0 B within 47.52 μs
INFO  [2023-01-06 20:43:46,570] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:46,570] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 50.49 μs
INFO  [2023-01-06 20:43:46,570] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:STRUCTURE}): 0 entries, 0 B within 47.06 μs
INFO  [2023-01-06 20:43:46,570] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 46.63 μs
INFO  [2023-01-06 20:43:46,570] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 46.00 μs
INFO  [2023-01-06 20:43:46,578] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-06 20:43:46,578] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-06 20:43:46,598] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1
INFO  [2023-01-06 20:43:46,598] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9
INFO  [2023-01-06 20:43:46,653] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:DATASET}): 0 entries, 0 B within 77.60 μs
INFO  [2023-01-06 20:43:46,653] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:DATASET}): 0 entries, 0 B within 63.45 μs
INFO  [2023-01-06 20:43:46,653] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:SECONDARY_IDS}): 0 entries, 0 B within 35.38 μs
INFO  [2023-01-06 20:43:46,653] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:SECONDARY_IDS}): 0 entries, 0 B within 42.25 μs
INFO  [2023-01-06 20:43:46,653] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:TABLES}): 0 entries, 0 B within 26.45 μs
INFO  [2023-01-06 20:43:46,653] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:TABLES}): 0 entries, 0 B within 29.66 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 39.38 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 54.27 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:IMPORTS}): 0 entries, 0 B within 30.25 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:IMPORTS}): 0 entries, 0 B within 27.46 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:CONCEPTS}): 0 entries, 0 B within 26.85 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:CONCEPTS}): 0 entries, 0 B within 28.45 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:WORKER}): 0 entries, 0 B within 26.72 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:WORKER}): 0 entries, 0 B within 35.73 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:BUCKETS}): 0 entries, 0 B within 34.85 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:BUCKETS}): 0 entries, 0 B within 26.40 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9:C_BLOCKS}): 0 entries, 0 B within 34.89 μs
INFO  [2023-01-06 20:43:46,654] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1:C_BLOCKS}): 0 entries, 0 B within 25.99 μs
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:46,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:46,765] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user
INFO  [2023-01-06 20:43:46,766] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:46,767] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-06 20:43:46,767] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-06 20:43:46,767] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9
INFO  [2023-01-06 20:43:46,767] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1
INFO  [2023-01-06 20:43:46,778] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:46,855] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1
INFO  [2023-01-06 20:43:46,855] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9
INFO  [2023-01-06 20:43:46,878] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:46,879] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:46,887] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:46,956] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_PermissionRoleHandlingTest_7771e079-a0ad-4885-8cdb-fa90ad5c8ee1
INFO  [2023-01-06 20:43:46,956] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_PermissionRoleHandlingTest_86b6bf38-43c1-45ab-ad3a-e569b99f37b9
INFO  [2023-01-06 20:43:47,071] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test PermissionRoleHandlingTest
INFO  [2023-01-06 20:43:47,074] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test RestartTest
INFO  [2023-01-06 20:43:47,077] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:43:47,113] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:DATASET}): 0 entries, 0 B within 91.93 μs
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:SECONDARY_IDS}): 0 entries, 0 B within 53.30 μs
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:TABLES}): 0 entries, 0 B within 44.18 μs
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 47.50 μs
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:IMPORTS}): 0 entries, 0 B within 41.57 μs
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:CONCEPTS}): 0 entries, 0 B within 40.98 μs
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:47,122] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 44.13 μs
INFO  [2023-01-06 20:43:47,123] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:STRUCTURE}): 0 entries, 0 B within 70.24 μs
INFO  [2023-01-06 20:43:47,123] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 51.39 μs
INFO  [2023-01-06 20:43:47,123] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 42.28 μs
INFO  [2023-01-06 20:43:47,130] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RestartTest, name=RestartTest]
INFO  [2023-01-06 20:43:47,130] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RestartTest, name=RestartTest]
INFO  [2023-01-06 20:43:47,159] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
INFO  [2023-01-06 20:43:47,164] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:DATASET}): 0 entries, 0 B within 71.58 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:SECONDARY_IDS}): 0 entries, 0 B within 35.08 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:TABLES}): 0 entries, 0 B within 25.74 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 28.54 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:IMPORTS}): 0 entries, 0 B within 24.28 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:CONCEPTS}): 0 entries, 0 B within 24.84 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:WORKER}): 0 entries, 0 B within 37.49 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:BUCKETS}): 0 entries, 0 B within 25.46 μs
INFO  [2023-01-06 20:43:47,183] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:C_BLOCKS}): 0 entries, 0 B within 23.67 μs
INFO  [2023-01-06 20:43:47,188] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:47,188] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:47,188] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:DATASET}): 0 entries, 0 B within 69.87 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:SECONDARY_IDS}): 0 entries, 0 B within 36.48 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:TABLES}): 0 entries, 0 B within 27.14 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 32.85 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:IMPORTS}): 0 entries, 0 B within 25.73 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:CONCEPTS}): 0 entries, 0 B within 29.26 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:WORKER}): 0 entries, 0 B within 25.37 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:BUCKETS}): 0 entries, 0 B within 23.81 μs
INFO  [2023-01-06 20:43:47,191] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:C_BLOCKS}): 0 entries, 0 B within 24.45 μs
INFO  [2023-01-06 20:43:47,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:47,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:47,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:47,317] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[RestartTest.secondary]
INFO  [2023-01-06 20:43:47,319] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,321] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId RestartTest.secondary
INFO  [2023-01-06 20:43:47,321] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId RestartTest.secondary
INFO  [2023-01-06 20:43:47,430] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,430] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RestartTest.test_table
INFO  [2023-01-06 20:43:47,430] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RestartTest.test_table
INFO  [2023-01-06 20:43:47,566] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,676] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:43:47,677] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:43:47,677] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-06 20:43:47,677] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_RestartTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000318769sINFO  [2023-01-06 20:43:47,709] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:43:47,709] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:43:47,710] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@29492fbd)
INFO  [2023-01-06 20:43:47,713] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_RestartTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:47,713] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:43:47,713] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_RestartTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:43:47,729] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into RestartTest.test_table
127.0.0.1 - - [06/Jan/2023:20:43:47 +0000] "POST /admin/datasets/RestartTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_RestartTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:43:47,730] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,733] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:43:47,746] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:43:47,746] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:43:47,752] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:43:47,753] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RestartTest.test_table.test_table], containing 4 entries.
WARN  [2023-01-06 20:43:47,754] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:43:47,755] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RestartTest.test_table.test_table.1
INFO  [2023-01-06 20:43:47,758] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RestartTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:43:47,760] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RestartTest.test_table.test_table.0
INFO  [2023-01-06 20:43:47,867] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,873] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:43:47,888] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:43:47,910] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[RestartTest] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:43:47,911] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[20ee173f-9ebf-4681-9de1-7f230ab3b97a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest))]]
INFO  [2023-01-06 20:43:47,916] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a
INFO  [2023-01-06 20:43:47,916] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a
INFO  [2023-01-06 20:43:47,916] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a] with 0 results within PT0.000683S
INFO  [2023-01-06 20:43:47,917] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a] with 2 results within PT0.00157S
INFO  [2023-01-06 20:43:47,918] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a, workerId=RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16, startTime=2023-01-06T20:43:47.916082, finishTime=2023-01-06T20:43:47.916765) of size 0
127.0.0.1 - - [06/Jan/2023:20:43:47 +0000] "POST /api/datasets/RestartTest/queries HTTP/1.1" 201 1117 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:43:47,919] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a, workerId=RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5, startTime=2023-01-06T20:43:47.916134, finishTime=2023-01-06T20:43:47.917704) of size 2
INFO  [2023-01-06 20:43:47,920] com.bakdata.conquery.models.execution.ManagedExecution: DONE 20ee173f-9ebf-4681-9de1-7f230ab3b97a ManagedQuery within PT0.008068S
127.0.0.1 - - [06/Jan/2023:20:43:47 +0000] "GET /api/datasets/RestartTest/queries/RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a HTTP/1.1" 200 1352 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:43:47,965] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RestartTest], queryId=20ee173f-9ebf-4681-9de1-7f230ab3b97a, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:43:47.910748, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@641c983b[Count = 0], startTime=2023-01-06T20:43:47.911728, finishTime=2023-01-06T20:43:47.919796, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b9fa27a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@347fb4f9], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@775c0aaa], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1974858b, com.bakdata.conquery.models.query.ColumnDescriptor@deed5b1]) download on dataset Dataset[label=null, name=RestartTest] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:43:48,026] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RestartTest], queryId=20ee173f-9ebf-4681-9de1-7f230ab3b97a, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:43:47.910748, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@641c983b[Count = 0], startTime=2023-01-06T20:43:47.911728, finishTime=2023-01-06T20:43:47.919796, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b9fa27a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@347fb4f9], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@775c0aaa], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1974858b, com.bakdata.conquery.models.query.ColumnDescriptor@deed5b1]) on dataset Dataset[label=null, name=RestartTest]
127.0.0.1 - - [06/Jan/2023:20:43:48 +0000] "GET /api/datasets/RestartTest/result/RestartTest.20ee173f-9ebf-4681-9de1-7f230ab3b97a.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 81
INFO  [2023-01-06 20:43:48,039] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:43:48,068] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:DATASET}): 0 entries, 0 B within 97.11 μs
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:SECONDARY_IDS}): 0 entries, 0 B within 58.22 μs
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:TABLES}): 0 entries, 0 B within 39.52 μs
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 42.66 μs
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:IMPORTS}): 0 entries, 0 B within 36.84 μs
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:CONCEPTS}): 0 entries, 0 B within 36.78 μs
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:48,080] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 39.82 μs
INFO  [2023-01-06 20:43:48,081] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:STRUCTURE}): 0 entries, 0 B within 37.55 μs
INFO  [2023-01-06 20:43:48,081] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:WORKER_TO_BUCKETS}): 0 entries, 0 B within 39.12 μs
INFO  [2023-01-06 20:43:48,081] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:PRIMARY_DICTIONARY}): 0 entries, 0 B within 51.57 μs
INFO  [2023-01-06 20:43:48,088] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset1, name=testDataset1]
INFO  [2023-01-06 20:43:48,088] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset1, name=testDataset1]
INFO  [2023-01-06 20:43:48,116] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
INFO  [2023-01-06 20:43:48,121] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
INFO  [2023-01-06 20:43:48,121] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
INFO  [2023-01-06 20:43:48,131] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:DATASET}): 0 entries, 0 B within 151.3 μs
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:SECONDARY_IDS}): 0 entries, 0 B within 78.22 μs
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:TABLES}): 0 entries, 0 B within 60.31 μs
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 64.39 μs
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:IMPORTS}): 0 entries, 0 B within 54.56 μs
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:CONCEPTS}): 0 entries, 0 B within 64.09 μs
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:48,132] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 78.96 μs
INFO  [2023-01-06 20:43:48,133] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:STRUCTURE}): 0 entries, 0 B within 50.94 μs
INFO  [2023-01-06 20:43:48,133] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:WORKER_TO_BUCKETS}): 0 entries, 0 B within 49.02 μs
INFO  [2023-01-06 20:43:48,133] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:PRIMARY_DICTIONARY}): 0 entries, 0 B within 50.38 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:DATASET}): 0 entries, 0 B within 66.50 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:SECONDARY_IDS}): 0 entries, 0 B within 62.16 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:TABLES}): 0 entries, 0 B within 20.86 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 22.81 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:IMPORTS}): 0 entries, 0 B within 18.46 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:CONCEPTS}): 0 entries, 0 B within 28.13 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:WORKER}): 0 entries, 0 B within 18.29 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:BUCKETS}): 0 entries, 0 B within 18.45 μs
INFO  [2023-01-06 20:43:48,144] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:C_BLOCKS}): 0 entries, 0 B within 16.19 μs
INFO  [2023-01-06 20:43:48,145] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset2, name=testDataset2]
INFO  [2023-01-06 20:43:48,145] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:DATASET}): 0 entries, 0 B within 53.77 μs
INFO  [2023-01-06 20:43:48,145] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:SECONDARY_IDS}): 0 entries, 0 B within 32.09 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:TABLES}): 0 entries, 0 B within 26.89 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 30.65 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:IMPORTS}): 0 entries, 0 B within 27.81 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:CONCEPTS}): 0 entries, 0 B within 32.58 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:WORKER}): 0 entries, 0 B within 43.03 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:BUCKETS}): 0 entries, 0 B within 41.32 μs
INFO  [2023-01-06 20:43:48,146] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:C_BLOCKS}): 0 entries, 0 B within 42.66 μs
INFO  [2023-01-06 20:43:48,147] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset2, name=testDataset2]
INFO  [2023-01-06 20:43:48,149] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset1.worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,149] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset1.worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,149] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,149] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset1.worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,149] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset1.worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,149] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,171] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
INFO  [2023-01-06 20:43:48,176] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
INFO  [2023-01-06 20:43:48,176] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:DATASET}): 0 entries, 0 B within 88.90 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:SECONDARY_IDS}): 0 entries, 0 B within 55.20 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:TABLES}): 0 entries, 0 B within 46.43 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 50.94 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:IMPORTS}): 0 entries, 0 B within 44.80 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:CONCEPTS}): 0 entries, 0 B within 44.55 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 47.18 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:STRUCTURE}): 0 entries, 0 B within 44.97 μs
INFO  [2023-01-06 20:43:48,186] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:WORKER_TO_BUCKETS}): 0 entries, 0 B within 56.03 μs
INFO  [2023-01-06 20:43:48,187] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:PRIMARY_DICTIONARY}): 0 entries, 0 B within 46.18 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:DATASET}): 0 entries, 0 B within 74.87 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:SECONDARY_IDS}): 0 entries, 0 B within 29.99 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:TABLES}): 0 entries, 0 B within 55.45 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:DATASET}): 0 entries, 0 B within 66.32 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 58.71 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:SECONDARY_IDS}): 0 entries, 0 B within 35.77 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:IMPORTS}): 0 entries, 0 B within 41.52 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:TABLES}): 0 entries, 0 B within 43.11 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:CONCEPTS}): 0 entries, 0 B within 38.25 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.87 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:WORKER}): 0 entries, 0 B within 39.93 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:IMPORTS}): 0 entries, 0 B within 59.90 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:BUCKETS}): 0 entries, 0 B within 32.50 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:CONCEPTS}): 0 entries, 0 B within 31.89 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:C_BLOCKS}): 0 entries, 0 B within 29.11 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:WORKER}): 0 entries, 0 B within 33.50 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:BUCKETS}): 0 entries, 0 B within 24.78 μs
INFO  [2023-01-06 20:43:48,204] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:C_BLOCKS}): 0 entries, 0 B within 22.72 μs
INFO  [2023-01-06 20:43:48,205] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset3, name=testDataset3]
INFO  [2023-01-06 20:43:48,205] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset3, name=testDataset3]
INFO  [2023-01-06 20:43:48,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset2.worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset2.worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset2.worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset2.worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,218] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
INFO  [2023-01-06 20:43:48,229] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
INFO  [2023-01-06 20:43:48,229] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:DATASET}): 0 entries, 0 B within 94.70 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:SECONDARY_IDS}): 0 entries, 0 B within 56.35 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:TABLES}): 0 entries, 0 B within 49.73 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 52.49 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:IMPORTS}): 0 entries, 0 B within 46.41 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:CONCEPTS}): 0 entries, 0 B within 45.83 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 52.82 μs
INFO  [2023-01-06 20:43:48,234] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:STRUCTURE}): 0 entries, 0 B within 46.73 μs
INFO  [2023-01-06 20:43:48,235] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:WORKER_TO_BUCKETS}): 0 entries, 0 B within 63.20 μs
INFO  [2023-01-06 20:43:48,235] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:PRIMARY_DICTIONARY}): 0 entries, 0 B within 48.08 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:DATASET}): 0 entries, 0 B within 86.70 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:DATASET}): 0 entries, 0 B within 86.52 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:SECONDARY_IDS}): 0 entries, 0 B within 28.47 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:SECONDARY_IDS}): 0 entries, 0 B within 31.52 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:TABLES}): 0 entries, 0 B within 28.52 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:TABLES}): 0 entries, 0 B within 27.50 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 31.02 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 31.07 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:IMPORTS}): 0 entries, 0 B within 26.94 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:IMPORTS}): 0 entries, 0 B within 26.75 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:CONCEPTS}): 0 entries, 0 B within 28.48 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:CONCEPTS}): 0 entries, 0 B within 26.39 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:WORKER}): 0 entries, 0 B within 28.27 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:WORKER}): 0 entries, 0 B within 27.78 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:BUCKETS}): 0 entries, 0 B within 26.12 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:BUCKETS}): 0 entries, 0 B within 26.83 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:C_BLOCKS}): 0 entries, 0 B within 27.09 μs
INFO  [2023-01-06 20:43:48,278] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:C_BLOCKS}): 0 entries, 0 B within 28.50 μs
INFO  [2023-01-06 20:43:48,279] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset4, name=testDataset4]
INFO  [2023-01-06 20:43:48,279] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset4, name=testDataset4]
INFO  [2023-01-06 20:43:48,281] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset3.worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,281] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset3.worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,281] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,281] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset3.worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,281] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset3.worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,281] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,317] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
INFO  [2023-01-06 20:43:48,317] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
INFO  [2023-01-06 20:43:48,317] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
INFO  [2023-01-06 20:43:48,332] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:DATASET}): 0 entries, 0 B within 178.1 μs
INFO  [2023-01-06 20:43:48,332] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:SECONDARY_IDS}): 0 entries, 0 B within 50.03 μs
INFO  [2023-01-06 20:43:48,332] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:TABLES}): 0 entries, 0 B within 33.80 μs
INFO  [2023-01-06 20:43:48,332] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 37.55 μs
INFO  [2023-01-06 20:43:48,332] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:IMPORTS}): 0 entries, 0 B within 32.31 μs
INFO  [2023-01-06 20:43:48,333] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:CONCEPTS}): 0 entries, 0 B within 30.68 μs
INFO  [2023-01-06 20:43:48,333] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:48,333] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 35.98 μs
INFO  [2023-01-06 20:43:48,333] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:STRUCTURE}): 0 entries, 0 B within 30.99 μs
INFO  [2023-01-06 20:43:48,333] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:WORKER_TO_BUCKETS}): 0 entries, 0 B within 35.58 μs
INFO  [2023-01-06 20:43:48,333] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:PRIMARY_DICTIONARY}): 0 entries, 0 B within 30.64 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:DATASET}): 0 entries, 0 B within 67.13 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:SECONDARY_IDS}): 0 entries, 0 B within 25.30 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:TABLES}): 0 entries, 0 B within 21.17 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 23.51 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:IMPORTS}): 0 entries, 0 B within 23.13 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:CONCEPTS}): 0 entries, 0 B within 19.56 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:WORKER}): 0 entries, 0 B within 18.97 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:BUCKETS}): 0 entries, 0 B within 20.15 μs
INFO  [2023-01-06 20:43:48,340] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:C_BLOCKS}): 0 entries, 0 B within 17.15 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset5, name=testDataset5]
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:DATASET}): 0 entries, 0 B within 51.50 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:SECONDARY_IDS}): 0 entries, 0 B within 24.73 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:TABLES}): 0 entries, 0 B within 18.62 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.23 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:IMPORTS}): 0 entries, 0 B within 22.10 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:CONCEPTS}): 0 entries, 0 B within 18.90 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:WORKER}): 0 entries, 0 B within 17.91 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:BUCKETS}): 0 entries, 0 B within 17.51 μs
INFO  [2023-01-06 20:43:48,341] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:C_BLOCKS}): 0 entries, 0 B within 17.40 μs
INFO  [2023-01-06 20:43:48,342] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset5, name=testDataset5]
INFO  [2023-01-06 20:43:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset4.worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset4.worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset4.worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset4.worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,368] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
INFO  [2023-01-06 20:43:48,373] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
INFO  [2023-01-06 20:43:48,373] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:DATASET}): 0 entries, 0 B within 71.94 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:SECONDARY_IDS}): 0 entries, 0 B within 43.53 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:TABLES}): 0 entries, 0 B within 35.80 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.26 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:IMPORTS}): 0 entries, 0 B within 40.28 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:CONCEPTS}): 0 entries, 0 B within 33.38 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 35.61 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:STRUCTURE}): 0 entries, 0 B within 33.32 μs
INFO  [2023-01-06 20:43:48,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:WORKER_TO_BUCKETS}): 0 entries, 0 B within 32.96 μs
INFO  [2023-01-06 20:43:48,379] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:PRIMARY_DICTIONARY}): 0 entries, 0 B within 32.40 μs
INFO  [2023-01-06 20:43:48,393] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.userDelete@test$2eemail
INFO  [2023-01-06 20:43:48,393] com.bakdata.conquery.resources.admin.rest.AdminProcessor: Deleting Role[role.roleDelete]
INFO  [2023-01-06 20:43:48,394] com.bakdata.conquery.integration.tests.RestartTest: Shutting down for restart
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:DATASET}): 0 entries, 0 B within 99.81 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:DATASET}): 0 entries, 0 B within 103.9 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:SECONDARY_IDS}): 0 entries, 0 B within 53.15 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:SECONDARY_IDS}): 0 entries, 0 B within 83.82 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:TABLES}): 0 entries, 0 B within 81.72 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:TABLES}): 0 entries, 0 B within 50.24 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 34.38 μs
INFO  [2023-01-06 20:43:48,398] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 32.41 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:IMPORTS}): 0 entries, 0 B within 27.92 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:IMPORTS}): 0 entries, 0 B within 28.62 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:CONCEPTS}): 0 entries, 0 B within 28.30 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:CONCEPTS}): 0 entries, 0 B within 27.98 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:WORKER}): 0 entries, 0 B within 29.33 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:WORKER}): 0 entries, 0 B within 28.98 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:BUCKETS}): 0 entries, 0 B within 32.11 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:BUCKETS}): 0 entries, 0 B within 28.05 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:C_BLOCKS}): 0 entries, 0 B within 27.23 μs
INFO  [2023-01-06 20:43:48,399] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:C_BLOCKS}): 0 entries, 0 B within 27.42 μs
INFO  [2023-01-06 20:43:48,400] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset6, name=testDataset6]
INFO  [2023-01-06 20:43:48,400] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset6, name=testDataset6]
INFO  [2023-01-06 20:43:48,402] org.eclipse.jetty.server.AbstractConnector: Stopped application@21769ad{HTTP/1.1, (http/1.1)}{0.0.0.0:46651}
INFO  [2023-01-06 20:43:48,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset5.worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset5.worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,404] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset5.worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,404] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset5.worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,404] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,404] org.eclipse.jetty.server.AbstractConnector: Stopped admin@6403bd8c{HTTP/1.1, (http/1.1)}{0.0.0.0:42541}
INFO  [2023-01-06 20:43:48,412] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@2953400d{/,null,STOPPED}
INFO  [2023-01-06 20:43:48,414] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@577777bc{/,null,STOPPED}
INFO  [2023-01-06 20:43:48,415] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-06 20:43:48,427] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
INFO  [2023-01-06 20:43:48,432] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:DATASET}): 0 entries, 0 B within 59.55 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:SECONDARY_IDS}): 0 entries, 0 B within 26.02 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:TABLES}): 0 entries, 0 B within 18.46 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 24.54 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:IMPORTS}): 0 entries, 0 B within 17.20 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:CONCEPTS}): 0 entries, 0 B within 17.58 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:WORKER}): 0 entries, 0 B within 18.50 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:BUCKETS}): 0 entries, 0 B within 18.61 μs
INFO  [2023-01-06 20:43:48,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:C_BLOCKS}): 0 entries, 0 B within 18.33 μs
WARN  [2023-01-06 20:43:48,448] com.bakdata.conquery.models.jobs.JobExecutor: Tried to add a job to a closed JobManager: reacting to ForwardToWorker(workerId=testDataset6.worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3, text=RequestConsistency)
INFO  [2023-01-06 20:43:48,450] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:DATASET}): 0 entries, 0 B within 59.46 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:SECONDARY_IDS}): 0 entries, 0 B within 27.76 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:TABLES}): 0 entries, 0 B within 22.81 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 24.34 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:IMPORTS}): 0 entries, 0 B within 20.37 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:CONCEPTS}): 0 entries, 0 B within 19.04 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:WORKER}): 0 entries, 0 B within 21.05 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:BUCKETS}): 0 entries, 0 B within 19.61 μs
INFO  [2023-01-06 20:43:48,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:C_BLOCKS}): 0 entries, 0 B within 18.84 μs
INFO  [2023-01-06 20:43:48,458] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset6.worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:43:48,458] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset6.worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:43:48,458] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:43:48,502] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-06 20:43:48,547] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
INFO  [2023-01-06 20:43:48,579] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:DATASET}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:SECONDARY_IDS}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:TABLES}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:DICTIONARIES_META}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:IMPORTS}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:CONCEPTS}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:WORKER}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:BUCKETS}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:C_BLOCKS}
INFO  [2023-01-06 20:43:48,680] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
INFO  [2023-01-06 20:43:48,688] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
INFO  [2023-01-06 20:43:48,720] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
INFO  [2023-01-06 20:43:48,766] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:DATASET}
INFO  [2023-01-06 20:43:48,766] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:SECONDARY_IDS}
INFO  [2023-01-06 20:43:48,766] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:TABLES}
INFO  [2023-01-06 20:43:48,766] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:DICTIONARIES_META}
INFO  [2023-01-06 20:43:48,766] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:48,766] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:IMPORTS}
INFO  [2023-01-06 20:43:48,767] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:CONCEPTS}
INFO  [2023-01-06 20:43:48,767] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:WORKER}
INFO  [2023-01-06 20:43:48,767] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:BUCKETS}
INFO  [2023-01-06 20:43:48,767] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:C_BLOCKS}
INFO  [2023-01-06 20:43:48,767] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
INFO  [2023-01-06 20:43:48,773] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
INFO  [2023-01-06 20:43:48,847] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:DATASET}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:SECONDARY_IDS}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:TABLES}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:DICTIONARIES_META}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:IMPORTS}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:CONCEPTS}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:WORKER}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:BUCKETS}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:C_BLOCKS}
INFO  [2023-01-06 20:43:48,947] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
INFO  [2023-01-06 20:43:48,954] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
INFO  [2023-01-06 20:43:48,966] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:DATASET}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:TABLES}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:IMPORTS}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:CONCEPTS}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:WORKER}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:BUCKETS}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:C_BLOCKS}
INFO  [2023-01-06 20:43:49,066] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
INFO  [2023-01-06 20:43:49,073] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
INFO  [2023-01-06 20:43:49,101] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:DATASET}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:TABLES}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:IMPORTS}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:CONCEPTS}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:WORKER}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:BUCKETS}
INFO  [2023-01-06 20:43:49,200] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:C_BLOCKS}
INFO  [2023-01-06 20:43:49,201] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
INFO  [2023-01-06 20:43:49,207] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:DATASET}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:TABLES}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:IMPORTS}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:CONCEPTS}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:WORKER}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:BUCKETS}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:C_BLOCKS}
INFO  [2023-01-06 20:43:49,217] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
INFO  [2023-01-06 20:43:49,224] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
INFO  [2023-01-06 20:43:49,253] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
INFO  [2023-01-06 20:43:49,341] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:DATASET}
INFO  [2023-01-06 20:43:49,341] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,341] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:TABLES}
INFO  [2023-01-06 20:43:49,341] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:IMPORTS}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:CONCEPTS}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:WORKER}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:BUCKETS}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:C_BLOCKS}
INFO  [2023-01-06 20:43:49,342] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
INFO  [2023-01-06 20:43:49,349] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:43:49,349] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:43:49,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-06 20:43:49,350] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:43:49,358] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-06 20:43:49,458] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
INFO  [2023-01-06 20:43:49,486] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:DATASET}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:TABLES}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:IMPORTS}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:CONCEPTS}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:WORKER}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:BUCKETS}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:C_BLOCKS}
INFO  [2023-01-06 20:43:49,567] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
INFO  [2023-01-06 20:43:49,576] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
INFO  [2023-01-06 20:43:49,617] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
INFO  [2023-01-06 20:43:49,717] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:DATASET}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:TABLES}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:IMPORTS}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:CONCEPTS}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:WORKER}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:BUCKETS}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:C_BLOCKS}
INFO  [2023-01-06 20:43:49,718] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
INFO  [2023-01-06 20:43:49,725] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
INFO  [2023-01-06 20:43:49,761] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:DATASET}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:TABLES}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:IMPORTS}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:CONCEPTS}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:WORKER}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:BUCKETS}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:C_BLOCKS}
INFO  [2023-01-06 20:43:49,763] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
INFO  [2023-01-06 20:43:49,770] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
INFO  [2023-01-06 20:43:49,801] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:DATASET}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:SECONDARY_IDS}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:TABLES}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:DICTIONARIES_META}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:IMPORTS}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:CONCEPTS}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:WORKER}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:BUCKETS}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:C_BLOCKS}
INFO  [2023-01-06 20:43:49,901] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
INFO  [2023-01-06 20:43:49,908] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
INFO  [2023-01-06 20:43:49,980] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
INFO  [2023-01-06 20:43:50,080] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:DATASET}
INFO  [2023-01-06 20:43:50,080] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:SECONDARY_IDS}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:TABLES}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:DICTIONARIES_META}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:IMPORTS}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:CONCEPTS}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:WORKER}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:BUCKETS}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:C_BLOCKS}
INFO  [2023-01-06 20:43:50,081] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
INFO  [2023-01-06 20:43:50,088] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
INFO  [2023-01-06 20:43:50,151] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:DATASET}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:SECONDARY_IDS}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:TABLES}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:DICTIONARIES_META}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:IMPORTS}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:CONCEPTS}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:WORKER}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:BUCKETS}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:C_BLOCKS}
INFO  [2023-01-06 20:43:50,251] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
INFO  [2023-01-06 20:43:50,258] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:DATASET}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:SECONDARY_IDS}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:TABLES}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:DICTIONARIES_META}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:IMPORTS}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:CONCEPTS}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:WORKER}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:BUCKETS}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:C_BLOCKS}
INFO  [2023-01-06 20:43:50,267] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
INFO  [2023-01-06 20:43:50,274] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:43:50,274] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:43:50,274] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-06 20:43:50,274] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:43:50,364] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-06 20:43:50,423] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset1
INFO  [2023-01-06 20:43:50,490] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset1
INFO  [2023-01-06 20:43:50,590] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset1
INFO  [2023-01-06 20:43:50,590] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:DATASET}
INFO  [2023-01-06 20:43:50,590] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:SECONDARY_IDS}
INFO  [2023-01-06 20:43:50,590] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:TABLES}
INFO  [2023-01-06 20:43:50,590] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:DICTIONARIES_META}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:IMPORTS}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:CONCEPTS}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:ID_MAPPING_META}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:STRUCTURE}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:50,591] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
INFO  [2023-01-06 20:43:50,599] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset2
INFO  [2023-01-06 20:43:50,668] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset2
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset2
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:DATASET}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:SECONDARY_IDS}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:TABLES}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:DICTIONARIES_META}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:IMPORTS}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:CONCEPTS}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:ID_MAPPING_META}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:STRUCTURE}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:50,768] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
INFO  [2023-01-06 20:43:50,774] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast RestartTest
INFO  [2023-01-06 20:43:50,871] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow RestartTest
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of RestartTest
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:DATASET}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:SECONDARY_IDS}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:TABLES}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:DICTIONARIES_META}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:IMPORTS}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:CONCEPTS}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:ID_MAPPING_META}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:STRUCTURE}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:50,968] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
INFO  [2023-01-06 20:43:50,974] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset3
INFO  [2023-01-06 20:43:50,996] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset3
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset3
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:DATASET}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:SECONDARY_IDS}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:TABLES}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:DICTIONARIES_META}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:IMPORTS}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:CONCEPTS}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:ID_MAPPING_META}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:STRUCTURE}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:51,097] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
INFO  [2023-01-06 20:43:51,104] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset4
INFO  [2023-01-06 20:43:51,180] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset4
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset4
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:DATASET}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:SECONDARY_IDS}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:TABLES}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:DICTIONARIES_META}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:IMPORTS}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:CONCEPTS}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:ID_MAPPING_META}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:STRUCTURE}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:51,182] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
INFO  [2023-01-06 20:43:51,188] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset5
INFO  [2023-01-06 20:43:51,244] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset5
INFO  [2023-01-06 20:43:51,344] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset5
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:DATASET}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:SECONDARY_IDS}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:TABLES}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:DICTIONARIES_META}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:IMPORTS}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:CONCEPTS}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:ID_MAPPING_META}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:STRUCTURE}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:51,345] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
INFO  [2023-01-06 20:43:51,352] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset6
INFO  [2023-01-06 20:43:51,388] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset6
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset6
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:DATASET}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:SECONDARY_IDS}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:TABLES}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:DICTIONARIES_META}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:DICTIONARIES_DATA}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:IMPORTS}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:CONCEPTS}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:ID_MAPPING_META}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:ID_MAPPING_DATA}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:STRUCTURE}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:WORKER_TO_BUCKETS}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:PRIMARY_DICTIONARY}
INFO  [2023-01-06 20:43:51,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
INFO  [2023-01-06 20:43:51,496] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
INFO  [2023-01-06 20:43:51,496] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
INFO  [2023-01-06 20:43:51,502] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
INFO  [2023-01-06 20:43:51,502] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
INFO  [2023-01-06 20:43:51,513] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
INFO  [2023-01-06 20:43:51,513] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
INFO  [2023-01-06 20:43:51,522] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
INFO  [2023-01-06 20:43:51,522] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
INFO  [2023-01-06 20:43:51,528] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
INFO  [2023-01-06 20:43:51,528] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
INFO  [2023-01-06 20:43:51,536] com.bakdata.conquery.integration.tests.RestartTest: Restarting
INFO  [2023-01-06 20:43:51,536] com.bakdata.conquery.util.support.TestConquery: Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:43:51]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:DATASET}): 1 entries, 51 B within 658.6 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:DATASET}): 1 entries, 51 B within 1.691 ms
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:SECONDARY_IDS}): 0 entries, 0 B within 102.6 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:SECONDARY_IDS}): 0 entries, 0 B within 117.1 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:DATASET}): 1 entries, 49 B within 94.50 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:TABLES}): 0 entries, 0 B within 66.80 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:TABLES}): 0 entries, 0 B within 70.22 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store SECONDARY_IDS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 78.43 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:SECONDARY_IDS}): 1 entries, 70 B within 153.8 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 76.00 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:IMPORTS}): 0 entries, 0 B within 90.05 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:IMPORTS}): 0 entries, 0 B within 74.88 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:CONCEPTS}): 0 entries, 0 B within 82.81 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:TABLES}): 1 entries, 187 B within 214.3 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:CONCEPTS}): 0 entries, 0 B within 66.36 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	Done reading Dataset[label=testDataset3, name=testDataset3] / com.bakdata.conquery.io.storage.NamespaceStorage
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	Done reading Dataset[label=testDataset4, name=testDataset4] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:DATASET}): 1 entries, 51 B within 238.0 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:SECONDARY_IDS}): 0 entries, 0 B within 108.4 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:TABLES}): 0 entries, 0 B within 99.23 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 110.0 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:IMPORTS}): 0 entries, 0 B within 71.68 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:CONCEPTS}): 0 entries, 0 B within 58.70 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	Done reading Dataset[label=testDataset5, name=testDataset5] / com.bakdata.conquery.io.storage.NamespaceStorage
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:DATASET}): 1 entries, 51 B within 187.6 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:SECONDARY_IDS}): 0 entries, 0 B within 88.73 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:TABLES}): 0 entries, 0 B within 73.60 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 67.23 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:IMPORTS}): 0 entries, 0 B within 79.47 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:CONCEPTS}): 0 entries, 0 B within 81.52 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	Done reading Dataset[label=testDataset6, name=testDataset6] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 4.994 ms
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 8.346 ms
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 4.232 ms
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 8.393 ms
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:STRUCTURE}): 0 entries, 0 B within 91.23 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:STRUCTURE}): 0 entries, 0 B within 98.79 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:STRUCTURE}): 0 entries, 0 B within 87.93 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:STRUCTURE}): 0 entries, 0 B within 145.4 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:WORKER_TO_BUCKETS}): 1 entries, 12 B within 162.2 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:WORKER_TO_BUCKETS}): 1 entries, 12 B within 151.4 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:WORKER_TO_BUCKETS}): 1 entries, 12 B within 153.4 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 8.871 ms
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5:PRIMARY_DICTIONARY}): 0 entries, 0 B within 71.83 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5	DONE reading Storage
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4:PRIMARY_DICTIONARY}): 0 entries, 0 B within 67.89 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4	DONE reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:WORKER_TO_BUCKETS}): 1 entries, 12 B within 184.7 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3:PRIMARY_DICTIONARY}): 0 entries, 0 B within 95.27 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3	DONE reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6:PRIMARY_DICTIONARY}): 0 entries, 0 B within 128.9 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6	DONE reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:DATASET}): 1 entries, 51 B within 169.0 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:IMPORTS}): 1 entries, 466 B within 3.953 ms
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:SECONDARY_IDS}): 0 entries, 0 B within 95.94 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:TABLES}): 0 entries, 0 B within 56.57 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 95.33 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:IMPORTS}): 0 entries, 0 B within 101.5 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:CONCEPTS}): 0 entries, 0 B within 93.17 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	Done reading Dataset[label=testDataset2, name=testDataset2] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:DATASET}): 1 entries, 51 B within 183.5 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:SECONDARY_IDS}): 0 entries, 0 B within 71.18 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 473.0 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:TABLES}): 0 entries, 0 B within 67.65 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:STRUCTURE}): 0 entries, 0 B within 108.0 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 63.87 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:IMPORTS}): 0 entries, 0 B within 70.55 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:WORKER_TO_BUCKETS}): 1 entries, 12 B within 182.7 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:CONCEPTS}): 0 entries, 0 B within 65.59 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	Done reading Dataset[label=testDataset1, name=testDataset1] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2:PRIMARY_DICTIONARY}): 0 entries, 0 B within 171.0 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2	DONE reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 396.4 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:STRUCTURE}): 0 entries, 0 B within 61.82 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:WORKER_TO_BUCKETS}): 1 entries, 12 B within 104.3 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1:PRIMARY_DICTIONARY}): 0 entries, 0 B within 60.85 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1	DONE reading Storage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:CONCEPTS}): 1 entries, 452 B within 5.259 ms
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	Done reading Dataset[label=RestartTest, name=RestartTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 255.4 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:STRUCTURE}): 0 entries, 0 B within 29.36 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:WORKER_TO_BUCKETS}): 1 entries, 226 B within 165.9 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest:PRIMARY_DICTIONARY}): 1 entries, 97 B within 773.2 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest	DONE reading Storage
[INFO] [2023-01-06 20:43:51]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_testDataset5), NamespacedStorage(pathName=dataset_testDataset4), NamespacedStorage(pathName=dataset_testDataset3), NamespacedStorage(pathName=dataset_testDataset6), NamespacedStorage(pathName=dataset_testDataset2), NamespacedStorage(pathName=dataset_testDataset1), NamespacedStorage(pathName=dataset_RestartTest)]
[INFO] [2023-01-06 20:43:51]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:43:51]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 2 entries, 328 B within 1.949 ms
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 1 entries, 150 B within 488.2 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 1 entries, 203 B within 458.0 μs
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 1 entries, 523 B within 12.36 ms
[DEBUG] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:51]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 88.67 μs
[INFO] [2023-01-06 20:43:51]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@574d6455
[DEBUG] [2023-01-06 20:43:51]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:43:51]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:43:51]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_15
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_16
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_17
[INFO] [2023-01-06 20:43:51]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:43:51]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:43:51]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:43:51]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:DATASET}): 1 entries, 51 B within 191.4 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:SECONDARY_IDS}): 0 entries, 0 B within 55.68 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:TABLES}): 0 entries, 0 B within 40.63 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 51.36 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:IMPORTS}): 0 entries, 0 B within 34.71 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:CONCEPTS}): 0 entries, 0 B within 33.86 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	Done reading Dataset[label=testDataset1, name=testDataset1] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:WORKER}): 1 entries, 125 B within 130.6 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:BUCKETS}): 0 entries, 0 B within 41.02 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd:C_BLOCKS}): 0 entries, 0 B within 37.42 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:DATASET}): 1 entries, 51 B within 183.2 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:SECONDARY_IDS}): 0 entries, 0 B within 60.35 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:TABLES}): 0 entries, 0 B within 31.27 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 31.48 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:IMPORTS}): 0 entries, 0 B within 27.12 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:CONCEPTS}): 0 entries, 0 B within 33.30 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	Done reading Dataset[label=testDataset4, name=testDataset4] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:WORKER}): 1 entries, 125 B within 94.03 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:BUCKETS}): 0 entries, 0 B within 30.90 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390:C_BLOCKS}): 0 entries, 0 B within 29.21 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:DATASET}): 1 entries, 51 B within 230.3 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:SECONDARY_IDS}): 0 entries, 0 B within 62.81 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:TABLES}): 0 entries, 0 B within 47.74 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 73.80 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:IMPORTS}): 0 entries, 0 B within 56.49 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:CONCEPTS}): 0 entries, 0 B within 79.76 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	Done reading Dataset[label=testDataset2, name=testDataset2] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:WORKER}): 1 entries, 125 B within 163.8 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:BUCKETS}): 0 entries, 0 B within 58.98 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24:C_BLOCKS}): 0 entries, 0 B within 57.12 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:DATASET}): 1 entries, 51 B within 117.2 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:SECONDARY_IDS}): 0 entries, 0 B within 31.69 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:TABLES}): 0 entries, 0 B within 24.57 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 24.30 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:IMPORTS}): 0 entries, 0 B within 20.87 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:CONCEPTS}): 0 entries, 0 B within 21.01 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	Done reading Dataset[label=testDataset6, name=testDataset6] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:WORKER}): 1 entries, 125 B within 79.24 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:BUCKETS}): 0 entries, 0 B within 23.41 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04:C_BLOCKS}): 0 entries, 0 B within 20.27 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:DATASET}): 1 entries, 51 B within 155.9 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:SECONDARY_IDS}): 0 entries, 0 B within 52.64 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:TABLES}): 0 entries, 0 B within 43.01 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.00 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:IMPORTS}): 0 entries, 0 B within 39.11 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:CONCEPTS}): 0 entries, 0 B within 39.07 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	Done reading Dataset[label=testDataset4, name=testDataset4] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:WORKER}): 1 entries, 125 B within 113.5 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:BUCKETS}): 0 entries, 0 B within 30.61 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d:C_BLOCKS}): 0 entries, 0 B within 22.79 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:DATASET}): 1 entries, 51 B within 176.2 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:SECONDARY_IDS}): 0 entries, 0 B within 51.00 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:TABLES}): 0 entries, 0 B within 38.69 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 50.08 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:IMPORTS}): 0 entries, 0 B within 31.61 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:CONCEPTS}): 0 entries, 0 B within 39.78 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	Done reading Dataset[label=testDataset5, name=testDataset5] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:WORKER}): 1 entries, 125 B within 104.8 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:BUCKETS}): 0 entries, 0 B within 37.98 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8:C_BLOCKS}): 0 entries, 0 B within 31.21 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:DATASET}): 1 entries, 51 B within 172.1 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:SECONDARY_IDS}): 0 entries, 0 B within 57.47 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:TABLES}): 0 entries, 0 B within 40.39 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 41.26 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:IMPORTS}): 0 entries, 0 B within 37.54 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:CONCEPTS}): 0 entries, 0 B within 35.74 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	Done reading Dataset[label=testDataset6, name=testDataset6] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:WORKER}): 1 entries, 125 B within 111.5 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:BUCKETS}): 0 entries, 0 B within 68.90 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3:C_BLOCKS}): 0 entries, 0 B within 36.90 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:DATASET}): 1 entries, 49 B within 155.3 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store SECONDARY_IDS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:SECONDARY_IDS}): 1 entries, 70 B within 135.5 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:TABLES}): 1 entries, 187 B within 156.5 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 850.7 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:IMPORTS}): 1 entries, 466 B within 3.918 ms
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:DATASET}): 1 entries, 51 B within 213.7 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:SECONDARY_IDS}): 0 entries, 0 B within 72.75 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:TABLES}): 0 entries, 0 B within 57.06 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 62.72 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:IMPORTS}): 0 entries, 0 B within 55.53 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:CONCEPTS}): 0 entries, 0 B within 52.83 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	Done reading Dataset[label=testDataset5, name=testDataset5] / com.bakdata.conquery.io.storage.WorkerStorage
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:CONCEPTS}): 1 entries, 452 B within 6.261 ms
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	Done reading Dataset[label=RestartTest, name=RestartTest] / com.bakdata.conquery.io.storage.WorkerStorage
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:WORKER}): 1 entries, 125 B within 173.1 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:BUCKETS}): 0 entries, 0 B within 73.18 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:WORKER}): 1 entries, 124 B within 144.0 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64:C_BLOCKS}): 0 entries, 0 B within 52.72 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:BUCKETS}): 1 entries, 346 B within 1.782 ms
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16:C_BLOCKS}): 1 entries, 213 B within 203.0 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:DATASET}): 1 entries, 51 B within 173.9 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:SECONDARY_IDS}): 0 entries, 0 B within 76.19 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:TABLES}): 0 entries, 0 B within 77.08 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 79.86 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:IMPORTS}): 0 entries, 0 B within 69.02 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:CONCEPTS}): 0 entries, 0 B within 67.46 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	Done reading Dataset[label=testDataset3, name=testDataset3] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:WORKER}): 1 entries, 125 B within 160.0 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:BUCKETS}): 0 entries, 0 B within 72.07 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414:C_BLOCKS}): 0 entries, 0 B within 74.92 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:DATASET}): 1 entries, 51 B within 257.2 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:SECONDARY_IDS}): 0 entries, 0 B within 88.43 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:TABLES}): 0 entries, 0 B within 70.05 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 77.01 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:IMPORTS}): 0 entries, 0 B within 68.62 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:CONCEPTS}): 0 entries, 0 B within 65.66 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	Done reading Dataset[label=testDataset3, name=testDataset3] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:WORKER}): 1 entries, 125 B within 175.8 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:BUCKETS}): 0 entries, 0 B within 73.76 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051:C_BLOCKS}): 0 entries, 0 B within 67.03 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:DATASET}): 1 entries, 49 B within 138.5 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store SECONDARY_IDS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:SECONDARY_IDS}): 1 entries, 70 B within 109.5 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:TABLES}): 1 entries, 187 B within 145.2 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:DATASET}): 1 entries, 51 B within 104.5 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 674.8 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:SECONDARY_IDS}): 0 entries, 0 B within 45.09 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:TABLES}): 0 entries, 0 B within 57.58 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 36.35 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:IMPORTS}): 0 entries, 0 B within 33.71 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:CONCEPTS}): 0 entries, 0 B within 28.08 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	Done reading Dataset[label=testDataset2, name=testDataset2] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:WORKER}): 1 entries, 125 B within 85.18 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:BUCKETS}): 0 entries, 0 B within 29.41 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e:C_BLOCKS}): 0 entries, 0 B within 29.13 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e	DONE reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:IMPORTS}): 1 entries, 466 B within 3.248 ms
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:CONCEPTS}): 1 entries, 452 B within 4.725 ms
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	Done reading Dataset[label=RestartTest, name=RestartTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:WORKER}): 1 entries, 124 B within 99.69 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:BUCKETS}): 1 entries, 358 B within 1.428 ms
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5:C_BLOCKS}): 1 entries, 213 B within 195.9 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5	DONE reading Storage
[INFO] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd)), WorkerStorage(worker=NamedImpl(name=worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24)), WorkerStorage(worker=NamedImpl(name=worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d)), WorkerStorage(worker=NamedImpl(name=worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3)), WorkerStorage(worker=NamedImpl(name=worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64)), WorkerStorage(worker=NamedImpl(name=worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051)), WorkerStorage(worker=NamedImpl(name=worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5))]
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 7
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	BEGIN reading Storage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:DATASET}): 1 entries, 51 B within 238.8 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:SECONDARY_IDS}): 0 entries, 0 B within 66.61 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:TABLES}): 0 entries, 0 B within 54.42 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 58.31 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:IMPORTS}): 0 entries, 0 B within 71.52 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:CONCEPTS}): 0 entries, 0 B within 78.79 μs
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	Done reading Dataset[label=testDataset1, name=testDataset1] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:WORKER}): 1 entries, 125 B within 158.8 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:BUCKETS}): 0 entries, 0 B within 69.28 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25:C_BLOCKS}): 0 entries, 0 B within 81.21 μs
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25	DONE reading Storage
[INFO] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390)), WorkerStorage(worker=NamedImpl(name=worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04)), WorkerStorage(worker=NamedImpl(name=worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8)), WorkerStorage(worker=NamedImpl(name=worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16)), WorkerStorage(worker=NamedImpl(name=worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414)), WorkerStorage(worker=NamedImpl(name=worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e)), WorkerStorage(worker=NamedImpl(name=worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25))]
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 7
[DEBUG] [2023-01-06 20:43:52]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:52260 connected, waiting for identity
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:52262 connected, waiting for identity
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52260	Sending worker identity 'worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d'
[INFO] [2023-01-06 20:43:52]	c.b.c.c.ShardNode	/127.0.0.1:52262	Sending worker identity 'worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25'
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:52260` registered.
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:52262` registered.
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Imports of worker testDataset3.worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Buckets of worker testDataset3.worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Imports of worker RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16 are consistent with the manager: 1 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Buckets of worker RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16 are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Imports of worker testDataset6.worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Buckets of worker testDataset6.worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Imports of worker RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5 are consistent with the manager: 1 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Buckets of worker RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5 are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Imports of worker testDataset2.worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Buckets of worker testDataset2.worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Imports of worker testDataset6.worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Buckets of worker testDataset6.worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Imports of worker testDataset5.worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Buckets of worker testDataset5.worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Imports of worker testDataset4.worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Buckets of worker testDataset4.worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Imports of worker testDataset1.worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Buckets of worker testDataset1.worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Imports of worker testDataset5.worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Buckets of worker testDataset5.worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Imports of worker testDataset2.worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Buckets of worker testDataset2.worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Imports of worker testDataset4.worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Buckets of worker testDataset4.worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Imports of worker testDataset3.worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Buckets of worker testDataset3.worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Consistency check was successful
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Imports of worker testDataset1.worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Buckets of worker testDataset1.worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Consistency check was successful
[WARN] [2023-01-06 20:43:52]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:52]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:43:52]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:43:52]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:43:52]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-06 20:43:52]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest6175202265804509010/tmp_RestartTest for Support
[INFO] [2023-01-06 20:43:52]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:43:52]	c.b.c.i.t.RestartTest		Restart complete
[INFO] [2023-01-06 20:43:52]	c.b.c.i.j.AbstractQueryEngineTest		SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
[INFO] [2023-01-06 20:43:52]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[RestartTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:52]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[195bd00c-afc4-448b-bcfb-9e64d4467fb4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest))]]
127.0.0.1 - - [06/Jan/2023:20:43:52 +0000] "POST /api/datasets/RestartTest/queries HTTP/1.1" 201 1117 "-" "Conquery (test client)" 25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ExecuteQuery	Worker[RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16, /127.0.0.1:52262]	Started ConceptQuery RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.q.QueryExecutor	Worker[RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16, /127.0.0.1:52262]	QueryPlan for Query[RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = RestartTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=connector, name=connector], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.ExecuteQuery	Worker[RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5, /127.0.0.1:52260]	Started ConceptQuery RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4
[INFO] [2023-01-06 20:43:52]	c.b.c.m.q.r.ShardResult		FINISHED Query[RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4] with 0 results within PT0.001953S
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.q.QueryExecutor	Worker[RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5, /127.0.0.1:52260]	QueryPlan for Query[RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = RestartTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=connector, name=connector], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:43:52]	c.b.c.m.q.r.ShardResult		FINISHED Query[RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4] with 2 results within PT0.001214S
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=RestartTest, name=RestartTest]	Received ShardResult(queryId=RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4, workerId=RestartTest.worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16, startTime=2023-01-06T20:43:52.675564, finishTime=2023-01-06T20:43:52.677517) of size 0
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.q.ManagedQuery	Dataset[label=RestartTest, name=RestartTest]	Received Result[size=0] for Query[RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4]
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=RestartTest, name=RestartTest]	Received ShardResult(queryId=RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4, workerId=RestartTest.worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5, startTime=2023-01-06T20:43:52.677385, finishTime=2023-01-06T20:43:52.678599) of size 2
[DEBUG] [2023-01-06 20:43:52]	c.b.c.m.q.ManagedQuery	Dataset[label=RestartTest, name=RestartTest]	Received Result[size=2] for Query[RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4]
[INFO] [2023-01-06 20:43:52]	c.b.c.m.e.ManagedExecution	Dataset[label=RestartTest, name=RestartTest]	DONE 195bd00c-afc4-448b-bcfb-9e64d4467fb4 ManagedQuery within PT0.017114S
127.0.0.1 - - [06/Jan/2023:20:43:52 +0000] "GET /api/datasets/RestartTest/queries/RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4 HTTP/1.1" 200 1353 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:43:52]	c.b.c.r.a.ResultCsvResource	user.SUPERUSER@SUPERUSER	Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=RestartTest, name=RestartTest], queryId=195bd00c-afc4-448b-bcfb-9e64d4467fb4, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:43:52.659575, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@13c7ff4b[Count = 0], startTime=2023-01-06T20:43:52.664314, finishTime=2023-01-06T20:43:52.681428, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5f019bcc), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@768d3a19], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@24520b3d], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@8c3955b, com.bakdata.conquery.models.query.ColumnDescriptor@5b4fa074]) download on dataset Dataset[label=RestartTest, name=RestartTest] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
[INFO] [2023-01-06 20:43:52]	c.b.c.i.r.c.ResultCsvProcessor	SUPERUSER@SUPERUSER	Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=RestartTest, name=RestartTest], queryId=195bd00c-afc4-448b-bcfb-9e64d4467fb4, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:43:52.659575, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@13c7ff4b[Count = 0], startTime=2023-01-06T20:43:52.664314, finishTime=2023-01-06T20:43:52.681428, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5f019bcc), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@768d3a19], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@24520b3d], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@8c3955b, com.bakdata.conquery.models.query.ColumnDescriptor@5b4fa074]) on dataset Dataset[label=RestartTest, name=RestartTest]
127.0.0.1 - - [06/Jan/2023:20:43:52 +0000] "GET /api/datasets/RestartTest/result/RestartTest.195bd00c-afc4-448b-bcfb-9e64d4467fb4.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 44
[INFO] [2023-01-06 20:43:52]	c.b.c.i.j.AbstractQueryEngineTest		INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset1, name=testDataset1]
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset1
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset1, name=testDataset1]
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset1
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[INFO] [2023-01-06 20:43:52]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[INFO] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset1
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset2
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[INFO] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset1_a986958b-edbb-4e3f-bc6a-c7689b786d25
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[DEBUG] [2023-01-06 20:43:52]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[INFO] [2023-01-06 20:43:52]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset1_fd1162a9-904e-4825-9ad7-325fbdf088bd
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset2, name=testDataset2]
[INFO] [2023-01-06 20:43:52]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset2, name=testDataset2]
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[INFO] [2023-01-06 20:43:52]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset2
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[INFO] [2023-01-06 20:43:53]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset2
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset2_1950364f-66fb-45a7-b1a6-c6afd85baf24
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset2_f84e5772-3df7-4657-8454-917e65c4f38e
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset3, name=testDataset3]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset3, name=testDataset3]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[INFO] [2023-01-06 20:43:53]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset3_81e925c5-9358-490d-97ed-86b4034e9051
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset3_2c32002c-d30e-4d23-b2c1-ddb561e72414
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset4, name=testDataset4]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset4, name=testDataset4]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset4
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[INFO] [2023-01-06 20:43:53]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset4
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset4_36446cbd-8a22-4951-8481-90ee8d1c1390
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset4_45c707be-a178-433b-99b7-4b5d2522d63d
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset5, name=testDataset5]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset5, name=testDataset5]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset5
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset5_7a0ad388-d5be-4512-ae22-29d2292c2a64
[INFO] [2023-01-06 20:43:53]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset5
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset6
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset6, name=testDataset6]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset5_6f754bf8-8dc5-4896-9f61-3e502d0f84c8
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset6, name=testDataset6]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_testDataset6_521a7891-475f-48a9-ac0c-a19ee53c19d3
[INFO] [2023-01-06 20:43:53]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_testDataset6
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor		Closing Job Manager fast RestartTest
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RestartTest, name=RestartTest]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[DEBUG] [2023-01-06 20:43:53]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[INFO] [2023-01-06 20:43:53]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_testDataset6_0f7db983-c926-4aa8-9e8d-d816fe5a9f04
[INFO] [2023-01-06 20:43:53]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RestartTest, name=RestartTest]
[INFO] [2023-01-06 20:43:53]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[INFO] [2023-01-06 20:43:54]	c.b.c.m.j.JobExecutor		Closing Job Manager slow RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[INFO] [2023-01-06 20:43:54]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[INFO] [2023-01-06 20:43:54]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RestartTest_e505d457-c7ba-4cc7-9348-f12180dfc3c5
[INFO] [2023-01-06 20:43:54]	c.b.c.m.w.Namespace		Removing namespace storage of RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[INFO] [2023-01-06 20:43:54]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RestartTest
[INFO] [2023-01-06 20:43:54]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[INFO] [2023-01-06 20:43:54]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RestartTest_e09aa65f-bb17-45f4-aef6-ac5f0a348d16
[INFO] [2023-01-06 20:43:54]	c.b.c.i.IntegrationTest$Wrapper	RestartTest	SUCCESS integration test RestartTest
[INFO] [2023-01-06 20:43:54]	c.b.c.i.IntegrationTest$Wrapper	ReusedQueryTest	STARTING integration test ReusedQueryTest
[INFO] [2023-01-06 20:43:54]	c.b.c.u.s.TestConquery	ReusedQueryTest	Setting up dataset
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:DATASET}): 0 entries, 0 B within 145.7 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:SECONDARY_IDS}): 0 entries, 0 B within 67.52 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:TABLES}): 0 entries, 0 B within 61.53 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 64.27 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:IMPORTS}): 0 entries, 0 B within 59.44 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:CONCEPTS}): 0 entries, 0 B within 57.51 μs
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.NamespacedStorage	ReusedQueryTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 62.03 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:STRUCTURE}): 0 entries, 0 B within 64.88 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 68.90 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 57.52 μs
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:DATASET}): 0 entries, 0 B within 104.6 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:SECONDARY_IDS}): 0 entries, 0 B within 33.49 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:TABLES}): 0 entries, 0 B within 26.87 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 33.72 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:IMPORTS}): 0 entries, 0 B within 23.93 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:CONCEPTS}): 0 entries, 0 B within 27.31 μs
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:WORKER}): 0 entries, 0 B within 27.25 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:BUCKETS}): 0 entries, 0 B within 21.62 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6:C_BLOCKS}): 0 entries, 0 B within 21.50 μs
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Imports of worker ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Buckets of worker ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Consistency check was successful
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:DATASET}): 0 entries, 0 B within 88.55 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:SECONDARY_IDS}): 0 entries, 0 B within 36.91 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:TABLES}): 0 entries, 0 B within 31.82 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 26.85 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:IMPORTS}): 0 entries, 0 B within 22.79 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:CONCEPTS}): 0 entries, 0 B within 22.32 μs
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:WORKER}): 0 entries, 0 B within 21.83 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:BUCKETS}): 0 entries, 0 B within 21.41 μs
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:54]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747:C_BLOCKS}): 0 entries, 0 B within 22.63 μs
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Imports of worker ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Buckets of worker ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:54]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:54]	c.b.c.r.a.r.AdminDatasetProcessor	ReusedQueryTest	Received new SecondaryId[ReusedQueryTest.secondary]
[INFO] [2023-01-06 20:43:54]	c.b.c.r.a.r.AdminDatasetProcessor	ReusedQueryTest	Received new SecondaryId[ReusedQueryTest.ignored]
[INFO] [2023-01-06 20:43:54]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received update of SecondaryId ReusedQueryTest.secondary
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received update of SecondaryId ReusedQueryTest.secondary
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received update of SecondaryId ReusedQueryTest.ignored
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received update of SecondaryId ReusedQueryTest.ignored
[INFO] [2023-01-06 20:43:54]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received update of Table ReusedQueryTest.table
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received update of Table ReusedQueryTest.table
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received update of Table ReusedQueryTest.table2
[INFO] [2023-01-06 20:43:54]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received update of Table ReusedQueryTest.table2
[INFO] [2023-01-06 20:43:54]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Updating Concept[ReusedQueryTest.concept]
[DEBUG] [2023-01-06 20:43:54]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Updating Concept[ReusedQueryTest.concept]
[INFO] [2023-01-06 20:43:54]	c.b.c.c.PreprocessorCommand	ReusedQueryTest	Preprocessing from command line config.
[INFO] [2023-01-06 20:43:54]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:43:54]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:43:54]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)	Required to preprocess 465 B in total
[INFO] [2023-01-06 20:43:54]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)
██████████████████████████▌                       ▌  53%	est. time remaining: 0.063550107s[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[ignored] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=a, suffix=a)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn			ignored: StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=a, suffix=a) -> StringTypeSingleton(singleValue=a)
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.StringParser		Reduced strings by the 'f_' prefix and '' suffix
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.StringParser			Chosen encoding is Base16LowerCase
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.StringParser			Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@4972be90(est. 100 B)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn			sid: StringParser(super=Parser(lines=6, nullLines=1), encoding=Base16LowerCase, prefix=f_, suffix=) -> StringTypePrefixSuffix(subType=StringTypeEncoded(encoding=Base16LowerCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=4], numberType=ByteArrayStore())), prefix=f_, suffix=)
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.RealParser		Max ULP = 1.1920928955078125E-7
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn			value: RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7) -> DoubleArrayStore()
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@603591d9), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@2f987ef5), dateReader=com.bakdata.conquery.util.DateReader@1365e37c, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table, name=table]:table[0/content.csv]		datum: DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@603591d9), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@2f987ef5), dateReader=com.bakdata.conquery.util.DateReader@1365e37c, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false) -> DateRangeTypeDateRange(minStore=IntegerDateStore(store=ShortArrayStore()), maxStore=IntegerDateStore(store=ShortArrayStore()))
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing Headers
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing header
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing data
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=table, name=table]:table[0/content.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.001319201s[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.StringParser		Reduced strings by the 'f_' prefix and '' suffix
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@47b549ce), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@438ad5e0), dateReader=com.bakdata.conquery.util.DateReader@68843f5, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.RealParser	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Max ULP = 9.5367431640625E-7
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.StringParser			Chosen encoding is Base16LowerCase
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn			datum: DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@47b549ce), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@438ad5e0), dateReader=com.bakdata.conquery.util.DateReader@68843f5, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false) -> DateRangeTypeDateRange(minStore=IntegerDateStore(store=ShortArrayStore()), maxStore=IntegerDateStore(store=ShortArrayStore()))
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]		value: RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7) -> DoubleArrayStore()
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.p.s.StringParser			Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@5c64d1cb(est. 83 B)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.PPColumn			sid: StringParser(super=Parser(lines=6, nullLines=1), encoding=Base16LowerCase, prefix=f_, suffix=) -> StringTypePrefixSuffix(subType=StringTypeEncoded(encoding=Base16LowerCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=3], numberType=ByteArrayStore())), prefix=f_, suffix=)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing data
[INFO] [2023-01-06 20:43:55]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:43:55]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-06 20:43:55]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:43:55]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing table into ReusedQueryTest.table
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /admin/datasets/ReusedQueryTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ReusedQueryTest%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.w.Namespace	Job Manager slow ReusedQueryTest	Assigning Bucket[0] to Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received new WorkerInformation(size = 0,dataset = ReusedQueryTest)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received new WorkerInformation(size = 1,dataset = ReusedQueryTest)
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:43:55]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received Dictionary[ReusedQueryTest.table#ReusedQueryTest$2etable$2esid] of size 4.
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received Dictionary[ReusedQueryTest.table#ReusedQueryTest$2etable$2esid] of size 4.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Start sending 1 Buckets
[WARN] [2023-01-06 20:43:55]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	One or more Children are not done yet
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received Import[ReusedQueryTest.table.table], containing 6 entries.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received Import[ReusedQueryTest.table.table], containing 6 entries.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ImportBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received ReusedQueryTest.table.table.0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Adding Bucket[ReusedQueryTest.table.table.0]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.WorkerStorage		Adding CBlock[ReusedQueryTest.table.table.0.ReusedQueryTest.concept.connector1]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing table2 into ReusedQueryTest.table2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Mapped 0 new ids
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Updating bucket assignments.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /admin/datasets/ReusedQueryTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ReusedQueryTest%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received new WorkerInformation(size = 1,dataset = ReusedQueryTest)
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received new WorkerInformation(size = 0,dataset = ReusedQueryTest)
[INFO] [2023-01-06 20:43:55]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:43:55]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received Dictionary[ReusedQueryTest.table2#ReusedQueryTest$2etable2$2esid] of size 3.
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received Dictionary[ReusedQueryTest.table2#ReusedQueryTest$2etable2$2esid] of size 3.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Start sending 1 Buckets
[WARN] [2023-01-06 20:43:55]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	One or more Children are not done yet
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received Import[ReusedQueryTest.table2.table2], containing 6 entries.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Received Import[ReusedQueryTest.table2.table2], containing 6 entries.
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ImportBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Received ReusedQueryTest.table2.table2.0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Adding Bucket[ReusedQueryTest.table2.table2.0]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.WorkerStorage		Adding CBlock[ReusedQueryTest.table2.table2.0.ReusedQueryTest.concept.connector2]
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[f6ae9fe5-313c-4a19-87f0-13634548b1b1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1594 "-" "Conquery (test client)" 17
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@27cfee4c`
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@5eb3bf60`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 0 results within PT0.006178S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.344134, finishTime=2023-01-06T20:43:55.350312) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 2 results within PT0.009726S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.344170, finishTime=2023-01-06T20:43:55.353896) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE f6ae9fe5-313c-4a19-87f0-13634548b1b1 ManagedQuery within PT0.026366S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1 HTTP/1.1" 200 1846 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[1034713a-645a-48c5-a447-061aec2e3968] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started ConceptQuery ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started ConceptQuery ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1014 "-" "Conquery (test client)" 11
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968] with 0 results within PT0.001537S
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.419966, finishTime=2023-01-06T20:43:55.421503) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968] with 2 results within PT0.003362S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.420327, finishTime=2023-01-06T20:43:55.423689) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 1034713a-645a-48c5-a447-061aec2e3968 ManagedQuery within PT0.007999S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.1034713a-645a-48c5-a447-061aec2e3968 HTTP/1.1" 200 1265 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	User[user.SUPERUSER@SUPERUSER] reexecuted Query[ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ReusedQueryTest], queryId=f6ae9fe5-313c-4a19-87f0-13634548b1b1, label=concept	@§$, creationTime=2023-01-06T20:43:55.330356, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6613c58c[Count = 0], startTime=2023-01-06T20:43:55.332769, finishTime=2023-01-06T20:43:55.359135, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b0ff645), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@768d3a19], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@24520b3d], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1e39b124, com.bakdata.conquery.models.query.ColumnDescriptor@6ae02301, com.bakdata.conquery.models.query.ColumnDescriptor@1227b474])]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[f6ae9fe5-313c-4a19-87f0-13634548b1b1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@24be02df`
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@557a868c`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 0 results within PT0.000391S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1/reexecute HTTP/1.1" 200 1615 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.465650, finishTime=2023-01-06T20:43:55.466041) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 2 results within PT0.002765S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.465182, finishTime=2023-01-06T20:43:55.467947) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE f6ae9fe5-313c-4a19-87f0-13634548b1b1 ManagedQuery within PT0.00906S
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[f6ae9fe5-313c-4a19-87f0-13634548b1b1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@318484c3`
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@6be8fa24`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 0 results within PT0.000443S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.489802, finishTime=2023-01-06T20:43:55.490245) of size 0
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1614 "-" "Conquery (test client)" 6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 2 results within PT0.002273S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.489811, finishTime=2023-01-06T20:43:55.492084) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE f6ae9fe5-313c-4a19-87f0-13634548b1b1 ManagedQuery within PT0.005515S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1 HTTP/1.1" 200 1845 "-" "Conquery (test client)" 3
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[55ae38b5-ed1c-480c-b4ce-f4149467e57b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1608 "-" "Conquery (test client)" 8
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@46f3df15`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b] with 0 results within PT0.000702S
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.528213, finishTime=2023-01-06T20:43:55.528915) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@46c1c66b`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b] with 1 results within PT0.003892S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.530296, finishTime=2023-01-06T20:43:55.534188) of size 1
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=1] for Query[ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 55ae38b5-ed1c-480c-b4ce-f4149467e57b ManagedQuery within PT0.010573S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.55ae38b5-ed1c-480c-b4ce-f4149467e57b HTTP/1.1" 200 1859 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[f6ae9fe5-313c-4a19-87f0-13634548b1b1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@6a7db30e`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 0 results within PT0.000428S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1613 "-" "Conquery (test client)" 4
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@2c61329`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.571874, finishTime=2023-01-06T20:43:55.572302) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 2 results within PT0.007534S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.572015, finishTime=2023-01-06T20:43:55.579549) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE f6ae9fe5-313c-4a19-87f0-13634548b1b1 ManagedQuery within PT0.010124S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1 HTTP/1.1" 200 1845 "-" "Conquery (test client)" 2
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[f6ae9fe5-313c-4a19-87f0-13634548b1b1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@2e091e4d`
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@632a1136`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 0 results within PT0.00034S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.606447, finishTime=2023-01-06T20:43:55.606787) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1] with 2 results within PT0.001872S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1614 "-" "Conquery (test client)" 7
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.606315, finishTime=2023-01-06T20:43:55.608187) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE f6ae9fe5-313c-4a19-87f0-13634548b1b1 ManagedQuery within PT0.004432S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.f6ae9fe5-313c-4a19-87f0-13634548b1b1 HTTP/1.1" 200 1845 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[ec702267-a110-4f5c-8cf5-ffbafcd4d4f3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started ConceptQuery ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started ConceptQuery ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3] with 0 results within PT0.000698S
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1014 "-" "Conquery (test client)" 6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.640219, finishTime=2023-01-06T20:43:55.640917) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3] with 2 results within PT0.001976S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.640338, finishTime=2023-01-06T20:43:55.642314) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE ec702267-a110-4f5c-8cf5-ffbafcd4d4f3 ManagedQuery within PT0.005755S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.ec702267-a110-4f5c-8cf5-ffbafcd4d4f3 HTTP/1.1" 200 1265 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[8506ccde-15dd-4e3d-b41b-33e144823aa4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@452bf349`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4] with 0 results within PT0.000388S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.691726, finishTime=2023-01-06T20:43:55.692114) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4]
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@58c4083d`
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1182 "-" "Conquery (test client)" 25
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4] with 2 results within PT0.003048S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.692614, finishTime=2023-01-06T20:43:55.695662) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 8506ccde-15dd-4e3d-b41b-33e144823aa4 ManagedQuery within PT0.008724S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.8506ccde-15dd-4e3d-b41b-33e144823aa4 HTTP/1.1" 200 1433 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:43:55]	c.b.c.a.QueryProcessor	user.shareholder	Query posted on Dataset[ReusedQueryTest] by User[{user.shareholder].
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.ExecutionManager	user.shareholder	Executing Query[a6c74421-d8b5-4128-ac42-8c86e85bb339] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Started SecondaryIdQuery ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339
[WARN] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	Entities for query are empty
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	QueryPlan for Query[ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@66e65516`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	Started SecondaryIdQuery ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, /127.0.0.1:52260]	FINISHED Query[ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339] with 0 results within PT0.000387S
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, /127.0.0.1:52262]	QueryPlan for Query[ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@4146d172`
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339, workerId=ReusedQueryTest.worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6, startTime=2023-01-06T20:43:55.738397, finishTime=2023-01-06T20:43:55.738784) of size 0
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339]
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 953 "-" "Conquery (test client)" 9
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:55]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339] with 2 results within PT0.003729S
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339, workerId=ReusedQueryTest.worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747, startTime=2023-01-06T20:43:55.738778, finishTime=2023-01-06T20:43:55.742507) of size 2
[DEBUG] [2023-01-06 20:43:55]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE a6c74421-d8b5-4128-ac42-8c86e85bb339 ManagedQuery within PT0.007304S
127.0.0.1 - - [06/Jan/2023:20:43:55 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.a6c74421-d8b5-4128-ac42-8c86e85bb339 HTTP/1.1" 200 967 "-" "Conquery (test client)" 3
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ReusedQueryTest
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[INFO] [2023-01-06 20:43:55]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ReusedQueryTest
[INFO] [2023-01-06 20:43:55]	c.b.c.m.w.Namespace		Removing namespace storage of ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[INFO] [2023-01-06 20:43:55]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[INFO] [2023-01-06 20:43:55]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ReusedQueryTest_8927962f-2981-468c-bcc4-6a3c92444747
[INFO] [2023-01-06 20:43:55]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[DEBUG] [2023-01-06 20:43:55]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[INFO] [2023-01-06 20:43:55]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ReusedQueryTest_6d40d21e-6f6d-4ff7-aeec-404573c4b4a6
[INFO] [2023-01-06 20:43:56]	c.b.c.i.IntegrationTest$Wrapper	ReusedQueryTest	SUCCESS integration test ReusedQueryTest
[INFO] [2023-01-06 20:43:56]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingOnGroupTest	STARTING integration test RoleHandlingOnGroupTest
[INFO] [2023-01-06 20:43:56]	c.b.c.u.s.TestConquery	RoleHandlingOnGroupTest	Setting up dataset
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:DATASET}): 0 entries, 0 B within 177.4 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:SECONDARY_IDS}): 0 entries, 0 B within 80.47 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:TABLES}): 0 entries, 0 B within 52.89 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 58.50 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:IMPORTS}): 0 entries, 0 B within 50.79 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:CONCEPTS}): 0 entries, 0 B within 49.30 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.NamespacedStorage	RoleHandlingOnGroupTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 52.67 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:STRUCTURE}): 0 entries, 0 B within 48.90 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 50.31 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 48.40 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:DATASET}): 0 entries, 0 B within 100.6 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:SECONDARY_IDS}): 0 entries, 0 B within 33.85 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:TABLES}): 0 entries, 0 B within 35.20 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.57 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:IMPORTS}): 0 entries, 0 B within 91.80 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:CONCEPTS}): 0 entries, 0 B within 30.14 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:WORKER}): 0 entries, 0 B within 33.79 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:BUCKETS}): 0 entries, 0 B within 31.81 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467:C_BLOCKS}): 0 entries, 0 B within 31.32 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:DATASET}): 0 entries, 0 B within 71.40 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:SECONDARY_IDS}): 0 entries, 0 B within 32.54 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:TABLES}): 0 entries, 0 B within 30.18 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 46.68 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:IMPORTS}): 0 entries, 0 B within 30.37 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:CONCEPTS}): 0 entries, 0 B within 24.82 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:WORKER}): 0 entries, 0 B within 25.82 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:BUCKETS}): 0 entries, 0 B within 24.66 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d:C_BLOCKS}): 0 entries, 0 B within 24.20 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Imports of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Buckets of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Imports of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Buckets of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:56]	c.b.c.u.s.TestConquery	RoleHandlingOnGroupTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:56]	c.b.c.m.a.AuthorizationController	RoleHandlingOnGroupTest	Security manager registered
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.MetaStorage	RoleHandlingOnGroupTest	Remove User = user.user
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	RoleHandlingOnGroupTest	Closing Job Manager fast RoleHandlingOnGroupTest
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	RoleHandlingOnGroupTest	Closing Job Manager slow RoleHandlingOnGroupTest
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[INFO] [2023-01-06 20:43:56]	c.b.c.m.w.Namespace	RoleHandlingOnGroupTest	Removing namespace storage of RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[INFO] [2023-01-06 20:43:56]	c.b.c.m.c.XodusStoreFactory	RoleHandlingOnGroupTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingOnGroupTest
[INFO] [2023-01-06 20:43:56]	c.b.c.u.s.TestConquery	RoleHandlingOnGroupTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[INFO] [2023-01-06 20:43:56]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingOnGroupTest_b5cb7571-96a6-4e9b-87d6-7e4ed84c5467
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[INFO] [2023-01-06 20:43:56]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingOnGroupTest_b680415f-c5f8-43f7-b68a-7a5211205c4d
[INFO] [2023-01-06 20:43:56]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingOnGroupTest	SUCCESS integration test RoleHandlingOnGroupTest
[INFO] [2023-01-06 20:43:56]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingTest	STARTING integration test RoleHandlingTest
[INFO] [2023-01-06 20:43:56]	c.b.c.u.s.TestConquery	RoleHandlingTest	Setting up dataset
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:DATASET}): 0 entries, 0 B within 166.0 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 110.7 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:TABLES}): 0 entries, 0 B within 130.1 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 86.65 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:IMPORTS}): 0 entries, 0 B within 74.39 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:CONCEPTS}): 0 entries, 0 B within 70.75 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.NamespacedStorage	RoleHandlingTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 75.98 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:STRUCTURE}): 0 entries, 0 B within 69.03 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 69.75 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 67.55 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:DATASET}): 0 entries, 0 B within 110.1 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:SECONDARY_IDS}): 0 entries, 0 B within 24.06 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:TABLES}): 0 entries, 0 B within 17.98 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.86 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:IMPORTS}): 0 entries, 0 B within 18.99 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:CONCEPTS}): 0 entries, 0 B within 19.89 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:WORKER}): 0 entries, 0 B within 17.39 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:BUCKETS}): 0 entries, 0 B within 17.55 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96:C_BLOCKS}): 0 entries, 0 B within 16.57 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Imports of worker RoleHandlingTest.worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Buckets of worker RoleHandlingTest.worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Consistency check was successful
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:DATASET}): 0 entries, 0 B within 82.56 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:SECONDARY_IDS}): 0 entries, 0 B within 37.82 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:TABLES}): 0 entries, 0 B within 29.34 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 32.16 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:IMPORTS}): 0 entries, 0 B within 26.25 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:CONCEPTS}): 0 entries, 0 B within 25.89 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:WORKER}): 0 entries, 0 B within 45.57 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:BUCKETS}): 0 entries, 0 B within 26.36 μs
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e:C_BLOCKS}): 0 entries, 0 B within 27.08 μs
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Imports of worker RoleHandlingTest.worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Buckets of worker RoleHandlingTest.worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:56]	c.b.c.u.s.TestConquery	RoleHandlingTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:56]	c.b.c.m.a.AuthorizationController	RoleHandlingTest	Security manager registered
[INFO] [2023-01-06 20:43:56]	c.b.c.i.s.MetaStorage	RoleHandlingTest	Remove User = user.user
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	RoleHandlingTest	Closing Job Manager fast RoleHandlingTest
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[INFO] [2023-01-06 20:43:56]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	RoleHandlingTest	Closing Job Manager slow RoleHandlingTest
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[INFO] [2023-01-06 20:43:56]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[INFO] [2023-01-06 20:43:56]	c.b.c.m.w.Namespace	RoleHandlingTest	Removing namespace storage of RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-06 20:43:56]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[INFO] [2023-01-06 20:43:56]	c.b.c.m.c.XodusStoreFactory	RoleHandlingTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleHandlingTest
[INFO] [2023-01-06 20:43:56]	c.b.c.u.s.TestConquery	RoleHandlingTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[INFO] [2023-01-06 20:43:57]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleHandlingTest_5d32034d-d472-4ab5-b8dc-fa8b0f9bbb96
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[INFO] [2023-01-06 20:43:57]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleHandlingTest_f7d78a45-54e6-4eeb-b4ea-612610c0fa2e
[INFO] [2023-01-06 20:43:57]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingTest	SUCCESS integration test RoleHandlingTest
[INFO] [2023-01-06 20:43:57]	c.b.c.i.IntegrationTest$Wrapper	RoleUITest	STARTING integration test RoleUITest
[INFO] [2023-01-06 20:43:57]	c.b.c.u.s.TestConquery	RoleUITest	Setting up dataset
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:DATASET}): 0 entries, 0 B within 182.2 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:SECONDARY_IDS}): 0 entries, 0 B within 82.33 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:TABLES}): 0 entries, 0 B within 63.48 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 67.05 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:IMPORTS}): 0 entries, 0 B within 61.62 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:CONCEPTS}): 0 entries, 0 B within 60.21 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.NamespacedStorage	RoleUITest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 75.17 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:STRUCTURE}): 0 entries, 0 B within 58.36 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 82.98 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 60.57 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=RoleUITest, name=RoleUITest]
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=RoleUITest, name=RoleUITest]
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:DATASET}): 0 entries, 0 B within 89.79 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:DATASET}): 0 entries, 0 B within 68.81 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:SECONDARY_IDS}): 0 entries, 0 B within 47.68 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:SECONDARY_IDS}): 0 entries, 0 B within 46.66 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:TABLES}): 0 entries, 0 B within 37.47 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:TABLES}): 0 entries, 0 B within 36.39 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.22 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 39.82 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:IMPORTS}): 0 entries, 0 B within 37.52 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:IMPORTS}): 0 entries, 0 B within 37.70 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:CONCEPTS}): 0 entries, 0 B within 35.43 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:CONCEPTS}): 0 entries, 0 B within 35.97 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:WORKER}): 0 entries, 0 B within 36.45 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:WORKER}): 0 entries, 0 B within 38.32 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:BUCKETS}): 0 entries, 0 B within 44.76 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:BUCKETS}): 0 entries, 0 B within 41.55 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13:C_BLOCKS}): 0 entries, 0 B within 38.33 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0:C_BLOCKS}): 0 entries, 0 B within 36.95 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Imports of worker RoleUITest.worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Buckets of worker RoleUITest.worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Consistency check was successful
[INFO] [2023-01-06 20:43:57]	c.b.c.u.s.TestConquery	RoleUITest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Imports of worker RoleUITest.worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Buckets of worker RoleUITest.worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Consistency check was successful
[DEBUG] [2023-01-06 20:43:57]	c.b.c.m.a.AuthorizationController	RoleUITest	Security manager registered
127.0.0.1 - - [06/Jan/2023:20:43:57 +0000] "GET /admin/roles/role.testMandatorName HTTP/1.1" 200 197 "-" "Conquery (test client)" 13
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.MetaStorage	RoleUITest	Remove User = user.testUser@test$2ede
[INFO] [2023-01-06 20:43:57]	c.b.c.m.j.JobExecutor	RoleUITest	Closing Job Manager fast RoleUITest
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RoleUITest, name=RoleUITest]
[INFO] [2023-01-06 20:43:57]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RoleUITest, name=RoleUITest]
[INFO] [2023-01-06 20:43:57]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[INFO] [2023-01-06 20:43:57]	c.b.c.m.j.JobExecutor	RoleUITest	Closing Job Manager slow RoleUITest
[INFO] [2023-01-06 20:43:57]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[INFO] [2023-01-06 20:43:57]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[INFO] [2023-01-06 20:43:57]	c.b.c.m.w.Namespace	RoleUITest	Removing namespace storage of RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[INFO] [2023-01-06 20:43:57]	c.b.c.m.c.XodusStoreFactory	RoleUITest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_RoleUITest
[INFO] [2023-01-06 20:43:57]	c.b.c.u.s.TestConquery	RoleUITest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[INFO] [2023-01-06 20:43:57]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_RoleUITest_82247ccb-c277-4a62-a2ad-0144974c0bb0
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[INFO] [2023-01-06 20:43:57]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_RoleUITest_e6bcbc7f-54c3-4cfd-86da-2eb663ad4b13
[INFO] [2023-01-06 20:43:57]	c.b.c.i.IntegrationTest$Wrapper	RoleUITest	SUCCESS integration test RoleUITest
[INFO] [2023-01-06 20:43:57]	c.b.c.i.IntegrationTest$Wrapper	SecondaryIdEndpointTest	STARTING integration test SecondaryIdEndpointTest
[INFO] [2023-01-06 20:43:57]	c.b.c.u.s.TestConquery	SecondaryIdEndpointTest	Setting up dataset
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:DATASET}): 0 entries, 0 B within 119.8 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:SECONDARY_IDS}): 0 entries, 0 B within 74.42 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:TABLES}): 0 entries, 0 B within 62.43 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 67.81 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:IMPORTS}): 0 entries, 0 B within 55.49 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:CONCEPTS}): 0 entries, 0 B within 44.95 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.NamespacedStorage	SecondaryIdEndpointTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 42.18 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:STRUCTURE}): 0 entries, 0 B within 37.35 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 36.81 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 39.10 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:DATASET}): 0 entries, 0 B within 78.63 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:SECONDARY_IDS}): 0 entries, 0 B within 21.69 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:TABLES}): 0 entries, 0 B within 19.90 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 30.96 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:IMPORTS}): 0 entries, 0 B within 16.86 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:CONCEPTS}): 0 entries, 0 B within 40.69 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:WORKER}): 0 entries, 0 B within 26.68 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:BUCKETS}): 0 entries, 0 B within 17.37 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725:C_BLOCKS}): 0 entries, 0 B within 16.48 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Imports of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Buckets of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Consistency check was successful
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:DATASET}): 0 entries, 0 B within 88.18 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:SECONDARY_IDS}): 0 entries, 0 B within 45.52 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:TABLES}): 0 entries, 0 B within 31.30 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 35.46 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:IMPORTS}): 0 entries, 0 B within 30.84 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:CONCEPTS}): 0 entries, 0 B within 29.83 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:WORKER}): 0 entries, 0 B within 30.32 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:BUCKETS}): 0 entries, 0 B within 30.65 μs
[DEBUG] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:57]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee:C_BLOCKS}): 0 entries, 0 B within 29.65 μs
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Imports of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Buckets of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:57]	c.b.c.u.s.TestConquery	SecondaryIdEndpointTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:57]	c.b.c.m.a.AuthorizationController	SecondaryIdEndpointTest	Security manager registered
[INFO] [2023-01-06 20:43:57]	c.b.c.r.a.r.AdminDatasetProcessor	user.SUPERUSER@SUPERUSER	Received new SecondaryId[SecondaryIdEndpointTest.description-NAME]
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee, /127.0.0.1:52262]	Received update of SecondaryId SecondaryIdEndpointTest.description-NAME
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725, /127.0.0.1:52260]	Received update of SecondaryId SecondaryIdEndpointTest.description-NAME
127.0.0.1 - - [06/Jan/2023:20:43:57 +0000] "POST /admin/datasets/SecondaryIdEndpointTest/secondaryId HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
[INFO] [2023-01-06 20:43:57]	c.b.c.i.t.SecondaryIdEndpointTest	SecondaryIdEndpointTest	InboundJaxrsResponse{context=ClientResponse{method=POST, uri=http://localhost:42541/admin/datasets/SecondaryIdEndpointTest/secondaryId, status=204, reason=No Content}}
[WARN] [2023-01-06 20:43:57]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	There are no displayable concepts in the dataset SecondaryIdEndpointTest
[WARN] [2023-01-06 20:43:57]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	No concepts could be collected for user.SUPERUSER@SUPERUSER on dataset SecondaryIdEndpointTest. The subject is possibly lacking the permission to use them.
127.0.0.1 - - [06/Jan/2023:20:43:57 +0000] "GET /api/datasets/SecondaryIdEndpointTest/concepts HTTP/1.1" 200 150 "-" "Conquery (test client)" 16
[INFO] [2023-01-06 20:43:57]	c.b.c.i.t.SecondaryIdEndpointTest	SecondaryIdEndpointTest	[FESecondaryId(id=SecondaryIdEndpointTest.description-NAME, label=description-LABEL, description=description-DESCRIPTION)]
127.0.0.1 - - [06/Jan/2023:20:43:57 +0000] "POST /admin/datasets/SecondaryIdEndpointTest/tables HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.UpdateTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725, /127.0.0.1:52260]	Received update of Table SecondaryIdEndpointTest.table
[INFO] [2023-01-06 20:43:57]	c.b.c.m.m.n.s.UpdateTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee, /127.0.0.1:52262]	Received update of Table SecondaryIdEndpointTest.table
127.0.0.1 - - [06/Jan/2023:20:43:57 +0000] "GET /admin/datasets/SecondaryIdEndpointTest HTTP/1.1" 200 79 "-" "Conquery (test client)" 5
[ERROR] [2023-01-06 20:43:58]	c.b.c.r.a.r.AdminDatasetProcessor	user.SUPERUSER@SUPERUSER	SecondaryId[SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )] still present on [SecondaryIdEndpointTest.table]
127.0.0.1 - - [06/Jan/2023:20:43:58 +0000] "DELETE /admin/datasets/SecondaryIdEndpointTest/secondaryId/SecondaryIdEndpointTest.description-NAME HTTP/1.1" 403 92 "-" "Conquery (test client)" 11
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee, /127.0.0.1:52262]	Received update of Table Table[label=table, name=table]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725, /127.0.0.1:52260]	Received update of Table Table[label=table, name=table]
127.0.0.1 - - [06/Jan/2023:20:43:58 +0000] "DELETE /admin/datasets/SecondaryIdEndpointTest/tables/SecondaryIdEndpointTest.table HTTP/1.1" 200 2 "-" "Conquery (test client)" 12
[INFO] [2023-01-06 20:43:58]	c.b.c.r.a.r.AdminDatasetProcessor	user.SUPERUSER@SUPERUSER	Deleting SecondaryId[SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )]
127.0.0.1 - - [06/Jan/2023:20:43:58 +0000] "DELETE /admin/datasets/SecondaryIdEndpointTest/secondaryId/SecondaryIdEndpointTest.description-NAME HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee, /127.0.0.1:52262]	Received Deletion of SecondaryId SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725, /127.0.0.1:52260]	Received Deletion of SecondaryId SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )
[WARN] [2023-01-06 20:43:58]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	There are no displayable concepts in the dataset SecondaryIdEndpointTest
[WARN] [2023-01-06 20:43:58]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	No concepts could be collected for user.SUPERUSER@SUPERUSER on dataset SecondaryIdEndpointTest. The subject is possibly lacking the permission to use them.
127.0.0.1 - - [06/Jan/2023:20:43:58 +0000] "GET /api/datasets/SecondaryIdEndpointTest/concepts HTTP/1.1" 200 33 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:43:58]	c.b.c.i.t.SecondaryIdEndpointTest	SecondaryIdEndpointTest	[]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	SecondaryIdEndpointTest	Closing Job Manager fast SecondaryIdEndpointTest
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	SecondaryIdEndpointTest	Closing Job Manager slow SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[INFO] [2023-01-06 20:43:58]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SecondaryIdEndpointTest_443210f9-e287-4076-bd43-a0138dc76725
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[INFO] [2023-01-06 20:43:58]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SecondaryIdEndpointTest_a90d0166-5683-487d-8fb3-f8729d4fdbee
[INFO] [2023-01-06 20:43:58]	c.b.c.m.w.Namespace	SecondaryIdEndpointTest	Removing namespace storage of SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[INFO] [2023-01-06 20:43:58]	c.b.c.m.c.XodusStoreFactory	SecondaryIdEndpointTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SecondaryIdEndpointTest
[INFO] [2023-01-06 20:43:58]	c.b.c.u.s.TestConquery	SecondaryIdEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:58]	c.b.c.i.IntegrationTest$Wrapper	SecondaryIdEndpointTest	SUCCESS integration test SecondaryIdEndpointTest
[INFO] [2023-01-06 20:43:58]	c.b.c.i.IntegrationTest$Wrapper	SuperPermissionTest	STARTING integration test SuperPermissionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.u.s.TestConquery	SuperPermissionTest	Setting up dataset
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:DATASET}): 0 entries, 0 B within 124.3 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:SECONDARY_IDS}): 0 entries, 0 B within 69.66 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:TABLES}): 0 entries, 0 B within 54.31 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 57.60 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:IMPORTS}): 0 entries, 0 B within 49.83 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:CONCEPTS}): 0 entries, 0 B within 49.64 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.NamespacedStorage	SuperPermissionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 55.25 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:STRUCTURE}): 0 entries, 0 B within 50.58 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 50.02 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 62.51 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:DATASET}): 0 entries, 0 B within 75.16 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:SECONDARY_IDS}): 0 entries, 0 B within 21.60 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:TABLES}): 0 entries, 0 B within 16.99 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 19.84 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:IMPORTS}): 0 entries, 0 B within 17.08 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:CONCEPTS}): 0 entries, 0 B within 16.59 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:WORKER}): 0 entries, 0 B within 18.53 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:BUCKETS}): 0 entries, 0 B within 27.11 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0:C_BLOCKS}): 0 entries, 0 B within 27.56 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:DATASET}): 0 entries, 0 B within 67.34 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:SECONDARY_IDS}): 0 entries, 0 B within 39.03 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:TABLES}): 0 entries, 0 B within 41.58 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 28.70 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:IMPORTS}): 0 entries, 0 B within 24.37 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:CONCEPTS}): 0 entries, 0 B within 22.27 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:WORKER}): 0 entries, 0 B within 19.40 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:BUCKETS}): 0 entries, 0 B within 24.41 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c:C_BLOCKS}): 0 entries, 0 B within 17.96 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Imports of worker SuperPermissionTest.worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Buckets of worker SuperPermissionTest.worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Imports of worker SuperPermissionTest.worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Buckets of worker SuperPermissionTest.worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:58]	c.b.c.u.s.TestConquery	SuperPermissionTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:58]	c.b.c.m.a.AuthorizationController	SuperPermissionTest	Security manager registered
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.MetaStorage	SuperPermissionTest	Remove User = user.user
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	SuperPermissionTest	Closing Job Manager fast SuperPermissionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	SuperPermissionTest	Closing Job Manager slow SuperPermissionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[INFO] [2023-01-06 20:43:58]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[INFO] [2023-01-06 20:43:58]	c.b.c.m.w.Namespace	SuperPermissionTest	Removing namespace storage of SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.m.c.XodusStoreFactory	SuperPermissionTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_SuperPermissionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.u.s.TestConquery	SuperPermissionTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[INFO] [2023-01-06 20:43:58]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_SuperPermissionTest_84e80cd1-1fcf-430d-872a-9807d5c86dc0
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[INFO] [2023-01-06 20:43:58]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_SuperPermissionTest_19cfb912-b1da-4e4c-b584-95572dbb540c
[INFO] [2023-01-06 20:43:58]	c.b.c.i.IntegrationTest$Wrapper	SuperPermissionTest	SUCCESS integration test SuperPermissionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.i.IntegrationTest$Wrapper	ConceptUpdateAndDeletionTest	STARTING integration test ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:43:58]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Setting up dataset
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}): 0 entries, 0 B within 137.9 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 75.76 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}): 0 entries, 0 B within 64.90 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 68.36 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}): 0 entries, 0 B within 62.13 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}): 0 entries, 0 B within 62.48 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.NamespacedStorage	ConceptUpdateAndDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 66.01 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}): 0 entries, 0 B within 63.95 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 61.79 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 62.42 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DATASET}): 0 entries, 0 B within 90.03 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:SECONDARY_IDS}): 0 entries, 0 B within 34.68 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:TABLES}): 0 entries, 0 B within 21.73 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 27.20 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:IMPORTS}): 0 entries, 0 B within 24.40 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:CONCEPTS}): 0 entries, 0 B within 24.46 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:WORKER}): 0 entries, 0 B within 21.68 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:BUCKETS}): 0 entries, 0 B within 21.60 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:C_BLOCKS}): 0 entries, 0 B within 21.83 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DATASET}): 0 entries, 0 B within 72.75 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:SECONDARY_IDS}): 0 entries, 0 B within 27.68 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:TABLES}): 0 entries, 0 B within 24.71 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 27.13 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:IMPORTS}): 0 entries, 0 B within 22.99 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:CONCEPTS}): 0 entries, 0 B within 21.63 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:WORKER}): 0 entries, 0 B within 20.56 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:BUCKETS}): 0 entries, 0 B within 28.12 μs
[DEBUG] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:43:58]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:C_BLOCKS}): 0 entries, 0 B within 21.90 μs
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:43:58]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:43:58]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.UpdateTable	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Received update of Table ConceptUpdateAndDeletionTest.test_table
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.UpdateTable	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Received update of Table ConceptUpdateAndDeletionTest.test_table
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[INFO] [2023-01-06 20:43:59]	c.b.c.c.PreprocessorCommand	ConceptUpdateAndDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-06 20:43:59]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:43:59]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)	Required to preprocess 94 B in total
[INFO] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000505477s[INFO] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:43:59]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:43:59]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3885d1e5)
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3885d1e5) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@74e53fc7(est. 81 B)
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		test_column: StringParser(super=Parser(lines=4, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=3], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Headers
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing header
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing data
[INFO] [2023-01-06 20:43:59]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:43:59]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 1 Jobs:
[INFO] [2023-01-06 20:43:59]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 4 Entities.
[INFO] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into ConceptUpdateAndDeletionTest.test_table
127.0.0.1 - - [06/Jan/2023:20:43:59 +0000] "POST /admin/datasets/ConceptUpdateAndDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ConceptUpdateAndDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 21
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Mapped 4 new ids
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.w.Namespace	Job Manager slow ConceptUpdateAndDeletionTest	Assigning Bucket[0] to Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.w.Namespace	Job Manager slow ConceptUpdateAndDeletionTest	Assigning Bucket[1] to Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6]
[INFO] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Received new WorkerInformation(size = 1,dataset = ConceptUpdateAndDeletionTest)
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Received new WorkerInformation(size = 1,dataset = ConceptUpdateAndDeletionTest)
[INFO] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:43:59]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ConceptUpdateAndDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Received Dictionary[ConceptUpdateAndDeletionTest.test_table#ConceptUpdateAndDeletionTest$2etest_table$2etest_column] of size 3.
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Received Dictionary[ConceptUpdateAndDeletionTest.test_table#ConceptUpdateAndDeletionTest$2etest_table$2etest_column] of size 3.
[INFO] [2023-01-06 20:43:59]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-06 20:43:59]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ConceptUpdateAndDeletionTest	One or more Children are not done yet
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.AddImport	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Received Import[ConceptUpdateAndDeletionTest.test_table.test_table], containing 4 entries.
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.AddImport	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Received Import[ConceptUpdateAndDeletionTest.test_table.test_table], containing 4 entries.
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.ImportBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Received ConceptUpdateAndDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Adding Bucket[ConceptUpdateAndDeletionTest.test_table.test_table.0]
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.ImportBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Received ConceptUpdateAndDeletionTest.test_table.test_table.1
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Adding Bucket[ConceptUpdateAndDeletionTest.test_table.test_table.1]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state before update
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query before update
[INFO] [2023-01-06 20:43:59]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ConceptUpdateAndDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:59]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest))]]
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Started ConceptQuery ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Started ConceptQuery ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	QueryPlan for Query[ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	QueryPlan for Query[ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:43:59]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a] with 0 results within PT0.000878S
[INFO] [2023-01-06 20:43:59]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a] with 1 results within PT0.001137S
127.0.0.1 - - [06/Jan/2023:20:43:59 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 201 1147 "-" "Conquery (test client)" 9
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, startTime=2023-01-06T20:43:59.513053, finishTime=2023-01-06T20:43:59.513931) of size 0
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=0] for Query[ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a]
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, startTime=2023-01-06T20:43:59.512962, finishTime=2023-01-06T20:43:59.514099) of size 1
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:59]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	DONE d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a ManagedQuery within PT0.005091S
127.0.0.1 - - [06/Jan/2023:20:43:59 +0000] "GET /api/datasets/ConceptUpdateAndDeletionTest/queries/ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a HTTP/1.1" 200 1450 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Query before update executed
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing  update
127.0.0.1 - - [06/Jan/2023:20:43:59 +0000] "PUT /admin/datasets/ConceptUpdateAndDeletionTest/concepts HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Update executed
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after update
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after update
[INFO] [2023-01-06 20:43:59]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ConceptUpdateAndDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:43:59]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[889c12c9-0576-4e5e-9c33-779d7ddc4789] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest))]]
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	Started ConceptQuery ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	Started ConceptQuery ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:52260]	QueryPlan for Query[ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:52262]	QueryPlan for Query[ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:43:59]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789] with 1 results within PT0.000663S
[INFO] [2023-01-06 20:43:59]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789] with 1 results within PT0.000976S
127.0.0.1 - - [06/Jan/2023:20:43:59 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 201 1147 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, startTime=2023-01-06T20:43:59.838956, finishTime=2023-01-06T20:43:59.839619) of size 1
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789]
[DEBUG] [2023-01-06 20:43:59]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:43:59]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, startTime=2023-01-06T20:43:59.838841, finishTime=2023-01-06T20:43:59.839817) of size 1
[DEBUG] [2023-01-06 20:43:59]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789]
[INFO] [2023-01-06 20:43:59]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	DONE 889c12c9-0576-4e5e-9c33-779d7ddc4789 ManagedQuery within PT0.002909S
127.0.0.1 - - [06/Jan/2023:20:43:59 +0000] "GET /api/datasets/ConceptUpdateAndDeletionTest/queries/ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789 HTTP/1.1" 200 1450 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:43:59]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:43:59]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Query after update executed
[INFO] [2023-01-06 20:43:59]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DATASET}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:TABLES}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:IMPORTS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:CONCEPTS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:WORKER}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:BUCKETS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:C_BLOCKS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:52262	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DATASET}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:TABLES}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:IMPORTS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:CONCEPTS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:WORKER}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:BUCKETS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:C_BLOCKS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:52260	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:00]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:00]	c.b.c.m.w.Namespace		Closing namespace storage of ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
[INFO] [2023-01-06 20:44:00]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:44:00]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}): 1 entries, 83 B within 223.4 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 30.06 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}): 1 entries, 183 B within 245.5 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 807.8 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}): 1 entries, 553 B within 2.925 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}): 1 entries, 508 B within 3.763 ms
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 429.2 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}): 0 entries, 0 B within 26.30 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 328 B within 121.6 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 114 B within 603.5 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	DONE reading Storage
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest)]
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.098 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 97.62 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 80.03 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 2 entries, 1.1 KiB within 11.92 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 75.12 μs
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@5171ca3f
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:44:00]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:44:00]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_18
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_19
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_20
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:44:00]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:44:00]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:44:00]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DATASET}): 1 entries, 83 B within 123.6 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:SECONDARY_IDS}): 0 entries, 0 B within 37.77 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:TABLES}): 1 entries, 183 B within 128.8 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 676.8 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:IMPORTS}): 1 entries, 553 B within 2.809 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DATASET}): 1 entries, 83 B within 118.7 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:SECONDARY_IDS}): 0 entries, 0 B within 42.39 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:TABLES}): 1 entries, 183 B within 157.1 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 639.8 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:CONCEPTS}): 1 entries, 508 B within 4.029 ms
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:WORKER}): 1 entries, 159 B within 98.91 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:IMPORTS}): 1 entries, 553 B within 3.059 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:BUCKETS}): 1 entries, 410 B within 1.217 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:C_BLOCKS}): 1 entries, 248 B within 148.9 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	DONE reading Storage
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea))]
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:CONCEPTS}): 1 entries, 508 B within 4.090 ms
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:WORKER}): 1 entries, 159 B within 87.70 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:BUCKETS}): 1 entries, 398 B within 1.450 ms
[DEBUG] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:00]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:C_BLOCKS}): 1 entries, 249 B within 161.4 μs
[DEBUG] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	DONE reading Storage
[INFO] [2023-01-06 20:44:00]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6))]
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:00]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53000 connected, waiting for identity
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	/127.0.0.1:53000	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53002 connected, waiting for identity
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	/127.0.0.1:53002	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	/127.0.0.1:53000	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6'
[INFO] [2023-01-06 20:44:00]	c.b.c.c.ShardNode	/127.0.0.1:53002	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea'
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53002` registered.
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53000` registered.
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6 are consistent with the manager: 1 Imports
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6 are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea are consistent with the manager: 1 Imports
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:00]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[WARN] [2023-01-06 20:44:00]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:00]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:44:00]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:01]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:44:01]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-06 20:44:01]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest for Support
[INFO] [2023-01-06 20:44:01]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:01]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after re-start
[INFO] [2023-01-06 20:44:01]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after restart.
[INFO] [2023-01-06 20:44:01]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ConceptUpdateAndDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:01]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[e45213ab-1a47-4c12-b55e-2999e548a52f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest))]]
127.0.0.1 - - [06/Jan/2023:20:44:01 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 201 1147 "-" "Conquery (test client)" 49
[DEBUG] [2023-01-06 20:44:01]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:01]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:53000]	Started ConceptQuery ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f
[DEBUG] [2023-01-06 20:44:01]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:53000]	QueryPlan for Query[ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:01]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:53002]	Started ConceptQuery ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f
[DEBUG] [2023-01-06 20:44:01]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:53002]	QueryPlan for Query[ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:01]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f] with 1 results within PT0.001007S
[INFO] [2023-01-06 20:44:01]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f] with 1 results within PT0.00103S
[INFO] [2023-01-06 20:44:01]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, startTime=2023-01-06T20:44:01.159978, finishTime=2023-01-06T20:44:01.161008) of size 1
[DEBUG] [2023-01-06 20:44:01]	c.b.c.m.q.ManagedQuery	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f]
[INFO] [2023-01-06 20:44:01]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, startTime=2023-01-06T20:44:01.159573, finishTime=2023-01-06T20:44:01.160580) of size 1
[DEBUG] [2023-01-06 20:44:01]	c.b.c.m.q.ManagedQuery	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f]
[INFO] [2023-01-06 20:44:01]	c.b.c.m.e.ManagedExecution	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	DONE e45213ab-1a47-4c12-b55e-2999e548a52f ManagedQuery within PT0.03043S
127.0.0.1 - - [06/Jan/2023:20:44:01 +0000] "GET /api/datasets/ConceptUpdateAndDeletionTest/queries/ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f HTTP/1.1" 200 1451 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:44:01]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:01]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Issuing deletion of import ConceptUpdateAndDeletionTest.test_tree
[INFO] [2023-01-06 20:44:01]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:01]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:53002]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:44:01]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:53000]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:44:01]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea, /127.0.0.1:53002]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:01]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6, /127.0.0.1:53000]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[INFO] [2023-01-06 20:44:01]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after deletion
[INFO] [2023-01-06 20:44:01]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after deletion (EXPECTING AN EXCEPTION IN THE LOGS!)
[DEBUG] [2023-01-06 20:44:01]	c.b.c.i.j.ConqueryJsonExceptionMapper	user.SUPERUSER@SUPERUSER	Unable to process JSON in request 'http://localhost:46651/api/datasets/ConceptUpdateAndDeletionTest/queries'
com.fasterxml.jackson.databind.JsonMappingException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector (through reference chain: com.bakdata.conquery.apiv1.query.ConceptQuery["root"]->com.bakdata.conquery.apiv1.query.concept.specific.CQConcept["tables"]->java.util.ArrayList[0]->com.bakdata.conquery.apiv1.query.concept.filter.CQTable["id"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:254)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:155)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:286)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.impl.ManagedReferenceProperty.deserializeAndSet(ManagedReferenceProperty.java:62)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:295)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:138)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:278)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:282)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:65)
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:26)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:252)
	... 96 common frames omitted
Caused by: com.bakdata.conquery.io.jackson.serializer.IdReferenceResolvingException: Could not find entry `ConceptUpdateAndDeletionTest.test_tree.test_column` of type com.bakdata.conquery.models.datasets.concepts.Connector
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 123]
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:55)
	... 99 common frames omitted
127.0.0.1 - - [06/Jan/2023:20:44:01 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 400 208 "-" "Conquery (test client)" 14
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DATASET}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:TABLES}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:IMPORTS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:CONCEPTS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:WORKER}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:BUCKETS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:C_BLOCKS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:01]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:01]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53002	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-06 20:44:01]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DATASET}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:TABLES}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:IMPORTS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:CONCEPTS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:WORKER}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:BUCKETS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:C_BLOCKS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:01]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:01]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53000	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-06 20:44:01]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:01]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:01]	c.b.c.m.w.Namespace		Closing namespace storage of ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-06 20:44:01]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:44:02]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}): 1 entries, 83 B within 129.1 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 29.99 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}): 1 entries, 183 B within 182.1 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 710.5 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}): 1 entries, 553 B within 2.867 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}): 0 entries, 0 B within 30.85 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 215.1 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}): 0 entries, 0 B within 23.74 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 328 B within 117.7 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 114 B within 493.4 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest	DONE reading Storage
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest)]
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 13.31 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 68.09 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 50.03 μs
[WARN] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [ConceptUpdateAndDeletionTest.889c12c9-0576-4e5e-9c33-779d7ddc4789]
[WARN] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [ConceptUpdateAndDeletionTest.d3a8bff1-fbaa-4a89-8257-84ddd00a4a8a]
[WARN] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [ConceptUpdateAndDeletionTest.e45213ab-1a47-4c12-b55e-2999e548a52f]
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	3
	Key read failure:	0 (0.00%)
	Value read failure:	3 (100.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 0 entries, 0 B within 11.00 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 45.85 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@59508b21
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:44:02]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:44:02]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_21
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_22
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_23
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:44:02]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:44:02]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:44:02]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:DATASET}): 1 entries, 83 B within 131.5 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:SECONDARY_IDS}): 0 entries, 0 B within 28.39 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:TABLES}): 1 entries, 183 B within 161.8 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 656.4 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:IMPORTS}): 1 entries, 553 B within 2.642 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:CONCEPTS}): 0 entries, 0 B within 31.11 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:WORKER}): 1 entries, 159 B within 88.37 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:BUCKETS}): 1 entries, 398 B within 1.239 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6:C_BLOCKS}): 0 entries, 0 B within 27.35 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6	DONE reading Storage
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6))]
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:DATASET}): 1 entries, 83 B within 131.1 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:SECONDARY_IDS}): 0 entries, 0 B within 35.39 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:TABLES}): 1 entries, 183 B within 144.2 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 784.5 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:IMPORTS}): 1 entries, 553 B within 2.798 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:CONCEPTS}): 0 entries, 0 B within 27.29 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:WORKER}): 1 entries, 159 B within 90.43 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:BUCKETS}): 1 entries, 410 B within 1.194 ms
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea:C_BLOCKS}): 0 entries, 0 B within 25.44 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea	DONE reading Storage
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea))]
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:02]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53136 connected, waiting for identity
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode	/127.0.0.1:53136	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53138 connected, waiting for identity
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode	/127.0.0.1:53138	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode	/127.0.0.1:53136	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea'
[INFO] [2023-01-06 20:44:02]	c.b.c.c.ShardNode	/127.0.0.1:53138	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6'
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53138` registered.
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53136` registered.
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea are consistent with the manager: 1 Imports
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6 are consistent with the manager: 1 Imports
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6 are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[WARN] [2023-01-06 20:44:02]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:02]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:44:02]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:02]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest6175202265804509010/tmp_ConceptUpdateAndDeletionTest for Support
[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:02]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after restart
[INFO] [2023-01-06 20:44:02]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after restart (EXPECTING AN EXCEPTION IN THE LOGS!)
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.j.ConqueryJsonExceptionMapper	user.SUPERUSER@SUPERUSER	Unable to process JSON in request 'http://localhost:46651/api/datasets/ConceptUpdateAndDeletionTest/queries'
com.fasterxml.jackson.databind.JsonMappingException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector (through reference chain: com.bakdata.conquery.apiv1.query.ConceptQuery["root"]->com.bakdata.conquery.apiv1.query.concept.specific.CQConcept["tables"]->java.util.ArrayList[0]->com.bakdata.conquery.apiv1.query.concept.filter.CQTable["id"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:254)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:155)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:286)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.impl.ManagedReferenceProperty.deserializeAndSet(ManagedReferenceProperty.java:62)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:295)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:138)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:278)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:282)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:65)
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:26)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:252)
	... 92 common frames omitted
Caused by: com.bakdata.conquery.io.jackson.serializer.IdReferenceResolvingException: Could not find entry `ConceptUpdateAndDeletionTest.test_tree.test_column` of type com.bakdata.conquery.models.datasets.concepts.Connector
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 123]
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:55)
	... 95 common frames omitted
127.0.0.1 - - [06/Jan/2023:20:44:02 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 400 208 "-" "Conquery (test client)" 11
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[INFO] [2023-01-06 20:44:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[INFO] [2023-01-06 20:44:02]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[INFO] [2023-01-06 20:44:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:02]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_490fd1a5-8d85-45be-8561-1c3156acf7ea
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_5661b40a-05c8-42ac-8f38-6c7afee7d8e6
[INFO] [2023-01-06 20:44:02]	c.b.c.m.w.Namespace		Removing namespace storage of ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:02]	c.b.c.i.IntegrationTest$Wrapper	ConceptUpdateAndDeletionTest	SUCCESS integration test ConceptUpdateAndDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.i.IntegrationTest$Wrapper	DatasetDeletionTest	STARTING integration test DatasetDeletionTest
[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Setting up dataset
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:DATASET}): 0 entries, 0 B within 86.30 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 33.66 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:TABLES}): 0 entries, 0 B within 32.26 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 61.25 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:IMPORTS}): 0 entries, 0 B within 56.78 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:CONCEPTS}): 0 entries, 0 B within 34.93 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.NamespacedStorage	DatasetDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 36.59 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:STRUCTURE}): 0 entries, 0 B within 34.80 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 28.12 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 28.22 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:DATASET}): 0 entries, 0 B within 109.9 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:SECONDARY_IDS}): 0 entries, 0 B within 39.71 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:TABLES}): 0 entries, 0 B within 30.78 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 35.77 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:IMPORTS}): 0 entries, 0 B within 32.81 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:CONCEPTS}): 0 entries, 0 B within 26.34 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:WORKER}): 0 entries, 0 B within 30.35 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:BUCKETS}): 0 entries, 0 B within 30.13 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168:C_BLOCKS}): 0 entries, 0 B within 30.99 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Imports of worker DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Buckets of worker DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Consistency check was successful
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:DATASET}): 0 entries, 0 B within 103.8 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:SECONDARY_IDS}): 0 entries, 0 B within 38.14 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:TABLES}): 0 entries, 0 B within 30.72 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 33.66 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:IMPORTS}): 0 entries, 0 B within 30.07 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:CONCEPTS}): 0 entries, 0 B within 28.40 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:WORKER}): 0 entries, 0 B within 29.27 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:BUCKETS}): 0 entries, 0 B within 28.63 μs
[DEBUG] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:02]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945:C_BLOCKS}): 0 entries, 0 B within 27.13 μs
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Imports of worker DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Buckets of worker DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:02]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:02]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:03]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:03]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received update of Table DatasetDeletionTest.test_table
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received update of Table DatasetDeletionTest.test_table2
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received update of Table DatasetDeletionTest.test_table
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received update of Table DatasetDeletionTest.test_table2
[INFO] [2023-01-06 20:44:03]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Updating Concept[DatasetDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Updating Concept[DatasetDeletionTest.test_tree]
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand	DatasetDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.045301228s[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1b0f3e8c)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1b0f3e8c) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@1a062159(est. 62 B)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00068565s[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@70e9ed8b)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@70e9ed8b) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@751ab1a5(est. 62 B)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-06 20:44:03]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:03]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into DatasetDeletionTest.test_table
127.0.0.1 - - [06/Jan/2023:20:44:03 +0000] "POST /admin/datasets/DatasetDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_DatasetDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest	Assigning Bucket[0] to Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received new WorkerInformation(size = 0,dataset = DatasetDeletionTest)
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:03]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received Dictionary[DatasetDeletionTest.test_table#DatasetDeletionTest$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received Dictionary[DatasetDeletionTest.test_table#DatasetDeletionTest$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Start sending 1 Buckets
[WARN] [2023-01-06 20:44:03]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	One or more Children are not done yet
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received Import[DatasetDeletionTest.test_table.test_table], containing 2 entries.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into DatasetDeletionTest.test_table2
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest	Assigning Bucket[1] to Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168]
127.0.0.1 - - [06/Jan/2023:20:44:03 +0000] "POST /admin/datasets/DatasetDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_DatasetDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
[INFO] [2023-01-06 20:44:03]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest)
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest)
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received Import[DatasetDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received DatasetDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Adding Bucket[DatasetDeletionTest.test_table.test_table.0]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:03]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received Dictionary[DatasetDeletionTest.test_table2#DatasetDeletionTest$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Start sending 2 Buckets
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received Dictionary[DatasetDeletionTest.test_table2#DatasetDeletionTest$2etest_table2$2etest_column] of size 2.
[WARN] [2023-01-06 20:44:03]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	One or more Children are not done yet
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest.test_table.test_table.0.DatasetDeletionTest.test_tree.test_column]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received Import[DatasetDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received DatasetDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Adding Bucket[DatasetDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received Import[DatasetDeletionTest.test_table2.test_table2], containing 2 entries.
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest.test_table2.test_table2.0.DatasetDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received DatasetDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Adding Bucket[DatasetDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest.test_table2.test_table2.1.DatasetDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:03]	c.b.c.i.t.d.DatasetDeletionTest		Checking state before deletion
[INFO] [2023-01-06 20:44:03]	c.b.c.i.t.d.DatasetDeletionTest		Executing query before deletion
[INFO] [2023-01-06 20:44:03]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[DatasetDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:03]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DatasetDeletionTest))]]
127.0.0.1 - - [06/Jan/2023:20:44:03 +0000] "POST /api/datasets/DatasetDeletionTest/queries HTTP/1.1" 201 1215 "-" "Conquery (test client)" 18
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Started ConceptQuery DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Started ConceptQuery DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	QueryPlan for Query[DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	QueryPlan for Query[DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:03]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd] with 0 results within PT0.001595S
[INFO] [2023-01-06 20:44:03]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd] with 2 results within PT0.002067S
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest]	Received ShardResult(queryId=DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd, workerId=DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, startTime=2023-01-06T20:44:03.758476, finishTime=2023-01-06T20:44:03.760071) of size 0
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest]	Received Result[size=0] for Query[DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest]	Received ShardResult(queryId=DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd, workerId=DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, startTime=2023-01-06T20:44:03.758388, finishTime=2023-01-06T20:44:03.760455) of size 2
[DEBUG] [2023-01-06 20:44:03]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest]	Received Result[size=2] for Query[DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=DatasetDeletionTest]	DONE 4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd ManagedQuery within PT0.015956S
127.0.0.1 - - [06/Jan/2023:20:44:03 +0000] "GET /api/datasets/DatasetDeletionTest/queries/DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd HTTP/1.1" 200 1483 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:44:03]	c.b.c.i.t.d.DatasetDeletionTest		Issuing deletion of import Dataset[label=null, name=DatasetDeletionTest]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Removing CBlock[DatasetDeletionTest.test_table.test_table.0.DatasetDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Removing CBlock[DatasetDeletionTest.test_table2.test_table2.1.DatasetDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Removing Concept[DatasetDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Removing CBlock[DatasetDeletionTest.test_table2.test_table2.0.DatasetDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Removing Concept[DatasetDeletionTest.test_tree]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Deleting Import[NamedImpl(name=test_table)]
[INFO] [2023-01-06 20:44:03]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received update of Table Table[label=test_table, name=test_table]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Deleting Import[NamedImpl(name=test_table)]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Removing Bucket[DatasetDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Removing Bucket[DatasetDeletionTest.test_table.test_table.0]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168, /127.0.0.1:53138]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received update of Table Table[label=test_table, name=test_table]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Removing Bucket[DatasetDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945, /127.0.0.1:53136]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DatasetDeletionTest
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DatasetDeletionTest
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[INFO] [2023-01-06 20:44:03]	c.b.c.m.w.Namespace		Removing namespace storage of DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[INFO] [2023-01-06 20:44:03]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-06 20:44:03]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[INFO] [2023-01-06 20:44:03]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest
[INFO] [2023-01-06 20:44:04]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[INFO] [2023-01-06 20:44:04]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest_4e93fd66-ebed-4365-9bdb-355102286945
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[INFO] [2023-01-06 20:44:04]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest_22d26612-e173-4892-a9ca-241452e33168
[INFO] [2023-01-06 20:44:04]	c.b.c.i.t.d.DatasetDeletionTest		Checking state after deletion
[INFO] [2023-01-06 20:44:04]	c.b.c.u.s.TestConquery		Setting up dataset
[WARN] [2023-01-06 20:44:04]	o.g.j.i.Errors	user.SUPERUSER@SUPERUSER	The following warnings have been detected: WARNING: Unknown HK2 failure detected:
MultiException stack 1 of 3
org.glassfish.jersey.server.ParamException$PathParamException: HTTP 404 Not Found
	at org.glassfish.jersey.server.internal.inject.PathParamValueParamProvider$PathParamValueProvider.apply(PathParamValueParamProvider.java:94)
	at org.glassfish.jersey.server.internal.inject.PathParamValueParamProvider$PathParamValueProvider.apply(PathParamValueParamProvider.java:79)
	at org.glassfish.jersey.server.internal.inject.ParamInjectionResolver.resolve(ParamInjectionResolver.java:97)
	at org.glassfish.jersey.inject.hk2.InjectionResolverWrapper.resolve(InjectionResolverWrapper.java:62)
	at org.jvnet.hk2.internal.ClazzCreator.resolve(ClazzCreator.java:188)
	at org.jvnet.hk2.internal.ClazzCreator.resolveAllDependencies(ClazzCreator.java:211)
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:334)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
	at org.glassfish.jersey.inject.hk2.RequestContext.findOrCreate(RequestContext.java:59)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.NoSuchElementException: Did not find Dataset[DatasetDeletionTest] in [[]]
	at com.bakdata.conquery.models.worker.DatasetRegistry.findRegistry(DatasetRegistry.java:80)
	at com.bakdata.conquery.models.worker.IdResolveContext.resolve(IdResolveContext.java:43)
	at com.bakdata.conquery.io.jackson.NamespacedIdRefParamConverter.fromString(NamespacedIdRefParamConverter.java:23)
	at com.bakdata.conquery.io.jackson.NamespacedIdRefParamConverter.fromString(NamespacedIdRefParamConverter.java:12)
	at org.glassfish.jersey.server.internal.inject.AbstractParamValueExtractor.convert(AbstractParamValueExtractor.java:116)
	at org.glassfish.jersey.server.internal.inject.AbstractParamValueExtractor.fromString(AbstractParamValueExtractor.java:107)
	at org.glassfish.jersey.server.internal.inject.SingleValueExtractor.extract(SingleValueExtractor.java:61)
	at org.glassfish.jersey.server.internal.inject.PathParamValueParamProvider$PathParamValueProvider.apply(PathParamValueParamProvider.java:92)
	... 77 more
MultiException stack 2 of 3
java.lang.IllegalArgumentException: While attempting to resolve the dependencies of com.bakdata.conquery.resources.api.QueryResource errors were found
	at org.jvnet.hk2.internal.ClazzCreator.resolveAllDependencies(ClazzCreator.java:224)
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:334)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
	at org.glassfish.jersey.inject.hk2.RequestContext.findOrCreate(RequestContext.java:59)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
MultiException stack 3 of 3
java.lang.IllegalStateException: Unable to perform operation: resolve on com.bakdata.conquery.resources.api.QueryResource
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:363)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
	at org.glassfish.jersey.inject.hk2.RequestContext.findOrCreate(RequestContext.java:59)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)


127.0.0.1 - - [06/Jan/2023:20:44:04 +0000] "POST /api/datasets/DatasetDeletionTest/queries HTTP/1.1" 404 43 "-" "Conquery (test client)" 7
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:DATASET}): 0 entries, 0 B within 119.5 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:SECONDARY_IDS}): 0 entries, 0 B within 56.34 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:TABLES}): 0 entries, 0 B within 45.09 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 50.41 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:IMPORTS}): 0 entries, 0 B within 45.93 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:CONCEPTS}): 0 entries, 0 B within 44.60 μs
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.NamespacedStorage		Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 46.35 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:STRUCTURE}): 0 entries, 0 B within 47.29 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:WORKER_TO_BUCKETS}): 0 entries, 0 B within 42.09 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore		While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:PRIMARY_DICTIONARY}): 0 entries, 0 B within 41.82 μs
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:DATASET}): 0 entries, 0 B within 92.50 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:SECONDARY_IDS}): 0 entries, 0 B within 43.03 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:TABLES}): 0 entries, 0 B within 45.11 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 33.62 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:IMPORTS}): 0 entries, 0 B within 24.81 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:CONCEPTS}): 0 entries, 0 B within 22.78 μs
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:WORKER}): 0 entries, 0 B within 34.42 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:BUCKETS}): 0 entries, 0 B within 23.89 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:C_BLOCKS}): 0 entries, 0 B within 25.03 μs
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Consistency check was successful
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:DATASET}): 0 entries, 0 B within 73.25 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:SECONDARY_IDS}): 0 entries, 0 B within 29.99 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:TABLES}): 0 entries, 0 B within 23.43 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 29.36 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:IMPORTS}): 0 entries, 0 B within 24.47 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:CONCEPTS}): 0 entries, 0 B within 30.39 μs
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:WORKER}): 0 entries, 0 B within 22.72 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:BUCKETS}): 0 entries, 0 B within 23.61 μs
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:04]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:C_BLOCKS}): 0 entries, 0 B within 21.50 μs
[INFO] [2023-01-06 20:44:04]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Consistency check was successful
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received update of Table DatasetDeletionTest[1].test_table
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received update of Table DatasetDeletionTest[1].test_table
[INFO] [2023-01-06 20:44:04]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received update of Table DatasetDeletionTest[1].test_table2
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received update of Table DatasetDeletionTest[1].test_table2
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand		Preprocessing from command line config.
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.064191955s[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@af980d4)
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@af980d4) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@2b640de5(est. 62 B)
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000878543s[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1ce56297)
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1ce56297) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@382b1248(est. 62 B)
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-06 20:44:04]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:04]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into DatasetDeletionTest[1].test_table
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Updating bucket assignments.
127.0.0.1 - - [06/Jan/2023:20:44:04 +0000] "POST /admin/datasets/DatasetDeletionTest%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_DatasetDeletionTest%5B1%5D%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest[1]	Assigning Bucket[0] to Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received new WorkerInformation(size = 0,dataset = DatasetDeletionTest[1])
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest[1])
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into DatasetDeletionTest[1].test_table2
[INFO] [2023-01-06 20:44:04]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:44:04 +0000] "POST /admin/datasets/DatasetDeletionTest%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_DatasetDeletionTest%5B1%5D%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:04]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received Dictionary[DatasetDeletionTest[1].test_table#DatasetDeletionTest[1]$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received Dictionary[DatasetDeletionTest[1].test_table#DatasetDeletionTest[1]$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Start sending 1 Buckets
[WARN] [2023-01-06 20:44:04]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	One or more Children are not done yet
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Updating bucket assignments.
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest[1]	Assigning Bucket[1] to Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest[1])
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest[1])
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received Import[DatasetDeletionTest[1].test_table.test_table], containing 2 entries.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received Import[DatasetDeletionTest[1].test_table.test_table], containing 2 entries.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received DatasetDeletionTest[1].test_table.test_table.0
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Adding Bucket[DatasetDeletionTest[1].test_table.test_table.0]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:04]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received Dictionary[DatasetDeletionTest[1].test_table2#DatasetDeletionTest[1]$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received Dictionary[DatasetDeletionTest[1].test_table2#DatasetDeletionTest[1]$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Start sending 2 Buckets
[WARN] [2023-01-06 20:44:04]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	One or more Children are not done yet
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received Import[DatasetDeletionTest[1].test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Received DatasetDeletionTest[1].test_table2.test_table2.0
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Adding Bucket[DatasetDeletionTest[1].test_table2.test_table2.0]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received Import[DatasetDeletionTest[1].test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Received DatasetDeletionTest[1].test_table2.test_table2.1
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Adding Bucket[DatasetDeletionTest[1].test_table2.test_table2.1]
[INFO] [2023-01-06 20:44:04]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Updating Concept[DatasetDeletionTest[1].test_tree]
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Updating Concept[DatasetDeletionTest[1].test_tree]
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest[1].test_table2.test_table2.1.DatasetDeletionTest[1].test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest[1].test_table.test_table.0.DatasetDeletionTest[1].test_tree.test_column]
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest[1].test_table2.test_table2.0.DatasetDeletionTest[1].test_tree.test_column2]
[INFO] [2023-01-06 20:44:04]	c.b.c.i.t.d.DatasetDeletionTest		Executing query after re-import
[INFO] [2023-01-06 20:44:04]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[DatasetDeletionTest[1]] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:04]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[20c3358d-c6e9-443e-b6c4-384299d74348] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DatasetDeletionTest[1]))]]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	Started ConceptQuery DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	Started ConceptQuery DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53138]	QueryPlan for Query[DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53136]	QueryPlan for Query[DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:04]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348] with 2 results within PT0.001202S
[INFO] [2023-01-06 20:44:04]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348] with 0 results within PT0.001033S
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, startTime=2023-01-06T20:44:04.936519, finishTime=2023-01-06T20:44:04.937552) of size 0
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest[1]]	Received Result[size=0] for Query[DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348]
[INFO] [2023-01-06 20:44:04]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, startTime=2023-01-06T20:44:04.936252, finishTime=2023-01-06T20:44:04.937454) of size 2
[DEBUG] [2023-01-06 20:44:04]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest[1]]	Received Result[size=2] for Query[DatasetDeletionTest[1].20c3358d-c6e9-443e-b6c4-384299d74348]
127.0.0.1 - - [06/Jan/2023:20:44:04 +0000] "POST /api/datasets/DatasetDeletionTest%5B1%5D/queries HTTP/1.1" 201 1228 "-" "Conquery (test client)" 11
[INFO] [2023-01-06 20:44:04]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=DatasetDeletionTest[1]]	DONE 20c3358d-c6e9-443e-b6c4-384299d74348 ManagedQuery within PT0.00642S
[DEBUG] [2023-01-06 20:44:04]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
127.0.0.1 - - [06/Jan/2023:20:44:04 +0000] "GET /api/datasets/DatasetDeletionTest%5B1%5D/queries/DatasetDeletionTest%5B1%5D.20c3358d-c6e9-443e-b6c4-384299d74348 HTTP/1.1" 200 1747 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:44:04]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:DATASET}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:TABLES}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:IMPORTS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:CONCEPTS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:WORKER}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:BUCKETS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:C_BLOCKS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53138	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:DATASET}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:TABLES}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:IMPORTS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:CONCEPTS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:WORKER}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:BUCKETS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:C_BLOCKS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53136	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:05]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:05]	c.b.c.m.w.Namespace		Closing namespace storage of DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:DATASET}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:TABLES}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:IMPORTS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:CONCEPTS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:ID_MAPPING_META}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:ID_MAPPING_DATA}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:STRUCTURE}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:WORKER_TO_BUCKETS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:PRIMARY_DICTIONARY}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
[INFO] [2023-01-06 20:44:05]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:44:05]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:DATASET}): 1 entries, 71 B within 137.4 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:SECONDARY_IDS}): 0 entries, 0 B within 35.89 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:TABLES}): 2 entries, 356 B within 220.8 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.092 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:IMPORTS}): 2 entries, 1 KiB within 4.065 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:CONCEPTS}): 1 entries, 642 B within 4.682 ms
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	Done reading Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 311.1 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:STRUCTURE}): 0 entries, 0 B within 30.24 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:WORKER_TO_BUCKETS}): 1 entries, 343 B within 123.5 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]:PRIMARY_DICTIONARY}): 1 entries, 108 B within 644.6 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]	DONE reading Storage
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_DatasetDeletionTest[1])]
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 625.9 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 50.19 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 38.75 μs
[WARN] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [DatasetDeletionTest.4dd8a9e6-3c15-4ace-b8dc-95a4c0cfe6bd]
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	1 (50.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 1 entries, 615 B within 6.428 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 48.68 μs
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@39ed81f7
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:44:05]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:44:05]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_24
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_25
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_26
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:44:05]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:44:05]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:44:05]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:DATASET}): 1 entries, 71 B within 132.2 μs
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:DATASET}): 1 entries, 71 B within 147.4 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:SECONDARY_IDS}): 0 entries, 0 B within 49.45 μs
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:SECONDARY_IDS}): 0 entries, 0 B within 55.94 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:TABLES}): 2 entries, 356 B within 242.1 μs
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:TABLES}): 2 entries, 356 B within 264.9 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.336 ms
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.325 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:IMPORTS}): 2 entries, 1 KiB within 3.251 ms
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:IMPORTS}): 2 entries, 1 KiB within 3.254 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:CONCEPTS}): 1 entries, 642 B within 4.092 ms
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:CONCEPTS}): 1 entries, 642 B within 4.092 ms
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	Done reading Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]] / com.bakdata.conquery.io.storage.WorkerStorage
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	Done reading Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:WORKER}): 1 entries, 147 B within 89.92 μs
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:WORKER}): 1 entries, 147 B within 83.56 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:BUCKETS}): 1 entries, 384 B within 1.341 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store BUCKETS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:BUCKETS}): 2 entries, 770 B within 1.465 ms
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6:C_BLOCKS}): 1 entries, 240 B within 149.3 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6	DONE reading Storage
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6))]
[DEBUG] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	While processing store C_BLOCKS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:05]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9:C_BLOCKS}): 2 entries, 477 B within 214.0 μs
[DEBUG] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9	DONE reading Storage
[INFO] [2023-01-06 20:44:05]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9))]
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:05]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53448 connected, waiting for identity
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	/127.0.0.1:53448	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53452 connected, waiting for identity
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	/127.0.0.1:53452	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	/127.0.0.1:53448	Sending worker identity 'worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9'
[INFO] [2023-01-06 20:44:05]	c.b.c.c.ShardNode	/127.0.0.1:53452	Sending worker identity 'worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6'
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53452` registered.
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53448` registered.
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9 are consistent with the manager: 2 Imports
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9 are consistent with the manager: 2 Buckets
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Consistency check was successful
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6 are consistent with the manager: 2 Imports
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6 are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:05]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Consistency check was successful
[WARN] [2023-01-06 20:44:06]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:06]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:44:06]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:06]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest6175202265804509010/tmp_DatasetDeletionTest[1] for Support
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:06]	c.b.c.i.t.d.DatasetDeletionTest		Checking state after re-start
[INFO] [2023-01-06 20:44:06]	c.b.c.i.t.d.DatasetDeletionTest		Executing query after restart
[INFO] [2023-01-06 20:44:06]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[DatasetDeletionTest[1]] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:06]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[7044b78a-a370-4260-a423-c641238c610c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DatasetDeletionTest[1]))]]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53448]	Started ConceptQuery DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c
[DEBUG] [2023-01-06 20:44:06]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, /127.0.0.1:53448]	QueryPlan for Query[DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
127.0.0.1 - - [06/Jan/2023:20:44:06 +0000] "POST /api/datasets/DatasetDeletionTest%5B1%5D/queries HTTP/1.1" 201 1228 "-" "Conquery (test client)" 43
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:06]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c] with 2 results within PT0.001875S
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9, startTime=2023-01-06T20:44:06.318069, finishTime=2023-01-06T20:44:06.319944) of size 2
[DEBUG] [2023-01-06 20:44:06]	c.b.c.m.q.ManagedQuery	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received Result[size=2] for Query[DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53452]	Started ConceptQuery DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c
[DEBUG] [2023-01-06 20:44:06]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, /127.0.0.1:53452]	QueryPlan for Query[DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:06]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c] with 0 results within PT0.001311S
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6, startTime=2023-01-06T20:44:06.323792, finishTime=2023-01-06T20:44:06.325103) of size 0
[DEBUG] [2023-01-06 20:44:06]	c.b.c.m.q.ManagedQuery	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received Result[size=0] for Query[DatasetDeletionTest[1].7044b78a-a370-4260-a423-c641238c610c]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.e.ManagedExecution	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	DONE 7044b78a-a370-4260-a423-c641238c610c ManagedQuery within PT0.033863S
127.0.0.1 - - [06/Jan/2023:20:44:06 +0000] "GET /api/datasets/DatasetDeletionTest%5B1%5D/queries/DatasetDeletionTest%5B1%5D.7044b78a-a370-4260-a423-c641238c610c HTTP/1.1" 200 1748 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:44:06]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:06]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[INFO] [2023-01-06 20:44:06]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:06]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:06]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_DatasetDeletionTest[1]_104869e9-337e-4633-b857-acb5ca5c96f9
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:06]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_DatasetDeletionTest[1]_1bf05e0e-2fee-445d-8fb6-68ee595f1cd6
[INFO] [2023-01-06 20:44:06]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.w.Namespace		Removing namespace storage of DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_DatasetDeletionTest[1]
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:06]	c.b.c.i.IntegrationTest$Wrapper	DatasetDeletionTest	SUCCESS integration test DatasetDeletionTest
[INFO] [2023-01-06 20:44:06]	c.b.c.i.IntegrationTest$Wrapper	ImportDeletionTest	STARTING integration test ImportDeletionTest
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery	ImportDeletionTest	Setting up dataset
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:DATASET}): 0 entries, 0 B within 154.8 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 105.7 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:TABLES}): 0 entries, 0 B within 77.01 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 77.90 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:IMPORTS}): 0 entries, 0 B within 70.98 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:CONCEPTS}): 0 entries, 0 B within 90.93 μs
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.NamespacedStorage	ImportDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 73.78 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:STRUCTURE}): 0 entries, 0 B within 70.19 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 69.04 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 103.4 μs
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:DATASET}): 0 entries, 0 B within 91.51 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:SECONDARY_IDS}): 0 entries, 0 B within 38.48 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:TABLES}): 0 entries, 0 B within 25.98 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 43.34 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:IMPORTS}): 0 entries, 0 B within 26.18 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:CONCEPTS}): 0 entries, 0 B within 22.86 μs
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:WORKER}): 0 entries, 0 B within 22.73 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:BUCKETS}): 0 entries, 0 B within 22.83 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:C_BLOCKS}): 0 entries, 0 B within 23.47 μs
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Consistency check was successful
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:DATASET}): 0 entries, 0 B within 79.58 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:SECONDARY_IDS}): 0 entries, 0 B within 29.63 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:TABLES}): 0 entries, 0 B within 20.03 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 27.06 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:IMPORTS}): 0 entries, 0 B within 18.98 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:CONCEPTS}): 0 entries, 0 B within 21.82 μs
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:WORKER}): 0 entries, 0 B within 19.12 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:BUCKETS}): 0 entries, 0 B within 18.04 μs
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:06]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:C_BLOCKS}): 0 entries, 0 B within 17.88 μs
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received update of Table ImportDeletionTest.test_table
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received update of Table ImportDeletionTest.test_table
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received update of Table ImportDeletionTest.test_table2
[INFO] [2023-01-06 20:44:06]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received update of Table ImportDeletionTest.test_table2
[INFO] [2023-01-06 20:44:06]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Updating Concept[ImportDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:06]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Updating Concept[ImportDeletionTest.test_tree]
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	ImportDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.050389173s[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7efcd4d9)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7efcd4d9) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@223163c3(est. 62 B)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000723783s[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@4146eb38)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@305698cc(est. 62 B)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@4146eb38) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into ImportDeletionTest.test_table
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "POST /admin/datasets/ImportDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ImportDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.w.Namespace	Job Manager slow ImportDeletionTest	Assigning Bucket[0] to Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received new WorkerInformation(size = 0,dataset = ImportDeletionTest)
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:07]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received Dictionary[ImportDeletionTest.test_table#ImportDeletionTest$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received Dictionary[ImportDeletionTest.test_table#ImportDeletionTest$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Start sending 1 Buckets
[WARN] [2023-01-06 20:44:07]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	One or more Children are not done yet
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into ImportDeletionTest.test_table2
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.w.Namespace	Job Manager slow ImportDeletionTest	Assigning Bucket[1] to Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "POST /admin/datasets/ImportDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_ImportDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
[INFO] [2023-01-06 20:44:07]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received Import[ImportDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received Import[ImportDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:07]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	Max cannot be decreased.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received ImportDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Adding Bucket[ImportDeletionTest.test_table.test_table.0]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-06 20:44:07]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	One or more Children are not done yet
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table.test_table.0.ImportDeletionTest.test_tree.test_column]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received ImportDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received ImportDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.0.ImportDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.1.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Checking state before deletion
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Executing query before deletion
[INFO] [2023-01-06 20:44:07]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[8a91b117-81d4-4f10-aa5e-83956b41b49b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Started ConceptQuery ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Started ConceptQuery ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	QueryPlan for Query[ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	QueryPlan for Query[ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b] with 0 results within PT0.000798S
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b, workerId=ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, startTime=2023-01-06T20:44:07.304784, finishTime=2023-01-06T20:44:07.305582) of size 0
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b] with 2 results within PT0.001825S
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b, workerId=ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, startTime=2023-01-06T20:44:07.304898, finishTime=2023-01-06T20:44:07.306723) of size 2
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=2] for Query[ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ImportDeletionTest]	DONE 8a91b117-81d4-4f10-aa5e-83956b41b49b ManagedQuery within PT0.005253S
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.8a91b117-81d4-4f10-aa5e-83956b41b49b HTTP/1.1" 200 1474 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Issuing deletion of import ImportDeletionTest.test_table2.test_table2
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "DELETE /admin/datasets/ImportDeletionTest/tables/ImportDeletionTest.test_table2/imports/ImportDeletionTest.test_table2.test_table2 HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
[INFO] [2023-01-06 20:44:07]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.RemoveImportJob	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Removing CBlock[ImportDeletionTest.test_table2.test_table2.0.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.RemoveImportJob	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Removing Bucket[ImportDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Removing CBlock[ImportDeletionTest.test_table2.test_table2.1.ImportDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Removing Bucket[ImportDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Checking state after deletion
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Executing query after deletion
[INFO] [2023-01-06 20:44:07]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[b2e2df32-e364-4a0d-9e3b-c7316f290585] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Started ConceptQuery ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Started ConceptQuery ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	QueryPlan for Query[ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	QueryPlan for Query[ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585] with 1 results within PT0.001099S
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 5
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585, workerId=ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, startTime=2023-01-06T20:44:07.461539, finishTime=2023-01-06T20:44:07.462638) of size 1
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=1] for Query[ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585] with 0 results within PT0.002005S
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585, workerId=ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, startTime=2023-01-06T20:44:07.461707, finishTime=2023-01-06T20:44:07.463712) of size 0
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ImportDeletionTest]	DONE b2e2df32-e364-4a0d-9e3b-c7316f290585 ManagedQuery within PT0.008109S
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.b2e2df32-e364-4a0d-9e3b-c7316f290585 HTTP/1.1" 200 1475 "-" "Conquery (test client)" 3
[INFO] [2023-01-06 20:44:07]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand		Preprocessing from command line config.
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	EXISTS ALREADY
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)		HASH OUTDATED
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 75 B in total
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000569733s[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=3, min=1, average=1.500000, max=2}
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=3, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@7ff3ce4)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@7ff3ce4) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@299be2c8(est. 80 B)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=3, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=3], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-06 20:44:07]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 1 Jobs:
[INFO] [2023-01-06 20:44:07]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob		Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob		Importing test_table2 into ImportDeletionTest.test_table2
[INFO] [2023-01-06 20:44:07]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Mapped 0 new ids
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Updating bucket assignments.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:07]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 3.
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 3.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-06 20:44:07]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	One or more Children are not done yet
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 3 entries.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Received ImportDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.0.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 3 entries.
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Received ImportDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.1.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Checking state after re-import
[INFO] [2023-01-06 20:44:07]	c.b.c.i.t.d.ImportDeletionTest		Executing query after re-import
[INFO] [2023-01-06 20:44:07]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[7402b71f-576e-48c6-ac30-fb1f916ff748] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	Started ConceptQuery ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	Started ConceptQuery ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53448]	QueryPlan for Query[ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53452]	QueryPlan for Query[ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748] with 2 results within PT0.001588S
[INFO] [2023-01-06 20:44:07]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748] with 0 results within PT0.001481S
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 7
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748, workerId=ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, startTime=2023-01-06T20:44:07.791227, finishTime=2023-01-06T20:44:07.792815) of size 2
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=2] for Query[ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748]
[INFO] [2023-01-06 20:44:07]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748, workerId=ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, startTime=2023-01-06T20:44:07.791371, finishTime=2023-01-06T20:44:07.792852) of size 0
[DEBUG] [2023-01-06 20:44:07]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748]
[DEBUG] [2023-01-06 20:44:07]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:07]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ImportDeletionTest]	DONE 7402b71f-576e-48c6-ac30-fb1f916ff748 ManagedQuery within PT0.004186S
127.0.0.1 - - [06/Jan/2023:20:44:07 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.7402b71f-576e-48c6-ac30-fb1f916ff748 HTTP/1.1" 200 1475 "-" "Conquery (test client)" 4
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-06 20:44:07]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:DATASET}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:TABLES}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:IMPORTS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:CONCEPTS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:WORKER}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:BUCKETS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:C_BLOCKS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53452	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:DATASET}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:TABLES}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:IMPORTS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:CONCEPTS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:WORKER}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:BUCKETS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:C_BLOCKS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53448	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ImportDeletionTest
[INFO] [2023-01-06 20:44:08]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ImportDeletionTest
[INFO] [2023-01-06 20:44:08]	c.b.c.m.w.Namespace		Closing namespace storage of ImportDeletionTest
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:DATASET}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:TABLES}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:IMPORTS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:CONCEPTS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:STRUCTURE}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
[INFO] [2023-01-06 20:44:08]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:44:08]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:DATASET}): 1 entries, 63 B within 201.1 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 62.04 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:TABLES}): 2 entries, 348 B within 398.4 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.524 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:IMPORTS}): 2 entries, 1,012 B within 4.771 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:CONCEPTS}): 1 entries, 622 B within 6.092 ms
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	Done reading Dataset[label=ImportDeletionTest, name=ImportDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 250.0 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:STRUCTURE}): 0 entries, 0 B within 38.46 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 315 B within 131.0 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 104 B within 565.1 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest	DONE reading Storage
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_ImportDeletionTest)]
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.271 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 92.66 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 72.79 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	3
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 3 entries, 1.8 KiB within 10.71 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 88.13 μs
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@3efecf77
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:44:08]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:44:08]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_27
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_28
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_29
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:44:08]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:44:08]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:44:08]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:DATASET}): 1 entries, 63 B within 132.5 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:SECONDARY_IDS}): 0 entries, 0 B within 26.17 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:TABLES}): 2 entries, 348 B within 213.2 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 771.5 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:DATASET}): 1 entries, 63 B within 93.63 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:SECONDARY_IDS}): 0 entries, 0 B within 33.34 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:TABLES}): 2 entries, 348 B within 174.9 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 858.2 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:IMPORTS}): 2 entries, 1,012 B within 2.799 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:IMPORTS}): 2 entries, 1,012 B within 3.672 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:CONCEPTS}): 1 entries, 622 B within 3.704 ms
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	Done reading Dataset[label=ImportDeletionTest, name=ImportDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:WORKER}): 1 entries, 138 B within 74.72 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store BUCKETS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:BUCKETS}): 2 entries, 746 B within 1.351 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	While processing store C_BLOCKS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185:C_BLOCKS}): 2 entries, 461 B within 208.3 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185	DONE reading Storage
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185))]
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:CONCEPTS}): 1 entries, 622 B within 3.998 ms
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	Done reading Dataset[label=ImportDeletionTest, name=ImportDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:WORKER}): 1 entries, 138 B within 97.90 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:BUCKETS}): 1 entries, 377 B within 1.235 ms
[DEBUG] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:08]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160:C_BLOCKS}): 1 entries, 235 B within 172.9 μs
[DEBUG] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160	DONE reading Storage
[INFO] [2023-01-06 20:44:08]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160))]
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:08]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53726 connected, waiting for identity
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	/127.0.0.1:53726	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53728 connected, waiting for identity
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	/127.0.0.1:53726	Sending worker identity 'worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160'
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	/127.0.0.1:53728	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:08]	c.b.c.c.ShardNode	/127.0.0.1:53728	Sending worker identity 'worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185'
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53726` registered.
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53728` registered.
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160 are consistent with the manager: 2 Imports
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160 are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185 are consistent with the manager: 2 Imports
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185 are consistent with the manager: 2 Buckets
[INFO] [2023-01-06 20:44:08]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Consistency check was successful
[WARN] [2023-01-06 20:44:08]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:09]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:44:09]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:09]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest6175202265804509010/tmp_ImportDeletionTest for Support
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:09]	c.b.c.i.t.d.ImportDeletionTest		Checking state after re-start
[INFO] [2023-01-06 20:44:09]	c.b.c.i.t.d.ImportDeletionTest		Executing query after re-import
[INFO] [2023-01-06 20:44:09]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:09]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[fca79b0a-23c4-4a74-928c-7d37df12ae3b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
127.0.0.1 - - [06/Jan/2023:20:44:09 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 83
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53728]	Started ConceptQuery ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53726]	Started ConceptQuery ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b
[DEBUG] [2023-01-06 20:44:09]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, /127.0.0.1:53728]	QueryPlan for Query[ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:09]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, /127.0.0.1:53726]	QueryPlan for Query[ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:09]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b] with 0 results within PT0.001369S
[INFO] [2023-01-06 20:44:09]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b] with 2 results within PT0.001639S
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b, workerId=ImportDeletionTest.worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160, startTime=2023-01-06T20:44:09.234193, finishTime=2023-01-06T20:44:09.235562) of size 0
[DEBUG] [2023-01-06 20:44:09]	c.b.c.m.q.ManagedQuery	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b]
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b, workerId=ImportDeletionTest.worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185, startTime=2023-01-06T20:44:09.234148, finishTime=2023-01-06T20:44:09.235787) of size 2
[DEBUG] [2023-01-06 20:44:09]	c.b.c.m.q.ManagedQuery	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received Result[size=2] for Query[ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b]
[INFO] [2023-01-06 20:44:09]	c.b.c.m.e.ManagedExecution	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	DONE fca79b0a-23c4-4a74-928c-7d37df12ae3b ManagedQuery within PT0.065947S
127.0.0.1 - - [06/Jan/2023:20:44:09 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.fca79b0a-23c4-4a74-928c-7d37df12ae3b HTTP/1.1" 200 1476 "-" "Conquery (test client)" 3
[INFO] [2023-01-06 20:44:09]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ImportDeletionTest
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[INFO] [2023-01-06 20:44:09]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[INFO] [2023-01-06 20:44:09]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[INFO] [2023-01-06 20:44:09]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ImportDeletionTest
[INFO] [2023-01-06 20:44:09]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:09]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[INFO] [2023-01-06 20:44:09]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_ImportDeletionTest_1d4fc122-88a7-4d5c-a404-2a3a1df5b160
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:09]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_ImportDeletionTest_f02d8fae-8348-4946-a2b0-d69831439185
[INFO] [2023-01-06 20:44:09]	c.b.c.m.w.Namespace		Removing namespace storage of ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[INFO] [2023-01-06 20:44:09]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_ImportDeletionTest
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:09]	c.b.c.i.IntegrationTest$Wrapper	ImportDeletionTest	SUCCESS integration test ImportDeletionTest
[INFO] [2023-01-06 20:44:09]	c.b.c.i.IntegrationTest$Wrapper	TableDeletionTest	STARTING integration test TableDeletionTest
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery	TableDeletionTest	Setting up dataset
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:DATASET}): 0 entries, 0 B within 145.6 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 79.76 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:TABLES}): 0 entries, 0 B within 69.12 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 74.10 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:IMPORTS}): 0 entries, 0 B within 67.59 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:CONCEPTS}): 0 entries, 0 B within 65.98 μs
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.NamespacedStorage	TableDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 69.29 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:STRUCTURE}): 0 entries, 0 B within 73.29 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 66.47 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 65.60 μs
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:DATASET}): 0 entries, 0 B within 84.69 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:SECONDARY_IDS}): 0 entries, 0 B within 27.58 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:TABLES}): 0 entries, 0 B within 20.34 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 30.61 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:IMPORTS}): 0 entries, 0 B within 18.64 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:CONCEPTS}): 0 entries, 0 B within 18.54 μs
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:WORKER}): 0 entries, 0 B within 15.89 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:BUCKETS}): 0 entries, 0 B within 17.95 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:C_BLOCKS}): 0 entries, 0 B within 19.06 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:DATASET}): 0 entries, 0 B within 67.84 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:SECONDARY_IDS}): 0 entries, 0 B within 34.79 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:TABLES}): 0 entries, 0 B within 24.24 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.21 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:IMPORTS}): 0 entries, 0 B within 35.04 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:CONCEPTS}): 0 entries, 0 B within 41.46 μs
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:WORKER}): 0 entries, 0 B within 30.25 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:BUCKETS}): 0 entries, 0 B within 21.94 μs
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:09]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:C_BLOCKS}): 0 entries, 0 B within 24.95 μs
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315 are consistent with the manager: 0 Imports
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315 are consistent with the manager: 0 Buckets
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received update of Table TableDeletionTest.test_table
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received update of Table TableDeletionTest.test_table
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-06 20:44:09]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-06 20:44:09]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Updating Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:09]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Updating Concept[TableDeletionTest.test_tree]
[INFO] [2023-01-06 20:44:09]	c.b.c.c.PreprocessorCommand	TableDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.055772755s[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@30b91756)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@30b91756) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@55f91548(est. 62 B)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000857462s[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@287b4784)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@287b4784) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@cc793d5(est. 62 B)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-06 20:44:10]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into TableDeletionTest.test_table
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Updating bucket assignments.
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "POST /admin/datasets/TableDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_TableDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.w.Namespace	Job Manager slow TableDeletionTest	Assigning Bucket[0] to Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received new WorkerInformation(size = 0,dataset = TableDeletionTest)
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:10]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received Dictionary[TableDeletionTest.test_table#TableDeletionTest$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received Dictionary[TableDeletionTest.test_table#TableDeletionTest$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Start sending 1 Buckets
[WARN] [2023-01-06 20:44:10]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	One or more Children are not done yet
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received Import[TableDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received TableDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Adding Bucket[TableDeletionTest.test_table.test_table.0]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received Import[TableDeletionTest.test_table.test_table], containing 2 entries.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table.test_table.0.TableDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into TableDeletionTest.test_table2
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.w.Namespace	Job Manager slow TableDeletionTest	Assigning Bucket[1] to Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "POST /admin/datasets/TableDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_TableDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:10]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Start sending 2 Buckets
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[WARN] [2023-01-06 20:44:10]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	One or more Children are not done yet
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received TableDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Adding Bucket[TableDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.0.TableDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received TableDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Adding Bucket[TableDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.1.TableDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:10]	c.b.c.i.t.d.TableDeletionTest		Checking state before deletion
[INFO] [2023-01-06 20:44:10]	c.b.c.i.t.d.TableDeletionTest		Executing query before deletion
[INFO] [2023-01-06 20:44:10]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[TableDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:10]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[1b1211d8-767c-4c30-afcf-2888448a8faa] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TableDeletionTest))]]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Started ConceptQuery TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Started ConceptQuery TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	QueryPlan for Query[TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	QueryPlan for Query[TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:10]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa] with 0 results within PT0.000962S
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 201 1208 "-" "Conquery (test client)" 9
[INFO] [2023-01-06 20:44:10]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa] with 2 results within PT0.00183S
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa, workerId=TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, startTime=2023-01-06T20:44:10.313006, finishTime=2023-01-06T20:44:10.313968) of size 0
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=0] for Query[TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa, workerId=TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, startTime=2023-01-06T20:44:10.312875, finishTime=2023-01-06T20:44:10.314705) of size 2
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=2] for Query[TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=TableDeletionTest]	DONE 1b1211d8-767c-4c30-afcf-2888448a8faa ManagedQuery within PT0.00661S
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "GET /api/datasets/TableDeletionTest/queries/TableDeletionTest.1b1211d8-767c-4c30-afcf-2888448a8faa HTTP/1.1" 200 1467 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:44:10]	c.b.c.i.t.d.TableDeletionTest		Issuing deletion of import TableDeletionTest.test_table2
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "DELETE /admin/datasets/TableDeletionTest/tables/TableDeletionTest.test_table2 HTTP/1.1" 409 31 "-" "Conquery (test client)" 7
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Removing CBlock[TableDeletionTest.test_table.test_table.0.TableDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Removing CBlock[TableDeletionTest.test_table2.test_table2.0.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Removing Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Removing CBlock[TableDeletionTest.test_table2.test_table2.1.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Removing Concept[TableDeletionTest.test_tree]
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "DELETE /admin/datasets/TableDeletionTest/tables/TableDeletionTest.test_table2 HTTP/1.1" 200 2 "-" "Conquery (test client)" 6
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.RemoveImportJob	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Removing Bucket[TableDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.RemoveImportJob	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Removing Bucket[TableDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.RemoveTable	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.RemoveTable	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:10]	c.b.c.i.t.d.TableDeletionTest		Checking state after deletion
[INFO] [2023-01-06 20:44:10]	c.b.c.i.t.d.TableDeletionTest		Executing query after deletion
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.j.ConqueryJsonExceptionMapper	user.SUPERUSER@SUPERUSER	Unable to process JSON in request 'http://localhost:46651/api/datasets/TableDeletionTest/queries'
com.fasterxml.jackson.databind.JsonMappingException: Error while resolving entry TableDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector (through reference chain: com.bakdata.conquery.apiv1.query.ConceptQuery["root"]->com.bakdata.conquery.apiv1.query.concept.specific.CQConcept["tables"]->java.util.ArrayList[0]->com.bakdata.conquery.apiv1.query.concept.filter.CQTable["id"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:254)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:155)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:286)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.impl.ManagedReferenceProperty.deserializeAndSet(ManagedReferenceProperty.java:62)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:295)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:138)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:278)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:282)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while resolving entry TableDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:65)
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:26)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:252)
	... 96 common frames omitted
Caused by: com.bakdata.conquery.io.jackson.serializer.IdReferenceResolvingException: Could not find entry `TableDeletionTest.test_tree.test_column` of type com.bakdata.conquery.models.datasets.concepts.Connector
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 123]
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:55)
	... 99 common frames omitted
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 400 197 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand		Preprocessing from command line config.
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	EXISTS ALREADY
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)		HASH STILL VALID
[INFO] [2023-01-06 20:44:10]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 0 B in total
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into TableDeletionTest.test_table2
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Mapped 0 new ids
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Updating bucket assignments.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:10 +0000] "POST /admin/datasets/TableDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest6175202265804509010%2Ftmp_TableDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-06 20:44:10]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-06 20:44:10]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	One or more Children are not done yet
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Received TableDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Adding Bucket[TableDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-06 20:44:10]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Received TableDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Adding Bucket[TableDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-06 20:44:10]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Updating Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Updating Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.1.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table.test_table.0.TableDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-06 20:44:10]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.0.TableDeletionTest.test_tree.test_column2]
[INFO] [2023-01-06 20:44:11]	c.b.c.i.t.d.TableDeletionTest		Checking state after re-import
[INFO] [2023-01-06 20:44:11]	c.b.c.i.t.d.TableDeletionTest		Executing query after re-import
[INFO] [2023-01-06 20:44:11]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[TableDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:11]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[3ae28ad0-0536-43da-a00d-0c3e867b4078] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TableDeletionTest))]]
[INFO] [2023-01-06 20:44:11]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	Started ConceptQuery TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078
[INFO] [2023-01-06 20:44:11]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	Started ConceptQuery TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53726]	QueryPlan for Query[TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53728]	QueryPlan for Query[TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:11]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078] with 0 results within PT0.001275S
127.0.0.1 - - [06/Jan/2023:20:44:11 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 201 1208 "-" "Conquery (test client)" 7
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:11]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078, workerId=TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, startTime=2023-01-06T20:44:11.051956, finishTime=2023-01-06T20:44:11.053231) of size 0
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=0] for Query[TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078]
[INFO] [2023-01-06 20:44:11]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078] with 2 results within PT0.002558S
[INFO] [2023-01-06 20:44:11]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078, workerId=TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, startTime=2023-01-06T20:44:11.051878, finishTime=2023-01-06T20:44:11.054436) of size 2
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=2] for Query[TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078]
[INFO] [2023-01-06 20:44:11]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=TableDeletionTest]	DONE 3ae28ad0-0536-43da-a00d-0c3e867b4078 ManagedQuery within PT0.005472S
127.0.0.1 - - [06/Jan/2023:20:44:11 +0000] "GET /api/datasets/TableDeletionTest/queries/TableDeletionTest.3ae28ad0-0536-43da-a00d-0c3e867b4078 HTTP/1.1" 200 1467 "-" "Conquery (test client)" 3
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:DATASET}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:TABLES}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:IMPORTS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:CONCEPTS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:WORKER}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:BUCKETS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:C_BLOCKS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53728	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:DATASET}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:TABLES}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:IMPORTS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:CONCEPTS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:WORKER}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:BUCKETS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:C_BLOCKS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:53726	Disconnected from ManagerNode
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:43067]	Client 'null' disconnected 
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager fast TableDeletionTest
[INFO] [2023-01-06 20:44:11]	c.b.c.m.j.JobExecutor		Closing Job Manager slow TableDeletionTest
[INFO] [2023-01-06 20:44:11]	c.b.c.m.w.Namespace		Closing namespace storage of TableDeletionTest
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:DATASET}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:TABLES}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:IMPORTS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:CONCEPTS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:STRUCTURE}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
[INFO] [2023-01-06 20:44:11]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest6175202265804509010
[DEBUG] [2023-01-06 20:44:11]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:DATASET}): 1 entries, 61 B within 208.9 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 50.66 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:TABLES}): 2 entries, 346 B within 324.8 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.451 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:IMPORTS}): 2 entries, 1,002 B within 4.504 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:CONCEPTS}): 1 entries, 617 B within 4.773 ms
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	Done reading Dataset[label=TableDeletionTest, name=TableDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 224.0 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:STRUCTURE}): 0 entries, 0 B within 23.93 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 308 B within 99.84 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 103 B within 492.0 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest	DONE reading Storage
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_TableDeletionTest)]
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.109 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 86.73 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 86.77 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}): 2 entries, 1.2 KiB within 9.452 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 68.81 μs
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@57d920f6
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-06 20:44:11]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-06 20:44:11]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_30
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_31
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_32
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-06 20:44:11]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-06 20:44:11]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-06 20:44:11]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:DATASET}): 1 entries, 61 B within 138.6 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:SECONDARY_IDS}): 0 entries, 0 B within 36.74 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:TABLES}): 2 entries, 346 B within 235.5 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 957.9 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	BEGIN reading Storage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:DATASET}): 1 entries, 61 B within 115.7 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:SECONDARY_IDS}): 0 entries, 0 B within 41.93 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:TABLES}): 2 entries, 346 B within 225.1 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.112 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:IMPORTS}): 2 entries, 1,002 B within 4.026 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:IMPORTS}): 2 entries, 1,002 B within 3.764 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:CONCEPTS}): 1 entries, 617 B within 4.191 ms
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	Done reading Dataset[label=TableDeletionTest, name=TableDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:WORKER}): 1 entries, 136 B within 89.35 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:BUCKETS}): 1 entries, 369 B within 1.421 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d:C_BLOCKS}): 1 entries, 230 B within 151.6 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d	DONE reading Storage
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d))]
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:CONCEPTS}): 1 entries, 617 B within 4.762 ms
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	Done reading Dataset[label=TableDeletionTest, name=TableDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:WORKER}): 1 entries, 136 B within 77.47 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store BUCKETS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:BUCKETS}): 2 entries, 740 B within 1.738 ms
[DEBUG] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	While processing store C_BLOCKS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-06 20:44:11]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315:C_BLOCKS}): 2 entries, 457 B within 240.9 μs
[DEBUG] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315	DONE reading Storage
[INFO] [2023-01-06 20:44:11]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315))]
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-06 20:44:11]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:43067
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:11]	c.b.c.c.ShardNode	/127.0.0.1:53994	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:12]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53994 connected, waiting for identity
[INFO] [2023-01-06 20:44:12]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:43067
[INFO] [2023-01-06 20:44:12]	c.b.c.c.ShardNode	/127.0.0.1:53994	Sending worker identity 'worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315'
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53994` registered.
[INFO] [2023-01-06 20:44:12]	c.b.c.c.ShardNode	/127.0.0.1:53998	Connected to ManagerNode @ /127.0.0.1:43067
[INFO] [2023-01-06 20:44:12]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:43067]	New client /127.0.0.1:53998 connected, waiting for identity
[INFO] [2023-01-06 20:44:12]	c.b.c.c.ShardNode	/127.0.0.1:53998	Sending worker identity 'worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d'
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315 are consistent with the manager: 2 Imports
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315 are consistent with the manager: 2 Buckets
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Consistency check was successful
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:53998` registered.
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d are consistent with the manager: 2 Imports
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d are consistent with the manager: 1 Buckets
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Consistency check was successful
[WARN] [2023-01-06 20:44:12]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:12]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-06 20:44:12]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-06 20:44:12]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-06 20:44:12]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-06 20:44:12]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest6175202265804509010/tmp_TableDeletionTest for Support
[INFO] [2023-01-06 20:44:12]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:12]	c.b.c.i.t.d.TableDeletionTest		Checking state after re-start
[INFO] [2023-01-06 20:44:12]	c.b.c.i.t.d.TableDeletionTest		Executing query after re-import
[INFO] [2023-01-06 20:44:12]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[TableDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-06 20:44:12]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TableDeletionTest))]]
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53994]	Started ConceptQuery TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a
127.0.0.1 - - [06/Jan/2023:20:44:12 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 201 1208 "-" "Conquery (test client)" 77
[DEBUG] [2023-01-06 20:44:12]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, /127.0.0.1:53994]	QueryPlan for Query[TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-06 20:44:12]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a] with 2 results within PT0.001135S
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53998]	Started ConceptQuery TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a
[DEBUG] [2023-01-06 20:44:12]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, /127.0.0.1:53998]	QueryPlan for Query[TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-06 20:44:12]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a] with 0 results within PT0.000772S
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a, workerId=TableDeletionTest.worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d, startTime=2023-01-06T20:44:12.368075, finishTime=2023-01-06T20:44:12.368847) of size 0
[DEBUG] [2023-01-06 20:44:12]	c.b.c.m.q.ManagedQuery	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received Result[size=0] for Query[TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a]
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a, workerId=TableDeletionTest.worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315, startTime=2023-01-06T20:44:12.366394, finishTime=2023-01-06T20:44:12.367529) of size 2
[DEBUG] [2023-01-06 20:44:12]	c.b.c.m.q.ManagedQuery	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received Result[size=2] for Query[TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a]
[INFO] [2023-01-06 20:44:12]	c.b.c.m.e.ManagedExecution	Dataset[label=TableDeletionTest, name=TableDeletionTest]	DONE 0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a ManagedQuery within PT0.051816S
127.0.0.1 - - [06/Jan/2023:20:44:12 +0000] "GET /api/datasets/TableDeletionTest/queries/TableDeletionTest.0c5eb7f9-4f16-40cd-bb5f-ebb926f2187a HTTP/1.1" 200 1467 "-" "Conquery (test client)" 5
[INFO] [2023-01-06 20:44:12]	c.b.c.m.j.JobExecutor		Closing Job Manager fast TableDeletionTest
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[INFO] [2023-01-06 20:44:12]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[INFO] [2023-01-06 20:44:12]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[INFO] [2023-01-06 20:44:12]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:12]	c.b.c.m.j.JobExecutor		Closing Job Manager slow TableDeletionTest
[INFO] [2023-01-06 20:44:12]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:12]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[INFO] [2023-01-06 20:44:12]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node0/worker_worker_TableDeletionTest_8a2b4412-2934-4f6d-9feb-6e867a7ff315
[INFO] [2023-01-06 20:44:12]	c.b.c.m.w.Namespace		Removing namespace storage of TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[INFO] [2023-01-06 20:44:12]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[DEBUG] [2023-01-06 20:44:12]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:12]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/shard-node1/worker_worker_TableDeletionTest_c8315fc8-e850-4f58-9804-396bf106784d
[INFO] [2023-01-06 20:44:12]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-06 20:44:12]	c.b.c.i.IntegrationTest$Wrapper	TableDeletionTest	SUCCESS integration test TableDeletionTest
[INFO] [2023-01-06 20:44:12]	c.b.c.u.s.TestConquery	TableDeletionTest	Working in temporary directory /tmp/conqueryIntegrationTest6499145362419833625
INFO  [2023-01-06 20:44:12,665] io.dropwizard.server.DefaultServerFactory: Registering jersey handler with root path prefix: /
INFO  [2023-01-06 20:44:12,665] io.dropwizard.server.DefaultServerFactory: Registering admin handler with root path prefix: /
INFO  [2023-01-06 20:44:12,669] com.bakdata.conquery.commands.ManagerNode: Opening MetaStorage
INFO  [2023-01-06 20:44:12,669] com.bakdata.conquery.commands.ManagerNode: Loading MetaStorage
INFO  [2023-01-06 20:44:12,669] com.bakdata.conquery.commands.ManagerNode: MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@348e89f1
WARN  [2023-01-06 20:44:12,669] com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm: 
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
INFO  [2023-01-06 20:44:12,669] com.bakdata.conquery.models.auth.AuthorizationController: Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_33
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_34
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_35
INFO  [2023-01-06 20:44:12,669] com.bakdata.conquery.commands.ManagerNode: Registering ResourcesProvider
INFO  [2023-01-06 20:44:12,673] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
INFO  [2023-01-06 20:44:12,673] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
INFO  [2023-01-06 20:44:12,690] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-06 20:44:12,690] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-06 20:44:12,691] io.dropwizard.server.ServerFactory: Starting Conquery


 ▄████████  ▄██████▄  ███▄▄▄▄   ████████▄   ███    █▄     ▄████████    ▄████████ ▄██   ▄   
███    ███ ███    ███ ███▀▀▀██▄ ███    ███  ███    ███   ███    ███   ███    ███ ███   ██▄ 
███    █▀  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▀    ███    ███ ███▄▄▄███ 
███        ███    ███ ███   ███ ███    ███  ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀ ▀▀▀▀▀▀███ 
███        ███    ███ ███   ███ ███    ███  ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀   ▄██   ███ 
███    █▄  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▄  ▀███████████ ███   ███ 
███    ███ ███    ███ ███   ███ ███  ▀ ███  ███    ███   ███    ███   ███    ███ ███   ███ 
████████▀   ▀██████▀   ▀█   █▀   ▀██████▀▄█ ████████▀    ██████████   ███    ███  ▀█████▀  
                                                                      ███    ███           
Version: 0.0.0-SNAPSHOT(${git.commit.id.describe})

           
INFO  [2023-01-06 20:44:12,693] org.eclipse.jetty.setuid.SetUIDListener: Opened application@72f04ef6{HTTP/1.1, (http/1.1)}{0.0.0.0:43121}
INFO  [2023-01-06 20:44:12,693] org.eclipse.jetty.setuid.SetUIDListener: Opened admin@6f06231a{HTTP/1.1, (http/1.1)}{0.0.0.0:37917}
INFO  [2023-01-06 20:44:12,693] org.eclipse.jetty.server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.17+8-post-Ubuntu-1ubuntu218.04
INFO  [2023-01-06 20:44:12,700] com.bakdata.conquery.commands.ManagerNode: Started ManagerNode @ /0:0:0:0:0:0:0:0:45779
INFO  [2023-01-06 20:44:12,701] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:45779
INFO  [2023-01-06 20:44:12,702] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:45779
INFO  [2023-01-06 20:44:12,702] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:59356 connected, waiting for identity
INFO  [2023-01-06 20:44:12,703] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:45779
INFO  [2023-01-06 20:44:12,729] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:59358 connected, waiting for identity
INFO  [2023-01-06 20:44:12,729] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:45779
INFO  [2023-01-06 20:44:12,733] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:59356` registered.
INFO  [2023-01-06 20:44:12,735] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:59358` registered.
INFO  [2023-01-06 20:44:12,815] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /api/config/frontend (com.bakdata.conquery.resources.api.ConfigResource)
    GET     /api/datasets (com.bakdata.conquery.resources.api.DatasetsResource)
    GET     /api/datasets/{dataset}/concepts (com.bakdata.conquery.resources.api.DatasetResource)
    GET     /api/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/resolve (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/autocomplete (com.bakdata.conquery.resources.api.FilterResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/resolve (com.bakdata.conquery.resources.api.FilterResource)
    GET     /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    POST    /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    DELETE  /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    PATCH   /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-queries (com.bakdata.conquery.resources.api.FormResource)
    GET     /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/upload (com.bakdata.conquery.resources.api.QueryResource)
    DELETE  /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    PATCH   /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/cancel (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/reexecute (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/result/{query}.arrf (com.bakdata.conquery.resources.api.ResultArrowFileResource)
    GET     /api/datasets/{dataset}/result/{query}.arrs (com.bakdata.conquery.resources.api.ResultArrowStreamResource)
    GET     /api/datasets/{dataset}/result/{query}.csv (com.bakdata.conquery.resources.api.ResultCsvResource)
    GET     /api/datasets/{dataset}/result/{query}.xlsx (com.bakdata.conquery.resources.api.ResultExcelResource)
    GET     /api/me (com.bakdata.conquery.resources.api.MeResource)

WARN  [2023-01-06 20:44:12,816] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

INFO  [2023-01-06 20:44:12,844] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-06 20:44:12,845] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@1c824fb3{/,null,AVAILABLE}
INFO  [2023-01-06 20:44:12,845] io.dropwizard.setup.AdminEnvironment: tasks = 

    POST    /tasks/permission-cleanup (com.bakdata.conquery.tasks.PermissionCleanupTask)
    POST    /tasks/report-consistency (com.bakdata.conquery.tasks.ReportConsistencyTask)
    POST    /tasks/log-level (io.dropwizard.servlets.tasks.LogConfigurationTask)
    POST    /tasks/form-scanner (com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner)
    POST    /tasks/clear-filter-source-search (com.bakdata.conquery.tasks.ClearFilterSourceSearch)
    POST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)
    POST    /tasks/query-cleanup (com.bakdata.conquery.tasks.QueryCleanupTask)
    POST    /tasks/shutdown (com.bakdata.conquery.resources.admin.ShutdownTask)

WARN  [2023-01-06 20:44:12,845] io.dropwizard.setup.AdminEnvironment: 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
INFO  [2023-01-06 20:44:12,900] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin/auth-overview/csv (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/auth-overview/csv/group/{groupId} (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    POST    /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    DELETE  /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    GET     /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    POST    /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/label (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/secondaryId (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/secondaryId/{secondaryId} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/structure (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    DELETE  /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    POST    /admin/datasets/{dataset}/update-matching-stats (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/weight (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/jobs (com.bakdata.conquery.resources.admin.rest.AdminResource)
    POST    /admin/jobs/{jobId}/cancel (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/logout (com.bakdata.conquery.resources.admin.rest.AdminResource)
    DELETE  /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    POST    /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    GET     /admin/queries (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    DELETE  /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    GET     /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/script (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/upload (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    GET     /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)

WARN  [2023-01-06 20:44:12,900] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

INFO  [2023-01-06 20:44:12,937] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin-ui/ (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/auth-overview (com.bakdata.conquery.resources.admin.ui.AuthOverviewUIResource)
    GET     /admin-ui/datasets (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset} (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.ui.ConceptsUIResource)
    GET     /admin-ui/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table}/import/{importId} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/groups (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/groups/{groupId} (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/jobs (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/queries (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/roles (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/roles/{roleId} (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/script (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/users (com.bakdata.conquery.resources.admin.ui.UserUIResource)
    GET     /admin-ui/users/{userId} (com.bakdata.conquery.resources.admin.ui.UserUIResource)

WARN  [2023-01-06 20:44:12,937] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

INFO  [2023-01-06 20:44:12,966] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-06 20:44:12,966] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@30b4f96c{/,null,AVAILABLE}
INFO  [2023-01-06 20:44:12,972] org.eclipse.jetty.server.AbstractConnector: Started application@72f04ef6{HTTP/1.1, (http/1.1)}{0.0.0.0:43121}
INFO  [2023-01-06 20:44:12,974] org.eclipse.jetty.server.AbstractConnector: Started admin@6f06231a{HTTP/1.1, (http/1.1)}{0.0.0.0:37917}
INFO  [2023-01-06 20:44:12,974] org.eclipse.jetty.server.Server: Started @57499ms
INFO  [2023-01-06 20:44:13,101] com.bakdata.conquery.util.support.TestConquery: Working in temporary directory /tmp/conqueryIntegrationTest6499145362419833625
INFO  [2023-01-06 20:44:13,136] io.dropwizard.server.DefaultServerFactory: Registering jersey handler with root path prefix: /
INFO  [2023-01-06 20:44:13,136] io.dropwizard.server.DefaultServerFactory: Registering admin handler with root path prefix: /
INFO  [2023-01-06 20:44:13,144] com.bakdata.conquery.commands.ManagerNode: Opening MetaStorage
INFO  [2023-01-06 20:44:13,144] com.bakdata.conquery.commands.ManagerNode: Loading MetaStorage
INFO  [2023-01-06 20:44:13,144] com.bakdata.conquery.commands.ManagerNode: MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@7452a620
WARN  [2023-01-06 20:44:13,144] com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm: 
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
INFO  [2023-01-06 20:44:13,144] com.bakdata.conquery.models.auth.AuthorizationController: Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_36
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_37
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_38
INFO  [2023-01-06 20:44:13,145] com.bakdata.conquery.commands.ManagerNode: Registering ResourcesProvider
INFO  [2023-01-06 20:44:13,149] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
INFO  [2023-01-06 20:44:13,149] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
INFO  [2023-01-06 20:44:13,242] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-06 20:44:13,243] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-06 20:44:13,244] io.dropwizard.server.ServerFactory: Starting Conquery


 ▄████████  ▄██████▄  ███▄▄▄▄   ████████▄   ███    █▄     ▄████████    ▄████████ ▄██   ▄   
███    ███ ███    ███ ███▀▀▀██▄ ███    ███  ███    ███   ███    ███   ███    ███ ███   ██▄ 
███    █▀  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▀    ███    ███ ███▄▄▄███ 
███        ███    ███ ███   ███ ███    ███  ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀ ▀▀▀▀▀▀███ 
███        ███    ███ ███   ███ ███    ███  ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀   ▄██   ███ 
███    █▄  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▄  ▀███████████ ███   ███ 
███    ███ ███    ███ ███   ███ ███  ▀ ███  ███    ███   ███    ███   ███    ███ ███   ███ 
████████▀   ▀██████▀   ▀█   █▀   ▀██████▀▄█ ████████▀    ██████████   ███    ███  ▀█████▀  
                                                                      ███    ███           
Version: 0.0.0-SNAPSHOT(${git.commit.id.describe})

           
INFO  [2023-01-06 20:44:13,300] org.eclipse.jetty.setuid.SetUIDListener: Opened application@5acbe4e9{HTTP/1.1, (http/1.1)}{0.0.0.0:34843}
INFO  [2023-01-06 20:44:13,300] org.eclipse.jetty.setuid.SetUIDListener: Opened admin@6cfcbc1e{HTTP/1.1, (http/1.1)}{0.0.0.0:45265}
INFO  [2023-01-06 20:44:13,300] org.eclipse.jetty.server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.17+8-post-Ubuntu-1ubuntu218.04
INFO  [2023-01-06 20:44:13,301] com.bakdata.conquery.commands.ManagerNode: Started ManagerNode @ /0:0:0:0:0:0:0:0:38057
INFO  [2023-01-06 20:44:13,302] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:38057
INFO  [2023-01-06 20:44:13,308] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:48612 connected, waiting for identity
INFO  [2023-01-06 20:44:13,308] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:38057
INFO  [2023-01-06 20:44:13,310] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:38057
INFO  [2023-01-06 20:44:13,311] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:48616 connected, waiting for identity
INFO  [2023-01-06 20:44:13,311] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:38057
INFO  [2023-01-06 20:44:13,335] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:48616` registered.
INFO  [2023-01-06 20:44:13,335] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:48612` registered.
INFO  [2023-01-06 20:44:13,408] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /api/config/frontend (com.bakdata.conquery.resources.api.ConfigResource)
    GET     /api/datasets (com.bakdata.conquery.resources.api.DatasetsResource)
    GET     /api/datasets/{dataset}/concepts (com.bakdata.conquery.resources.api.DatasetResource)
    GET     /api/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/resolve (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/autocomplete (com.bakdata.conquery.resources.api.FilterResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/resolve (com.bakdata.conquery.resources.api.FilterResource)
    GET     /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    POST    /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    DELETE  /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    PATCH   /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-queries (com.bakdata.conquery.resources.api.FormResource)
    GET     /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/upload (com.bakdata.conquery.resources.api.QueryResource)
    DELETE  /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    PATCH   /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/cancel (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/reexecute (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/result/{query}.arrf (com.bakdata.conquery.resources.api.ResultArrowFileResource)
    GET     /api/datasets/{dataset}/result/{query}.arrs (com.bakdata.conquery.resources.api.ResultArrowStreamResource)
    GET     /api/datasets/{dataset}/result/{query}.csv (com.bakdata.conquery.resources.api.ResultCsvResource)
    GET     /api/datasets/{dataset}/result/{query}.xlsx (com.bakdata.conquery.resources.api.ResultExcelResource)
    GET     /api/me (com.bakdata.conquery.resources.api.MeResource)

WARN  [2023-01-06 20:44:13,408] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

INFO  [2023-01-06 20:44:13,450] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-06 20:44:13,450] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@6ae5981f{/,null,AVAILABLE}
INFO  [2023-01-06 20:44:13,450] io.dropwizard.setup.AdminEnvironment: tasks = 

    POST    /tasks/permission-cleanup (com.bakdata.conquery.tasks.PermissionCleanupTask)
    POST    /tasks/report-consistency (com.bakdata.conquery.tasks.ReportConsistencyTask)
    POST    /tasks/log-level (io.dropwizard.servlets.tasks.LogConfigurationTask)
    POST    /tasks/form-scanner (com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner)
    POST    /tasks/clear-filter-source-search (com.bakdata.conquery.tasks.ClearFilterSourceSearch)
    POST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)
    POST    /tasks/query-cleanup (com.bakdata.conquery.tasks.QueryCleanupTask)
    POST    /tasks/shutdown (com.bakdata.conquery.resources.admin.ShutdownTask)

WARN  [2023-01-06 20:44:13,450] io.dropwizard.setup.AdminEnvironment: 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
INFO  [2023-01-06 20:44:13,508] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin/auth-overview/csv (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/auth-overview/csv/group/{groupId} (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    POST    /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    DELETE  /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    GET     /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    POST    /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/label (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/secondaryId (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/secondaryId/{secondaryId} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/structure (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    DELETE  /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    POST    /admin/datasets/{dataset}/update-matching-stats (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/weight (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/jobs (com.bakdata.conquery.resources.admin.rest.AdminResource)
    POST    /admin/jobs/{jobId}/cancel (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/logout (com.bakdata.conquery.resources.admin.rest.AdminResource)
    DELETE  /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    POST    /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    GET     /admin/queries (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    DELETE  /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    GET     /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/script (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/upload (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    GET     /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)

WARN  [2023-01-06 20:44:13,508] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

INFO  [2023-01-06 20:44:13,546] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin-ui/ (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/auth-overview (com.bakdata.conquery.resources.admin.ui.AuthOverviewUIResource)
    GET     /admin-ui/datasets (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset} (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.ui.ConceptsUIResource)
    GET     /admin-ui/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table}/import/{importId} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/groups (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/groups/{groupId} (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/jobs (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/queries (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/roles (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/roles/{roleId} (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/script (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/users (com.bakdata.conquery.resources.admin.ui.UserUIResource)
    GET     /admin-ui/users/{userId} (com.bakdata.conquery.resources.admin.ui.UserUIResource)

WARN  [2023-01-06 20:44:13,546] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

INFO  [2023-01-06 20:44:13,575] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-06 20:44:13,576] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@a984ac7{/,null,AVAILABLE}
INFO  [2023-01-06 20:44:13,580] org.eclipse.jetty.server.AbstractConnector: Started application@5acbe4e9{HTTP/1.1, (http/1.1)}{0.0.0.0:34843}
INFO  [2023-01-06 20:44:13,581] org.eclipse.jetty.server.AbstractConnector: Started admin@6cfcbc1e{HTTP/1.1, (http/1.1)}{0.0.0.0:45265}
INFO  [2023-01-06 20:44:13,581] org.eclipse.jetty.server.Server: Started @58106ms
INFO  [2023-01-06 20:44:13,701] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT Test
INFO  [2023-01-06 20:44:13,701] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:13,702] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:13,709] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-06 20:44:13,709] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-06 20:44:13,709] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:13,709] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:13,716] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:13,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_e1ea1168-0d99-4a98-bcbb-ca5038b1c3ff are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:13,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_e1ea1168-0d99-4a98-bcbb-ca5038b1c3ff are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:13,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:13,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_34036a18-6768-4e3e-917b-168083706f85 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:13,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_34036a18-6768-4e3e-917b-168083706f85 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:13,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:13,853] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:13,853] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT$20Test.table
INFO  [2023-01-06 20:44:13,855] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT$20Test.table
INFO  [2023-01-06 20:44:14,010] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:14,149] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:14,155] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:14,155] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 76 B in total
INFO  [2023-01-06 20:44:14,155] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000275814sINFO  [2023-01-06 20:44:14,183] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:14,183] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3eff6764)
INFO  [2023-01-06 20:44:14,184] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:14,187] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:14,187] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:14,187] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:14,213] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:14 +0000] "POST /admin/datasets/BIG_MULTI_SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:44:14,216] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:14,218] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:14,219] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:14,219] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:14,221] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-06 20:44:14,222] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:14,235] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-06 20:44:14,235] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-06 20:44:14,235] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT$20Test.table.table.1
INFO  [2023-01-06 20:44:14,236] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT$20Test.table.table.0
INFO  [2023-01-06 20:44:14,355] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT Test QUERY INIT
INFO  [2023-01-06 20:44:14,385] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:14,386] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[292b43ac-03cb-42c5-935a-459f20297328] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT Test))]]
127.0.0.1 - - [06/Jan/2023:20:44:14 +0000] "POST /api/datasets/BIG_MULTI_SELECT$20Test/queries HTTP/1.1" 201 1197 "-" "Conquery (test client)" 34
INFO  [2023-01-06 20:44:14,411] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328
INFO  [2023-01-06 20:44:14,411] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328
INFO  [2023-01-06 20:44:14,413] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328] with 0 results within PT0.001706S
INFO  [2023-01-06 20:44:14,413] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328] with 2 results within PT0.001754S
INFO  [2023-01-06 20:44:14,416] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328, workerId=BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_e1ea1168-0d99-4a98-bcbb-ca5038b1c3ff, startTime=2023-01-06T20:44:14.411826, finishTime=2023-01-06T20:44:14.413532) of size 0
INFO  [2023-01-06 20:44:14,416] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328, workerId=BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_34036a18-6768-4e3e-917b-168083706f85, startTime=2023-01-06T20:44:14.411835, finishTime=2023-01-06T20:44:14.413589) of size 2
INFO  [2023-01-06 20:44:14,417] com.bakdata.conquery.models.execution.ManagedExecution: DONE 292b43ac-03cb-42c5-935a-459f20297328 ManagedQuery within PT0.031089S
127.0.0.1 - - [06/Jan/2023:20:44:14 +0000] "GET /api/datasets/BIG_MULTI_SELECT$20Test/queries/BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328 HTTP/1.1" 200 1481 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:14,447] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT Test], queryId=292b43ac-03cb-42c5-935a-459f20297328, label=concept	@§$, creationTime=2023-01-06T20:44:14.386135, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22e5d833[Count = 0], startTime=2023-01-06T20:44:14.386379, finishTime=2023-01-06T20:44:14.417468, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2ba7a720), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3549344f, com.bakdata.conquery.models.query.ColumnDescriptor@39e15695]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:14,447] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT Test], queryId=292b43ac-03cb-42c5-935a-459f20297328, label=concept	@§$, creationTime=2023-01-06T20:44:14.386135, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22e5d833[Count = 0], startTime=2023-01-06T20:44:14.386379, finishTime=2023-01-06T20:44:14.417468, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2ba7a720), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3549344f, com.bakdata.conquery.models.query.ColumnDescriptor@39e15695]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT Test]
127.0.0.1 - - [06/Jan/2023:20:44:14 +0000] "GET /api/datasets/BIG_MULTI_SELECT%20Test/result/BIG_MULTI_SELECT$20Test.292b43ac-03cb-42c5-935a-459f20297328.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 30
INFO  [2023-01-06 20:44:14,473] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT Test on 3 rows
INFO  [2023-01-06 20:44:14,475] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT Test
INFO  [2023-01-06 20:44:14,476] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-06 20:44:14,477] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT Test_e1ea1168-0d99-4a98-bcbb-ca5038b1c3ff
INFO  [2023-01-06 20:44:14,477] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-06 20:44:14,477] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT Test_34036a18-6768-4e3e-917b-168083706f85
INFO  [2023-01-06 20:44:14,506] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT Test
INFO  [2023-01-06 20:44:14,510] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT Test_34036a18-6768-4e3e-917b-168083706f85
INFO  [2023-01-06 20:44:14,511] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT Test_e1ea1168-0d99-4a98-bcbb-ca5038b1c3ff
INFO  [2023-01-06 20:44:14,527] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT$20Test
INFO  [2023-01-06 20:44:14,527] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:14,646] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT Test
INFO  [2023-01-06 20:44:14,647] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-06 20:44:14,647] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:14,647] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:14,648] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-06 20:44:14,648] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-06 20:44:14,648] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:14,648] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:14,651] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_b09183df-3499-4598-8166-015b34601cdb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:14,651] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_b09183df-3499-4598-8166-015b34601cdb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:14,651] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:14,651] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_79ccfa39-216f-4289-adcc-37bc48b715c5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:14,651] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_79ccfa39-216f-4289-adcc-37bc48b715c5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:14,651] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:14,655] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:14,757] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:14,758] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_2VALUES$20Test.table
INFO  [2023-01-06 20:44:14,758] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_2VALUES$20Test.table
INFO  [2023-01-06 20:44:14,874] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:14,993] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:14,993] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:14,993] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 91 B in total
INFO  [2023-01-06 20:44:14,993] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_2VALUES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000431233sINFO  [2023-01-06 20:44:15,037] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:15,037] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@56c5e1f1)
INFO  [2023-01-06 20:44:15,038] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:15,041] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_2VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:15,041] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:15,041] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_2VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:15,055] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_2VALUES$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:15 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_2VALUES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT_2VALUES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:15,058] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:15,058] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:15,059] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:15,059] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:15,063] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:15,063] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_2VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:15,063] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_2VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:15,065] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_2VALUES$20Test.table.table.0
WARN  [2023-01-06 20:44:15,065] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:15,065] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_2VALUES$20Test.table.table.1
INFO  [2023-01-06 20:44:15,171] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_2VALUES Test QUERY INIT
INFO  [2023-01-06 20:44:15,191] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_2VALUES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:15,192] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[860269e9-5dfd-4247-b7ab-601dcae2bccd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_2VALUES Test))]]
INFO  [2023-01-06 20:44:15,196] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd
INFO  [2023-01-06 20:44:15,196] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd
INFO  [2023-01-06 20:44:15,197] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd] with 0 results within PT0.001024S
INFO  [2023-01-06 20:44:15,198] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd] with 3 results within PT0.002071S
INFO  [2023-01-06 20:44:15,199] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd, workerId=BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_b09183df-3499-4598-8166-015b34601cdb, startTime=2023-01-06T20:44:15.196739, finishTime=2023-01-06T20:44:15.197763) of size 0
127.0.0.1 - - [06/Jan/2023:20:44:15 +0000] "POST /api/datasets/BIG_MULTI_SELECT_2VALUES$20Test/queries HTTP/1.1" 201 1233 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:44:15,200] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd, workerId=BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_79ccfa39-216f-4289-adcc-37bc48b715c5, startTime=2023-01-06T20:44:15.196749, finishTime=2023-01-06T20:44:15.198820) of size 3
INFO  [2023-01-06 20:44:15,201] com.bakdata.conquery.models.execution.ManagedExecution: DONE 860269e9-5dfd-4247-b7ab-601dcae2bccd ManagedQuery within PT0.008828S
127.0.0.1 - - [06/Jan/2023:20:44:15 +0000] "GET /api/datasets/BIG_MULTI_SELECT_2VALUES$20Test/queries/BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd HTTP/1.1" 200 1548 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:15,238] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test], queryId=860269e9-5dfd-4247-b7ab-601dcae2bccd, label=concept	@§$, creationTime=2023-01-06T20:44:15.191881, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7655ff19[Count = 0], startTime=2023-01-06T20:44:15.192161, finishTime=2023-01-06T20:44:15.200989, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@37d89990), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_2VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4769abac, com.bakdata.conquery.models.query.ColumnDescriptor@4f0e9399]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:15,238] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test], queryId=860269e9-5dfd-4247-b7ab-601dcae2bccd, label=concept	@§$, creationTime=2023-01-06T20:44:15.191881, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7655ff19[Count = 0], startTime=2023-01-06T20:44:15.192161, finishTime=2023-01-06T20:44:15.200989, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@37d89990), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_2VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4769abac, com.bakdata.conquery.models.query.ColumnDescriptor@4f0e9399]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test]
127.0.0.1 - - [06/Jan/2023:20:44:15 +0000] "GET /api/datasets/BIG_MULTI_SELECT_2VALUES%20Test/result/BIG_MULTI_SELECT_2VALUES$20Test.860269e9-5dfd-4247-b7ab-601dcae2bccd.csv?pretty=false HTTP/1.1" 200 115 "-" "Conquery (test client)" 27
INFO  [2023-01-06 20:44:15,263] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_2VALUES Test on 4 rows
INFO  [2023-01-06 20:44:15,264] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-06 20:44:15,264] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-06 20:44:15,264] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-06 20:44:15,264] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_2VALUES Test_79ccfa39-216f-4289-adcc-37bc48b715c5
INFO  [2023-01-06 20:44:15,264] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_2VALUES Test_b09183df-3499-4598-8166-015b34601cdb
INFO  [2023-01-06 20:44:15,360] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-06 20:44:15,360] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_2VALUES Test_b09183df-3499-4598-8166-015b34601cdb
INFO  [2023-01-06 20:44:15,360] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_2VALUES Test_79ccfa39-216f-4289-adcc-37bc48b715c5
INFO  [2023-01-06 20:44:15,365] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_2VALUES$20Test
INFO  [2023-01-06 20:44:15,366] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:15,471] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-06 20:44:15,471] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:15,471] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:15,472] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:15,473] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:15,473] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:15,473] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:15,473] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:15,475] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_2f428cb5-4ed3-4b59-b6a3-9ac23f3ecc13 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:15,476] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_2f428cb5-4ed3-4b59-b6a3-9ac23f3ecc13 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:15,476] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:15,476] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_c6ff3f7d-4d74-4867-830b-acf94db2db60 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:15,476] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_c6ff3f7d-4d74-4867-830b-acf94db2db60 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:15,476] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:15,479] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:15,588] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:15,589] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-06 20:44:15,589] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-06 20:44:15,700] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:15,813] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:15,813] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:15,813] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 90 B in total
INFO  [2023-01-06 20:44:15,814] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000433177sINFO  [2023-01-06 20:44:15,858] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:15,858] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@62d85d39)
INFO  [2023-01-06 20:44:15,858] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:15,861] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:15,861] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:15,861] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:15,876] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:15 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_EMPTY_VALUES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT_EMPTY_VALUES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:15,878] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:15,878] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:15,879] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:15,879] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:15,880] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:15,881] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:15,881] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:44:15,882] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:15,882] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table.0
INFO  [2023-01-06 20:44:15,882] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table.1
INFO  [2023-01-06 20:44:15,987] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_EMPTY_VALUES Test QUERY INIT
INFO  [2023-01-06 20:44:16,007] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_EMPTY_VALUES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:16,008] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c86280c5-2136-4af7-8818-f0692781d9fc] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_EMPTY_VALUES Test))]]
INFO  [2023-01-06 20:44:16,012] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc
INFO  [2023-01-06 20:44:16,012] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc
INFO  [2023-01-06 20:44:16,013] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc] with 0 results within PT0.001103S
INFO  [2023-01-06 20:44:16,013] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc] with 2 results within PT0.001067S
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "POST /api/datasets/BIG_MULTI_SELECT_EMPTY_VALUES$20Test/queries HTTP/1.1" 201 1249 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:16,014] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc, workerId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_c6ff3f7d-4d74-4867-830b-acf94db2db60, startTime=2023-01-06T20:44:16.012452, finishTime=2023-01-06T20:44:16.013555) of size 0
INFO  [2023-01-06 20:44:16,014] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc, workerId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_2f428cb5-4ed3-4b59-b6a3-9ac23f3ecc13, startTime=2023-01-06T20:44:16.012513, finishTime=2023-01-06T20:44:16.013580) of size 2
INFO  [2023-01-06 20:44:16,015] com.bakdata.conquery.models.execution.ManagedExecution: DONE c86280c5-2136-4af7-8818-f0692781d9fc ManagedQuery within PT0.006992S
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "GET /api/datasets/BIG_MULTI_SELECT_EMPTY_VALUES$20Test/queries/BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc HTTP/1.1" 200 1583 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:44:16,048] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test], queryId=c86280c5-2136-4af7-8818-f0692781d9fc, label=concept	@§$, creationTime=2023-01-06T20:44:16.008325, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@421b4fa3[Count = 0], startTime=2023-01-06T20:44:16.008618, finishTime=2023-01-06T20:44:16.015610, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@42287eae), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7393e9f6, com.bakdata.conquery.models.query.ColumnDescriptor@3a4a8651]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:16,048] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test], queryId=c86280c5-2136-4af7-8818-f0692781d9fc, label=concept	@§$, creationTime=2023-01-06T20:44:16.008325, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@421b4fa3[Count = 0], startTime=2023-01-06T20:44:16.008618, finishTime=2023-01-06T20:44:16.015610, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@42287eae), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7393e9f6, com.bakdata.conquery.models.query.ColumnDescriptor@3a4a8651]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "GET /api/datasets/BIG_MULTI_SELECT_EMPTY_VALUES%20Test/result/BIG_MULTI_SELECT_EMPTY_VALUES$20Test.c86280c5-2136-4af7-8818-f0692781d9fc.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:16,067] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_EMPTY_VALUES Test on 3 rows
INFO  [2023-01-06 20:44:16,067] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:16,067] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:16,067] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:16,067] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_2f428cb5-4ed3-4b59-b6a3-9ac23f3ecc13
INFO  [2023-01-06 20:44:16,068] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_c6ff3f7d-4d74-4867-830b-acf94db2db60
INFO  [2023-01-06 20:44:16,077] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:16,080] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_c6ff3f7d-4d74-4867-830b-acf94db2db60
INFO  [2023-01-06 20:44:16,080] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_2f428cb5-4ed3-4b59-b6a3-9ac23f3ecc13
INFO  [2023-01-06 20:44:16,082] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_EMPTY_VALUES$20Test
INFO  [2023-01-06 20:44:16,082] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:16,188] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:16,188] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:16,188] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:16,188] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:16,189] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:16,190] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:16,190] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:16,190] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:16,191] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:16,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_9054269c-a22c-4fbd-b8cb-d1e45f443ed4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:16,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_9054269c-a22c-4fbd-b8cb-d1e45f443ed4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:16,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:16,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_87822da2-f6db-41a3-8061-ef672dbf5646 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:16,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_87822da2-f6db-41a3-8061-ef672dbf5646 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:16,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:16,300] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:16,300] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-06 20:44:16,300] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-06 20:44:16,417] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:16,531] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:16,531] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:16,531] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 92 B in total
INFO  [2023-01-06 20:44:16,531] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000395173sINFO  [2023-01-06 20:44:16,571] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:16,571] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@604d6e39)
INFO  [2023-01-06 20:44:16,571] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:16,575] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:16,575] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:16,575] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:16,589] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:16,591] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:16,591] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:16,591] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:16,591] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:16,593] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:16,593] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:16,593] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:44:16,594] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:16,594] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table.0
INFO  [2023-01-06 20:44:16,594] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table.1
INFO  [2023-01-06 20:44:16,700] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test QUERY INIT
INFO  [2023-01-06 20:44:16,733] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:16,734] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[25a07826-cb22-4635-ace1-5905f82107b3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test))]]
INFO  [2023-01-06 20:44:16,742] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3
INFO  [2023-01-06 20:44:16,742] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "POST /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test/queries HTTP/1.1" 201 1269 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:16,744] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3] with 0 results within PT0.001806S
INFO  [2023-01-06 20:44:16,744] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3] with 1 results within PT0.002088S
INFO  [2023-01-06 20:44:16,745] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_87822da2-f6db-41a3-8061-ef672dbf5646, startTime=2023-01-06T20:44:16.742597, finishTime=2023-01-06T20:44:16.744403) of size 0
INFO  [2023-01-06 20:44:16,745] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_9054269c-a22c-4fbd-b8cb-d1e45f443ed4, startTime=2023-01-06T20:44:16.742593, finishTime=2023-01-06T20:44:16.744681) of size 1
INFO  [2023-01-06 20:44:16,746] com.bakdata.conquery.models.execution.ManagedExecution: DONE 25a07826-cb22-4635-ace1-5905f82107b3 ManagedQuery within PT0.012315S
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test/queries/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3 HTTP/1.1" 200 1625 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:16,776] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test], queryId=25a07826-cb22-4635-ace1-5905f82107b3, label=concept	@§$, creationTime=2023-01-06T20:44:16.734036, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3de29583[Count = 0], startTime=2023-01-06T20:44:16.734418, finishTime=2023-01-06T20:44:16.746733, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@c375193), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@c952df2, com.bakdata.conquery.models.query.ColumnDescriptor@74aab757]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:16,777] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test], queryId=25a07826-cb22-4635-ace1-5905f82107b3, label=concept	@§$, creationTime=2023-01-06T20:44:16.734036, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3de29583[Count = 0], startTime=2023-01-06T20:44:16.734418, finishTime=2023-01-06T20:44:16.746733, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@c375193), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@c952df2, com.bakdata.conquery.models.query.ColumnDescriptor@74aab757]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
127.0.0.1 - - [06/Jan/2023:20:44:16 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER%20Test/result/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.25a07826-cb22-4635-ace1-5905f82107b3.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:44:16,795] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test on 2 rows
INFO  [2023-01-06 20:44:16,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:16,795] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:16,795] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:16,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_87822da2-f6db-41a3-8061-ef672dbf5646
INFO  [2023-01-06 20:44:16,796] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_9054269c-a22c-4fbd-b8cb-d1e45f443ed4
INFO  [2023-01-06 20:44:16,892] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:16,892] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_9054269c-a22c-4fbd-b8cb-d1e45f443ed4
INFO  [2023-01-06 20:44:16,892] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_87822da2-f6db-41a3-8061-ef672dbf5646
INFO  [2023-01-06 20:44:16,894] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test
INFO  [2023-01-06 20:44:16,894] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,000] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:17,000] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:17,000] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:17,000] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:17,002] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:17,002] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:17,002] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:17,002] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:17,004] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,004] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_75b7885b-ab0d-42be-ae47-473c637b2bb4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:17,004] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_75b7885b-ab0d-42be-ae47-473c637b2bb4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:17,004] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:17,005] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_fa37150a-3a25-4468-bc0e-394ed9c53f19 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:17,005] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_fa37150a-3a25-4468-bc0e-394ed9c53f19 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:17,005] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:17,113] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,114] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-06 20:44:17,114] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-06 20:44:17,232] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,353] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:17,354] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:17,354] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 90 B in total
INFO  [2023-01-06 20:44:17,354] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000427625sINFO  [2023-01-06 20:44:17,397] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:17,397] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1120325a)
INFO  [2023-01-06 20:44:17,397] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:17,401] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:17,401] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:17,401] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:17,424] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:17 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 24
INFO  [2023-01-06 20:44:17,434] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:17,434] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,435] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:17,435] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:17,436] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:17,437] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:17,437] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:44:17,437] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:17,438] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.0
INFO  [2023-01-06 20:44:17,438] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.1
INFO  [2023-01-06 20:44:17,545] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test QUERY INIT
INFO  [2023-01-06 20:44:17,556] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:17,557] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3ae71339-4d7f-492b-aa41-5cead0092ed0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test))]]
INFO  [2023-01-06 20:44:17,561] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0
INFO  [2023-01-06 20:44:17,561] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0
INFO  [2023-01-06 20:44:17,561] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0] with 0 results within PT0.000509S
INFO  [2023-01-06 20:44:17,562] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0] with 1 results within PT0.000965S
INFO  [2023-01-06 20:44:17,562] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_fa37150a-3a25-4468-bc0e-394ed9c53f19, startTime=2023-01-06T20:44:17.561208, finishTime=2023-01-06T20:44:17.561717) of size 0
INFO  [2023-01-06 20:44:17,563] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_75b7885b-ab0d-42be-ae47-473c637b2bb4, startTime=2023-01-06T20:44:17.561210, finishTime=2023-01-06T20:44:17.562175) of size 1
127.0.0.1 - - [06/Jan/2023:20:44:17 +0000] "POST /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test/queries HTTP/1.1" 201 1273 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:17,564] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3ae71339-4d7f-492b-aa41-5cead0092ed0 ManagedQuery within PT0.006633S
127.0.0.1 - - [06/Jan/2023:20:44:17 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test/queries/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0 HTTP/1.1" 200 1632 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:17,597] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test], queryId=3ae71339-4d7f-492b-aa41-5cead0092ed0, label=concept	@§$, creationTime=2023-01-06T20:44:17.557154, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3a0b3e68[Count = 0], startTime=2023-01-06T20:44:17.557452, finishTime=2023-01-06T20:44:17.564085, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@41255088), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@b7b6053, com.bakdata.conquery.models.query.ColumnDescriptor@39bf5006]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:17,597] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test], queryId=3ae71339-4d7f-492b-aa41-5cead0092ed0, label=concept	@§$, creationTime=2023-01-06T20:44:17.557154, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3a0b3e68[Count = 0], startTime=2023-01-06T20:44:17.557452, finishTime=2023-01-06T20:44:17.564085, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@41255088), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@b7b6053, com.bakdata.conquery.models.query.ColumnDescriptor@39bf5006]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
127.0.0.1 - - [06/Jan/2023:20:44:17 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2%20Test/result/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.3ae71339-4d7f-492b-aa41-5cead0092ed0.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:44:17,615] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test on 2 rows
INFO  [2023-01-06 20:44:17,616] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:17,616] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:17,616] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:17,616] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_75b7885b-ab0d-42be-ae47-473c637b2bb4
INFO  [2023-01-06 20:44:17,616] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_fa37150a-3a25-4468-bc0e-394ed9c53f19
INFO  [2023-01-06 20:44:17,714] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:17,714] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_fa37150a-3a25-4468-bc0e-394ed9c53f19
INFO  [2023-01-06 20:44:17,714] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_75b7885b-ab0d-42be-ae47-473c637b2bb4
INFO  [2023-01-06 20:44:17,741] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test
INFO  [2023-01-06 20:44:17,741] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,846] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:17,846] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT Test
INFO  [2023-01-06 20:44:17,846] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:17,847] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:17,848] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-06 20:44:17,848] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-06 20:44:17,848] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:17,848] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:17,850] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20Test.worker_COUNT$20Test_1ebdff5e-6c01-45d6-af50-556d250b0267 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:17,850] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20Test.worker_COUNT$20Test_1ebdff5e-6c01-45d6-af50-556d250b0267 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:17,850] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:17,850] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20Test.worker_COUNT$20Test_9b5f9e5a-e51c-40eb-8b86-fe5b1bf0b880 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:17,850] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20Test.worker_COUNT$20Test_9b5f9e5a-e51c-40eb-8b86-fe5b1bf0b880 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:17,850] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:17,854] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,958] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:17,958] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20Test.table
INFO  [2023-01-06 20:44:17,958] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20Test.table
INFO  [2023-01-06 20:44:18,073] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:18,190] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:18,190] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:18,190] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1 KiB in total
INFO  [2023-01-06 20:44:18,190] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000360824sINFO  [2023-01-06 20:44:18,228] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=36, min=1, average=7.200000, max=13}
INFO  [2023-01-06 20:44:18,228] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=36, nullLines=0), minParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@2c003c9e), maxParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@1d1196e), dateReader=com.bakdata.conquery.util.DateReader@5206127e, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:44:18,228] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=36, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-06 20:44:18,231] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:18,231] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:18,231] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:18,261] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:18 +0000] "POST /admin/datasets/COUNT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:44:18,263] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:18,264] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:18,264] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:18,264] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:18,267] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:18,268] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20Test.table.table], containing 36 entries.
INFO  [2023-01-06 20:44:18,268] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20Test.table.table], containing 36 entries.
INFO  [2023-01-06 20:44:18,269] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20Test.table.table.0
WARN  [2023-01-06 20:44:18,269] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:18,269] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20Test.table.table.1
INFO  [2023-01-06 20:44:18,375] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT Test QUERY INIT
INFO  [2023-01-06 20:44:18,398] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:18,398] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[09f91e62-1f6b-42e1-989a-906f0daeeb8f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT Test))]]
INFO  [2023-01-06 20:44:18,403] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f
INFO  [2023-01-06 20:44:18,403] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f
127.0.0.1 - - [06/Jan/2023:20:44:18 +0000] "POST /api/datasets/COUNT$20Test/queries HTTP/1.1" 201 1166 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:44:18,404] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f] with 1 results within PT0.001262S
INFO  [2023-01-06 20:44:18,404] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f] with 1 results within PT0.001446S
INFO  [2023-01-06 20:44:18,405] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f, workerId=COUNT$20Test.worker_COUNT$20Test_9b5f9e5a-e51c-40eb-8b86-fe5b1bf0b880, startTime=2023-01-06T20:44:18.403189, finishTime=2023-01-06T20:44:18.404451) of size 1
INFO  [2023-01-06 20:44:18,405] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f, workerId=COUNT$20Test.worker_COUNT$20Test_1ebdff5e-6c01-45d6-af50-556d250b0267, startTime=2023-01-06T20:44:18.403074, finishTime=2023-01-06T20:44:18.404520) of size 1
INFO  [2023-01-06 20:44:18,406] com.bakdata.conquery.models.execution.ManagedExecution: DONE 09f91e62-1f6b-42e1-989a-906f0daeeb8f ManagedQuery within PT0.007631S
127.0.0.1 - - [06/Jan/2023:20:44:18 +0000] "GET /api/datasets/COUNT$20Test/queries/COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f HTTP/1.1" 200 1405 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:18,434] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT Test], queryId=09f91e62-1f6b-42e1-989a-906f0daeeb8f, label=concept	@§$, creationTime=2023-01-06T20:44:18.398557, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@30336e17[Count = 0], startTime=2023-01-06T20:44:18.398825, finishTime=2023-01-06T20:44:18.406456, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3642119), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1491a5d, com.bakdata.conquery.models.query.ColumnDescriptor@7bf40be7]) download on dataset Dataset[label=null, name=COUNT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:18,434] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT Test], queryId=09f91e62-1f6b-42e1-989a-906f0daeeb8f, label=concept	@§$, creationTime=2023-01-06T20:44:18.398557, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@30336e17[Count = 0], startTime=2023-01-06T20:44:18.398825, finishTime=2023-01-06T20:44:18.406456, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3642119), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1491a5d, com.bakdata.conquery.models.query.ColumnDescriptor@7bf40be7]) on dataset Dataset[label=null, name=COUNT Test]
127.0.0.1 - - [06/Jan/2023:20:44:18 +0000] "GET /api/datasets/COUNT%20Test/result/COUNT$20Test.09f91e62-1f6b-42e1-989a-906f0daeeb8f.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:18,454] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT Test on 3 rows
INFO  [2023-01-06 20:44:18,454] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT Test
INFO  [2023-01-06 20:44:18,454] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-06 20:44:18,455] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-06 20:44:18,455] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT Test_9b5f9e5a-e51c-40eb-8b86-fe5b1bf0b880
INFO  [2023-01-06 20:44:18,455] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT Test_1ebdff5e-6c01-45d6-af50-556d250b0267
INFO  [2023-01-06 20:44:18,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT Test_1ebdff5e-6c01-45d6-af50-556d250b0267
INFO  [2023-01-06 20:44:18,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT Test
INFO  [2023-01-06 20:44:18,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT Test_9b5f9e5a-e51c-40eb-8b86-fe5b1bf0b880
INFO  [2023-01-06 20:44:18,569] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT$20Test
INFO  [2023-01-06 20:44:18,569] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:18,675] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT Test
INFO  [2023-01-06 20:44:18,676] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT distinct Test
INFO  [2023-01-06 20:44:18,676] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:18,676] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:18,677] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-06 20:44:18,677] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-06 20:44:18,677] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:18,677] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:18,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_49d820a7-808f-403b-bdbd-fa0eb29011fa are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:18,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_49d820a7-808f-403b-bdbd-fa0eb29011fa are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:18,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:18,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_929943d7-fdb8-4132-825d-dc201947d402 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:18,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_929943d7-fdb8-4132-825d-dc201947d402 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:18,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:18,684] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:18,787] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:18,787] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20distinct$20Test.table
INFO  [2023-01-06 20:44:18,788] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20distinct$20Test.table
INFO  [2023-01-06 20:44:18,909] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,022] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:19,023] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:19,023] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1 KiB in total
INFO  [2023-01-06 20:44:19,023] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT distinct Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000308622sINFO  [2023-01-06 20:44:19,055] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=36, min=1, average=7.200000, max=13}
INFO  [2023-01-06 20:44:19,055] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=36, nullLines=0), minParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@5ac835dc), maxParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@4b6a248a), dateReader=com.bakdata.conquery.util.DateReader@2900c82a, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:44:19,065] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=36, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-06 20:44:19,068] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:19,068] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:19,068] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:19,083] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT$20distinct$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "POST /admin/datasets/COUNT%20distinct%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT+distinct+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:19,085] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,085] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:19,085] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:19,085] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:19,088] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:19,088] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20distinct$20Test.table.table], containing 36 entries.
INFO  [2023-01-06 20:44:19,088] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20distinct$20Test.table.table], containing 36 entries.
WARN  [2023-01-06 20:44:19,089] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:19,089] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20distinct$20Test.table.table.0
INFO  [2023-01-06 20:44:19,089] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20distinct$20Test.table.table.1
INFO  [2023-01-06 20:44:19,195] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT distinct Test QUERY INIT
INFO  [2023-01-06 20:44:19,208] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT$20distinct$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:19,208] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[424f4a1c-fa0f-4846-ae04-cced872aa869] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT distinct Test))]]
INFO  [2023-01-06 20:44:19,212] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869
INFO  [2023-01-06 20:44:19,212] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869
INFO  [2023-01-06 20:44:19,213] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869] with 0 results within PT0.001697S
INFO  [2023-01-06 20:44:19,214] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869] with 2 results within PT0.001759S
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "POST /api/datasets/COUNT$20distinct$20Test/queries HTTP/1.1" 201 1210 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:19,214] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869, workerId=COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_49d820a7-808f-403b-bdbd-fa0eb29011fa, startTime=2023-01-06T20:44:19.212256, finishTime=2023-01-06T20:44:19.213953) of size 0
INFO  [2023-01-06 20:44:19,215] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869, workerId=COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_929943d7-fdb8-4132-825d-dc201947d402, startTime=2023-01-06T20:44:19.212237, finishTime=2023-01-06T20:44:19.213996) of size 2
INFO  [2023-01-06 20:44:19,216] com.bakdata.conquery.models.execution.ManagedExecution: DONE 424f4a1c-fa0f-4846-ae04-cced872aa869 ManagedQuery within PT0.007341S
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "GET /api/datasets/COUNT$20distinct$20Test/queries/COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869 HTTP/1.1" 200 1493 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:19,253] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT distinct Test], queryId=424f4a1c-fa0f-4846-ae04-cced872aa869, label=concept	@§$, creationTime=2023-01-06T20:44:19.208465, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@74b7125f[Count = 0], startTime=2023-01-06T20:44:19.208727, finishTime=2023-01-06T20:44:19.216068, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5076d9dd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@cf60fb3, com.bakdata.conquery.models.query.ColumnDescriptor@1f265d0]) download on dataset Dataset[label=null, name=COUNT distinct Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:19,253] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT distinct Test], queryId=424f4a1c-fa0f-4846-ae04-cced872aa869, label=concept	@§$, creationTime=2023-01-06T20:44:19.208465, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@74b7125f[Count = 0], startTime=2023-01-06T20:44:19.208727, finishTime=2023-01-06T20:44:19.216068, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5076d9dd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@cf60fb3, com.bakdata.conquery.models.query.ColumnDescriptor@1f265d0]) on dataset Dataset[label=null, name=COUNT distinct Test]
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "GET /api/datasets/COUNT%20distinct%20Test/result/COUNT$20distinct$20Test.424f4a1c-fa0f-4846-ae04-cced872aa869.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:19,256] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT distinct Test on 3 rows
INFO  [2023-01-06 20:44:19,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT distinct Test
INFO  [2023-01-06 20:44:19,256] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-06 20:44:19,256] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-06 20:44:19,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT distinct Test_929943d7-fdb8-4132-825d-dc201947d402
INFO  [2023-01-06 20:44:19,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT distinct Test_49d820a7-808f-403b-bdbd-fa0eb29011fa
INFO  [2023-01-06 20:44:19,277] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT distinct Test
INFO  [2023-01-06 20:44:19,278] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT distinct Test_49d820a7-808f-403b-bdbd-fa0eb29011fa
INFO  [2023-01-06 20:44:19,278] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT distinct Test_929943d7-fdb8-4132-825d-dc201947d402
INFO  [2023-01-06 20:44:19,289] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT$20distinct$20Test
INFO  [2023-01-06 20:44:19,289] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,394] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT distinct Test
INFO  [2023-01-06 20:44:19,395] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT multi distinct Test
INFO  [2023-01-06 20:44:19,395] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:19,395] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:19,396] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-06 20:44:19,396] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-06 20:44:19,396] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:19,396] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:19,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_ef918018-52c5-489a-94e7-abcb3fce7c35 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:19,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_ef918018-52c5-489a-94e7-abcb3fce7c35 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:19,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:19,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_f0d68b71-de62-45a8-8417-882095cc4b03 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:19,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_f0d68b71-de62-45a8-8417-882095cc4b03 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:19,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:19,401] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,505] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20multi$20distinct$20Test.table
INFO  [2023-01-06 20:44:19,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20multi$20distinct$20Test.table
INFO  [2023-01-06 20:44:19,623] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,735] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:19,735] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:19,735] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.2 KiB in total
INFO  [2023-01-06 20:44:19,735] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT multi distinct Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000332656sINFO  [2023-01-06 20:44:19,769] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=34, min=1, average=6.800000, max=13}
INFO  [2023-01-06 20:44:19,769] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[a] with StringParser(super=Parser(lines=34, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:19,769] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[b] with StringParser(super=Parser(lines=34, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:19,769] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=34, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-06 20:44:19,769] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=34, nullLines=0), minParser=DateParser(super=Parser(lines=34, nullLines=0), subType=IntegerParser(super=Parser(lines=34, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@5ad2eb8a), maxParser=DateParser(super=Parser(lines=34, nullLines=0), subType=IntegerParser(super=Parser(lines=34, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@37442227), dateReader=com.bakdata.conquery.util.DateReader@22f4d4fa, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:44:19,773] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT multi distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:19,773] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:19,773] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT multi distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:19,797] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT$20multi$20distinct$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "POST /admin/datasets/COUNT%20multi%20distinct%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT+multi+distinct+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:44:19,799] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:19,799] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:19,800] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:19,800] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:19,802] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:19,802] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20multi$20distinct$20Test.table.table], containing 34 entries.
INFO  [2023-01-06 20:44:19,802] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20multi$20distinct$20Test.table.table], containing 34 entries.
WARN  [2023-01-06 20:44:19,803] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:19,844] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20multi$20distinct$20Test.table.table.0
INFO  [2023-01-06 20:44:19,848] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20multi$20distinct$20Test.table.table.1
INFO  [2023-01-06 20:44:19,953] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT multi distinct Test QUERY INIT
INFO  [2023-01-06 20:44:19,964] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT$20multi$20distinct$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:19,965] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[cb4b7e3f-682e-4f33-bb4c-0d1d9850241f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT multi distinct Test))]]
INFO  [2023-01-06 20:44:19,967] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f
INFO  [2023-01-06 20:44:19,967] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "POST /api/datasets/COUNT$20multi$20distinct$20Test/queries HTTP/1.1" 201 1242 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:44:19,969] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f] with 2 results within PT0.001472S
INFO  [2023-01-06 20:44:19,969] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f] with 0 results within PT0.001414S
INFO  [2023-01-06 20:44:19,970] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f, workerId=COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_ef918018-52c5-489a-94e7-abcb3fce7c35, startTime=2023-01-06T20:44:19.968023, finishTime=2023-01-06T20:44:19.969437) of size 0
INFO  [2023-01-06 20:44:19,970] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f, workerId=COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_f0d68b71-de62-45a8-8417-882095cc4b03, startTime=2023-01-06T20:44:19.967817, finishTime=2023-01-06T20:44:19.969289) of size 2
INFO  [2023-01-06 20:44:19,971] com.bakdata.conquery.models.execution.ManagedExecution: DONE cb4b7e3f-682e-4f33-bb4c-0d1d9850241f ManagedQuery within PT0.006327S
127.0.0.1 - - [06/Jan/2023:20:44:19 +0000] "GET /api/datasets/COUNT$20multi$20distinct$20Test/queries/COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f HTTP/1.1" 200 1557 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:20,000] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT multi distinct Test], queryId=cb4b7e3f-682e-4f33-bb4c-0d1d9850241f, label=concept	@§$, creationTime=2023-01-06T20:44:19.965142, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5b6b4f7d[Count = 0], startTime=2023-01-06T20:44:19.965311, finishTime=2023-01-06T20:44:19.971638, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@51ceb4a8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT multi distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@213ee469, com.bakdata.conquery.models.query.ColumnDescriptor@1c374815]) download on dataset Dataset[label=null, name=COUNT multi distinct Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:20,000] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT multi distinct Test], queryId=cb4b7e3f-682e-4f33-bb4c-0d1d9850241f, label=concept	@§$, creationTime=2023-01-06T20:44:19.965142, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5b6b4f7d[Count = 0], startTime=2023-01-06T20:44:19.965311, finishTime=2023-01-06T20:44:19.971638, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@51ceb4a8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT multi distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@213ee469, com.bakdata.conquery.models.query.ColumnDescriptor@1c374815]) on dataset Dataset[label=null, name=COUNT multi distinct Test]
127.0.0.1 - - [06/Jan/2023:20:44:20 +0000] "GET /api/datasets/COUNT%20multi%20distinct%20Test/result/COUNT$20multi$20distinct$20Test.cb4b7e3f-682e-4f33-bb4c-0d1d9850241f.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:44:20,017] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT multi distinct Test on 3 rows
INFO  [2023-01-06 20:44:20,017] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT multi distinct Test
INFO  [2023-01-06 20:44:20,018] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-06 20:44:20,018] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-06 20:44:20,018] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT multi distinct Test_ef918018-52c5-489a-94e7-abcb3fce7c35
INFO  [2023-01-06 20:44:20,018] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT multi distinct Test_f0d68b71-de62-45a8-8417-882095cc4b03
INFO  [2023-01-06 20:44:20,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT multi distinct Test_ef918018-52c5-489a-94e7-abcb3fce7c35
INFO  [2023-01-06 20:44:20,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT multi distinct Test_f0d68b71-de62-45a8-8417-882095cc4b03
INFO  [2023-01-06 20:44:20,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT multi distinct Test
INFO  [2023-01-06 20:44:20,116] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT$20multi$20distinct$20Test
INFO  [2023-01-06 20:44:20,116] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,254] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT multi distinct Test
INFO  [2023-01-06 20:44:20,254] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_QUARTERS Test
INFO  [2023-01-06 20:44:20,254] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:20,254] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:20,256] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-06 20:44:20,256] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-06 20:44:20,256] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:20,256] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:20,258] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_b6932da8-4fd8-4257-8e5d-5ddf804cb614 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:20,258] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_b6932da8-4fd8-4257-8e5d-5ddf804cb614 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:20,258] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:20,258] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_91b5a3e0-eada-4e5e-9566-4c78ca960ee5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:20,258] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_91b5a3e0-eada-4e5e-9566-4c78ca960ee5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:20,258] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:20,262] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,373] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test.table
INFO  [2023-01-06 20:44:20,373] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test.table
INFO  [2023-01-06 20:44:20,489] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,604] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:20,604] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:20,604] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.4 KiB in total
INFO  [2023-01-06 20:44:20,604] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000259869sINFO  [2023-01-06 20:44:20,631] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=38, min=1, average=3.166667, max=6}
INFO  [2023-01-06 20:44:20,631] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=38, nullLines=0), minParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=16436), dateReader=com.bakdata.conquery.util.DateReader@7cdff12e), maxParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=17166, maxValue=17166), dateReader=com.bakdata.conquery.util.DateReader@34228039), dateReader=com.bakdata.conquery.util.DateReader@6afaadad, onlyQuarters=false, maxValue=17166, minValue=16436, anyOpen=false)
INFO  [2023-01-06 20:44:20,632] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=17117), dateReader=com.bakdata.conquery.util.DateReader@5e127698)
INFO  [2023-01-06 20:44:20,634] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:20,634] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:20,634] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:20,658] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_QUARTERS$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:20 +0000] "POST /admin/datasets/COUNT_QUARTERS%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT_QUARTERS+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:20,660] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,661] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:20,661] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:20,661] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:20,664] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:20,664] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test.table.table], containing 38 entries.
INFO  [2023-01-06 20:44:20,665] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test.table.table], containing 38 entries.
INFO  [2023-01-06 20:44:20,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.0
INFO  [2023-01-06 20:44:20,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.1
WARN  [2023-01-06 20:44:20,667] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:20,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.2
INFO  [2023-01-06 20:44:20,668] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.3
INFO  [2023-01-06 20:44:20,776] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_QUARTERS Test QUERY INIT
INFO  [2023-01-06 20:44:20,794] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_QUARTERS$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:20,795] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[32f5bf2c-d152-4035-a404-45ae1b721473] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test))]]
INFO  [2023-01-06 20:44:20,797] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473
INFO  [2023-01-06 20:44:20,798] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473
127.0.0.1 - - [06/Jan/2023:20:44:20 +0000] "POST /api/datasets/COUNT_QUARTERS$20Test/queries HTTP/1.1" 201 1202 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:44:20,799] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473] with 2 results within PT0.001622S
INFO  [2023-01-06 20:44:20,799] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473] with 4 results within PT0.001769S
INFO  [2023-01-06 20:44:20,800] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473, workerId=COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_b6932da8-4fd8-4257-8e5d-5ddf804cb614, startTime=2023-01-06T20:44:20.797507, finishTime=2023-01-06T20:44:20.799129) of size 2
INFO  [2023-01-06 20:44:20,801] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473, workerId=COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_91b5a3e0-eada-4e5e-9566-4c78ca960ee5, startTime=2023-01-06T20:44:20.798061, finishTime=2023-01-06T20:44:20.799830) of size 4
INFO  [2023-01-06 20:44:20,801] com.bakdata.conquery.models.execution.ManagedExecution: DONE 32f5bf2c-d152-4035-a404-45ae1b721473 ManagedQuery within PT0.006885S
127.0.0.1 - - [06/Jan/2023:20:44:20 +0000] "GET /api/datasets/COUNT_QUARTERS$20Test/queries/COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473 HTTP/1.1" 200 1477 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:20,829] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test], queryId=32f5bf2c-d152-4035-a404-45ae1b721473, label=concept	@§$, creationTime=2023-01-06T20:44:20.794854, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3100bc1f[Count = 0], startTime=2023-01-06T20:44:20.795043, finishTime=2023-01-06T20:44:20.801928, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@303b11b5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1be7678c, com.bakdata.conquery.models.query.ColumnDescriptor@5cb6f6ee]) download on dataset Dataset[label=null, name=COUNT_QUARTERS Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:20,829] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test], queryId=32f5bf2c-d152-4035-a404-45ae1b721473, label=concept	@§$, creationTime=2023-01-06T20:44:20.794854, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3100bc1f[Count = 0], startTime=2023-01-06T20:44:20.795043, finishTime=2023-01-06T20:44:20.801928, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@303b11b5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1be7678c, com.bakdata.conquery.models.query.ColumnDescriptor@5cb6f6ee]) on dataset Dataset[label=null, name=COUNT_QUARTERS Test]
127.0.0.1 - - [06/Jan/2023:20:44:20 +0000] "GET /api/datasets/COUNT_QUARTERS%20Test/result/COUNT_QUARTERS$20Test.32f5bf2c-d152-4035-a404-45ae1b721473.csv?pretty=false HTTP/1.1" 200 171 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:44:20,850] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT_QUARTERS Test on 7 rows
INFO  [2023-01-06 20:44:20,850] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_QUARTERS Test
INFO  [2023-01-06 20:44:20,851] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-06 20:44:20,851] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-06 20:44:20,851] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test_b6932da8-4fd8-4257-8e5d-5ddf804cb614
INFO  [2023-01-06 20:44:20,852] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test_91b5a3e0-eada-4e5e-9566-4c78ca960ee5
INFO  [2023-01-06 20:44:20,857] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test_b6932da8-4fd8-4257-8e5d-5ddf804cb614
INFO  [2023-01-06 20:44:20,857] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_QUARTERS Test
INFO  [2023-01-06 20:44:20,857] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test_91b5a3e0-eada-4e5e-9566-4c78ca960ee5
INFO  [2023-01-06 20:44:20,871] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_QUARTERS$20Test
INFO  [2023-01-06 20:44:20,871] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,976] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_QUARTERS Test
INFO  [2023-01-06 20:44:20,977] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_QUARTERS Test
INFO  [2023-01-06 20:44:20,977] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:20,977] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:20,978] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-06 20:44:20,978] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-06 20:44:20,978] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:20,978] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_731e808f-9135-412c-8186-33ac55f5daed are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_731e808f-9135-412c-8186-33ac55f5daed are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_c5f70afd-18c7-4391-84e1-c2901260d9cc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_c5f70afd-18c7-4391-84e1-c2901260d9cc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:20,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:21,088] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:21,088] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test[1].table
INFO  [2023-01-06 20:44:21,089] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test[1].table
INFO  [2023-01-06 20:44:21,203] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:21,315] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:21,315] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:21,315] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 205 B in total
INFO  [2023-01-06 20:44:21,315] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000507608sINFO  [2023-01-06 20:44:21,366] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:21,367] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=16436, maxValue=16436), dateReader=com.bakdata.conquery.util.DateReader@1b27f497)
INFO  [2023-01-06 20:44:21,367] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateRangeParser(super=Parser(lines=5, nullLines=0), minParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=16436, maxValue=16587), dateReader=com.bakdata.conquery.util.DateReader@20f479a2), maxParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=16466, maxValue=16800), dateReader=com.bakdata.conquery.util.DateReader@588fc94d), dateReader=com.bakdata.conquery.util.DateReader@2a3af2bf, onlyQuarters=false, maxValue=16800, minValue=16436, anyOpen=false)
INFO  [2023-01-06 20:44:21,371] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:21,371] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:21,371] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:21,392] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_QUARTERS$20Test[1].table
127.0.0.1 - - [06/Jan/2023:20:44:21 +0000] "POST /admin/datasets/COUNT_QUARTERS%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT_QUARTERS+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:44:21,394] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:21,394] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:21,395] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:21,395] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:21,396] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:21,397] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test[1].table.table], containing 5 entries.
INFO  [2023-01-06 20:44:21,397] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test[1].table.table], containing 5 entries.
WARN  [2023-01-06 20:44:21,398] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:21,398] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test[1].table.table.0
INFO  [2023-01-06 20:44:21,398] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test[1].table.table.1
INFO  [2023-01-06 20:44:21,503] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_QUARTERS Test QUERY INIT
INFO  [2023-01-06 20:44:21,527] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_QUARTERS$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:21,528] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4f3b0b19-f104-43be-a928-21c4c0cc37ac] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test[1]))]]
INFO  [2023-01-06 20:44:21,533] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test[1].4f3b0b19-f104-43be-a928-21c4c0cc37ac
INFO  [2023-01-06 20:44:21,533] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test[1].4f3b0b19-f104-43be-a928-21c4c0cc37ac
127.0.0.1 - - [06/Jan/2023:20:44:21 +0000] "POST /api/datasets/COUNT_QUARTERS$20Test%5B1%5D/queries HTTP/1.1" 201 1214 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:21,534] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test[1].4f3b0b19-f104-43be-a928-21c4c0cc37ac] with 1 results within PT0.001669S
INFO  [2023-01-06 20:44:21,535] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test[1].4f3b0b19-f104-43be-a928-21c4c0cc37ac] with 1 results within PT0.001841S
INFO  [2023-01-06 20:44:21,535] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test[1].4f3b0b19-f104-43be-a928-21c4c0cc37ac, workerId=COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_c5f70afd-18c7-4391-84e1-c2901260d9cc, startTime=2023-01-06T20:44:21.533249, finishTime=2023-01-06T20:44:21.534918) of size 1
INFO  [2023-01-06 20:44:21,535] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test[1].4f3b0b19-f104-43be-a928-21c4c0cc37ac, workerId=COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_731e808f-9135-412c-8186-33ac55f5daed, startTime=2023-01-06T20:44:21.533238, finishTime=2023-01-06T20:44:21.535079) of size 1
INFO  [2023-01-06 20:44:21,536] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4f3b0b19-f104-43be-a928-21c4c0cc37ac ManagedQuery within PT0.008141S
127.0.0.1 - - [06/Jan/2023:20:44:21 +0000] "GET /api/datasets/COUNT_QUARTERS$20Test%5B1%5D/queries/COUNT_QUARTERS$20Test%5B1%5D.4f3b0b19-f104-43be-a928-21c4c0cc37ac HTTP/1.1" 200 1748 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:21,566] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test[1]], queryId=4f3b0b19-f104-43be-a928-21c4c0cc37ac, label=concept	@§$, creationTime=2023-01-06T20:44:21.528345, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7f4b078c[Count = 0], startTime=2023-01-06T20:44:21.528729, finishTime=2023-01-06T20:44:21.536870, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1dff2aad), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@143f8317, com.bakdata.conquery.models.query.ColumnDescriptor@61f194ea]) download on dataset Dataset[label=null, name=COUNT_QUARTERS Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:21,567] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test[1]], queryId=4f3b0b19-f104-43be-a928-21c4c0cc37ac, label=concept	@§$, creationTime=2023-01-06T20:44:21.528345, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7f4b078c[Count = 0], startTime=2023-01-06T20:44:21.528729, finishTime=2023-01-06T20:44:21.536870, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1dff2aad), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@143f8317, com.bakdata.conquery.models.query.ColumnDescriptor@61f194ea]) on dataset Dataset[label=null, name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-06 20:44:21,586] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT_QUARTERS Test on 3 rows
127.0.0.1 - - [06/Jan/2023:20:44:21 +0000] "GET /api/datasets/COUNT_QUARTERS%20Test%5B1%5D/result/COUNT_QUARTERS$20Test%5B1%5D.4f3b0b19-f104-43be-a928-21c4c0cc37ac.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:21,586] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_QUARTERS Test[1]
INFO  [2023-01-06 20:44:21,589] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-06 20:44:21,589] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-06 20:44:21,589] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test[1]_c5f70afd-18c7-4391-84e1-c2901260d9cc
INFO  [2023-01-06 20:44:21,589] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test[1]_731e808f-9135-412c-8186-33ac55f5daed
INFO  [2023-01-06 20:44:21,689] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_QUARTERS Test[1]
INFO  [2023-01-06 20:44:21,689] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test[1]_731e808f-9135-412c-8186-33ac55f5daed
INFO  [2023-01-06 20:44:21,689] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test[1]_c5f70afd-18c7-4391-84e1-c2901260d9cc
INFO  [2023-01-06 20:44:21,698] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_QUARTERS$20Test[1]
INFO  [2023-01-06 20:44:21,698] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:21,803] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_QUARTERS Test
INFO  [2023-01-06 20:44:21,804] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNTfalse Test
INFO  [2023-01-06 20:44:21,804] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:21,804] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:21,806] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-06 20:44:21,806] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-06 20:44:21,806] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:21,806] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:21,810] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:21,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_a186d70d-6272-4855-98fc-1f5629233774 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:21,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_a186d70d-6272-4855-98fc-1f5629233774 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:21,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:21,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_400e011e-d64d-4715-9f59-b8dba32b0c5d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:21,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_400e011e-d64d-4715-9f59-b8dba32b0c5d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:21,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:21,916] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:21,917] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNTfalse$20Test.table
INFO  [2023-01-06 20:44:21,917] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNTfalse$20Test.table
INFO  [2023-01-06 20:44:22,029] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:22,142] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:22,143] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:22,143] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1 KiB in total
INFO  [2023-01-06 20:44:22,143] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNTfalse Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00041625sINFO  [2023-01-06 20:44:22,185] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=36, min=1, average=7.200000, max=13}
INFO  [2023-01-06 20:44:22,186] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=36, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-06 20:44:22,186] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=36, nullLines=0), minParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@5473790e), maxParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@5dbf681a), dateReader=com.bakdata.conquery.util.DateReader@3ed00608, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:44:22,188] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNTfalse Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:22,188] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:22,188] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNTfalse Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:22,213] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNTfalse$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:22 +0000] "POST /admin/datasets/COUNTfalse%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNTfalse+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:44:22,215] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:22,215] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:22,215] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:22,215] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:22,217] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:22,217] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNTfalse$20Test.table.table], containing 36 entries.
INFO  [2023-01-06 20:44:22,217] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNTfalse$20Test.table.table], containing 36 entries.
WARN  [2023-01-06 20:44:22,218] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:22,218] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNTfalse$20Test.table.table.0
INFO  [2023-01-06 20:44:22,219] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNTfalse$20Test.table.table.1
INFO  [2023-01-06 20:44:22,324] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNTfalse Test QUERY INIT
INFO  [2023-01-06 20:44:22,354] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNTfalse$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:22,355] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1bf757b4-68f1-4fed-b8ae-55aa87f963a8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNTfalse Test))]]
INFO  [2023-01-06 20:44:22,360] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8
INFO  [2023-01-06 20:44:22,360] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8
127.0.0.1 - - [06/Jan/2023:20:44:22 +0000] "POST /api/datasets/COUNTfalse$20Test/queries HTTP/1.1" 201 1186 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:22,361] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8] with 1 results within PT0.001319S
INFO  [2023-01-06 20:44:22,361] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8] with 1 results within PT0.00128S
INFO  [2023-01-06 20:44:22,362] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8, workerId=COUNTfalse$20Test.worker_COUNTfalse$20Test_a186d70d-6272-4855-98fc-1f5629233774, startTime=2023-01-06T20:44:22.360375, finishTime=2023-01-06T20:44:22.361655) of size 1
INFO  [2023-01-06 20:44:22,362] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8, workerId=COUNTfalse$20Test.worker_COUNTfalse$20Test_400e011e-d64d-4715-9f59-b8dba32b0c5d, startTime=2023-01-06T20:44:22.360299, finishTime=2023-01-06T20:44:22.361618) of size 1
INFO  [2023-01-06 20:44:22,363] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1bf757b4-68f1-4fed-b8ae-55aa87f963a8 ManagedQuery within PT0.008137S
127.0.0.1 - - [06/Jan/2023:20:44:22 +0000] "GET /api/datasets/COUNTfalse$20Test/queries/COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8 HTTP/1.1" 200 1445 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:22,397] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNTfalse Test], queryId=1bf757b4-68f1-4fed-b8ae-55aa87f963a8, label=concept	@§$, creationTime=2023-01-06T20:44:22.355191, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2d9e2136[Count = 0], startTime=2023-01-06T20:44:22.355494, finishTime=2023-01-06T20:44:22.363631, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@55c03f5a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNTfalse Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@678cd965, com.bakdata.conquery.models.query.ColumnDescriptor@47165323]) download on dataset Dataset[label=null, name=COUNTfalse Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:22,397] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNTfalse Test], queryId=1bf757b4-68f1-4fed-b8ae-55aa87f963a8, label=concept	@§$, creationTime=2023-01-06T20:44:22.355191, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2d9e2136[Count = 0], startTime=2023-01-06T20:44:22.355494, finishTime=2023-01-06T20:44:22.363631, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@55c03f5a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNTfalse Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@678cd965, com.bakdata.conquery.models.query.ColumnDescriptor@47165323]) on dataset Dataset[label=null, name=COUNTfalse Test]
127.0.0.1 - - [06/Jan/2023:20:44:22 +0000] "GET /api/datasets/COUNTfalse%20Test/result/COUNTfalse$20Test.1bf757b4-68f1-4fed-b8ae-55aa87f963a8.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:44:22,416] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNTfalse Test on 3 rows
INFO  [2023-01-06 20:44:22,416] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNTfalse Test
INFO  [2023-01-06 20:44:22,417] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-06 20:44:22,417] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-06 20:44:22,417] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNTfalse Test_a186d70d-6272-4855-98fc-1f5629233774
INFO  [2023-01-06 20:44:22,417] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNTfalse Test_400e011e-d64d-4715-9f59-b8dba32b0c5d
INFO  [2023-01-06 20:44:22,514] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNTfalse Test_a186d70d-6272-4855-98fc-1f5629233774
INFO  [2023-01-06 20:44:22,514] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNTfalse Test
INFO  [2023-01-06 20:44:22,514] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNTfalse Test_400e011e-d64d-4715-9f59-b8dba32b0c5d
INFO  [2023-01-06 20:44:22,518] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNTfalse$20Test
INFO  [2023-01-06 20:44:22,518] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:22,627] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNTfalse Test
INFO  [2023-01-06 20:44:22,628] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_REAL Test
INFO  [2023-01-06 20:44:22,628] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:22,628] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:22,629] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-06 20:44:22,629] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-06 20:44:22,629] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:22,629] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_e1436179-8690-4b01-aa57-fc06643b8087 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_e1436179-8690-4b01-aa57-fc06643b8087 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_e11d3c95-15c7-4453-a9aa-b18fc61bc085 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_e11d3c95-15c7-4453-a9aa-b18fc61bc085 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:22,636] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:22,743] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:22,743] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test.table
INFO  [2023-01-06 20:44:22,743] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test.table
INFO  [2023-01-06 20:44:22,858] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:22,977] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:22,977] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:22,978] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 147 B in total
INFO  [2023-01-06 20:44:22,978] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_REAL Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000524648sINFO  [2023-01-06 20:44:23,031] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:44:23,031] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[validity] with DateParser(super=Parser(lines=6, nullLines=1), subType=IntegerParser(super=Parser(lines=6, nullLines=1), minValue=10986, maxValue=10988), dateReader=com.bakdata.conquery.util.DateReader@1c71bb06)
INFO  [2023-01-06 20:44:23,031] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=1), subType=IntegerParser(super=Parser(lines=6, nullLines=1), minValue=10977, maxValue=10997), dateReader=com.bakdata.conquery.util.DateReader@6c0db9d4)
INFO  [2023-01-06 20:44:23,034] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_REAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:23,034] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:23,034] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_REAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:23,056] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_REAL$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:23 +0000] "POST /admin/datasets/NUMBER_REAL%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_REAL+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:44:23,058] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:23,058] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:23,058] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:23,059] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:23,061] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:23,063] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:23,063] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:23,064] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test.table.table.0
WARN  [2023-01-06 20:44:23,064] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:23,064] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test.table.table.1
INFO  [2023-01-06 20:44:23,171] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_REAL Test QUERY INIT
INFO  [2023-01-06 20:44:23,207] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_REAL$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:23,208] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[25bf23c7-d645-4d74-b9cf-ea9e0a145267] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test))]]
INFO  [2023-01-06 20:44:23,213] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267
INFO  [2023-01-06 20:44:23,213] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267
127.0.0.1 - - [06/Jan/2023:20:44:23 +0000] "POST /api/datasets/NUMBER_REAL$20Test/queries HTTP/1.1" 201 1293 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:23,215] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267] with 0 results within PT0.001457S
INFO  [2023-01-06 20:44:23,215] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267] with 2 results within PT0.001864S
INFO  [2023-01-06 20:44:23,216] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267, workerId=NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_e1436179-8690-4b01-aa57-fc06643b8087, startTime=2023-01-06T20:44:23.213962, finishTime=2023-01-06T20:44:23.215419) of size 0
INFO  [2023-01-06 20:44:23,216] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267, workerId=NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_e11d3c95-15c7-4453-a9aa-b18fc61bc085, startTime=2023-01-06T20:44:23.213846, finishTime=2023-01-06T20:44:23.215710) of size 2
INFO  [2023-01-06 20:44:23,216] com.bakdata.conquery.models.execution.ManagedExecution: DONE 25bf23c7-d645-4d74-b9cf-ea9e0a145267 ManagedQuery within PT0.008677S
127.0.0.1 - - [06/Jan/2023:20:44:23 +0000] "GET /api/datasets/NUMBER_REAL$20Test/queries/NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267 HTTP/1.1" 200 1556 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:23,238] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test], queryId=25bf23c7-d645-4d74-b9cf-ea9e0a145267, label=concept	@§$, creationTime=2023-01-06T20:44:23.207945, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77680357[Count = 0], startTime=2023-01-06T20:44:23.208237, finishTime=2023-01-06T20:44:23.216914, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@758f11f4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@559e13a8, com.bakdata.conquery.models.query.ColumnDescriptor@15663ed9]) download on dataset Dataset[label=null, name=NUMBER_REAL Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:23,239] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test], queryId=25bf23c7-d645-4d74-b9cf-ea9e0a145267, label=concept	@§$, creationTime=2023-01-06T20:44:23.207945, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77680357[Count = 0], startTime=2023-01-06T20:44:23.208237, finishTime=2023-01-06T20:44:23.216914, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@758f11f4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@559e13a8, com.bakdata.conquery.models.query.ColumnDescriptor@15663ed9]) on dataset Dataset[label=null, name=NUMBER_REAL Test]
127.0.0.1 - - [06/Jan/2023:20:44:23 +0000] "GET /api/datasets/NUMBER_REAL%20Test/result/NUMBER_REAL$20Test.25bf23c7-d645-4d74-b9cf-ea9e0a145267.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 32
INFO  [2023-01-06 20:44:23,269] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_REAL Test on 3 rows
INFO  [2023-01-06 20:44:23,270] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_REAL Test
INFO  [2023-01-06 20:44:23,270] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-06 20:44:23,270] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-06 20:44:23,270] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test_e1436179-8690-4b01-aa57-fc06643b8087
INFO  [2023-01-06 20:44:23,270] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test_e11d3c95-15c7-4453-a9aa-b18fc61bc085
INFO  [2023-01-06 20:44:23,329] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_REAL Test
INFO  [2023-01-06 20:44:23,330] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test_e11d3c95-15c7-4453-a9aa-b18fc61bc085
INFO  [2023-01-06 20:44:23,334] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test_e1436179-8690-4b01-aa57-fc06643b8087
INFO  [2023-01-06 20:44:23,368] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_REAL$20Test
INFO  [2023-01-06 20:44:23,368] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:23,473] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_REAL Test
INFO  [2023-01-06 20:44:23,473] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DIFFSUM_INTEGER Test
INFO  [2023-01-06 20:44:23,473] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:23,473] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:23,475] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-06 20:44:23,475] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-06 20:44:23,475] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:23,475] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:23,476] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:23,477] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_cf5fb706-0da0-4e7a-8e27-b5361693371f are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:23,477] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_cf5fb706-0da0-4e7a-8e27-b5361693371f are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:23,477] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:23,477] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_be7ad15f-19ed-4665-8a5e-b47b430dd14f are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:23,477] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_be7ad15f-19ed-4665-8a5e-b47b430dd14f are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:23,477] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:23,584] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:23,585] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DIFFSUM_INTEGER$20Test.table
INFO  [2023-01-06 20:44:23,585] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DIFFSUM_INTEGER$20Test.table
INFO  [2023-01-06 20:44:23,712] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:23,829] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:23,830] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:23,830] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 439 B in total
INFO  [2023-01-06 20:44:23,830] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DIFFSUM_INTEGER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000486041sINFO  [2023-01-06 20:44:23,879] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=21, min=1, average=1.750000, max=3}
INFO  [2023-01-06 20:44:23,880] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[minus] with IntegerParser(super=Parser(lines=21, nullLines=0), minValue=0, maxValue=200)
INFO  [2023-01-06 20:44:23,880] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14835, maxValue=17614), dateReader=com.bakdata.conquery.util.DateReader@66a7bcba)
INFO  [2023-01-06 20:44:23,880] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with IntegerParser(super=Parser(lines=21, nullLines=0), minValue=50, maxValue=250)
INFO  [2023-01-06 20:44:23,886] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DIFFSUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:23,886] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:23,886] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DIFFSUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:23,907] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DIFFSUM_INTEGER$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:23 +0000] "POST /admin/datasets/DIFFSUM_INTEGER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DIFFSUM_INTEGER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:23,909] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:23,909] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:23,910] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:23,910] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:23,912] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:23,913] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DIFFSUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-06 20:44:23,913] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DIFFSUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-06 20:44:23,915] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.0
INFO  [2023-01-06 20:44:23,915] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.1
WARN  [2023-01-06 20:44:23,915] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:23,915] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.2
INFO  [2023-01-06 20:44:23,917] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.3
INFO  [2023-01-06 20:44:24,022] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DIFFSUM_INTEGER Test QUERY INIT
INFO  [2023-01-06 20:44:24,040] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DIFFSUM_INTEGER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:24,040] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[16f35f69-1cff-4871-bf83-9498f2e443ad] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DIFFSUM_INTEGER Test))]]
INFO  [2023-01-06 20:44:24,044] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad
INFO  [2023-01-06 20:44:24,044] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad
INFO  [2023-01-06 20:44:24,047] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad] with 3 results within PT0.002378S
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "POST /api/datasets/DIFFSUM_INTEGER$20Test/queries HTTP/1.1" 201 1210 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:24,047] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad] with 4 results within PT0.002766S
INFO  [2023-01-06 20:44:24,048] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad, workerId=DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_be7ad15f-19ed-4665-8a5e-b47b430dd14f, startTime=2023-01-06T20:44:24.044935, finishTime=2023-01-06T20:44:24.047313) of size 3
INFO  [2023-01-06 20:44:24,048] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad, workerId=DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_cf5fb706-0da0-4e7a-8e27-b5361693371f, startTime=2023-01-06T20:44:24.045034, finishTime=2023-01-06T20:44:24.047800) of size 4
INFO  [2023-01-06 20:44:24,049] com.bakdata.conquery.models.execution.ManagedExecution: DONE 16f35f69-1cff-4871-bf83-9498f2e443ad ManagedQuery within PT0.008429S
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "GET /api/datasets/DIFFSUM_INTEGER$20Test/queries/DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad HTTP/1.1" 200 1489 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:24,075] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DIFFSUM_INTEGER Test], queryId=16f35f69-1cff-4871-bf83-9498f2e443ad, label=concept	@§$, creationTime=2023-01-06T20:44:24.040692, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3d02e148[Count = 0], startTime=2023-01-06T20:44:24.040949, finishTime=2023-01-06T20:44:24.049378, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@42990c11), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DIFFSUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@69890c45, com.bakdata.conquery.models.query.ColumnDescriptor@5690cb04]) download on dataset Dataset[label=null, name=DIFFSUM_INTEGER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:24,075] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DIFFSUM_INTEGER Test], queryId=16f35f69-1cff-4871-bf83-9498f2e443ad, label=concept	@§$, creationTime=2023-01-06T20:44:24.040692, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3d02e148[Count = 0], startTime=2023-01-06T20:44:24.040949, finishTime=2023-01-06T20:44:24.049378, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@42990c11), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DIFFSUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@69890c45, com.bakdata.conquery.models.query.ColumnDescriptor@5690cb04]) on dataset Dataset[label=null, name=DIFFSUM_INTEGER Test]
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "GET /api/datasets/DIFFSUM_INTEGER%20Test/result/DIFFSUM_INTEGER$20Test.16f35f69-1cff-4871-bf83-9498f2e443ad.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:44:24,096] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest DIFFSUM_INTEGER Test on 8 rows
INFO  [2023-01-06 20:44:24,096] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DIFFSUM_INTEGER Test
INFO  [2023-01-06 20:44:24,096] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-06 20:44:24,097] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-06 20:44:24,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DIFFSUM_INTEGER Test_be7ad15f-19ed-4665-8a5e-b47b430dd14f
INFO  [2023-01-06 20:44:24,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DIFFSUM_INTEGER Test_cf5fb706-0da0-4e7a-8e27-b5361693371f
INFO  [2023-01-06 20:44:24,175] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DIFFSUM_INTEGER Test
INFO  [2023-01-06 20:44:24,176] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DIFFSUM_INTEGER Test_cf5fb706-0da0-4e7a-8e27-b5361693371f
INFO  [2023-01-06 20:44:24,176] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DIFFSUM_INTEGER Test_be7ad15f-19ed-4665-8a5e-b47b430dd14f
INFO  [2023-01-06 20:44:24,217] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DIFFSUM_INTEGER$20Test
INFO  [2023-01-06 20:44:24,217] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:24,324] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DIFFSUM_INTEGER Test
INFO  [2023-01-06 20:44:24,324] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_INTEGER Test
INFO  [2023-01-06 20:44:24,324] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:24,324] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:24,325] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-06 20:44:24,325] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-06 20:44:24,325] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:24,325] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:24,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_3ae9125e-c825-48bb-ad0d-deb643a951b9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:24,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_3ae9125e-c825-48bb-ad0d-deb643a951b9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:24,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:24,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_c34e70ba-260b-4501-9709-45f7f768d443 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:24,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_c34e70ba-260b-4501-9709-45f7f768d443 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:24,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:24,331] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:24,435] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:24,436] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test.table
INFO  [2023-01-06 20:44:24,436] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test.table
INFO  [2023-01-06 20:44:24,551] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:24,665] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:24,666] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:24,666] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 448 B in total
INFO  [2023-01-06 20:44:24,666] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_INTEGER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000445126sINFO  [2023-01-06 20:44:24,711] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=21, min=1, average=1.750000, max=3}
INFO  [2023-01-06 20:44:24,711] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[minus] with RealParser(super=Parser(lines=21, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.52587890625E-5)
INFO  [2023-01-06 20:44:24,711] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with RealParser(super=Parser(lines=21, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.52587890625E-5)
INFO  [2023-01-06 20:44:24,711] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14835, maxValue=17614), dateReader=com.bakdata.conquery.util.DateReader@6751e7d6)
INFO  [2023-01-06 20:44:24,714] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:24,714] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:24,714] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:24,734] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_INTEGER$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "POST /admin/datasets/SUM_INTEGER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SUM_INTEGER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:44:24,736] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:24,736] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:24,736] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:24,736] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:24,737] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:24,738] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-06 20:44:24,738] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test.table.table], containing 21 entries.
WARN  [2023-01-06 20:44:24,739] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:24,739] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.0
INFO  [2023-01-06 20:44:24,739] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.1
INFO  [2023-01-06 20:44:24,740] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.3
INFO  [2023-01-06 20:44:24,740] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.2
INFO  [2023-01-06 20:44:24,845] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_INTEGER Test QUERY INIT
INFO  [2023-01-06 20:44:24,866] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_INTEGER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:24,867] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5052cb02-51bb-4c81-8417-370ec6393acb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test))]]
INFO  [2023-01-06 20:44:24,871] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb
INFO  [2023-01-06 20:44:24,871] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "POST /api/datasets/SUM_INTEGER$20Test/queries HTTP/1.1" 201 1191 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:24,872] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb] with 4 results within PT0.001541S
INFO  [2023-01-06 20:44:24,873] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb] with 3 results within PT0.001492S
INFO  [2023-01-06 20:44:24,873] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb, workerId=SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_c34e70ba-260b-4501-9709-45f7f768d443, startTime=2023-01-06T20:44:24.871116, finishTime=2023-01-06T20:44:24.872657) of size 4
INFO  [2023-01-06 20:44:24,874] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb, workerId=SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_3ae9125e-c825-48bb-ad0d-deb643a951b9, startTime=2023-01-06T20:44:24.871737, finishTime=2023-01-06T20:44:24.873229) of size 3
INFO  [2023-01-06 20:44:24,878] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5052cb02-51bb-4c81-8417-370ec6393acb ManagedQuery within PT0.011686S
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "GET /api/datasets/SUM_INTEGER$20Test/queries/SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb HTTP/1.1" 200 1455 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:24,914] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test], queryId=5052cb02-51bb-4c81-8417-370ec6393acb, label=concept	@§$, creationTime=2023-01-06T20:44:24.867056, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3ba8311f[Count = 0], startTime=2023-01-06T20:44:24.867247, finishTime=2023-01-06T20:44:24.878933, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@35177a0c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3df127ba, com.bakdata.conquery.models.query.ColumnDescriptor@28d5d3]) download on dataset Dataset[label=null, name=SUM_INTEGER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:24,915] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test], queryId=5052cb02-51bb-4c81-8417-370ec6393acb, label=concept	@§$, creationTime=2023-01-06T20:44:24.867056, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3ba8311f[Count = 0], startTime=2023-01-06T20:44:24.867247, finishTime=2023-01-06T20:44:24.878933, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@35177a0c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3df127ba, com.bakdata.conquery.models.query.ColumnDescriptor@28d5d3]) on dataset Dataset[label=null, name=SUM_INTEGER Test]
127.0.0.1 - - [06/Jan/2023:20:44:24 +0000] "GET /api/datasets/SUM_INTEGER%20Test/result/SUM_INTEGER$20Test.5052cb02-51bb-4c81-8417-370ec6393acb.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:24,934] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SUM_INTEGER Test on 8 rows
INFO  [2023-01-06 20:44:24,934] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_INTEGER Test
INFO  [2023-01-06 20:44:24,934] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-06 20:44:24,934] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-06 20:44:24,935] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test_3ae9125e-c825-48bb-ad0d-deb643a951b9
INFO  [2023-01-06 20:44:24,935] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test_c34e70ba-260b-4501-9709-45f7f768d443
INFO  [2023-01-06 20:44:25,033] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_INTEGER Test
INFO  [2023-01-06 20:44:25,033] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test_c34e70ba-260b-4501-9709-45f7f768d443
INFO  [2023-01-06 20:44:25,033] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test_3ae9125e-c825-48bb-ad0d-deb643a951b9
INFO  [2023-01-06 20:44:25,039] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_INTEGER$20Test
INFO  [2023-01-06 20:44:25,039] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:25,153] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_INTEGER Test
INFO  [2023-01-06 20:44:25,153] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM Test
INFO  [2023-01-06 20:44:25,153] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:25,153] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:25,154] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-06 20:44:25,154] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-06 20:44:25,154] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:25,154] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:25,156] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_7502d01e-95b4-437c-809e-b24f4758f0fb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:25,156] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_7502d01e-95b4-437c-809e-b24f4758f0fb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:25,156] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:25,156] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_4fcd89fc-2c95-45b0-ade3-fffdf1afe0c1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:25,156] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_4fcd89fc-2c95-45b0-ade3-fffdf1afe0c1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:25,156] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:25,160] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:25,266] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:25,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM$20Test.table
INFO  [2023-01-06 20:44:25,267] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM$20Test.table
INFO  [2023-01-06 20:44:25,388] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:25,506] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:25,506] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:25,506] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 166 B in total
INFO  [2023-01-06 20:44:25,506] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000442089sINFO  [2023-01-06 20:44:25,551] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:44:25,551] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14610, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@2179fed4), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14640, maxValue=15928), dateReader=com.bakdata.conquery.util.DateReader@3092d979), dateReader=com.bakdata.conquery.util.DateReader@53959651, onlyQuarters=false, maxValue=15928, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:44:25,553] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:25,553] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:25,553] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:25,573] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DURATION_SUM$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:25 +0000] "POST /admin/datasets/DURATION_SUM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DURATION_SUM+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:44:25,575] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:25,576] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:25,576] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:25,576] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:25,579] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:25,579] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:25,579] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:25,581] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:25,581] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM$20Test.table.table.0
INFO  [2023-01-06 20:44:25,581] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM$20Test.table.table.1
INFO  [2023-01-06 20:44:25,688] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM Test QUERY INIT
INFO  [2023-01-06 20:44:25,698] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:25,698] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[64290d1d-e51b-479c-b242-a59fb5713ac6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM Test))]]
INFO  [2023-01-06 20:44:25,700] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6
INFO  [2023-01-06 20:44:25,700] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6
INFO  [2023-01-06 20:44:25,701] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6] with 1 results within PT0.000813S
127.0.0.1 - - [06/Jan/2023:20:44:25 +0000] "POST /api/datasets/DURATION_SUM$20Test/queries HTTP/1.1" 201 1197 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:44:25,701] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6] with 2 results within PT0.001209S
INFO  [2023-01-06 20:44:25,702] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6, workerId=DURATION_SUM$20Test.worker_DURATION_SUM$20Test_4fcd89fc-2c95-45b0-ade3-fffdf1afe0c1, startTime=2023-01-06T20:44:25.700663, finishTime=2023-01-06T20:44:25.701476) of size 1
INFO  [2023-01-06 20:44:25,702] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6, workerId=DURATION_SUM$20Test.worker_DURATION_SUM$20Test_7502d01e-95b4-437c-809e-b24f4758f0fb, startTime=2023-01-06T20:44:25.700667, finishTime=2023-01-06T20:44:25.701876) of size 2
INFO  [2023-01-06 20:44:25,702] com.bakdata.conquery.models.execution.ManagedExecution: DONE 64290d1d-e51b-479c-b242-a59fb5713ac6 ManagedQuery within PT0.004225S
127.0.0.1 - - [06/Jan/2023:20:44:25 +0000] "GET /api/datasets/DURATION_SUM$20Test/queries/DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6 HTTP/1.1" 200 1464 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:25,733] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM Test], queryId=64290d1d-e51b-479c-b242-a59fb5713ac6, label=concept	@§$, creationTime=2023-01-06T20:44:25.698314, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5bc1893[Count = 0], startTime=2023-01-06T20:44:25.698459, finishTime=2023-01-06T20:44:25.702684, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@15e5b30d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@17f22ebf, com.bakdata.conquery.models.query.ColumnDescriptor@119c37fa]) download on dataset Dataset[label=null, name=DURATION_SUM Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:25,733] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM Test], queryId=64290d1d-e51b-479c-b242-a59fb5713ac6, label=concept	@§$, creationTime=2023-01-06T20:44:25.698314, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5bc1893[Count = 0], startTime=2023-01-06T20:44:25.698459, finishTime=2023-01-06T20:44:25.702684, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@15e5b30d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@17f22ebf, com.bakdata.conquery.models.query.ColumnDescriptor@119c37fa]) on dataset Dataset[label=null, name=DURATION_SUM Test]
127.0.0.1 - - [06/Jan/2023:20:44:25 +0000] "GET /api/datasets/DURATION_SUM%20Test/result/DURATION_SUM$20Test.64290d1d-e51b-479c-b242-a59fb5713ac6.csv?pretty=false HTTP/1.1" 200 91 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:44:25,753] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest DURATION_SUM Test on 4 rows
INFO  [2023-01-06 20:44:25,753] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM Test
INFO  [2023-01-06 20:44:25,754] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-06 20:44:25,754] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-06 20:44:25,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM Test_4fcd89fc-2c95-45b0-ade3-fffdf1afe0c1
INFO  [2023-01-06 20:44:25,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM Test_7502d01e-95b4-437c-809e-b24f4758f0fb
INFO  [2023-01-06 20:44:25,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM Test
INFO  [2023-01-06 20:44:25,755] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM Test_7502d01e-95b4-437c-809e-b24f4758f0fb
INFO  [2023-01-06 20:44:25,755] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM Test_4fcd89fc-2c95-45b0-ade3-fffdf1afe0c1
INFO  [2023-01-06 20:44:25,781] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM$20Test
INFO  [2023-01-06 20:44:25,781] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:25,887] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM Test
INFO  [2023-01-06 20:44:25,888] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM_2 Test
INFO  [2023-01-06 20:44:25,888] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:25,888] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:25,893] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-06 20:44:25,893] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:25,893] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-06 20:44:25,893] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:25,898] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_13bdc6e7-36aa-4ffb-bd48-78b2d8358022 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:25,898] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_13bdc6e7-36aa-4ffb-bd48-78b2d8358022 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:25,898] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:25,898] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_7992d283-0778-4d0d-afa4-6d7490dc61ed are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:25,898] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_7992d283-0778-4d0d-afa4-6d7490dc61ed are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:25,898] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:25,900] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,005] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,005] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_2$20Test.table
INFO  [2023-01-06 20:44:26,006] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_2$20Test.table
INFO  [2023-01-06 20:44:26,121] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,233] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:26,233] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:26,233] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 170 B in total
INFO  [2023-01-06 20:44:26,233] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000224666sINFO  [2023-01-06 20:44:26,256] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-06 20:44:26,256] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=7, nullLines=1), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14610, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@543b6649), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14640, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@1d393141), dateReader=com.bakdata.conquery.util.DateReader@76457059, onlyQuarters=false, maxValue=15927, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:44:26,259] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:26,259] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:26,259] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:26,275] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DURATION_SUM_2$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:26 +0000] "POST /admin/datasets/DURATION_SUM_2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DURATION_SUM_2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:26,277] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,277] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:26,278] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:26,278] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:26,279] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:26,280] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_2$20Test.table.table], containing 7 entries.
INFO  [2023-01-06 20:44:26,280] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_2$20Test.table.table], containing 7 entries.
WARN  [2023-01-06 20:44:26,280] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:26,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_2$20Test.table.table.0
INFO  [2023-01-06 20:44:26,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_2$20Test.table.table.1
INFO  [2023-01-06 20:44:26,386] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM_2 Test QUERY INIT
INFO  [2023-01-06 20:44:26,398] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM_2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:26,398] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d6a6c6a2-2b57-487f-96e4-fbabefaf4939] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_2 Test))]]
INFO  [2023-01-06 20:44:26,401] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939
INFO  [2023-01-06 20:44:26,401] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939
INFO  [2023-01-06 20:44:26,402] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939] with 3 results within PT0.000778S
127.0.0.1 - - [06/Jan/2023:20:44:26 +0000] "POST /api/datasets/DURATION_SUM_2$20Test/queries HTTP/1.1" 201 1205 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:26,402] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939] with 1 results within PT0.000843S
INFO  [2023-01-06 20:44:26,402] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939, workerId=DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_13bdc6e7-36aa-4ffb-bd48-78b2d8358022, startTime=2023-01-06T20:44:26.401561, finishTime=2023-01-06T20:44:26.402339) of size 3
INFO  [2023-01-06 20:44:26,403] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939, workerId=DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_7992d283-0778-4d0d-afa4-6d7490dc61ed, startTime=2023-01-06T20:44:26.401566, finishTime=2023-01-06T20:44:26.402409) of size 1
INFO  [2023-01-06 20:44:26,403] com.bakdata.conquery.models.execution.ManagedExecution: DONE d6a6c6a2-2b57-487f-96e4-fbabefaf4939 ManagedQuery within PT0.004615S
127.0.0.1 - - [06/Jan/2023:20:44:26 +0000] "GET /api/datasets/DURATION_SUM_2$20Test/queries/DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939 HTTP/1.1" 200 1480 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:44:26,426] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_2 Test], queryId=d6a6c6a2-2b57-487f-96e4-fbabefaf4939, label=concept	@§$, creationTime=2023-01-06T20:44:26.398369, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@279beb0f[Count = 0], startTime=2023-01-06T20:44:26.398579, finishTime=2023-01-06T20:44:26.403194, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6d31efa3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@34c71549, com.bakdata.conquery.models.query.ColumnDescriptor@7dadf87c]) download on dataset Dataset[label=null, name=DURATION_SUM_2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:26,426] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_2 Test], queryId=d6a6c6a2-2b57-487f-96e4-fbabefaf4939, label=concept	@§$, creationTime=2023-01-06T20:44:26.398369, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@279beb0f[Count = 0], startTime=2023-01-06T20:44:26.398579, finishTime=2023-01-06T20:44:26.403194, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6d31efa3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@34c71549, com.bakdata.conquery.models.query.ColumnDescriptor@7dadf87c]) on dataset Dataset[label=null, name=DURATION_SUM_2 Test]
127.0.0.1 - - [06/Jan/2023:20:44:26 +0000] "GET /api/datasets/DURATION_SUM_2%20Test/result/DURATION_SUM_2$20Test.d6a6c6a2-2b57-487f-96e4-fbabefaf4939.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:44:26,442] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest DURATION_SUM_2 Test on 5 rows
INFO  [2023-01-06 20:44:26,442] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM_2 Test
INFO  [2023-01-06 20:44:26,442] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-06 20:44:26,442] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-06 20:44:26,442] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_2 Test_13bdc6e7-36aa-4ffb-bd48-78b2d8358022
INFO  [2023-01-06 20:44:26,442] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_2 Test_7992d283-0778-4d0d-afa4-6d7490dc61ed
INFO  [2023-01-06 20:44:26,493] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM_2 Test
INFO  [2023-01-06 20:44:26,497] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_2 Test_13bdc6e7-36aa-4ffb-bd48-78b2d8358022
INFO  [2023-01-06 20:44:26,497] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_2 Test_7992d283-0778-4d0d-afa4-6d7490dc61ed
INFO  [2023-01-06 20:44:26,581] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM_2$20Test
INFO  [2023-01-06 20:44:26,581] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,686] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM_2 Test
INFO  [2023-01-06 20:44:26,686] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT Test
INFO  [2023-01-06 20:44:26,687] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:26,687] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:26,688] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-06 20:44:26,688] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-06 20:44:26,688] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:26,688] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:26,692] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_4e2f88ff-0a3c-49b2-87ce-2c6d6ee1af0b are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:26,692] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_4e2f88ff-0a3c-49b2-87ce-2c6d6ee1af0b are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:26,692] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:26,692] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_b4786b8d-3a56-4806-a8d1-727240b8b5bf are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:26,692] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_b4786b8d-3a56-4806-a8d1-727240b8b5bf are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:26,692] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:26,695] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,797] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:26,797] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT$20Test.table
INFO  [2023-01-06 20:44:26,797] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT$20Test.table
INFO  [2023-01-06 20:44:26,922] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,039] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:27,040] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:27,040] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 139 B in total
INFO  [2023-01-06 20:44:27,040] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000392683sINFO  [2023-01-06 20:44:27,080] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=8, min=1, average=1.600000, max=3}
INFO  [2023-01-06 20:44:27,080] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2b21fd0a)
INFO  [2023-01-06 20:44:27,080] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:27,083] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:27,083] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:27,083] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:27,097] com.bakdata.conquery.models.jobs.ImportJob: Importing table into MULTI_SELECT$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:27 +0000] "POST /admin/datasets/MULTI_SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:27,099] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,099] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:27,100] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:27,100] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:27,101] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:27,101] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT$20Test.table.table], containing 8 entries.
INFO  [2023-01-06 20:44:27,101] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT$20Test.table.table], containing 8 entries.
WARN  [2023-01-06 20:44:27,102] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:27,102] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT$20Test.table.table.0
INFO  [2023-01-06 20:44:27,102] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT$20Test.table.table.1
INFO  [2023-01-06 20:44:27,208] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT Test QUERY INIT
INFO  [2023-01-06 20:44:27,235] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:27,236] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a3702d0c-6630-4e26-a82c-2b58811d0a4b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT Test))]]
INFO  [2023-01-06 20:44:27,239] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b
INFO  [2023-01-06 20:44:27,239] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b
INFO  [2023-01-06 20:44:27,240] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b] with 2 results within PT0.001022S
INFO  [2023-01-06 20:44:27,240] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b] with 1 results within PT0.000943S
INFO  [2023-01-06 20:44:27,241] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b, workerId=MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_b4786b8d-3a56-4806-a8d1-727240b8b5bf, startTime=2023-01-06T20:44:27.239489, finishTime=2023-01-06T20:44:27.240511) of size 2
INFO  [2023-01-06 20:44:27,241] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b, workerId=MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_4e2f88ff-0a3c-49b2-87ce-2c6d6ee1af0b, startTime=2023-01-06T20:44:27.239701, finishTime=2023-01-06T20:44:27.240644) of size 1
INFO  [2023-01-06 20:44:27,241] com.bakdata.conquery.models.execution.ManagedExecution: DONE a3702d0c-6630-4e26-a82c-2b58811d0a4b ManagedQuery within PT0.005421S
127.0.0.1 - - [06/Jan/2023:20:44:27 +0000] "POST /api/datasets/MULTI_SELECT$20Test/queries HTTP/1.1" 201 1185 "-" "Conquery (test client)" 9
127.0.0.1 - - [06/Jan/2023:20:44:27 +0000] "GET /api/datasets/MULTI_SELECT$20Test/queries/MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b HTTP/1.1" 200 1452 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:27,269] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT Test], queryId=a3702d0c-6630-4e26-a82c-2b58811d0a4b, label=concept	@§$, creationTime=2023-01-06T20:44:27.235882, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4bad04df[Count = 0], startTime=2023-01-06T20:44:27.236136, finishTime=2023-01-06T20:44:27.241557, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@317cf7c8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@53fbc787, com.bakdata.conquery.models.query.ColumnDescriptor@4f9dc838]) download on dataset Dataset[label=null, name=MULTI_SELECT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:27,269] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT Test], queryId=a3702d0c-6630-4e26-a82c-2b58811d0a4b, label=concept	@§$, creationTime=2023-01-06T20:44:27.235882, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4bad04df[Count = 0], startTime=2023-01-06T20:44:27.236136, finishTime=2023-01-06T20:44:27.241557, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@317cf7c8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@53fbc787, com.bakdata.conquery.models.query.ColumnDescriptor@4f9dc838]) on dataset Dataset[label=null, name=MULTI_SELECT Test]
127.0.0.1 - - [06/Jan/2023:20:44:27 +0000] "GET /api/datasets/MULTI_SELECT%20Test/result/MULTI_SELECT$20Test.a3702d0c-6630-4e26-a82c-2b58811d0a4b.csv?pretty=false HTTP/1.1" 200 91 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:44:27,290] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest MULTI_SELECT Test on 4 rows
INFO  [2023-01-06 20:44:27,290] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT Test
INFO  [2023-01-06 20:44:27,291] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-06 20:44:27,291] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-06 20:44:27,291] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT Test_4e2f88ff-0a3c-49b2-87ce-2c6d6ee1af0b
INFO  [2023-01-06 20:44:27,291] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT Test_b4786b8d-3a56-4806-a8d1-727240b8b5bf
INFO  [2023-01-06 20:44:27,388] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT Test
INFO  [2023-01-06 20:44:27,390] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT Test_4e2f88ff-0a3c-49b2-87ce-2c6d6ee1af0b
INFO  [2023-01-06 20:44:27,390] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT Test_b4786b8d-3a56-4806-a8d1-727240b8b5bf
INFO  [2023-01-06 20:44:27,402] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT$20Test
INFO  [2023-01-06 20:44:27,402] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,508] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT Test
INFO  [2023-01-06 20:44:27,508] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_DECIMAL Test
INFO  [2023-01-06 20:44:27,508] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:27,508] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:27,509] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-06 20:44:27,510] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-06 20:44:27,510] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:27,510] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:27,512] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_4764e7b6-79b5-4571-8652-f450636adf99 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:27,512] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_4764e7b6-79b5-4571-8652-f450636adf99 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:27,512] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:27,512] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_8c293cc3-fb1d-409a-af58-7241115369bf are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:27,512] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_8c293cc3-fb1d-409a-af58-7241115369bf are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:27,512] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:27,516] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,622] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,623] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_DECIMAL$20Test.table
INFO  [2023-01-06 20:44:27,623] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_DECIMAL$20Test.table
INFO  [2023-01-06 20:44:27,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,854] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:27,854] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:27,854] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 355 B in total
INFO  [2023-01-06 20:44:27,854] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_DECIMAL Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000384806sINFO  [2023-01-06 20:44:27,893] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=18, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:44:27,893] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=18, nullLines=0), subType=IntegerParser(super=Parser(lines=18, nullLines=0), minValue=14835, maxValue=17612), dateReader=com.bakdata.conquery.util.DateReader@177a593c)
INFO  [2023-01-06 20:44:27,894] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with DecimalParser(super=Parser(lines=18, nullLines=3), maxScale=2, maxAbs=300)
INFO  [2023-01-06 20:44:27,900] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_DECIMAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:27,900] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:27,900] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_DECIMAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:27,914] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_DECIMAL$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:27 +0000] "POST /admin/datasets/NUMBER_DECIMAL%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_DECIMAL+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:27,917] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:27,917] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:27,917] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:27,917] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:27,920] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:27,921] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_DECIMAL$20Test.table.table], containing 18 entries.
INFO  [2023-01-06 20:44:27,921] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_DECIMAL$20Test.table.table], containing 18 entries.
INFO  [2023-01-06 20:44:27,922] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.0
WARN  [2023-01-06 20:44:27,922] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:27,922] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.1
INFO  [2023-01-06 20:44:27,922] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.2
INFO  [2023-01-06 20:44:27,923] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.3
INFO  [2023-01-06 20:44:28,028] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_DECIMAL Test QUERY INIT
INFO  [2023-01-06 20:44:28,055] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_DECIMAL$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:28,055] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[72145bde-27b1-4bec-a67f-ba250f29c08c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_DECIMAL Test))]]
INFO  [2023-01-06 20:44:28,059] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c
INFO  [2023-01-06 20:44:28,059] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c
INFO  [2023-01-06 20:44:28,061] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c] with 2 results within PT0.001777S
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "POST /api/datasets/NUMBER_DECIMAL$20Test/queries HTTP/1.1" 201 1203 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:28,061] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c] with 3 results within PT0.001765S
INFO  [2023-01-06 20:44:28,062] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c, workerId=NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_8c293cc3-fb1d-409a-af58-7241115369bf, startTime=2023-01-06T20:44:28.059550, finishTime=2023-01-06T20:44:28.061327) of size 2
INFO  [2023-01-06 20:44:28,062] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c, workerId=NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_4764e7b6-79b5-4571-8652-f450636adf99, startTime=2023-01-06T20:44:28.059794, finishTime=2023-01-06T20:44:28.061559) of size 3
INFO  [2023-01-06 20:44:28,062] com.bakdata.conquery.models.execution.ManagedExecution: DONE 72145bde-27b1-4bec-a67f-ba250f29c08c ManagedQuery within PT0.006864S
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "GET /api/datasets/NUMBER_DECIMAL$20Test/queries/NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c HTTP/1.1" 200 1478 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:28,093] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_DECIMAL Test], queryId=72145bde-27b1-4bec-a67f-ba250f29c08c, label=concept	@§$, creationTime=2023-01-06T20:44:28.055415, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3b0fe96c[Count = 0], startTime=2023-01-06T20:44:28.055671, finishTime=2023-01-06T20:44:28.062535, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ac8b506), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_DECIMAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@19f5893c, com.bakdata.conquery.models.query.ColumnDescriptor@7e413dac]) download on dataset Dataset[label=null, name=NUMBER_DECIMAL Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:28,093] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_DECIMAL Test], queryId=72145bde-27b1-4bec-a67f-ba250f29c08c, label=concept	@§$, creationTime=2023-01-06T20:44:28.055415, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3b0fe96c[Count = 0], startTime=2023-01-06T20:44:28.055671, finishTime=2023-01-06T20:44:28.062535, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ac8b506), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_DECIMAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@19f5893c, com.bakdata.conquery.models.query.ColumnDescriptor@7e413dac]) on dataset Dataset[label=null, name=NUMBER_DECIMAL Test]
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "GET /api/datasets/NUMBER_DECIMAL%20Test/result/NUMBER_DECIMAL$20Test.72145bde-27b1-4bec-a67f-ba250f29c08c.csv?pretty=false HTTP/1.1" 200 144 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:44:28,115] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_DECIMAL Test on 6 rows
INFO  [2023-01-06 20:44:28,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_DECIMAL Test
INFO  [2023-01-06 20:44:28,116] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-06 20:44:28,116] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-06 20:44:28,116] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_DECIMAL Test_4764e7b6-79b5-4571-8652-f450636adf99
INFO  [2023-01-06 20:44:28,116] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_DECIMAL Test_8c293cc3-fb1d-409a-af58-7241115369bf
INFO  [2023-01-06 20:44:28,214] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_DECIMAL Test_8c293cc3-fb1d-409a-af58-7241115369bf
INFO  [2023-01-06 20:44:28,214] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_DECIMAL Test_4764e7b6-79b5-4571-8652-f450636adf99
INFO  [2023-01-06 20:44:28,214] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_DECIMAL Test
INFO  [2023-01-06 20:44:28,223] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_DECIMAL$20Test
INFO  [2023-01-06 20:44:28,223] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:28,330] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_DECIMAL Test
INFO  [2023-01-06 20:44:28,330] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_INTEGER Test
INFO  [2023-01-06 20:44:28,330] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:28,330] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:28,332] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-06 20:44:28,332] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-06 20:44:28,332] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:28,332] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:28,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_4dd62b3c-8fd5-4550-a39f-94bf2c34a388 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:28,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_4dd62b3c-8fd5-4550-a39f-94bf2c34a388 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:28,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:28,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_2b5286df-f1da-4aea-86a0-04e9c4eaaa59 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:28,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_2b5286df-f1da-4aea-86a0-04e9c4eaaa59 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:28,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:28,338] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:28,442] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:28,442] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test.table
INFO  [2023-01-06 20:44:28,442] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test.table
INFO  [2023-01-06 20:44:28,564] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:28,675] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:28,675] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:28,675] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 255 B in total
INFO  [2023-01-06 20:44:28,675] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_INTEGER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000390619sINFO  [2023-01-06 20:44:28,715] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=9, sum=15, min=1, average=1.666667, max=2}
INFO  [2023-01-06 20:44:28,715] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=15, nullLines=0), subType=IntegerParser(super=Parser(lines=15, nullLines=0), minValue=14835, maxValue=17351), dateReader=com.bakdata.conquery.util.DateReader@5955dc18)
INFO  [2023-01-06 20:44:28,715] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=15, nullLines=3), minValue=50, maxValue=300)
INFO  [2023-01-06 20:44:28,723] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:28,723] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:28,723] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:28,737] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_INTEGER$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "POST /admin/datasets/NUMBER_INTEGER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_INTEGER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:28,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:28,740] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:28,740] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:28,740] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:28,743] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:44:28,743] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test.table.table], containing 15 entries.
INFO  [2023-01-06 20:44:28,743] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test.table.table], containing 15 entries.
INFO  [2023-01-06 20:44:28,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test.table.table.0
WARN  [2023-01-06 20:44:28,745] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:28,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test.table.table.1
INFO  [2023-01-06 20:44:28,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test.table.table.2
INFO  [2023-01-06 20:44:28,850] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_INTEGER Test QUERY INIT
INFO  [2023-01-06 20:44:28,860] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_INTEGER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:28,860] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f201b6a8-d190-4f18-b853-319879f83ab0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test))]]
INFO  [2023-01-06 20:44:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0
INFO  [2023-01-06 20:44:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "POST /api/datasets/NUMBER_INTEGER$20Test/queries HTTP/1.1" 201 1206 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:44:28,864] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0] with 2 results within PT0.001292S
INFO  [2023-01-06 20:44:28,864] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0] with 2 results within PT0.001699S
INFO  [2023-01-06 20:44:28,865] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0, workerId=NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_4dd62b3c-8fd5-4550-a39f-94bf2c34a388, startTime=2023-01-06T20:44:28.862947, finishTime=2023-01-06T20:44:28.864239) of size 2
INFO  [2023-01-06 20:44:28,865] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0, workerId=NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_2b5286df-f1da-4aea-86a0-04e9c4eaaa59, startTime=2023-01-06T20:44:28.862948, finishTime=2023-01-06T20:44:28.864647) of size 2
INFO  [2023-01-06 20:44:28,865] com.bakdata.conquery.models.execution.ManagedExecution: DONE f201b6a8-d190-4f18-b853-319879f83ab0 ManagedQuery within PT0.005052S
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "GET /api/datasets/NUMBER_INTEGER$20Test/queries/NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0 HTTP/1.1" 200 1481 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:28,907] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test], queryId=f201b6a8-d190-4f18-b853-319879f83ab0, label=concept	@§$, creationTime=2023-01-06T20:44:28.860198, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@b47ae6e[Count = 0], startTime=2023-01-06T20:44:28.860357, finishTime=2023-01-06T20:44:28.865409, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@53b0fde9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@573c22d1, com.bakdata.conquery.models.query.ColumnDescriptor@6f7da310]) download on dataset Dataset[label=null, name=NUMBER_INTEGER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:28,907] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test], queryId=f201b6a8-d190-4f18-b853-319879f83ab0, label=concept	@§$, creationTime=2023-01-06T20:44:28.860198, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@b47ae6e[Count = 0], startTime=2023-01-06T20:44:28.860357, finishTime=2023-01-06T20:44:28.865409, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@53b0fde9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@573c22d1, com.bakdata.conquery.models.query.ColumnDescriptor@6f7da310]) on dataset Dataset[label=null, name=NUMBER_INTEGER Test]
127.0.0.1 - - [06/Jan/2023:20:44:28 +0000] "GET /api/datasets/NUMBER_INTEGER%20Test/result/NUMBER_INTEGER$20Test.f201b6a8-d190-4f18-b853-319879f83ab0.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:44:28,927] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_INTEGER Test on 5 rows
INFO  [2023-01-06 20:44:28,927] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_INTEGER Test
INFO  [2023-01-06 20:44:28,927] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-06 20:44:28,927] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-06 20:44:28,928] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test_4dd62b3c-8fd5-4550-a39f-94bf2c34a388
INFO  [2023-01-06 20:44:28,928] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test_2b5286df-f1da-4aea-86a0-04e9c4eaaa59
INFO  [2023-01-06 20:44:28,932] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_INTEGER Test
INFO  [2023-01-06 20:44:28,933] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test_4dd62b3c-8fd5-4550-a39f-94bf2c34a388
INFO  [2023-01-06 20:44:28,933] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test_2b5286df-f1da-4aea-86a0-04e9c4eaaa59
INFO  [2023-01-06 20:44:28,945] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_INTEGER$20Test
INFO  [2023-01-06 20:44:28,945] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,052] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_INTEGER Test
INFO  [2023-01-06 20:44:29,052] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_INTEGER Test
INFO  [2023-01-06 20:44:29,052] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:29,052] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:29,053] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-06 20:44:29,053] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-06 20:44:29,053] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:29,053] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:29,055] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_0449425a-6f8a-42d9-8a51-e466ce0073af are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:29,055] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_0449425a-6f8a-42d9-8a51-e466ce0073af are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:29,055] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:29,055] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_dcfecb0d-a5e7-4100-94db-829ccf1582ed are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:29,055] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_dcfecb0d-a5e7-4100-94db-829ccf1582ed are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:29,055] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:29,059] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,164] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,164] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test[1].table
INFO  [2023-01-06 20:44:29,165] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test[1].table
INFO  [2023-01-06 20:44:29,279] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,388] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:29,388] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:29,388] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 41 B in total
INFO  [2023-01-06 20:44:29,388] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_INTEGER Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000221455sINFO  [2023-01-06 20:44:29,411] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=1, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:29,411] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=1, nullLines=0), minParser=DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=16511, maxValue=16511), dateReader=com.bakdata.conquery.util.DateReader@51d91497), maxParser=DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=16800, maxValue=16800), dateReader=com.bakdata.conquery.util.DateReader@1c088ef6), dateReader=com.bakdata.conquery.util.DateReader@4a7d5552, onlyQuarters=false, maxValue=16800, minValue=16511, anyOpen=false)
INFO  [2023-01-06 20:44:29,411] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=1, nullLines=0), minValue=50, maxValue=50)
INFO  [2023-01-06 20:44:29,414] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:29,414] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:29,414] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:29,427] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_INTEGER$20Test[1].table
127.0.0.1 - - [06/Jan/2023:20:44:29 +0000] "POST /admin/datasets/NUMBER_INTEGER%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_INTEGER+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:29,430] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,430] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:29,430] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:29,430] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:29,432] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:29,433] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test[1].table.table], containing 1 entries.
INFO  [2023-01-06 20:44:29,433] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test[1].table.table], containing 1 entries.
WARN  [2023-01-06 20:44:29,434] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:29,434] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test[1].table.table.0
INFO  [2023-01-06 20:44:29,550] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_INTEGER Test QUERY INIT
INFO  [2023-01-06 20:44:29,568] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_INTEGER$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:29,568] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test[1]))]]
INFO  [2023-01-06 20:44:29,572] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test[1].ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818
INFO  [2023-01-06 20:44:29,573] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test[1].ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818
WARN  [2023-01-06 20:44:29,573] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:44:29,573] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test[1].ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818] with 0 results within PT0.000159S
INFO  [2023-01-06 20:44:29,573] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test[1].ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818, workerId=NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_0449425a-6f8a-42d9-8a51-e466ce0073af, startTime=2023-01-06T20:44:29.573047, finishTime=2023-01-06T20:44:29.573206) of size 0
INFO  [2023-01-06 20:44:29,573] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test[1].ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818] with 1 results within PT0.000767S
INFO  [2023-01-06 20:44:29,574] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test[1].ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818, workerId=NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_dcfecb0d-a5e7-4100-94db-829ccf1582ed, startTime=2023-01-06T20:44:29.573027, finishTime=2023-01-06T20:44:29.573794) of size 1
INFO  [2023-01-06 20:44:29,574] com.bakdata.conquery.models.execution.ManagedExecution: DONE ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818 ManagedQuery within PT0.005637S
127.0.0.1 - - [06/Jan/2023:20:44:29 +0000] "POST /api/datasets/NUMBER_INTEGER$20Test%5B1%5D/queries HTTP/1.1" 201 1216 "-" "Conquery (test client)" 9
127.0.0.1 - - [06/Jan/2023:20:44:29 +0000] "GET /api/datasets/NUMBER_INTEGER$20Test%5B1%5D/queries/NUMBER_INTEGER$20Test%5B1%5D.ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818 HTTP/1.1" 200 1751 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:29,604] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test[1]], queryId=ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818, label=concept	@§$, creationTime=2023-01-06T20:44:29.568718, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@793f28b[Count = 0], startTime=2023-01-06T20:44:29.568997, finishTime=2023-01-06T20:44:29.574634, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@688f81d6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1661e601, com.bakdata.conquery.models.query.ColumnDescriptor@1f3fc02b]) download on dataset Dataset[label=null, name=NUMBER_INTEGER Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:29,605] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test[1]], queryId=ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818, label=concept	@§$, creationTime=2023-01-06T20:44:29.568718, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@793f28b[Count = 0], startTime=2023-01-06T20:44:29.568997, finishTime=2023-01-06T20:44:29.574634, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@688f81d6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1661e601, com.bakdata.conquery.models.query.ColumnDescriptor@1f3fc02b]) on dataset Dataset[label=null, name=NUMBER_INTEGER Test[1]]
127.0.0.1 - - [06/Jan/2023:20:44:29 +0000] "GET /api/datasets/NUMBER_INTEGER%20Test%5B1%5D/result/NUMBER_INTEGER$20Test%5B1%5D.ccd2d4d3-83d4-4fe6-8722-a06c9d8e3818.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:44:29,623] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_INTEGER Test on 2 rows
INFO  [2023-01-06 20:44:29,623] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_INTEGER Test[1]
INFO  [2023-01-06 20:44:29,624] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-06 20:44:29,624] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-06 20:44:29,624] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test[1]_0449425a-6f8a-42d9-8a51-e466ce0073af
INFO  [2023-01-06 20:44:29,624] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test[1]_dcfecb0d-a5e7-4100-94db-829ccf1582ed
INFO  [2023-01-06 20:44:29,653] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_INTEGER Test[1]
INFO  [2023-01-06 20:44:29,654] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test[1]_dcfecb0d-a5e7-4100-94db-829ccf1582ed
INFO  [2023-01-06 20:44:29,654] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test[1]_0449425a-6f8a-42d9-8a51-e466ce0073af
INFO  [2023-01-06 20:44:29,734] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_INTEGER$20Test[1]
INFO  [2023-01-06 20:44:29,734] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,839] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_INTEGER Test
INFO  [2023-01-06 20:44:29,840] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MONEY Test
INFO  [2023-01-06 20:44:29,840] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:29,840] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:29,841] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-06 20:44:29,841] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:29,841] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-06 20:44:29,841] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:29,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_090b3e50-c711-470c-9890-49513ebb3458 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:29,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_090b3e50-c711-470c-9890-49513ebb3458 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:29,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:29,844] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_6bd7e32b-5e31-4f4f-8fbf-04ea2a932447 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:29,844] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_6bd7e32b-5e31-4f4f-8fbf-04ea2a932447 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:29,844] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:29,847] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,950] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:29,950] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MONEY$20Test.table
INFO  [2023-01-06 20:44:29,950] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MONEY$20Test.table
INFO  [2023-01-06 20:44:30,063] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:30,175] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:30,175] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:30,176] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 355 B in total
INFO  [2023-01-06 20:44:30,176] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MONEY Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000287259sINFO  [2023-01-06 20:44:30,205] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=18, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:44:30,205] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=18, nullLines=0), subType=IntegerParser(super=Parser(lines=18, nullLines=0), minValue=14835, maxValue=17612), dateReader=com.bakdata.conquery.util.DateReader@377de01f)
INFO  [2023-01-06 20:44:30,205] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with MoneyParser(super=Parser(lines=18, nullLines=3), maxValue=30000, minValue=5000, moneyFactor=100)
INFO  [2023-01-06 20:44:30,215] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MONEY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:30,215] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:30,215] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MONEY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:30,237] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_MONEY$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:30 +0000] "POST /admin/datasets/NUMBER_MONEY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_MONEY+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:30,239] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:30,239] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:30,239] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:30,241] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:30,241] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MONEY$20Test.table.table], containing 18 entries.
INFO  [2023-01-06 20:44:30,242] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
WARN  [2023-01-06 20:44:30,242] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:30,242] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MONEY$20Test.table.table], containing 18 entries.
INFO  [2023-01-06 20:44:30,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.1
INFO  [2023-01-06 20:44:30,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.0
INFO  [2023-01-06 20:44:30,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.3
INFO  [2023-01-06 20:44:30,243] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.2
INFO  [2023-01-06 20:44:30,348] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MONEY Test QUERY INIT
INFO  [2023-01-06 20:44:30,362] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MONEY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:30,362] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[bbeb7f53-155c-47e7-b322-32433f37c961] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MONEY Test))]]
INFO  [2023-01-06 20:44:30,366] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961
INFO  [2023-01-06 20:44:30,366] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961
127.0.0.1 - - [06/Jan/2023:20:44:30 +0000] "POST /api/datasets/NUMBER_MONEY$20Test/queries HTTP/1.1" 201 1202 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:30,368] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961] with 2 results within PT0.001968S
INFO  [2023-01-06 20:44:30,368] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961] with 3 results within PT0.001977S
INFO  [2023-01-06 20:44:30,369] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961, workerId=NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_6bd7e32b-5e31-4f4f-8fbf-04ea2a932447, startTime=2023-01-06T20:44:30.366267, finishTime=2023-01-06T20:44:30.368235) of size 2
INFO  [2023-01-06 20:44:30,369] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961, workerId=NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_090b3e50-c711-470c-9890-49513ebb3458, startTime=2023-01-06T20:44:30.366513, finishTime=2023-01-06T20:44:30.368490) of size 3
INFO  [2023-01-06 20:44:30,369] com.bakdata.conquery.models.execution.ManagedExecution: DONE bbeb7f53-155c-47e7-b322-32433f37c961 ManagedQuery within PT0.006763S
127.0.0.1 - - [06/Jan/2023:20:44:30 +0000] "GET /api/datasets/NUMBER_MONEY$20Test/queries/NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961 HTTP/1.1" 200 1469 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:30,412] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MONEY Test], queryId=bbeb7f53-155c-47e7-b322-32433f37c961, label=concept	@§$, creationTime=2023-01-06T20:44:30.362402, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6830804[Count = 0], startTime=2023-01-06T20:44:30.362676, finishTime=2023-01-06T20:44:30.369439, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6dfaa6f8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MONEY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4698dcb1, com.bakdata.conquery.models.query.ColumnDescriptor@1859803b]) download on dataset Dataset[label=null, name=NUMBER_MONEY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:30,412] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MONEY Test], queryId=bbeb7f53-155c-47e7-b322-32433f37c961, label=concept	@§$, creationTime=2023-01-06T20:44:30.362402, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6830804[Count = 0], startTime=2023-01-06T20:44:30.362676, finishTime=2023-01-06T20:44:30.369439, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6dfaa6f8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MONEY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4698dcb1, com.bakdata.conquery.models.query.ColumnDescriptor@1859803b]) on dataset Dataset[label=null, name=NUMBER_MONEY Test]
127.0.0.1 - - [06/Jan/2023:20:44:30 +0000] "GET /api/datasets/NUMBER_MONEY%20Test/result/NUMBER_MONEY$20Test.bbeb7f53-155c-47e7-b322-32433f37c961.csv?pretty=false HTTP/1.1" 200 144 "-" "Conquery (test client)" 26
INFO  [2023-01-06 20:44:30,436] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_MONEY Test on 6 rows
INFO  [2023-01-06 20:44:30,436] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MONEY Test
INFO  [2023-01-06 20:44:30,437] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-06 20:44:30,437] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-06 20:44:30,437] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MONEY Test_6bd7e32b-5e31-4f4f-8fbf-04ea2a932447
INFO  [2023-01-06 20:44:30,437] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MONEY Test_090b3e50-c711-470c-9890-49513ebb3458
INFO  [2023-01-06 20:44:30,441] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MONEY Test
INFO  [2023-01-06 20:44:30,442] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MONEY$20Test
INFO  [2023-01-06 20:44:30,442] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:30,442] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MONEY Test_090b3e50-c711-470c-9890-49513ebb3458
INFO  [2023-01-06 20:44:30,442] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MONEY Test_6bd7e32b-5e31-4f4f-8fbf-04ea2a932447
INFO  [2023-01-06 20:44:30,549] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MONEY Test
INFO  [2023-01-06 20:44:30,549] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_REAL Test
INFO  [2023-01-06 20:44:30,550] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:30,550] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:30,551] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-06 20:44:30,551] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-06 20:44:30,551] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:30,551] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_c2b9a208-b958-4fbc-9250-125765dcfda5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_c2b9a208-b958-4fbc-9250-125765dcfda5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_368d8ddf-a5db-4fbe-80cd-5544eac4095d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_368d8ddf-a5db-4fbe-80cd-5544eac4095d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:30,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:30,658] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:30,658] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test[1].table
INFO  [2023-01-06 20:44:30,658] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test[1].table
INFO  [2023-01-06 20:44:30,772] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:30,885] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:30,885] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:30,885] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 355 B in total
INFO  [2023-01-06 20:44:30,885] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_REAL Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000378638sINFO  [2023-01-06 20:44:30,924] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=18, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:44:30,924] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=18, nullLines=0), subType=IntegerParser(super=Parser(lines=18, nullLines=0), minValue=14835, maxValue=17612), dateReader=com.bakdata.conquery.util.DateReader@2ec531d4)
INFO  [2023-01-06 20:44:30,924] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with RealParser(super=Parser(lines=18, nullLines=3), requiredPrecision=4.9E-324, floatULP=3.0517578125E-5)
INFO  [2023-01-06 20:44:30,926] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_REAL Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:30,926] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:30,926] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_REAL Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:30,941] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_REAL$20Test[1].table
127.0.0.1 - - [06/Jan/2023:20:44:30 +0000] "POST /admin/datasets/NUMBER_REAL%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_REAL+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:30,943] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:30,944] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:30,944] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:30,944] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:30,946] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:30,946] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test[1].table.table], containing 18 entries.
INFO  [2023-01-06 20:44:30,946] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test[1].table.table], containing 18 entries.
INFO  [2023-01-06 20:44:30,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.0
INFO  [2023-01-06 20:44:30,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.1
WARN  [2023-01-06 20:44:30,948] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:30,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.2
INFO  [2023-01-06 20:44:30,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.3
INFO  [2023-01-06 20:44:31,053] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_REAL Test QUERY INIT
INFO  [2023-01-06 20:44:31,072] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_REAL$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:31,072] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a59efc3d-52e7-46f4-8653-14bc740d43ad] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test[1]))]]
INFO  [2023-01-06 20:44:31,077] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test[1].a59efc3d-52e7-46f4-8653-14bc740d43ad
INFO  [2023-01-06 20:44:31,077] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test[1].a59efc3d-52e7-46f4-8653-14bc740d43ad
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "POST /api/datasets/NUMBER_REAL$20Test%5B1%5D/queries HTTP/1.1" 201 1203 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:31,079] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test[1].a59efc3d-52e7-46f4-8653-14bc740d43ad] with 3 results within PT0.002396S
INFO  [2023-01-06 20:44:31,079] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test[1].a59efc3d-52e7-46f4-8653-14bc740d43ad] with 2 results within PT0.002579S
INFO  [2023-01-06 20:44:31,080] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test[1].a59efc3d-52e7-46f4-8653-14bc740d43ad, workerId=NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_c2b9a208-b958-4fbc-9250-125765dcfda5, startTime=2023-01-06T20:44:31.077053, finishTime=2023-01-06T20:44:31.079449) of size 3
INFO  [2023-01-06 20:44:31,080] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test[1].a59efc3d-52e7-46f4-8653-14bc740d43ad, workerId=NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_368d8ddf-a5db-4fbe-80cd-5544eac4095d, startTime=2023-01-06T20:44:31.077046, finishTime=2023-01-06T20:44:31.079625) of size 2
INFO  [2023-01-06 20:44:31,080] com.bakdata.conquery.models.execution.ManagedExecution: DONE a59efc3d-52e7-46f4-8653-14bc740d43ad ManagedQuery within PT0.008096S
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "GET /api/datasets/NUMBER_REAL$20Test%5B1%5D/queries/NUMBER_REAL$20Test%5B1%5D.a59efc3d-52e7-46f4-8653-14bc740d43ad HTTP/1.1" 200 1714 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:31,114] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test[1]], queryId=a59efc3d-52e7-46f4-8653-14bc740d43ad, label=concept	@§$, creationTime=2023-01-06T20:44:31.072323, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3a96f038[Count = 0], startTime=2023-01-06T20:44:31.072565, finishTime=2023-01-06T20:44:31.080661, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@400315ad), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2f8460b4, com.bakdata.conquery.models.query.ColumnDescriptor@4c498e7c]) download on dataset Dataset[label=null, name=NUMBER_REAL Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:31,114] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test[1]], queryId=a59efc3d-52e7-46f4-8653-14bc740d43ad, label=concept	@§$, creationTime=2023-01-06T20:44:31.072323, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3a96f038[Count = 0], startTime=2023-01-06T20:44:31.072565, finishTime=2023-01-06T20:44:31.080661, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@400315ad), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2f8460b4, com.bakdata.conquery.models.query.ColumnDescriptor@4c498e7c]) on dataset Dataset[label=null, name=NUMBER_REAL Test[1]]
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "GET /api/datasets/NUMBER_REAL%20Test%5B1%5D/result/NUMBER_REAL$20Test%5B1%5D.a59efc3d-52e7-46f4-8653-14bc740d43ad.csv?pretty=false HTTP/1.1" 200 144 "-" "Conquery (test client)" 25
INFO  [2023-01-06 20:44:31,141] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_REAL Test on 6 rows
INFO  [2023-01-06 20:44:31,141] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_REAL Test[1]
INFO  [2023-01-06 20:44:31,141] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-06 20:44:31,141] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-06 20:44:31,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test[1]_c2b9a208-b958-4fbc-9250-125765dcfda5
INFO  [2023-01-06 20:44:31,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test[1]_368d8ddf-a5db-4fbe-80cd-5544eac4095d
INFO  [2023-01-06 20:44:31,151] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_REAL Test[1]
INFO  [2023-01-06 20:44:31,151] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test[1]_c2b9a208-b958-4fbc-9250-125765dcfda5
INFO  [2023-01-06 20:44:31,152] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test[1]_368d8ddf-a5db-4fbe-80cd-5544eac4095d
INFO  [2023-01-06 20:44:31,248] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_REAL$20Test[1]
INFO  [2023-01-06 20:44:31,248] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:31,355] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_REAL Test
INFO  [2023-01-06 20:44:31,356] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING FILTER Test
INFO  [2023-01-06 20:44:31,356] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:31,356] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:31,357] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-06 20:44:31,357] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-06 20:44:31,357] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:31,357] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:31,359] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_6b19f41f-527a-498c-9874-266aabb6e4f1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:31,359] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_6b19f41f-527a-498c-9874-266aabb6e4f1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:31,359] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:31,359] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_1d6e8000-93d0-42a5-852a-0d381a34aa21 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:31,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_1d6e8000-93d0-42a5-852a-0d381a34aa21 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:31,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:31,364] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:31,467] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:31,468] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING$20FILTER$20Test.table
INFO  [2023-01-06 20:44:31,468] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING$20FILTER$20Test.table
INFO  [2023-01-06 20:44:31,584] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:31,696] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:31,696] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:31,696] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 67 B in total
INFO  [2023-01-06 20:44:31,696] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING FILTER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000346715sINFO  [2023-01-06 20:44:31,731] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:31,732] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=2, nullLines=0), minParser=DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=16511, maxValue=17292), dateReader=com.bakdata.conquery.util.DateReader@5d2af260), maxParser=DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=16525, maxValue=17301), dateReader=com.bakdata.conquery.util.DateReader@762f9489), dateReader=com.bakdata.conquery.util.DateReader@594d61f4, onlyQuarters=false, maxValue=17301, minValue=16511, anyOpen=false)
INFO  [2023-01-06 20:44:31,732] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with RealParser(super=Parser(lines=2, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-06 20:44:31,734] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING FILTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:31,734] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:31,734] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING FILTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:31,754] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_MISSING$20FILTER$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "POST /admin/datasets/NUMBER_MISSING%20FILTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_MISSING+FILTER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:44:31,756] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:31,757] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:31,757] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:31,757] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:31,759] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:31,759] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING$20FILTER$20Test.table.table], containing 2 entries.
INFO  [2023-01-06 20:44:31,759] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING$20FILTER$20Test.table.table], containing 2 entries.
WARN  [2023-01-06 20:44:31,761] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:31,761] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING$20FILTER$20Test.table.table.0
INFO  [2023-01-06 20:44:31,866] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING FILTER Test QUERY INIT
INFO  [2023-01-06 20:44:31,883] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING$20FILTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:31,884] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4696ba65-1d72-47ec-9340-aff8ad728902] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING FILTER Test))]]
INFO  [2023-01-06 20:44:31,897] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902
INFO  [2023-01-06 20:44:31,897] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902
WARN  [2023-01-06 20:44:31,897] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:44:31,897] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902] with 0 results within PT0.000178S
INFO  [2023-01-06 20:44:31,898] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902, workerId=NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_1d6e8000-93d0-42a5-852a-0d381a34aa21, startTime=2023-01-06T20:44:31.897249, finishTime=2023-01-06T20:44:31.897427) of size 0
INFO  [2023-01-06 20:44:31,898] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902] with 1 results within PT0.000972S
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "POST /api/datasets/NUMBER_MISSING$20FILTER$20Test/queries HTTP/1.1" 201 1237 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:44:31,898] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902, workerId=NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_6b19f41f-527a-498c-9874-266aabb6e4f1, startTime=2023-01-06T20:44:31.897261, finishTime=2023-01-06T20:44:31.898233) of size 1
INFO  [2023-01-06 20:44:31,899] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4696ba65-1d72-47ec-9340-aff8ad728902 ManagedQuery within PT0.014704S
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "GET /api/datasets/NUMBER_MISSING$20FILTER$20Test/queries/NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902 HTTP/1.1" 200 1549 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:31,926] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING FILTER Test], queryId=4696ba65-1d72-47ec-9340-aff8ad728902, label=concept	@§$, creationTime=2023-01-06T20:44:31.884117, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2799f035[Count = 0], startTime=2023-01-06T20:44:31.884301, finishTime=2023-01-06T20:44:31.899005, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@9835c0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING FILTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2a824553, com.bakdata.conquery.models.query.ColumnDescriptor@4cdca106]) download on dataset Dataset[label=null, name=NUMBER_MISSING FILTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:31,926] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING FILTER Test], queryId=4696ba65-1d72-47ec-9340-aff8ad728902, label=concept	@§$, creationTime=2023-01-06T20:44:31.884117, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2799f035[Count = 0], startTime=2023-01-06T20:44:31.884301, finishTime=2023-01-06T20:44:31.899005, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@9835c0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING FILTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2a824553, com.bakdata.conquery.models.query.ColumnDescriptor@4cdca106]) on dataset Dataset[label=null, name=NUMBER_MISSING FILTER Test]
127.0.0.1 - - [06/Jan/2023:20:44:31 +0000] "GET /api/datasets/NUMBER_MISSING%20FILTER%20Test/result/NUMBER_MISSING$20FILTER$20Test.4696ba65-1d72-47ec-9340-aff8ad728902.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:31,945] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_MISSING FILTER Test on 2 rows
INFO  [2023-01-06 20:44:31,945] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING FILTER Test
INFO  [2023-01-06 20:44:31,946] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-06 20:44:31,946] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING FILTER Test_1d6e8000-93d0-42a5-852a-0d381a34aa21
INFO  [2023-01-06 20:44:31,946] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-06 20:44:31,946] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING FILTER Test_6b19f41f-527a-498c-9874-266aabb6e4f1
INFO  [2023-01-06 20:44:31,957] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING FILTER Test
INFO  [2023-01-06 20:44:31,959] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING FILTER Test_6b19f41f-527a-498c-9874-266aabb6e4f1
INFO  [2023-01-06 20:44:31,959] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING FILTER Test_1d6e8000-93d0-42a5-852a-0d381a34aa21
INFO  [2023-01-06 20:44:31,961] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING$20FILTER$20Test
INFO  [2023-01-06 20:44:31,961] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:32,161] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING FILTER Test
INFO  [2023-01-06 20:44:32,162] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Prefix Test
INFO  [2023-01-06 20:44:32,162] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:32,162] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:32,163] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-06 20:44:32,163] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-06 20:44:32,163] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:32,163] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:32,165] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Prefix$20Test.worker_Prefix$20Test_45520f16-9f9a-4f32-9ef6-89e5099f271a are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:32,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Prefix$20Test.worker_Prefix$20Test_45520f16-9f9a-4f32-9ef6-89e5099f271a are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:32,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:32,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Prefix$20Test.worker_Prefix$20Test_23e13c24-bf20-4e76-af4b-680d53800cea are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:32,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Prefix$20Test.worker_Prefix$20Test_23e13c24-bf20-4e76-af4b-680d53800cea are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:32,166] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:32,170] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:32,272] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:32,273] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Prefix$20Test.table
INFO  [2023-01-06 20:44:32,273] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Prefix$20Test.table
INFO  [2023-01-06 20:44:32,399] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:32,517] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:32,518] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:32,518] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 109 B in total
INFO  [2023-01-06 20:44:32,518] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Prefix Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00038483sINFO  [2023-01-06 20:44:32,557] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=6, min=1, average=1.200000, max=2}
INFO  [2023-01-06 20:44:32,557] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@19123cb)
INFO  [2023-01-06 20:44:32,557] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:32,560] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Prefix Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:32,560] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:32,560] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Prefix Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:32,584] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Prefix$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:32 +0000] "POST /admin/datasets/Prefix%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Prefix+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:44:32,586] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:32,587] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:32,588] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:32,588] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:32,591] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:32,591] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Prefix$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:32,592] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Prefix$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:32,593] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:32,593] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Prefix$20Test.table.table.0
INFO  [2023-01-06 20:44:32,594] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Prefix$20Test.table.table.1
INFO  [2023-01-06 20:44:32,699] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Prefix Test QUERY INIT
INFO  [2023-01-06 20:44:32,712] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Prefix$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:32,712] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[aa32d7c7-dffa-4bcc-829f-dedaefd58d54] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Prefix Test))]]
127.0.0.1 - - [06/Jan/2023:20:44:32 +0000] "POST /api/datasets/Prefix$20Test/queries HTTP/1.1" 201 1149 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:44:32,726] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54
INFO  [2023-01-06 20:44:32,726] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54
INFO  [2023-01-06 20:44:32,727] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54] with 1 results within PT0.001236S
INFO  [2023-01-06 20:44:32,727] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54] with 2 results within PT0.001452S
INFO  [2023-01-06 20:44:32,728] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54, workerId=Prefix$20Test.worker_Prefix$20Test_45520f16-9f9a-4f32-9ef6-89e5099f271a, startTime=2023-01-06T20:44:32.726218, finishTime=2023-01-06T20:44:32.727454) of size 1
INFO  [2023-01-06 20:44:32,728] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54, workerId=Prefix$20Test.worker_Prefix$20Test_23e13c24-bf20-4e76-af4b-680d53800cea, startTime=2023-01-06T20:44:32.726213, finishTime=2023-01-06T20:44:32.727665) of size 2
INFO  [2023-01-06 20:44:32,728] com.bakdata.conquery.models.execution.ManagedExecution: DONE aa32d7c7-dffa-4bcc-829f-dedaefd58d54 ManagedQuery within PT0.015986S
127.0.0.1 - - [06/Jan/2023:20:44:32 +0000] "GET /api/datasets/Prefix$20Test/queries/Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54 HTTP/1.1" 200 1393 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:32,749] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Prefix Test], queryId=aa32d7c7-dffa-4bcc-829f-dedaefd58d54, label=concept	@§$, creationTime=2023-01-06T20:44:32.712598, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@161623e0[Count = 0], startTime=2023-01-06T20:44:32.712965, finishTime=2023-01-06T20:44:32.728951, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5755c53c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Prefix Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6330d801, com.bakdata.conquery.models.query.ColumnDescriptor@bba2f43]) download on dataset Dataset[label=null, name=Prefix Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:32,749] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Prefix Test], queryId=aa32d7c7-dffa-4bcc-829f-dedaefd58d54, label=concept	@§$, creationTime=2023-01-06T20:44:32.712598, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@161623e0[Count = 0], startTime=2023-01-06T20:44:32.712965, finishTime=2023-01-06T20:44:32.728951, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5755c53c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Prefix Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6330d801, com.bakdata.conquery.models.query.ColumnDescriptor@bba2f43]) on dataset Dataset[label=null, name=Prefix Test]
127.0.0.1 - - [06/Jan/2023:20:44:32 +0000] "GET /api/datasets/Prefix%20Test/result/Prefix$20Test.aa32d7c7-dffa-4bcc-829f-dedaefd58d54.csv?pretty=false HTTP/1.1" 200 91 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:44:32,767] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest Prefix Test on 4 rows
INFO  [2023-01-06 20:44:32,767] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Prefix Test
INFO  [2023-01-06 20:44:32,767] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-06 20:44:32,767] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-06 20:44:32,768] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Prefix Test_45520f16-9f9a-4f32-9ef6-89e5099f271a
INFO  [2023-01-06 20:44:32,768] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Prefix Test_23e13c24-bf20-4e76-af4b-680d53800cea
INFO  [2023-01-06 20:44:32,865] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Prefix Test
INFO  [2023-01-06 20:44:32,865] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Prefix Test_23e13c24-bf20-4e76-af4b-680d53800cea
INFO  [2023-01-06 20:44:32,865] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Prefix Test_45520f16-9f9a-4f32-9ef6-89e5099f271a
INFO  [2023-01-06 20:44:32,894] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Prefix$20Test
INFO  [2023-01-06 20:44:32,894] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:32,999] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Prefix Test
INFO  [2023-01-06 20:44:33,000] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test QUARTERS_IN_YEAR Test
INFO  [2023-01-06 20:44:33,000] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:33,000] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:33,001] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-06 20:44:33,001] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-06 20:44:33,001] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:33,001] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:33,003] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_d4992eb8-ee28-4d8f-b358-f23f7749dba0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:33,003] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_d4992eb8-ee28-4d8f-b358-f23f7749dba0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:33,003] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:33,003] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_7064e30a-114d-4ce1-9555-8a674a297075 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:33,003] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_7064e30a-114d-4ce1-9555-8a674a297075 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:33,003] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:33,007] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,110] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,111] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTERS_IN_YEAR$20Test.table
INFO  [2023-01-06 20:44:33,111] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTERS_IN_YEAR$20Test.table
INFO  [2023-01-06 20:44:33,230] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,344] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:33,344] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:33,344] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.4 KiB in total
INFO  [2023-01-06 20:44:33,344] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_QUARTERS_IN_YEAR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000402437sINFO  [2023-01-06 20:44:33,386] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=38, min=1, average=3.166667, max=6}
INFO  [2023-01-06 20:44:33,386] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=38, nullLines=0), minParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=16801), dateReader=com.bakdata.conquery.util.DateReader@7b68dc97), maxParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16800, maxValue=17166), dateReader=com.bakdata.conquery.util.DateReader@2b432637), dateReader=com.bakdata.conquery.util.DateReader@20d29cfa, onlyQuarters=false, maxValue=17166, minValue=16436, anyOpen=false)
INFO  [2023-01-06 20:44:33,386] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=17117), dateReader=com.bakdata.conquery.util.DateReader@74445994)
INFO  [2023-01-06 20:44:33,389] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_QUARTERS_IN_YEAR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:33,389] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:33,389] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_QUARTERS_IN_YEAR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:33,408] com.bakdata.conquery.models.jobs.ImportJob: Importing table into QUARTERS_IN_YEAR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:33 +0000] "POST /admin/datasets/QUARTERS_IN_YEAR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_QUARTERS_IN_YEAR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:33,414] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,414] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:33,414] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:33,414] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:33,417] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:44:33,417] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTERS_IN_YEAR$20Test.table.table], containing 38 entries.
INFO  [2023-01-06 20:44:33,417] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTERS_IN_YEAR$20Test.table.table], containing 38 entries.
INFO  [2023-01-06 20:44:33,419] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.0
INFO  [2023-01-06 20:44:33,419] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.1
WARN  [2023-01-06 20:44:33,419] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:33,419] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.2
INFO  [2023-01-06 20:44:33,419] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.3
INFO  [2023-01-06 20:44:33,532] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: QUARTERS_IN_YEAR Test QUERY INIT
INFO  [2023-01-06 20:44:33,547] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[QUARTERS_IN_YEAR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:33,548] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[73722986-26bf-4b16-b3fc-a724f0314ccd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_QUARTERS_IN_YEAR Test))]]
INFO  [2023-01-06 20:44:33,551] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd
INFO  [2023-01-06 20:44:33,551] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd
127.0.0.1 - - [06/Jan/2023:20:44:33 +0000] "POST /api/datasets/QUARTERS_IN_YEAR$20Test/queries HTTP/1.1" 201 1210 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:33,554] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd] with 4 results within PT0.003076S
INFO  [2023-01-06 20:44:33,554] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd] with 3 results within PT0.003287S
INFO  [2023-01-06 20:44:33,555] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd, workerId=QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_7064e30a-114d-4ce1-9555-8a674a297075, startTime=2023-01-06T20:44:33.551425, finishTime=2023-01-06T20:44:33.554501) of size 4
INFO  [2023-01-06 20:44:33,555] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd, workerId=QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_d4992eb8-ee28-4d8f-b358-f23f7749dba0, startTime=2023-01-06T20:44:33.551224, finishTime=2023-01-06T20:44:33.554511) of size 3
INFO  [2023-01-06 20:44:33,555] com.bakdata.conquery.models.execution.ManagedExecution: DONE 73722986-26bf-4b16-b3fc-a724f0314ccd ManagedQuery within PT0.007149S
127.0.0.1 - - [06/Jan/2023:20:44:33 +0000] "GET /api/datasets/QUARTERS_IN_YEAR$20Test/queries/QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd HTTP/1.1" 200 1493 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:33,581] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTERS_IN_YEAR Test], queryId=73722986-26bf-4b16-b3fc-a724f0314ccd, label=concept	@§$, creationTime=2023-01-06T20:44:33.548262, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@777c2f4f[Count = 0], startTime=2023-01-06T20:44:33.548484, finishTime=2023-01-06T20:44:33.555633, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@42f46bd6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTERS_IN_YEAR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7756a8f, com.bakdata.conquery.models.query.ColumnDescriptor@3919a645]) download on dataset Dataset[label=null, name=QUARTERS_IN_YEAR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:33,582] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTERS_IN_YEAR Test], queryId=73722986-26bf-4b16-b3fc-a724f0314ccd, label=concept	@§$, creationTime=2023-01-06T20:44:33.548262, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@777c2f4f[Count = 0], startTime=2023-01-06T20:44:33.548484, finishTime=2023-01-06T20:44:33.555633, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@42f46bd6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTERS_IN_YEAR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7756a8f, com.bakdata.conquery.models.query.ColumnDescriptor@3919a645]) on dataset Dataset[label=null, name=QUARTERS_IN_YEAR Test]
127.0.0.1 - - [06/Jan/2023:20:44:33 +0000] "GET /api/datasets/QUARTERS_IN_YEAR%20Test/result/QUARTERS_IN_YEAR$20Test.73722986-26bf-4b16-b3fc-a724f0314ccd.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:44:33,602] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest QUARTERS_IN_YEAR Test on 8 rows
INFO  [2023-01-06 20:44:33,602] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast QUARTERS_IN_YEAR Test
INFO  [2023-01-06 20:44:33,603] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-06 20:44:33,603] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-06 20:44:33,603] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTERS_IN_YEAR Test_7064e30a-114d-4ce1-9555-8a674a297075
INFO  [2023-01-06 20:44:33,603] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTERS_IN_YEAR Test_d4992eb8-ee28-4d8f-b358-f23f7749dba0
INFO  [2023-01-06 20:44:33,701] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow QUARTERS_IN_YEAR Test
INFO  [2023-01-06 20:44:33,702] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTERS_IN_YEAR Test_d4992eb8-ee28-4d8f-b358-f23f7749dba0
INFO  [2023-01-06 20:44:33,702] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTERS_IN_YEAR Test_7064e30a-114d-4ce1-9555-8a674a297075
INFO  [2023-01-06 20:44:33,719] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of QUARTERS_IN_YEAR$20Test
INFO  [2023-01-06 20:44:33,719] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,834] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test QUARTERS_IN_YEAR Test
INFO  [2023-01-06 20:44:33,835] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SELECT Test
INFO  [2023-01-06 20:44:33,835] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:33,835] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:33,836] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-06 20:44:33,836] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-06 20:44:33,837] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:33,837] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:33,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT$20Test.worker_SELECT$20Test_62e03c50-95af-48eb-8b28-f74d45a0b7e7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:33,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT$20Test.worker_SELECT$20Test_62e03c50-95af-48eb-8b28-f74d45a0b7e7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:33,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:33,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT$20Test.worker_SELECT$20Test_f4037685-fea2-418a-b4e8-47c327feaf4c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:33,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT$20Test.worker_SELECT$20Test_f4037685-fea2-418a-b4e8-47c327feaf4c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:33,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:33,843] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,946] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:33,947] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT$20Test.table
INFO  [2023-01-06 20:44:33,947] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT$20Test.table
INFO  [2023-01-06 20:44:34,066] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:34,187] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:34,188] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:34,188] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 76 B in total
INFO  [2023-01-06 20:44:34,188] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000400105sINFO  [2023-01-06 20:44:34,228] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:34,228] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@53c8ffbe)
INFO  [2023-01-06 20:44:34,229] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:34,231] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:34,231] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:34,231] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:34,252] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SELECT$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:34 +0000] "POST /admin/datasets/SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:44:34,253] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:34,254] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:34,255] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:34,255] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:34,258] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:34,258] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-06 20:44:34,258] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-06 20:44:34,260] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT$20Test.table.table.0
WARN  [2023-01-06 20:44:34,260] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:34,260] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT$20Test.table.table.1
INFO  [2023-01-06 20:44:34,366] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SELECT Test QUERY INIT
INFO  [2023-01-06 20:44:34,377] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SELECT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:34,378] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e4d1778a-a9a0-46af-83d2-9300c1c2e02b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SELECT Test))]]
INFO  [2023-01-06 20:44:34,382] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b
INFO  [2023-01-06 20:44:34,383] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b
INFO  [2023-01-06 20:44:34,383] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b] with 2 results within PT0.001106S
INFO  [2023-01-06 20:44:34,383] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b] with 0 results within PT0.000693S
INFO  [2023-01-06 20:44:34,384] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b, workerId=SELECT$20Test.worker_SELECT$20Test_f4037685-fea2-418a-b4e8-47c327feaf4c, startTime=2023-01-06T20:44:34.382149, finishTime=2023-01-06T20:44:34.383255) of size 2
127.0.0.1 - - [06/Jan/2023:20:44:34 +0000] "POST /api/datasets/SELECT$20Test/queries HTTP/1.1" 201 1149 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:34,384] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b, workerId=SELECT$20Test.worker_SELECT$20Test_62e03c50-95af-48eb-8b28-f74d45a0b7e7, startTime=2023-01-06T20:44:34.383183, finishTime=2023-01-06T20:44:34.383876) of size 0
INFO  [2023-01-06 20:44:34,384] com.bakdata.conquery.models.execution.ManagedExecution: DONE e4d1778a-a9a0-46af-83d2-9300c1c2e02b ManagedQuery within PT0.006354S
127.0.0.1 - - [06/Jan/2023:20:44:34 +0000] "GET /api/datasets/SELECT$20Test/queries/SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b HTTP/1.1" 200 1392 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:34,417] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT Test], queryId=e4d1778a-a9a0-46af-83d2-9300c1c2e02b, label=concept	@§$, creationTime=2023-01-06T20:44:34.377998, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@76e18ac9[Count = 0], startTime=2023-01-06T20:44:34.378231, finishTime=2023-01-06T20:44:34.384585, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@34fb8f07), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@41f80f2a, com.bakdata.conquery.models.query.ColumnDescriptor@647d94b0]) download on dataset Dataset[label=null, name=SELECT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:34,417] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT Test], queryId=e4d1778a-a9a0-46af-83d2-9300c1c2e02b, label=concept	@§$, creationTime=2023-01-06T20:44:34.377998, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@76e18ac9[Count = 0], startTime=2023-01-06T20:44:34.378231, finishTime=2023-01-06T20:44:34.384585, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@34fb8f07), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@41f80f2a, com.bakdata.conquery.models.query.ColumnDescriptor@647d94b0]) on dataset Dataset[label=null, name=SELECT Test]
127.0.0.1 - - [06/Jan/2023:20:44:34 +0000] "GET /api/datasets/SELECT%20Test/result/SELECT$20Test.e4d1778a-a9a0-46af-83d2-9300c1c2e02b.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:44:34,438] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SELECT Test on 3 rows
INFO  [2023-01-06 20:44:34,439] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SELECT Test
INFO  [2023-01-06 20:44:34,439] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-06 20:44:34,439] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-06 20:44:34,439] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT Test_f4037685-fea2-418a-b4e8-47c327feaf4c
INFO  [2023-01-06 20:44:34,440] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT Test_62e03c50-95af-48eb-8b28-f74d45a0b7e7
INFO  [2023-01-06 20:44:34,537] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SELECT Test
INFO  [2023-01-06 20:44:34,538] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT Test_62e03c50-95af-48eb-8b28-f74d45a0b7e7
INFO  [2023-01-06 20:44:34,538] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT Test_f4037685-fea2-418a-b4e8-47c327feaf4c
INFO  [2023-01-06 20:44:34,560] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SELECT$20Test
INFO  [2023-01-06 20:44:34,561] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:34,666] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SELECT Test
INFO  [2023-01-06 20:44:34,666] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:34,666] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:34,667] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:34,668] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:34,668] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:34,668] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:34,668] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:34,670] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_8a9dfb28-6599-4e59-bf93-debee8324f20 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:34,670] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_8a9dfb28-6599-4e59-bf93-debee8324f20 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:34,670] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:34,670] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_b99336bc-b384-4eb5-9a1f-c565bbc04bf8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:34,670] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_b99336bc-b384-4eb5-9a1f-c565bbc04bf8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:34,670] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:34,674] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:34,784] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:34,784] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-06 20:44:34,784] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-06 20:44:34,895] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,005] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:35,005] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:35,005] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 90 B in total
INFO  [2023-01-06 20:44:35,005] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000428894sINFO  [2023-01-06 20:44:35,049] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:35,049] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@177690de)
INFO  [2023-01-06 20:44:35,049] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:35,052] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:35,052] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:35,052] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:35,075] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_EMPTY_VALUES$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "POST /admin/datasets/SINGLE_SELECT_EMPTY_VALUES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SINGLE_SELECT_EMPTY_VALUES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:35,077] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,077] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:35,078] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:35,078] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:35,080] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:35,080] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:35,080] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:35,082] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_EMPTY_VALUES$20Test.table.table.0
WARN  [2023-01-06 20:44:35,082] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:35,082] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_EMPTY_VALUES$20Test.table.table.1
INFO  [2023-01-06 20:44:35,187] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_EMPTY_VALUES Test QUERY INIT
INFO  [2023-01-06 20:44:35,204] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_EMPTY_VALUES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:35,204] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e47597d2-406c-4768-a071-404e3df02004] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_EMPTY_VALUES Test))]]
INFO  [2023-01-06 20:44:35,207] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004
INFO  [2023-01-06 20:44:35,207] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004
INFO  [2023-01-06 20:44:35,207] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004] with 0 results within PT0.00051S
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "POST /api/datasets/SINGLE_SELECT_EMPTY_VALUES$20Test/queries HTTP/1.1" 201 1229 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:44:35,208] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004] with 2 results within PT0.000962S
INFO  [2023-01-06 20:44:35,208] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004, workerId=SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_b99336bc-b384-4eb5-9a1f-c565bbc04bf8, startTime=2023-01-06T20:44:35.207308, finishTime=2023-01-06T20:44:35.207818) of size 0
INFO  [2023-01-06 20:44:35,209] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004, workerId=SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_8a9dfb28-6599-4e59-bf93-debee8324f20, startTime=2023-01-06T20:44:35.207311, finishTime=2023-01-06T20:44:35.208273) of size 2
INFO  [2023-01-06 20:44:35,209] com.bakdata.conquery.models.execution.ManagedExecution: DONE e47597d2-406c-4768-a071-404e3df02004 ManagedQuery within PT0.004177S
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "GET /api/datasets/SINGLE_SELECT_EMPTY_VALUES$20Test/queries/SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004 HTTP/1.1" 200 1552 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:35,236] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test], queryId=e47597d2-406c-4768-a071-404e3df02004, label=concept	@§$, creationTime=2023-01-06T20:44:35.204602, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6b22c49b[Count = 0], startTime=2023-01-06T20:44:35.204919, finishTime=2023-01-06T20:44:35.209096, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6f718814), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@528e79ba, com.bakdata.conquery.models.query.ColumnDescriptor@33894e47]) download on dataset Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:35,236] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test], queryId=e47597d2-406c-4768-a071-404e3df02004, label=concept	@§$, creationTime=2023-01-06T20:44:35.204602, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6b22c49b[Count = 0], startTime=2023-01-06T20:44:35.204919, finishTime=2023-01-06T20:44:35.209096, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6f718814), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@528e79ba, com.bakdata.conquery.models.query.ColumnDescriptor@33894e47]) on dataset Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test]
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "GET /api/datasets/SINGLE_SELECT_EMPTY_VALUES%20Test/result/SINGLE_SELECT_EMPTY_VALUES$20Test.e47597d2-406c-4768-a071-404e3df02004.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:44:35,256] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_EMPTY_VALUES Test on 3 rows
INFO  [2023-01-06 20:44:35,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:35,257] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:35,257] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-06 20:44:35,257] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_EMPTY_VALUES Test_b99336bc-b384-4eb5-9a1f-c565bbc04bf8
INFO  [2023-01-06 20:44:35,257] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_EMPTY_VALUES Test_8a9dfb28-6599-4e59-bf93-debee8324f20
INFO  [2023-01-06 20:44:35,276] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:35,277] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_EMPTY_VALUES Test_b99336bc-b384-4eb5-9a1f-c565bbc04bf8
INFO  [2023-01-06 20:44:35,277] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_EMPTY_VALUES Test_8a9dfb28-6599-4e59-bf93-debee8324f20
INFO  [2023-01-06 20:44:35,282] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_EMPTY_VALUES$20Test
INFO  [2023-01-06 20:44:35,282] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,387] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-06 20:44:35,387] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:35,388] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:35,388] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:35,388] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:35,388] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:35,389] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:35,389] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:35,390] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_cf684a1b-573b-418e-952b-0756a07f9922 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:35,390] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_cf684a1b-573b-418e-952b-0756a07f9922 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:35,390] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:35,390] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_20237f25-1259-45a6-ba1b-ea44f42dd602 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:35,390] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_20237f25-1259-45a6-ba1b-ea44f42dd602 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:35,390] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:35,394] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,498] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,498] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-06 20:44:35,498] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-06 20:44:35,622] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,737] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:35,737] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:35,737] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 92 B in total
INFO  [2023-01-06 20:44:35,738] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000416711sINFO  [2023-01-06 20:44:35,780] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:44:35,780] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3229ae9f)
INFO  [2023-01-06 20:44:35,780] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:35,783] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:35,783] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:35,783] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:35,801] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "POST /admin/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SINGLE_SELECT_SPEZIAL_CHARACTER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:44:35,802] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:35,802] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:35,803] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:35,803] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:35,804] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:35,805] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:44:35,805] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:44:35,805] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:35,806] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table.1
INFO  [2023-01-06 20:44:35,806] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table.0
INFO  [2023-01-06 20:44:35,911] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_SPEZIAL_CHARACTER Test QUERY INIT
INFO  [2023-01-06 20:44:35,923] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:35,924] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[243d51a4-f369-4433-ac1b-5cf0f806be4c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER Test))]]
INFO  [2023-01-06 20:44:35,926] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c
INFO  [2023-01-06 20:44:35,926] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c
INFO  [2023-01-06 20:44:35,927] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c] with 0 results within PT0.000836S
INFO  [2023-01-06 20:44:35,927] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c] with 1 results within PT0.001002S
INFO  [2023-01-06 20:44:35,928] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_20237f25-1259-45a6-ba1b-ea44f42dd602, startTime=2023-01-06T20:44:35.926989, finishTime=2023-01-06T20:44:35.927825) of size 0
INFO  [2023-01-06 20:44:35,928] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_cf684a1b-573b-418e-952b-0756a07f9922, startTime=2023-01-06T20:44:35.926971, finishTime=2023-01-06T20:44:35.927973) of size 1
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "POST /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test/queries HTTP/1.1" 201 1249 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:35,928] com.bakdata.conquery.models.execution.ManagedExecution: DONE 243d51a4-f369-4433-ac1b-5cf0f806be4c ManagedQuery within PT0.004106S
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test/queries/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c HTTP/1.1" 200 1591 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:44:35,947] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test], queryId=243d51a4-f369-4433-ac1b-5cf0f806be4c, label=concept	@§$, creationTime=2023-01-06T20:44:35.924229, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@15927fe0[Count = 0], startTime=2023-01-06T20:44:35.924434, finishTime=2023-01-06T20:44:35.928540, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@491af1f7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7bd54690, com.bakdata.conquery.models.query.ColumnDescriptor@2fe32a3c]) download on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:35,947] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test], queryId=243d51a4-f369-4433-ac1b-5cf0f806be4c, label=concept	@§$, creationTime=2023-01-06T20:44:35.924229, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@15927fe0[Count = 0], startTime=2023-01-06T20:44:35.924434, finishTime=2023-01-06T20:44:35.928540, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@491af1f7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7bd54690, com.bakdata.conquery.models.query.ColumnDescriptor@2fe32a3c]) on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
127.0.0.1 - - [06/Jan/2023:20:44:35 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER%20Test/result/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.243d51a4-f369-4433-ac1b-5cf0f806be4c.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:44:35,966] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_SPEZIAL_CHARACTER Test on 2 rows
INFO  [2023-01-06 20:44:35,966] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:35,967] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:35,967] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-06 20:44:35,967] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_20237f25-1259-45a6-ba1b-ea44f42dd602
INFO  [2023-01-06 20:44:35,967] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_cf684a1b-573b-418e-952b-0756a07f9922
INFO  [2023-01-06 20:44:35,989] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:35,989] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_cf684a1b-573b-418e-952b-0756a07f9922
INFO  [2023-01-06 20:44:35,990] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_20237f25-1259-45a6-ba1b-ea44f42dd602
INFO  [2023-01-06 20:44:36,006] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_SPEZIAL_CHARACTER$20Test
INFO  [2023-01-06 20:44:36,006] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:36,113] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-06 20:44:36,113] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:36,114] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:36,114] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:36,115] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:36,115] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:36,115] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:36,115] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:36,116] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:36,117] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_3ff5252a-65ff-4da6-84fb-ff4d2918367e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:36,117] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_3ff5252a-65ff-4da6-84fb-ff4d2918367e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:36,117] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:36,117] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_2659f3ba-4e1e-4ca8-b9b8-9b1c507a9188 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:36,117] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_2659f3ba-4e1e-4ca8-b9b8-9b1c507a9188 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:36,117] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:36,225] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:36,226] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-06 20:44:36,226] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-06 20:44:36,341] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:36,461] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:36,461] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:36,461] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 105 B in total
INFO  [2023-01-06 20:44:36,461] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000170122sINFO  [2023-01-06 20:44:36,479] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=6, min=1, average=1.200000, max=2}
INFO  [2023-01-06 20:44:36,479] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2ce30d98)
INFO  [2023-01-06 20:44:36,479] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:36,482] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:36,482] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:36,482] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:36,504] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:36 +0000] "POST /admin/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SINGLE_SELECT_SPEZIAL_CHARACTER2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:36,505] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:36,505] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:36,506] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:36,506] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:36,507] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:36,507] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:36,508] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:36,508] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:36,508] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.0
INFO  [2023-01-06 20:44:36,509] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.1
INFO  [2023-01-06 20:44:36,614] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_SPEZIAL_CHARACTER2 Test QUERY INIT
INFO  [2023-01-06 20:44:36,626] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:36,626] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e7929132-e217-4d86-8bd6-0e5b8447bd81] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test))]]
INFO  [2023-01-06 20:44:36,630] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81
INFO  [2023-01-06 20:44:36,630] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81
INFO  [2023-01-06 20:44:36,631] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81] with 1 results within PT0.000938S
INFO  [2023-01-06 20:44:36,631] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81] with 0 results within PT0.000874S
INFO  [2023-01-06 20:44:36,631] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_3ff5252a-65ff-4da6-84fb-ff4d2918367e, startTime=2023-01-06T20:44:36.630395, finishTime=2023-01-06T20:44:36.631333) of size 1
INFO  [2023-01-06 20:44:36,632] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_2659f3ba-4e1e-4ca8-b9b8-9b1c507a9188, startTime=2023-01-06T20:44:36.630503, finishTime=2023-01-06T20:44:36.631377) of size 0
INFO  [2023-01-06 20:44:36,632] com.bakdata.conquery.models.execution.ManagedExecution: DONE e7929132-e217-4d86-8bd6-0e5b8447bd81 ManagedQuery within PT0.005453S
127.0.0.1 - - [06/Jan/2023:20:44:36 +0000] "POST /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test/queries HTTP/1.1" 201 1253 "-" "Conquery (test client)" 9
127.0.0.1 - - [06/Jan/2023:20:44:36 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test/queries/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81 HTTP/1.1" 200 1600 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:36,655] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test], queryId=e7929132-e217-4d86-8bd6-0e5b8447bd81, label=concept	@§$, creationTime=2023-01-06T20:44:36.626428, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6a00facb[Count = 0], startTime=2023-01-06T20:44:36.626711, finishTime=2023-01-06T20:44:36.632164, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4fe1bb0f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7f3a8a72, com.bakdata.conquery.models.query.ColumnDescriptor@26845759]) download on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:36,655] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test], queryId=e7929132-e217-4d86-8bd6-0e5b8447bd81, label=concept	@§$, creationTime=2023-01-06T20:44:36.626428, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6a00facb[Count = 0], startTime=2023-01-06T20:44:36.626711, finishTime=2023-01-06T20:44:36.632164, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4fe1bb0f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7f3a8a72, com.bakdata.conquery.models.query.ColumnDescriptor@26845759]) on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
127.0.0.1 - - [06/Jan/2023:20:44:36 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2%20Test/result/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.e7929132-e217-4d86-8bd6-0e5b8447bd81.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:36,675] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_SPEZIAL_CHARACTER2 Test on 2 rows
INFO  [2023-01-06 20:44:36,675] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:36,676] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:36,676] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-06 20:44:36,676] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_2659f3ba-4e1e-4ca8-b9b8-9b1c507a9188
INFO  [2023-01-06 20:44:36,676] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_3ff5252a-65ff-4da6-84fb-ff4d2918367e
INFO  [2023-01-06 20:44:36,715] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:36,716] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_2659f3ba-4e1e-4ca8-b9b8-9b1c507a9188
INFO  [2023-01-06 20:44:36,720] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_3ff5252a-65ff-4da6-84fb-ff4d2918367e
INFO  [2023-01-06 20:44:36,809] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test
INFO  [2023-01-06 20:44:36,809] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:36,914] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-06 20:44:36,914] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-06 20:44:36,914] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:36,915] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:36,915] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-06 20:44:36,916] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-06 20:44:36,916] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:36,916] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:36,918] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_c2335671-89e7-487b-8f9c-f81106b89fb8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:36,918] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_c2335671-89e7-487b-8f9c-f81106b89fb8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:36,918] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:36,918] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_ab77e550-5047-46ff-bd58-3ffcf28a14ad are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:36,918] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_ab77e550-5047-46ff-bd58-3ffcf28a14ad are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:36,918] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:36,922] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,023] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,023] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table
INFO  [2023-01-06 20:44:37,024] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table
INFO  [2023-01-06 20:44:37,135] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,246] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:37,247] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:37,247] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 105 B in total
INFO  [2023-01-06 20:44:37,247] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000277713sINFO  [2023-01-06 20:44:37,275] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=6, min=1, average=1.200000, max=2}
INFO  [2023-01-06 20:44:37,275] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@e13bb4c)
INFO  [2023-01-06 20:44:37,275] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:37,278] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:37,278] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:37,279] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:37,294] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table
127.0.0.1 - - [06/Jan/2023:20:44:37 +0000] "POST /admin/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SINGLE_SELECT_SPEZIAL_CHARACTER3+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:37,295] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:37,295] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,296] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:37,296] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:37,297] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:37,298] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:37,298] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:37,298] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:37,298] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table.0
INFO  [2023-01-06 20:44:37,299] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table.1
INFO  [2023-01-06 20:44:37,404] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_SPEZIAL_CHARACTER3 Test QUERY INIT
INFO  [2023-01-06 20:44:37,416] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:37,418] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d4a23b00-78ed-4372-8c65-37843fa229e9] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test))]]
INFO  [2023-01-06 20:44:37,422] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9
INFO  [2023-01-06 20:44:37,422] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9
INFO  [2023-01-06 20:44:37,422] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9] with 0 results within PT0.000676S
INFO  [2023-01-06 20:44:37,422] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9] with 1 results within PT0.000558S
INFO  [2023-01-06 20:44:37,423] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_c2335671-89e7-487b-8f9c-f81106b89fb8, startTime=2023-01-06T20:44:37.422028, finishTime=2023-01-06T20:44:37.422704) of size 0
INFO  [2023-01-06 20:44:37,423] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_ab77e550-5047-46ff-bd58-3ffcf28a14ad, startTime=2023-01-06T20:44:37.422234, finishTime=2023-01-06T20:44:37.422792) of size 1
INFO  [2023-01-06 20:44:37,423] com.bakdata.conquery.models.execution.ManagedExecution: DONE d4a23b00-78ed-4372-8c65-37843fa229e9 ManagedQuery within PT0.00484S
127.0.0.1 - - [06/Jan/2023:20:44:37 +0000] "POST /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test/queries HTTP/1.1" 201 1251 "-" "Conquery (test client)" 10
127.0.0.1 - - [06/Jan/2023:20:44:37 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test/queries/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9 HTTP/1.1" 200 1597 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:44:37,455] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test], queryId=d4a23b00-78ed-4372-8c65-37843fa229e9, label=concept	@§$, creationTime=2023-01-06T20:44:37.418320, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6a9e937b[Count = 0], startTime=2023-01-06T20:44:37.418680, finishTime=2023-01-06T20:44:37.423520, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66a19163), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@392c6773, com.bakdata.conquery.models.query.ColumnDescriptor@2aabfec3]) download on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:37,455] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test], queryId=d4a23b00-78ed-4372-8c65-37843fa229e9, label=concept	@§$, creationTime=2023-01-06T20:44:37.418320, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6a9e937b[Count = 0], startTime=2023-01-06T20:44:37.418680, finishTime=2023-01-06T20:44:37.423520, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66a19163), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@392c6773, com.bakdata.conquery.models.query.ColumnDescriptor@2aabfec3]) on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
127.0.0.1 - - [06/Jan/2023:20:44:37 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3%20Test/result/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.d4a23b00-78ed-4372-8c65-37843fa229e9.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:37,474] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_SPEZIAL_CHARACTER3 Test on 2 rows
INFO  [2023-01-06 20:44:37,475] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-06 20:44:37,475] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-06 20:44:37,475] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-06 20:44:37,475] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_ab77e550-5047-46ff-bd58-3ffcf28a14ad
INFO  [2023-01-06 20:44:37,475] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_c2335671-89e7-487b-8f9c-f81106b89fb8
INFO  [2023-01-06 20:44:37,516] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-06 20:44:37,518] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_ab77e550-5047-46ff-bd58-3ffcf28a14ad
INFO  [2023-01-06 20:44:37,518] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_c2335671-89e7-487b-8f9c-f81106b89fb8
INFO  [2023-01-06 20:44:37,598] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test
INFO  [2023-01-06 20:44:37,598] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,704] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-06 20:44:37,704] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_DECIMAL
INFO  [2023-01-06 20:44:37,704] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:37,705] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:37,705] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-06 20:44:37,705] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-06 20:44:37,705] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:37,706] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:37,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DECIMAL.worker_SUM_DECIMAL_3dd98ab8-3bd9-4eef-ba05-c47eafe9e8a7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:37,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DECIMAL.worker_SUM_DECIMAL_3dd98ab8-3bd9-4eef-ba05-c47eafe9e8a7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:37,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:37,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DECIMAL.worker_SUM_DECIMAL_e9fbbad7-dd7d-4db3-a5b4-3f2d50a35ed8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:37,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DECIMAL.worker_SUM_DECIMAL_e9fbbad7-dd7d-4db3-a5b4-3f2d50a35ed8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:37,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:37,711] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,815] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:37,815] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DECIMAL.table
INFO  [2023-01-06 20:44:37,815] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DECIMAL.table
INFO  [2023-01-06 20:44:37,927] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,040] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:38,040] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:38,041] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 185 B in total
INFO  [2023-01-06 20:44:38,041] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_DECIMAL/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000315208sINFO  [2023-01-06 20:44:38,073] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=4}
INFO  [2023-01-06 20:44:38,073] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with DecimalParser(super=Parser(lines=10, nullLines=0), maxScale=0, maxAbs=250)
INFO  [2023-01-06 20:44:38,073] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=16511, maxValue=16514), dateReader=com.bakdata.conquery.util.DateReader@c2bd4f9)
INFO  [2023-01-06 20:44:38,076] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_DECIMAL/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:38,076] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:38,076] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_DECIMAL/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:38,095] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_DECIMAL.table
127.0.0.1 - - [06/Jan/2023:20:44:38 +0000] "POST /admin/datasets/SUM_DECIMAL/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SUM_DECIMAL%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:44:38,097] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,098] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:38,098] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:38,098] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:38,099] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:38,100] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DECIMAL.table.table], containing 10 entries.
INFO  [2023-01-06 20:44:38,100] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DECIMAL.table.table], containing 10 entries.
WARN  [2023-01-06 20:44:38,100] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:38,101] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DECIMAL.table.table.0
INFO  [2023-01-06 20:44:38,101] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DECIMAL.table.table.1
INFO  [2023-01-06 20:44:38,206] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_DECIMAL QUERY INIT
INFO  [2023-01-06 20:44:38,215] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_DECIMAL] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:38,215] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7321a475-f5d7-4c2c-b00c-8f4be8eccf69] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DECIMAL))]]
INFO  [2023-01-06 20:44:38,218] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69
INFO  [2023-01-06 20:44:38,218] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69
127.0.0.1 - - [06/Jan/2023:20:44:38 +0000] "POST /api/datasets/SUM_DECIMAL/queries HTTP/1.1" 201 1163 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:44:38,219] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69] with 3 results within PT0.000886S
INFO  [2023-01-06 20:44:38,219] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69] with 1 results within PT0.000954S
INFO  [2023-01-06 20:44:38,219] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69, workerId=SUM_DECIMAL.worker_SUM_DECIMAL_e9fbbad7-dd7d-4db3-a5b4-3f2d50a35ed8, startTime=2023-01-06T20:44:38.218409, finishTime=2023-01-06T20:44:38.219295) of size 3
INFO  [2023-01-06 20:44:38,219] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69, workerId=SUM_DECIMAL.worker_SUM_DECIMAL_3dd98ab8-3bd9-4eef-ba05-c47eafe9e8a7, startTime=2023-01-06T20:44:38.218413, finishTime=2023-01-06T20:44:38.219367) of size 1
INFO  [2023-01-06 20:44:38,220] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7321a475-f5d7-4c2c-b00c-8f4be8eccf69 ManagedQuery within PT0.004062S
127.0.0.1 - - [06/Jan/2023:20:44:38 +0000] "GET /api/datasets/SUM_DECIMAL/queries/SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69 HTTP/1.1" 200 1398 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:44:38,248] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DECIMAL], queryId=7321a475-f5d7-4c2c-b00c-8f4be8eccf69, label=concept	@§$, creationTime=2023-01-06T20:44:38.215847, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7436c88b[Count = 0], startTime=2023-01-06T20:44:38.216012, finishTime=2023-01-06T20:44:38.220074, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@379e5585), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DECIMAL)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@d830b09, com.bakdata.conquery.models.query.ColumnDescriptor@c7c4ab2]) download on dataset Dataset[label=null, name=SUM_DECIMAL] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:38,248] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DECIMAL], queryId=7321a475-f5d7-4c2c-b00c-8f4be8eccf69, label=concept	@§$, creationTime=2023-01-06T20:44:38.215847, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7436c88b[Count = 0], startTime=2023-01-06T20:44:38.216012, finishTime=2023-01-06T20:44:38.220074, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@379e5585), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DECIMAL)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@d830b09, com.bakdata.conquery.models.query.ColumnDescriptor@c7c4ab2]) on dataset Dataset[label=null, name=SUM_DECIMAL]
127.0.0.1 - - [06/Jan/2023:20:44:38 +0000] "GET /api/datasets/SUM_DECIMAL/result/SUM_DECIMAL.7321a475-f5d7-4c2c-b00c-8f4be8eccf69.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:44:38,265] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SUM_DECIMAL on 5 rows
INFO  [2023-01-06 20:44:38,265] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_DECIMAL
INFO  [2023-01-06 20:44:38,266] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-06 20:44:38,266] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-06 20:44:38,266] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DECIMAL_3dd98ab8-3bd9-4eef-ba05-c47eafe9e8a7
INFO  [2023-01-06 20:44:38,266] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DECIMAL_e9fbbad7-dd7d-4db3-a5b4-3f2d50a35ed8
INFO  [2023-01-06 20:44:38,305] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_DECIMAL
INFO  [2023-01-06 20:44:38,306] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DECIMAL_3dd98ab8-3bd9-4eef-ba05-c47eafe9e8a7
INFO  [2023-01-06 20:44:38,307] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DECIMAL_e9fbbad7-dd7d-4db3-a5b4-3f2d50a35ed8
INFO  [2023-01-06 20:44:38,401] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_DECIMAL
INFO  [2023-01-06 20:44:38,401] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,506] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_DECIMAL
INFO  [2023-01-06 20:44:38,506] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_INTEGER Test
INFO  [2023-01-06 20:44:38,506] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:38,506] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:38,508] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-06 20:44:38,508] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-06 20:44:38,508] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:38,508] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:38,509] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,509] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_55ab8181-d805-4c47-a6bd-6ea2a9121daf are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:38,509] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_55ab8181-d805-4c47-a6bd-6ea2a9121daf are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:38,509] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:38,510] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_f6c97256-939a-4cb9-92d6-a3e70a0926e7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:38,510] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_f6c97256-939a-4cb9-92d6-a3e70a0926e7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:38,510] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:38,617] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,618] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test[1].table
INFO  [2023-01-06 20:44:38,618] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test[1].table
INFO  [2023-01-06 20:44:38,736] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,848] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:38,848] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:38,848] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 185 B in total
INFO  [2023-01-06 20:44:38,849] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_INTEGER Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000395503sINFO  [2023-01-06 20:44:38,889] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=4}
INFO  [2023-01-06 20:44:38,889] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=16511, maxValue=16514), dateReader=com.bakdata.conquery.util.DateReader@121ed571)
INFO  [2023-01-06 20:44:38,889] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with IntegerParser(super=Parser(lines=10, nullLines=0), minValue=-50, maxValue=250)
INFO  [2023-01-06 20:44:38,892] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:38,892] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:38,892] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:38,912] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_INTEGER$20Test[1].table
127.0.0.1 - - [06/Jan/2023:20:44:38 +0000] "POST /admin/datasets/SUM_INTEGER%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SUM_INTEGER+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:44:38,913] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:38,914] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:38,914] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:38,914] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:38,917] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:38,917] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test[1].table.table], containing 10 entries.
INFO  [2023-01-06 20:44:38,917] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test[1].table.table], containing 10 entries.
WARN  [2023-01-06 20:44:38,919] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:38,919] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test[1].table.table.0
INFO  [2023-01-06 20:44:38,919] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test[1].table.table.1
INFO  [2023-01-06 20:44:39,025] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_INTEGER Test QUERY INIT
INFO  [2023-01-06 20:44:39,037] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_INTEGER$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:39,038] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b8645840-c3d4-429a-80f9-e719d256d3fd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test[1]))]]
INFO  [2023-01-06 20:44:39,042] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test[1].b8645840-c3d4-429a-80f9-e719d256d3fd
INFO  [2023-01-06 20:44:39,042] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test[1].b8645840-c3d4-429a-80f9-e719d256d3fd
INFO  [2023-01-06 20:44:39,043] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test[1].b8645840-c3d4-429a-80f9-e719d256d3fd] with 3 results within PT0.001127S
INFO  [2023-01-06 20:44:39,043] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test[1].b8645840-c3d4-429a-80f9-e719d256d3fd] with 1 results within PT0.001119S
127.0.0.1 - - [06/Jan/2023:20:44:39 +0000] "POST /api/datasets/SUM_INTEGER$20Test%5B1%5D/queries HTTP/1.1" 201 1206 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:39,044] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test[1].b8645840-c3d4-429a-80f9-e719d256d3fd, workerId=SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_f6c97256-939a-4cb9-92d6-a3e70a0926e7, startTime=2023-01-06T20:44:39.042685, finishTime=2023-01-06T20:44:39.043812) of size 3
INFO  [2023-01-06 20:44:39,044] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test[1].b8645840-c3d4-429a-80f9-e719d256d3fd, workerId=SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_55ab8181-d805-4c47-a6bd-6ea2a9121daf, startTime=2023-01-06T20:44:39.042750, finishTime=2023-01-06T20:44:39.043869) of size 1
INFO  [2023-01-06 20:44:39,044] com.bakdata.conquery.models.execution.ManagedExecution: DONE b8645840-c3d4-429a-80f9-e719d256d3fd ManagedQuery within PT0.006458S
127.0.0.1 - - [06/Jan/2023:20:44:39 +0000] "GET /api/datasets/SUM_INTEGER$20Test%5B1%5D/queries/SUM_INTEGER$20Test%5B1%5D.b8645840-c3d4-429a-80f9-e719d256d3fd HTTP/1.1" 200 1717 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:44:39,076] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test[1]], queryId=b8645840-c3d4-429a-80f9-e719d256d3fd, label=concept	@§$, creationTime=2023-01-06T20:44:39.038057, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@16900755[Count = 0], startTime=2023-01-06T20:44:39.038303, finishTime=2023-01-06T20:44:39.044761, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@767a8fe9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2cc67320, com.bakdata.conquery.models.query.ColumnDescriptor@3360c9f9]) download on dataset Dataset[label=null, name=SUM_INTEGER Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:39,076] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test[1]], queryId=b8645840-c3d4-429a-80f9-e719d256d3fd, label=concept	@§$, creationTime=2023-01-06T20:44:39.038057, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@16900755[Count = 0], startTime=2023-01-06T20:44:39.038303, finishTime=2023-01-06T20:44:39.044761, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@767a8fe9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2cc67320, com.bakdata.conquery.models.query.ColumnDescriptor@3360c9f9]) on dataset Dataset[label=null, name=SUM_INTEGER Test[1]]
127.0.0.1 - - [06/Jan/2023:20:44:39 +0000] "GET /api/datasets/SUM_INTEGER%20Test%5B1%5D/result/SUM_INTEGER$20Test%5B1%5D.b8645840-c3d4-429a-80f9-e719d256d3fd.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:44:39,094] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SUM_INTEGER Test on 5 rows
INFO  [2023-01-06 20:44:39,094] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_INTEGER Test[1]
INFO  [2023-01-06 20:44:39,094] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-06 20:44:39,094] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-06 20:44:39,095] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test[1]_f6c97256-939a-4cb9-92d6-a3e70a0926e7
INFO  [2023-01-06 20:44:39,095] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test[1]_55ab8181-d805-4c47-a6bd-6ea2a9121daf
INFO  [2023-01-06 20:44:39,108] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_INTEGER Test[1]
INFO  [2023-01-06 20:44:39,109] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test[1]_55ab8181-d805-4c47-a6bd-6ea2a9121daf
INFO  [2023-01-06 20:44:39,109] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test[1]_f6c97256-939a-4cb9-92d6-a3e70a0926e7
INFO  [2023-01-06 20:44:39,119] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_INTEGER$20Test[1]
INFO  [2023-01-06 20:44:39,119] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:39,225] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_INTEGER Test
INFO  [2023-01-06 20:44:39,226] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:39,226] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:39,226] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:39,227] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:39,227] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:39,227] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:39,227] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:39,229] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_001066e8-59cc-4ca2-bc45-2fcba551acdf are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:39,229] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_001066e8-59cc-4ca2-bc45-2fcba551acdf are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:39,229] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:39,229] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_485a4b4b-e178-4b41-8b15-5091b7c445a5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:39,229] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_485a4b4b-e178-4b41-8b15-5091b7c445a5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:39,229] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:39,234] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:39,374] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ABS-EXPORT-FORM$20SECONDARY_ID.secondary]
INFO  [2023-01-06 20:44:39,375] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ABS-EXPORT-FORM$20SECONDARY_ID.ignored]
INFO  [2023-01-06 20:44:39,375] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:39,376] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-06 20:44:39,376] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.ignored
INFO  [2023-01-06 20:44:39,377] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-06 20:44:39,416] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.ignored
INFO  [2023-01-06 20:44:39,524] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-06 20:44:39,524] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:39,524] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-06 20:44:39,524] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:39,524] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:39,630] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID IMPORT TABLES
INFO  [2023-01-06 20:44:39,661] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:39,809] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID IMPORT CONCEPTS
INFO  [2023-01-06 20:44:39,810] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:39,810] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:39,811] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:39,811] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:44:39,811] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.003482649sINFO  [2023-01-06 20:44:39,840] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:39,840] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5fee3c96)
INFO  [2023-01-06 20:44:39,840] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3db63637)
INFO  [2023-01-06 20:44:39,840] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@22726566), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2cb59f58), dateReader=com.bakdata.conquery.util.DateReader@288a0e16, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:39,840] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@50d3641)
INFO  [2023-01-06 20:44:39,840] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:39,847] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:39,847] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000656665sINFO  [2023-01-06 20:44:39,877] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-06 20:44:39,877] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@215e5776)
INFO  [2023-01-06 20:44:39,877] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:39,877] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:39,880] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:39,880] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:39,880] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:39,880] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:39,896] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:39 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ABS-EXPORT-FORM+SECONDARY_ID%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:39,897] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:39,898] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:39,898] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:39,900] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:39,900] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:39,900] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:39,901] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:39,903] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.1
WARN  [2023-01-06 20:44:39,903] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:39,904] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:39,904] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:39,906] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:39,906] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:39,906] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:39,906] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:39,922] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ABS-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:39,923] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:39,923] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:39,923] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:39 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ABS-EXPORT-FORM+SECONDARY_ID%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:44:39,923] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:39,923] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-06 20:44:39,924] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:39,924] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:39,924] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:39,924] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.table.table.0
INFO  [2023-01-06 20:44:40,030] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,035] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:40,057] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,062] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:40,078] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,078] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:40,078] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:40,191] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[283b843f-7ee3-42c3-a492-7d702b134723] in Datasets[[]]
INFO  [2023-01-06 20:44:40,209] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20SECONDARY_ID.283b843f-7ee3-42c3-a492-7d702b134723
INFO  [2023-01-06 20:44:40,209] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20SECONDARY_ID.283b843f-7ee3-42c3-a492-7d702b134723
INFO  [2023-01-06 20:44:40,215] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20SECONDARY_ID.283b843f-7ee3-42c3-a492-7d702b134723] with 0 results within PT0.005943S
INFO  [2023-01-06 20:44:40,218] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20SECONDARY_ID.283b843f-7ee3-42c3-a492-7d702b134723] with 1 results within PT0.009375S
INFO  [2023-01-06 20:44:40,219] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20SECONDARY_ID.283b843f-7ee3-42c3-a492-7d702b134723, workerId=ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_485a4b4b-e178-4b41-8b15-5091b7c445a5, startTime=2023-01-06T20:44:40.209485, finishTime=2023-01-06T20:44:40.215428) of size 0
INFO  [2023-01-06 20:44:40,221] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20SECONDARY_ID.283b843f-7ee3-42c3-a492-7d702b134723, workerId=ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_001066e8-59cc-4ca2-bc45-2fcba551acdf, startTime=2023-01-06T20:44:40.209484, finishTime=2023-01-06T20:44:40.218859) of size 1
INFO  [2023-01-06 20:44:40,221] com.bakdata.conquery.models.execution.ManagedExecution: DONE f1158d75-c206-48f4-83e9-e617183fa821 ManagedQuery within PT0.028966S
INFO  [2023-01-06 20:44:40,222] com.bakdata.conquery.models.execution.ManagedExecution: DONE 283b843f-7ee3-42c3-a492-7d702b134723 ManagedInternalForm within PT0.030166S
INFO  [2023-01-06 20:44:40,222] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID QUERIES EXECUTED
INFO  [2023-01-06 20:44:40,243] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID CSV TESTING: results
INFO  [2023-01-06 20:44:40,248] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:40,248] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:40,248] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:40,249] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM SECONDARY_ID_485a4b4b-e178-4b41-8b15-5091b7c445a5
INFO  [2023-01-06 20:44:40,249] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM SECONDARY_ID_001066e8-59cc-4ca2-bc45-2fcba551acdf
INFO  [2023-01-06 20:44:40,343] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:40,343] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM SECONDARY_ID_001066e8-59cc-4ca2-bc45-2fcba551acdf
INFO  [2023-01-06 20:44:40,343] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM SECONDARY_ID_485a4b4b-e178-4b41-8b15-5091b7c445a5
INFO  [2023-01-06 20:44:40,430] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS-EXPORT-FORM$20SECONDARY_ID
INFO  [2023-01-06 20:44:40,430] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,485] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:40,486] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:40,486] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:40,486] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:40,487] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:40,487] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:40,487] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:40,487] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:40,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_37301726-3f9d-46cb-b74a-44b977b0e020 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:40,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_37301726-3f9d-46cb-b74a-44b977b0e020 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:40,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:40,490] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_28deeba9-0254-4203-a114-4d76d7448ccd are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:40,490] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_28deeba9-0254-4203-a114-4d76d7448ccd are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:40,490] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:40,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,593] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,600] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,601] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-06 20:44:40,601] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-06 20:44:40,706] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-06 20:44:40,730] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,839] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:40,840] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:40,840] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:40,840] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-06 20:44:40,840] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000351011sINFO  [2023-01-06 20:44:40,876] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:40,876] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@9687ee7)
INFO  [2023-01-06 20:44:40,876] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@2be4855f)
INFO  [2023-01-06 20:44:40,876] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:40,876] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@6d889563)
INFO  [2023-01-06 20:44:40,876] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@55bd357d), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5b7ad1ba), dateReader=com.bakdata.conquery.util.DateReader@5f455e0d, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:40,889] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:40,889] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:40,889] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:40,917] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:40 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20ADD%20DEFAULT%20SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ABS-EXPORT-FORM+ADD+DEFAULT+SELECT+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:44:40,918] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:40,919] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:40,921] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:40,921] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:40,924] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:40,924] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:40,925] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:40,927] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:40,927] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:40,928] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:40,929] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:40,929] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:40,930] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:40,930] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.6
WARN  [2023-01-06 20:44:40,930] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:40,930] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:41,035] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,041] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:41,077] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,083] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,088] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:41,098] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,098] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:41,099] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:41,211] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f493acbe-eae6-4a72-b332-f9ba297afc79] in Datasets[[]]
INFO  [2023-01-06 20:44:41,236] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.f493acbe-eae6-4a72-b332-f9ba297afc79
INFO  [2023-01-06 20:44:41,236] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.f493acbe-eae6-4a72-b332-f9ba297afc79
INFO  [2023-01-06 20:44:41,240] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.f493acbe-eae6-4a72-b332-f9ba297afc79] with 0 results within PT0.003716S
INFO  [2023-01-06 20:44:41,240] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.f493acbe-eae6-4a72-b332-f9ba297afc79] with 1 results within PT0.003939S
INFO  [2023-01-06 20:44:41,241] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.f493acbe-eae6-4a72-b332-f9ba297afc79, workerId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_37301726-3f9d-46cb-b74a-44b977b0e020, startTime=2023-01-06T20:44:41.236367, finishTime=2023-01-06T20:44:41.240083) of size 0
INFO  [2023-01-06 20:44:41,241] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.f493acbe-eae6-4a72-b332-f9ba297afc79, workerId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_28deeba9-0254-4203-a114-4d76d7448ccd, startTime=2023-01-06T20:44:41.236694, finishTime=2023-01-06T20:44:41.240633) of size 1
INFO  [2023-01-06 20:44:41,241] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1b61094d-936f-4f78-99f8-711d750017ca ManagedQuery within PT0.030002S
INFO  [2023-01-06 20:44:41,242] com.bakdata.conquery.models.execution.ManagedExecution: DONE f493acbe-eae6-4a72-b332-f9ba297afc79 ManagedInternalForm within PT0.03105S
INFO  [2023-01-06 20:44:41,242] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:41,254] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-06 20:44:41,258] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:41,258] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:41,258] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:41,259] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_37301726-3f9d-46cb-b74a-44b977b0e020
INFO  [2023-01-06 20:44:41,259] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_28deeba9-0254-4203-a114-4d76d7448ccd
INFO  [2023-01-06 20:44:41,287] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:41,288] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_37301726-3f9d-46cb-b74a-44b977b0e020
INFO  [2023-01-06 20:44:41,288] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_28deeba9-0254-4203-a114-4d76d7448ccd
INFO  [2023-01-06 20:44:41,331] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test
INFO  [2023-01-06 20:44:41,331] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,404] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:41,405] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-06 20:44:41,405] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:41,405] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:41,406] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-06 20:44:41,407] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-06 20:44:41,407] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:41,407] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_e2463074-0a0b-49e2-b218-daed69a434fe are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_e2463074-0a0b-49e2-b218-daed69a434fe are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_ebe4d631-ba80-44da-92a6-5f7ce7ecb331 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_ebe4d631-ba80-44da-92a6-5f7ce7ecb331 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:41,410] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:41,513] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,520] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,520] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-06 20:44:41,520] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-06 20:44:41,626] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test IMPORT TABLES
INFO  [2023-01-06 20:44:41,641] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,752] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:41,753] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:41,753] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:41,753] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-06 20:44:41,753] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000406779sINFO  [2023-01-06 20:44:41,794] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:41,795] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@160fc0b2)
INFO  [2023-01-06 20:44:41,795] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@430610f4)
INFO  [2023-01-06 20:44:41,795] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:41,795] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@155869f9)
INFO  [2023-01-06 20:44:41,795] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3da7df1e), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@39f09f54), dateReader=com.bakdata.conquery.util.DateReader@62e9a6bc, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:41,798] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:41,798] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:41,798] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS-EXPORT-FORM WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:41,811] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:41 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20WITH%20SELECT%20SET%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ABS-EXPORT-FORM+WITH+SELECT+SET+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:44:41,811] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,813] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:41,814] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:41,814] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:41,817] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:41,817] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:41,817] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:41,819] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:41,819] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:41,819] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:41,820] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:41,820] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.5
WARN  [2023-01-06 20:44:41,821] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:41,821] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:41,821] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:41,822] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:41,927] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,932] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:41,980] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,985] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,991] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:41,997] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:41,997] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:41,997] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:42,106] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f9600edb-2136-4398-93ff-0367be1d633b] in Datasets[[]]
INFO  [2023-01-06 20:44:42,112] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.f9600edb-2136-4398-93ff-0367be1d633b
INFO  [2023-01-06 20:44:42,112] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.f9600edb-2136-4398-93ff-0367be1d633b
INFO  [2023-01-06 20:44:42,126] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.f9600edb-2136-4398-93ff-0367be1d633b] with 0 results within PT0.013638S
INFO  [2023-01-06 20:44:42,126] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.f9600edb-2136-4398-93ff-0367be1d633b] with 1 results within PT0.013774S
INFO  [2023-01-06 20:44:42,127] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.f9600edb-2136-4398-93ff-0367be1d633b, workerId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_e2463074-0a0b-49e2-b218-daed69a434fe, startTime=2023-01-06T20:44:42.112464, finishTime=2023-01-06T20:44:42.126102) of size 0
INFO  [2023-01-06 20:44:42,127] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.f9600edb-2136-4398-93ff-0367be1d633b, workerId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_ebe4d631-ba80-44da-92a6-5f7ce7ecb331, startTime=2023-01-06T20:44:42.112503, finishTime=2023-01-06T20:44:42.126277) of size 1
INFO  [2023-01-06 20:44:42,127] com.bakdata.conquery.models.execution.ManagedExecution: DONE c22e9ee4-7801-42a3-91f4-5c67c53a138b ManagedQuery within PT0.020256S
INFO  [2023-01-06 20:44:42,128] com.bakdata.conquery.models.execution.ManagedExecution: DONE f9600edb-2136-4398-93ff-0367be1d633b ManagedInternalForm within PT0.021168S
INFO  [2023-01-06 20:44:42,128] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:42,145] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test CSV TESTING: results
INFO  [2023-01-06 20:44:42,147] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-06 20:44:42,147] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-06 20:44:42,148] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-06 20:44:42,148] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM WITH SELECT SET Test_e2463074-0a0b-49e2-b218-daed69a434fe
INFO  [2023-01-06 20:44:42,148] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM WITH SELECT SET Test_ebe4d631-ba80-44da-92a6-5f7ce7ecb331
INFO  [2023-01-06 20:44:42,207] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-06 20:44:42,208] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM WITH SELECT SET Test_ebe4d631-ba80-44da-92a6-5f7ce7ecb331
INFO  [2023-01-06 20:44:42,208] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM WITH SELECT SET Test_e2463074-0a0b-49e2-b218-daed69a434fe
INFO  [2023-01-06 20:44:42,221] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test
INFO  [2023-01-06 20:44:42,221] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:42,403] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-06 20:44:42,404] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:42,404] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:42,404] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:42,405] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:42,405] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:42,405] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:42,405] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:42,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_ea84d0b7-a9b4-42f9-9384-a1579fd2ef4f are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:42,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_ea84d0b7-a9b4-42f9-9384-a1579fd2ef4f are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:42,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:42,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_2fb9da7e-7c15-4e2f-883b-107c370069c9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:42,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_2fb9da7e-7c15-4e2f-883b-107c370069c9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:42,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:42,412] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:42,511] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ADD$20DEFAULT$20SELECT$20Test.secondary]
INFO  [2023-01-06 20:44:42,512] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:42,513] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test.secondary
INFO  [2023-01-06 20:44:42,513] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test.secondary
INFO  [2023-01-06 20:44:42,621] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-06 20:44:42,621] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-06 20:44:42,621] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:42,621] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.table
INFO  [2023-01-06 20:44:42,621] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.table
INFO  [2023-01-06 20:44:42,726] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-06 20:44:42,737] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:42,848] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:42,848] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:42,848] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:42,848] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:42,848] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:44:42,848] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.004994781sINFO  [2023-01-06 20:44:42,890] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:42,890] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2de1c374)
INFO  [2023-01-06 20:44:42,890] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@6da809c3)
INFO  [2023-01-06 20:44:42,890] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:42,890] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@5c6084e1), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5c2f9f6a), dateReader=com.bakdata.conquery.util.DateReader@242dd047, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:42,890] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@19d3f2eb)
INFO  [2023-01-06 20:44:42,894] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:42,894] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000625384sINFO  [2023-01-06 20:44:42,912] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-06 20:44:42,912] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@921ada0)
INFO  [2023-01-06 20:44:42,912] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:42,912] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:42,918] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:42,918] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:42,918] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:42,918] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:42,950] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ADD$20DEFAULT$20SELECT$20Test.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:42 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ADD+DEFAULT+SELECT+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:44:42,952] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:42,952] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:42,952] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:42,954] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:42,954] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:42,954] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:42,955] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:42,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.2
WARN  [2023-01-06 20:44:42,956] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:42,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:42,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:42,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:42,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:42,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:42,957] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:42,973] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ADD$20DEFAULT$20SELECT$20Test.table
INFO  [2023-01-06 20:44:42,974] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:42,974] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:42,974] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:42 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ADD+DEFAULT+SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:42,974] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:42,975] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:42,975] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:42,975] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:42,975] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:42,975] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.table.table.0
INFO  [2023-01-06 20:44:43,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,097] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:43,107] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,115] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:43,130] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,130] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:43,130] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:43,237] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[09b5ab72-78cc-4fc5-8869-73d7483283fc] in Datasets[[]]
INFO  [2023-01-06 20:44:43,245] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test.09b5ab72-78cc-4fc5-8869-73d7483283fc
INFO  [2023-01-06 20:44:43,245] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test.09b5ab72-78cc-4fc5-8869-73d7483283fc
INFO  [2023-01-06 20:44:43,326] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test.09b5ab72-78cc-4fc5-8869-73d7483283fc] with 0 results within PT0.080415S
INFO  [2023-01-06 20:44:43,327] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test.09b5ab72-78cc-4fc5-8869-73d7483283fc] with 1 results within PT0.080999S
INFO  [2023-01-06 20:44:43,327] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test.09b5ab72-78cc-4fc5-8869-73d7483283fc, workerId=ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_2fb9da7e-7c15-4e2f-883b-107c370069c9, startTime=2023-01-06T20:44:43.246020, finishTime=2023-01-06T20:44:43.326435) of size 0
INFO  [2023-01-06 20:44:43,327] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test.09b5ab72-78cc-4fc5-8869-73d7483283fc, workerId=ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_ea84d0b7-a9b4-42f9-9384-a1579fd2ef4f, startTime=2023-01-06T20:44:43.246014, finishTime=2023-01-06T20:44:43.327013) of size 1
INFO  [2023-01-06 20:44:43,328] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0fc457f2-9856-440f-9df5-9a6c01b74957 ManagedQuery within PT0.090398S
INFO  [2023-01-06 20:44:43,328] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:43,328] com.bakdata.conquery.models.execution.ManagedExecution: DONE 09b5ab72-78cc-4fc5-8869-73d7483283fc ManagedInternalForm within PT0.091197S
INFO  [2023-01-06 20:44:43,329] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-06 20:44:43,330] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:43,330] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:43,330] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-06 20:44:43,333] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test_ea84d0b7-a9b4-42f9-9384-a1579fd2ef4f
INFO  [2023-01-06 20:44:43,335] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test_2fb9da7e-7c15-4e2f-883b-107c370069c9
INFO  [2023-01-06 20:44:43,425] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:43,425] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test_2fb9da7e-7c15-4e2f-883b-107c370069c9
INFO  [2023-01-06 20:44:43,425] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test_ea84d0b7-a9b4-42f9-9384-a1579fd2ef4f
INFO  [2023-01-06 20:44:43,485] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ADD$20DEFAULT$20SELECT$20Test
INFO  [2023-01-06 20:44:43,485] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,536] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:43,537] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:43,537] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:43,537] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:43,539] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-06 20:44:43,539] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-06 20:44:43,539] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:43,539] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:43,543] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_18efe3aa-84c7-47cb-b187-05a226e692f0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:43,543] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_18efe3aa-84c7-47cb-b187-05a226e692f0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:43,543] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:43,543] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_3bd847c7-510e-4f88-8d64-91d8994710f3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:43,543] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_3bd847c7-510e-4f88-8d64-91d8994710f3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:43,543] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:43,544] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,646] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ADD$20DEFAULT$20SELECT$20Test[1].secondary]
INFO  [2023-01-06 20:44:43,648] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,649] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test[1].secondary
INFO  [2023-01-06 20:44:43,649] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test[1].secondary
INFO  [2023-01-06 20:44:43,755] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:43,756] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm
INFO  [2023-01-06 20:44:43,756] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm
INFO  [2023-01-06 20:44:43,756] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].table
INFO  [2023-01-06 20:44:43,800] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].table
INFO  [2023-01-06 20:44:43,919] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-06 20:44:43,932] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,051] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:44,052] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:44,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:44,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:44,052] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:44:44,052] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[1]/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.014873802sINFO  [2023-01-06 20:44:44,176] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:44,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4ba98b75)
INFO  [2023-01-06 20:44:44,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:44,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@da76a2c)
INFO  [2023-01-06 20:44:44,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@1a47db81)
INFO  [2023-01-06 20:44:44,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@6e747447), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@53435ce2), dateReader=com.bakdata.conquery.util.DateReader@7c30dc70, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:44,183] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:44,183] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.001614501sINFO  [2023-01-06 20:44:44,216] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-06 20:44:44,216] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:44,216] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@57e8027a)
INFO  [2023-01-06 20:44:44,216] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:44,219] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:44,219] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:44,219] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:44,219] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:44,243] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:44 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ADD+DEFAULT+SELECT+Test%5B1%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:44:44,245] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:44,246] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:44,246] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:44,250] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:44,250] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:44,250] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:44,252] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:44,252] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:44,253] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:44,254] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:44,254] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:44,255] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.5
WARN  [2023-01-06 20:44:44,255] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:44,256] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:44,256] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:44,263] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ADD$20DEFAULT$20SELECT$20Test[1].table
INFO  [2023-01-06 20:44:44,263] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:44 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ADD+DEFAULT+SELECT+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:44,264] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,264] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:44,264] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:44,271] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-06 20:44:44,272] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:44,272] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].table.table], containing 6 entries.
INFO  [2023-01-06 20:44:44,272] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].table.table], containing 6 entries.
INFO  [2023-01-06 20:44:44,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].table.table.0
INFO  [2023-01-06 20:44:44,377] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,395] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:44,402] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,408] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:44,418] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,418] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:44,418] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:44,526] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[441a1561-59c4-4e80-8cc1-c4b77684a628] in Datasets[[]]
INFO  [2023-01-06 20:44:44,532] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[1].441a1561-59c4-4e80-8cc1-c4b77684a628
INFO  [2023-01-06 20:44:44,532] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[1].441a1561-59c4-4e80-8cc1-c4b77684a628
INFO  [2023-01-06 20:44:44,536] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[1].441a1561-59c4-4e80-8cc1-c4b77684a628] with 0 results within PT0.004047S
INFO  [2023-01-06 20:44:44,536] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[1].441a1561-59c4-4e80-8cc1-c4b77684a628] with 1 results within PT0.004111S
INFO  [2023-01-06 20:44:44,537] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[1].441a1561-59c4-4e80-8cc1-c4b77684a628, workerId=ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_18efe3aa-84c7-47cb-b187-05a226e692f0, startTime=2023-01-06T20:44:44.532184, finishTime=2023-01-06T20:44:44.536295) of size 1
INFO  [2023-01-06 20:44:44,537] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[1].441a1561-59c4-4e80-8cc1-c4b77684a628, workerId=ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_3bd847c7-510e-4f88-8d64-91d8994710f3, startTime=2023-01-06T20:44:44.532169, finishTime=2023-01-06T20:44:44.536216) of size 0
INFO  [2023-01-06 20:44:44,537] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4a55e832-9f62-4519-82e1-acc657a9a3dd ManagedQuery within PT0.011672S
INFO  [2023-01-06 20:44:44,539] com.bakdata.conquery.models.execution.ManagedExecution: DONE 441a1561-59c4-4e80-8cc1-c4b77684a628 ManagedInternalForm within PT0.012842S
INFO  [2023-01-06 20:44:44,539] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:44,557] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-06 20:44:44,559] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ADD DEFAULT SELECT Test[1]
INFO  [2023-01-06 20:44:44,559] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-06 20:44:44,559] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-06 20:44:44,560] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[1]_3bd847c7-510e-4f88-8d64-91d8994710f3
INFO  [2023-01-06 20:44:44,560] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[1]_18efe3aa-84c7-47cb-b187-05a226e692f0
INFO  [2023-01-06 20:44:44,657] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[1]_3bd847c7-510e-4f88-8d64-91d8994710f3
INFO  [2023-01-06 20:44:44,657] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[1]_18efe3aa-84c7-47cb-b187-05a226e692f0
INFO  [2023-01-06 20:44:44,657] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ADD DEFAULT SELECT Test[1]
INFO  [2023-01-06 20:44:44,672] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ADD$20DEFAULT$20SELECT$20Test[1]
INFO  [2023-01-06 20:44:44,672] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,825] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:44,825] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:44,825] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:44,825] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:44,826] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:44,827] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:44,827] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:44,827] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:44,829] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_960d350f-ea5c-4aae-b629-22612eb7badd are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:44,829] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_960d350f-ea5c-4aae-b629-22612eb7badd are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:44,829] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:44,829] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_42c6d530-da61-4ca9-8671-6f9ec607a726 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:44,829] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_42c6d530-da61-4ca9-8671-6f9ec607a726 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:44,829] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:44,833] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,933] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.secondary]
INFO  [2023-01-06 20:44:44,933] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:44,934] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-06 20:44:44,934] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-06 20:44:45,041] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-06 20:44:45,041] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-06 20:44:45,041] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,041] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:45,041] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:45,146] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID IMPORT TABLES
INFO  [2023-01-06 20:44:45,162] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,274] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID IMPORT CONCEPTS
INFO  [2023-01-06 20:44:45,275] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:45,275] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:45,275] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:45,275] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:44:45,275] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.002975679sINFO  [2023-01-06 20:44:45,300] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:45,300] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@570a1bd0)
INFO  [2023-01-06 20:44:45,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:45,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@55b8a0f5), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@754cb1e7), dateReader=com.bakdata.conquery.util.DateReader@3f29f8e1, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:45,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@266d1369)
INFO  [2023-01-06 20:44:45,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@51fb2424)
INFO  [2023-01-06 20:44:45,305] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:45,305] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000467392sINFO  [2023-01-06 20:44:45,323] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-06 20:44:45,323] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@26596753)
INFO  [2023-01-06 20:44:45,323] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:45,323] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:45,326] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:45,326] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:45,326] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:45,326] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:45,349] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:45 +0000] "POST /admin/datasets/ENTITY-DATE-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ENTITY-DATE-EXPORT-FORM+SECONDARY_ID%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:44:45,351] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:45,352] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:45,353] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:45,356] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:45,356] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:45,356] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:45,358] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:45,359] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:45,360] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:45,366] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.3
WARN  [2023-01-06 20:44:45,366] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:45,366] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:45,366] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:45,367] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:45,367] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:45,373] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:45,373] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:45,373] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:45,373] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:45 +0000] "POST /admin/datasets/ENTITY-DATE-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ENTITY-DATE-EXPORT-FORM+SECONDARY_ID%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:45,373] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,374] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-06 20:44:45,374] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:45,374] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:45,374] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:45,375] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table.table.0
INFO  [2023-01-06 20:44:45,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,487] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:45,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,500] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:45,506] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:45,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:45,613] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[fadc5e16-ddb7-4904-b094-0234456582f8] in Datasets[[]]
INFO  [2023-01-06 20:44:45,620] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.fadc5e16-ddb7-4904-b094-0234456582f8
INFO  [2023-01-06 20:44:45,620] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.fadc5e16-ddb7-4904-b094-0234456582f8
INFO  [2023-01-06 20:44:45,624] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.fadc5e16-ddb7-4904-b094-0234456582f8] with 0 results within PT0.0041S
INFO  [2023-01-06 20:44:45,625] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.fadc5e16-ddb7-4904-b094-0234456582f8] with 1 results within PT0.004543S
INFO  [2023-01-06 20:44:45,625] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.fadc5e16-ddb7-4904-b094-0234456582f8, workerId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_960d350f-ea5c-4aae-b629-22612eb7badd, startTime=2023-01-06T20:44:45.620796, finishTime=2023-01-06T20:44:45.624896) of size 0
INFO  [2023-01-06 20:44:45,625] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.fadc5e16-ddb7-4904-b094-0234456582f8, workerId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_42c6d530-da61-4ca9-8671-6f9ec607a726, startTime=2023-01-06T20:44:45.620462, finishTime=2023-01-06T20:44:45.625005) of size 1
INFO  [2023-01-06 20:44:45,625] com.bakdata.conquery.models.execution.ManagedExecution: DONE 6a4b899e-6a69-489a-aac6-d1ecedcd8756 ManagedQuery within PT0.012194S
INFO  [2023-01-06 20:44:45,626] com.bakdata.conquery.models.execution.ManagedExecution: DONE fadc5e16-ddb7-4904-b094-0234456582f8 ManagedInternalForm within PT0.012948S
INFO  [2023-01-06 20:44:45,626] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID QUERIES EXECUTED
INFO  [2023-01-06 20:44:45,639] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID CSV TESTING: results
INFO  [2023-01-06 20:44:45,641] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:45,641] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:45,641] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:45,642] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_960d350f-ea5c-4aae-b629-22612eb7badd
INFO  [2023-01-06 20:44:45,642] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_42c6d530-da61-4ca9-8671-6f9ec607a726
INFO  [2023-01-06 20:44:45,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:45,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_960d350f-ea5c-4aae-b629-22612eb7badd
INFO  [2023-01-06 20:44:45,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_42c6d530-da61-4ca9-8671-6f9ec607a726
INFO  [2023-01-06 20:44:45,778] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID
INFO  [2023-01-06 20:44:45,778] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,912] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:45,913] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:45,913] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:45,913] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:45,914] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-06 20:44:45,914] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-06 20:44:45,914] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:45,914] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_623e7778-4a4d-4c9d-98d7-00fb586650f8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_623e7778-4a4d-4c9d-98d7-00fb586650f8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_c4b4a56f-ae70-47b6-8f6d-48f9bfde68d5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_c4b4a56f-ae70-47b6-8f6d-48f9bfde68d5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:45,916] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:46,020] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,028] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,028] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm
INFO  [2023-01-06 20:44:46,028] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm
INFO  [2023-01-06 20:44:46,133] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-06 20:44:46,149] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,257] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:46,258] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:46,258] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:46,258] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-06 20:44:46,258] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[2]/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000411157sINFO  [2023-01-06 20:44:46,300] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:46,300] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4d3efa84)
INFO  [2023-01-06 20:44:46,300] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:46,300] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@750070ae)
INFO  [2023-01-06 20:44:46,300] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@2b17e11f), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5ed77746), dateReader=com.bakdata.conquery.util.DateReader@7a6824ce, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:46,300] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@6f24f4a)
INFO  [2023-01-06 20:44:46,304] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:46,304] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:46,304] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ADD DEFAULT SELECT Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:46,322] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:46 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ADD+DEFAULT+SELECT+Test%5B2%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:46,323] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,323] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:46,324] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:46,324] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:46,325] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:46,326] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:46,326] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.2
WARN  [2023-01-06 20:44:46,327] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:46,327] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:46,331] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:46,448] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,453] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:46,498] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,503] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,509] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:46,520] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,521] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:46,521] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:46,632] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9755f76f-80f3-446d-827d-95ae533a759c] in Datasets[[]]
INFO  [2023-01-06 20:44:46,642] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[2].9755f76f-80f3-446d-827d-95ae533a759c
INFO  [2023-01-06 20:44:46,642] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[2].9755f76f-80f3-446d-827d-95ae533a759c
INFO  [2023-01-06 20:44:46,645] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[2].9755f76f-80f3-446d-827d-95ae533a759c] with 0 results within PT0.002406S
INFO  [2023-01-06 20:44:46,645] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[2].9755f76f-80f3-446d-827d-95ae533a759c] with 1 results within PT0.003169S
INFO  [2023-01-06 20:44:46,646] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[2].9755f76f-80f3-446d-827d-95ae533a759c, workerId=ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_c4b4a56f-ae70-47b6-8f6d-48f9bfde68d5, startTime=2023-01-06T20:44:46.642821, finishTime=2023-01-06T20:44:46.645227) of size 0
INFO  [2023-01-06 20:44:46,647] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[2].9755f76f-80f3-446d-827d-95ae533a759c, workerId=ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_623e7778-4a4d-4c9d-98d7-00fb586650f8, startTime=2023-01-06T20:44:46.642600, finishTime=2023-01-06T20:44:46.645769) of size 1
INFO  [2023-01-06 20:44:46,647] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9ceabecc-cbbe-4cfc-a803-940c53114608 ManagedQuery within PT0.015014S
INFO  [2023-01-06 20:44:46,648] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:46,648] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9755f76f-80f3-446d-827d-95ae533a759c ManagedInternalForm within PT0.016222S
INFO  [2023-01-06 20:44:46,658] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-06 20:44:46,661] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ADD DEFAULT SELECT Test[2]
INFO  [2023-01-06 20:44:46,661] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-06 20:44:46,661] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-06 20:44:46,662] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[2]_c4b4a56f-ae70-47b6-8f6d-48f9bfde68d5
INFO  [2023-01-06 20:44:46,662] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[2]_623e7778-4a4d-4c9d-98d7-00fb586650f8
INFO  [2023-01-06 20:44:46,714] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ADD DEFAULT SELECT Test[2]
INFO  [2023-01-06 20:44:46,715] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[2]_623e7778-4a4d-4c9d-98d7-00fb586650f8
INFO  [2023-01-06 20:44:46,715] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[2]_c4b4a56f-ae70-47b6-8f6d-48f9bfde68d5
INFO  [2023-01-06 20:44:46,728] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ADD$20DEFAULT$20SELECT$20Test[2]
INFO  [2023-01-06 20:44:46,728] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,826] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ADD DEFAULT SELECT Test
INFO  [2023-01-06 20:44:46,826] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test WITH SELECT SET Test
INFO  [2023-01-06 20:44:46,827] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:46,827] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:46,828] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-06 20:44:46,828] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:46,828] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-06 20:44:46,828] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_46da1f51-d10d-479b-8c3b-ec7d4dc0ac1c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_46da1f51-d10d-479b-8c3b-ec7d4dc0ac1c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_aac0b41d-c0f2-48ac-a9bc-e76298638032 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_aac0b41d-c0f2-48ac-a9bc-e76298638032 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:46,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:46,934] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:46,942] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-06 20:44:46,942] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-06 20:44:47,047] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test IMPORT TABLES
INFO  [2023-01-06 20:44:47,064] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,176] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:47,177] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:47,177] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:47,177] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-06 20:44:47,177] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000191951sINFO  [2023-01-06 20:44:47,197] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:47,197] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4273160e)
INFO  [2023-01-06 20:44:47,197] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:47,197] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@50b3f9cc)
INFO  [2023-01-06 20:44:47,197] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@1c60b410)
INFO  [2023-01-06 20:44:47,197] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@9e91322), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@19c6b5e2), dateReader=com.bakdata.conquery.util.DateReader@915fcb4, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:47,200] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:47,200] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:47,200] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:47,217] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-06 20:44:47,218] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:44:47 +0000] "POST /admin/datasets/WITH%20SELECT%20SET%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_WITH+SELECT+SET+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:47,219] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:47,221] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:47,221] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:47,225] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:47,240] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:47,240] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:47,241] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:47,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:47,243] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:47,244] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.4
WARN  [2023-01-06 20:44:47,245] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:47,245] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:47,245] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:47,246] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:47,246] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:47,351] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,356] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:47,397] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,402] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,407] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:47,414] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,415] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:47,415] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:47,523] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7714cdf3-617d-405f-862f-9cc15c315c98] in Datasets[[]]
INFO  [2023-01-06 20:44:47,533] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form WITH$20SELECT$20SET$20Test.7714cdf3-617d-405f-862f-9cc15c315c98
INFO  [2023-01-06 20:44:47,534] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form WITH$20SELECT$20SET$20Test.7714cdf3-617d-405f-862f-9cc15c315c98
INFO  [2023-01-06 20:44:47,535] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[WITH$20SELECT$20SET$20Test.7714cdf3-617d-405f-862f-9cc15c315c98] with 0 results within PT0.001779S
INFO  [2023-01-06 20:44:47,535] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=WITH$20SELECT$20SET$20Test.7714cdf3-617d-405f-862f-9cc15c315c98, workerId=WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_46da1f51-d10d-479b-8c3b-ec7d4dc0ac1c, startTime=2023-01-06T20:44:47.533688, finishTime=2023-01-06T20:44:47.535467) of size 0
INFO  [2023-01-06 20:44:47,536] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[WITH$20SELECT$20SET$20Test.7714cdf3-617d-405f-862f-9cc15c315c98] with 1 results within PT0.002007S
INFO  [2023-01-06 20:44:47,537] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=WITH$20SELECT$20SET$20Test.7714cdf3-617d-405f-862f-9cc15c315c98, workerId=WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_aac0b41d-c0f2-48ac-a9bc-e76298638032, startTime=2023-01-06T20:44:47.534072, finishTime=2023-01-06T20:44:47.536079) of size 1
INFO  [2023-01-06 20:44:47,537] com.bakdata.conquery.models.execution.ManagedExecution: DONE f6350de9-1646-43e4-8a28-1a9818e67d72 ManagedQuery within PT0.01361S
INFO  [2023-01-06 20:44:47,538] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7714cdf3-617d-405f-862f-9cc15c315c98 ManagedInternalForm within PT0.01456S
INFO  [2023-01-06 20:44:47,538] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:47,551] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test CSV TESTING: results
INFO  [2023-01-06 20:44:47,553] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast WITH SELECT SET Test
INFO  [2023-01-06 20:44:47,553] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-06 20:44:47,553] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-06 20:44:47,553] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_WITH SELECT SET Test_46da1f51-d10d-479b-8c3b-ec7d4dc0ac1c
INFO  [2023-01-06 20:44:47,554] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_WITH SELECT SET Test_aac0b41d-c0f2-48ac-a9bc-e76298638032
INFO  [2023-01-06 20:44:47,650] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow WITH SELECT SET Test
INFO  [2023-01-06 20:44:47,650] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_WITH SELECT SET Test_46da1f51-d10d-479b-8c3b-ec7d4dc0ac1c
INFO  [2023-01-06 20:44:47,650] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_WITH SELECT SET Test_aac0b41d-c0f2-48ac-a9bc-e76298638032
INFO  [2023-01-06 20:44:47,751] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of WITH$20SELECT$20SET$20Test
INFO  [2023-01-06 20:44:47,751] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,822] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test WITH SELECT SET Test
INFO  [2023-01-06 20:44:47,822] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:47,822] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:47,822] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:47,824] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-06 20:44:47,824] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:47,824] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-06 20:44:47,824] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:47,825] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,825] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_e49f8ef7-10c7-4f92-ae2e-a273d556cb5c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:47,825] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_e49f8ef7-10c7-4f92-ae2e-a273d556cb5c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:47,825] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:47,826] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_f83e0ae0-f3e2-42b0-9505-d42e37e8ba28 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:47,826] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_f83e0ae0-f3e2-42b0-9505-d42e37e8ba28 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:47,826] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:47,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,948] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:47,949] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_stamm
INFO  [2023-01-06 20:44:47,949] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_stamm
INFO  [2023-01-06 20:44:47,991] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_tage_range
INFO  [2023-01-06 20:44:47,992] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_tage_range
INFO  [2023-01-06 20:44:48,097] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-06 20:44:48,127] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,249] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:48,249] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:48,250] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:48,250] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:48,250] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-06 20:44:48,250] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.028534413sINFO  [2023-01-06 20:44:48,288] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:48,288] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@7f098531)
INFO  [2023-01-06 20:44:48,288] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@550d5f32)
INFO  [2023-01-06 20:44:48,288] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:48,288] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@34d096b1)
INFO  [2023-01-06 20:44:48,288] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@2f24cabf), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2776117b), dateReader=com.bakdata.conquery.util.DateReader@3eaff1b1, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:48,291] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:48,291] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000590357sINFO  [2023-01-06 20:44:48,310] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:48,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@2ac28937)
INFO  [2023-01-06 20:44:48,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@1e4dd99)
INFO  [2023-01-06 20:44:48,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@790be387), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@38ba8f87), dateReader=com.bakdata.conquery.util.DateReader@14ff44d7, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-06 20:44:48,312] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:48,312] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:48,312] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:48,312] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:48,330] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:48 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:48,333] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:48,334] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:48,334] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:48,337] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:48,337] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:48,338] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:48,339] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:48,340] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:48,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:48,342] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:48,342] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:48,342] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:48,342] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.6
WARN  [2023-01-06 20:44:48,342] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:48,343] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:48,351] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test.vers_tage_range
INFO  [2023-01-06 20:44:48,351] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:48,351] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:48,351] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:48 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:44:48,352] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,352] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:48,352] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:48,352] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:48,352] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.0
INFO  [2023-01-06 20:44:48,352] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.2
INFO  [2023-01-06 20:44:48,353] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.1
INFO  [2023-01-06 20:44:48,353] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.3
WARN  [2023-01-06 20:44:48,353] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:48,397] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.4
INFO  [2023-01-06 20:44:48,397] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.6
INFO  [2023-01-06 20:44:48,400] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.5
INFO  [2023-01-06 20:44:48,400] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.7
INFO  [2023-01-06 20:44:48,504] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,510] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:48,547] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,552] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,558] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:48,569] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,570] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:48,570] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:48,681] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0dfb135e-3acc-489e-ada8-5a6292169e4e] in Datasets[[]]
INFO  [2023-01-06 20:44:48,703] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test.0dfb135e-3acc-489e-ada8-5a6292169e4e
INFO  [2023-01-06 20:44:48,703] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test.0dfb135e-3acc-489e-ada8-5a6292169e4e
INFO  [2023-01-06 20:44:48,708] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test.0dfb135e-3acc-489e-ada8-5a6292169e4e] with 1 results within PT0.004629S
INFO  [2023-01-06 20:44:48,708] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test.0dfb135e-3acc-489e-ada8-5a6292169e4e] with 1 results within PT0.00463S
INFO  [2023-01-06 20:44:48,709] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test.0dfb135e-3acc-489e-ada8-5a6292169e4e, workerId=REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_e49f8ef7-10c7-4f92-ae2e-a273d556cb5c, startTime=2023-01-06T20:44:48.703525, finishTime=2023-01-06T20:44:48.708155) of size 1
INFO  [2023-01-06 20:44:48,709] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test.0dfb135e-3acc-489e-ada8-5a6292169e4e, workerId=REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_f83e0ae0-f3e2-42b0-9505-d42e37e8ba28, startTime=2023-01-06T20:44:48.703511, finishTime=2023-01-06T20:44:48.708140) of size 1
INFO  [2023-01-06 20:44:48,709] com.bakdata.conquery.models.execution.ManagedExecution: DONE 696b3d70-efd0-4386-ba52-75ee170505c4 ManagedQuery within PT0.027509S
INFO  [2023-01-06 20:44:48,711] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:48,711] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0dfb135e-3acc-489e-ada8-5a6292169e4e ManagedInternalForm within PT0.030045S
INFO  [2023-01-06 20:44:48,724] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-06 20:44:48,728] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:48,728] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-06 20:44:48,728] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-06 20:44:48,729] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test_f83e0ae0-f3e2-42b0-9505-d42e37e8ba28
INFO  [2023-01-06 20:44:48,729] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test_e49f8ef7-10c7-4f92-ae2e-a273d556cb5c
INFO  [2023-01-06 20:44:48,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test_e49f8ef7-10c7-4f92-ae2e-a273d556cb5c
INFO  [2023-01-06 20:44:48,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test_f83e0ae0-f3e2-42b0-9505-d42e37e8ba28
INFO  [2023-01-06 20:44:48,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:48,853] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test
INFO  [2023-01-06 20:44:48,853] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,976] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:48,977] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:48,977] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:48,977] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:48,978] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-06 20:44:48,978] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-06 20:44:48,978] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:48,978] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_f372111a-9923-4e01-925a-fcf880e3c5a7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_f372111a-9923-4e01-925a-fcf880e3c5a7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_83ea48f8-8cbe-43bd-9132-a9c03b821422 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_83ea48f8-8cbe-43bd-9132-a9c03b821422 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:48,980] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:49,084] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,092] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_stamm
INFO  [2023-01-06 20:44:49,092] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_stamm
INFO  [2023-01-06 20:44:49,139] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_tage_range
INFO  [2023-01-06 20:44:49,139] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_tage_range
INFO  [2023-01-06 20:44:49,249] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-06 20:44:49,264] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,376] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:49,376] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:49,377] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:49,377] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:49,377] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-06 20:44:49,377] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[1]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.033535618sINFO  [2023-01-06 20:44:49,422] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:49,422] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@6368b9b8)
INFO  [2023-01-06 20:44:49,422] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:49,422] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@32a9eaa0)
INFO  [2023-01-06 20:44:49,422] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@56711d9b)
INFO  [2023-01-06 20:44:49,422] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@20db862d), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@45d3b837), dateReader=com.bakdata.conquery.util.DateReader@7e26ba17, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:49,425] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:49,426] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[1]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000659329sINFO  [2023-01-06 20:44:49,444] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:49,444] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@50129daf)
INFO  [2023-01-06 20:44:49,444] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@4e73ce7c)
INFO  [2023-01-06 20:44:49,444] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@89c9e28), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@1f76ab89), dateReader=com.bakdata.conquery.util.DateReader@5e95f3af, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-06 20:44:49,446] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[1]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:49,446] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:49,446] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:49,446] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[1]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:49,464] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[1].vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:49 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B1%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:49,465] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:49,465] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:49,465] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:49,467] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:49,467] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:49,467] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:49,468] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:49,468] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:49,473] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:49,473] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:49,473] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.3
WARN  [2023-01-06 20:44:49,473] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:49,473] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:49,474] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:49,474] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:49,494] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[1].vers_tage_range
INFO  [2023-01-06 20:44:49,494] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:49,494] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:49,494] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:49 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B1%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:49,495] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,495] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:49,495] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:49,495] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:49,495] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.0
INFO  [2023-01-06 20:44:49,495] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.2
INFO  [2023-01-06 20:44:49,496] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.1
WARN  [2023-01-06 20:44:49,496] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:49,496] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.3
INFO  [2023-01-06 20:44:49,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.5
INFO  [2023-01-06 20:44:49,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.4
INFO  [2023-01-06 20:44:49,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.7
INFO  [2023-01-06 20:44:49,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.6
INFO  [2023-01-06 20:44:49,645] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,650] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:49,696] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,701] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,706] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:49,712] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:49,713] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:49,713] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:49,820] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[553ddf00-f4f0-4c2c-bc35-b234a496710d] in Datasets[[]]
INFO  [2023-01-06 20:44:49,826] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[1].553ddf00-f4f0-4c2c-bc35-b234a496710d
INFO  [2023-01-06 20:44:49,826] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[1].553ddf00-f4f0-4c2c-bc35-b234a496710d
INFO  [2023-01-06 20:44:49,833] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[1].553ddf00-f4f0-4c2c-bc35-b234a496710d] with 1 results within PT0.007476S
INFO  [2023-01-06 20:44:49,833] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[1].553ddf00-f4f0-4c2c-bc35-b234a496710d] with 1 results within PT0.00783S
INFO  [2023-01-06 20:44:49,834] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[1].553ddf00-f4f0-4c2c-bc35-b234a496710d, workerId=REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_83ea48f8-8cbe-43bd-9132-a9c03b821422, startTime=2023-01-06T20:44:49.826037, finishTime=2023-01-06T20:44:49.833513) of size 1
INFO  [2023-01-06 20:44:49,834] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[1].553ddf00-f4f0-4c2c-bc35-b234a496710d, workerId=REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_f372111a-9923-4e01-925a-fcf880e3c5a7, startTime=2023-01-06T20:44:49.826036, finishTime=2023-01-06T20:44:49.833866) of size 1
INFO  [2023-01-06 20:44:49,834] com.bakdata.conquery.models.execution.ManagedExecution: DONE 529aa25b-2286-4104-a6ce-68b958e15d8a ManagedQuery within PT0.013901S
INFO  [2023-01-06 20:44:49,835] com.bakdata.conquery.models.execution.ManagedExecution: DONE 553ddf00-f4f0-4c2c-bc35-b234a496710d ManagedInternalForm within PT0.014749S
INFO  [2023-01-06 20:44:49,835] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:49,862] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-06 20:44:49,863] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[1]
INFO  [2023-01-06 20:44:49,864] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-06 20:44:49,864] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-06 20:44:49,864] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[1]_83ea48f8-8cbe-43bd-9132-a9c03b821422
INFO  [2023-01-06 20:44:49,864] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[1]_f372111a-9923-4e01-925a-fcf880e3c5a7
INFO  [2023-01-06 20:44:49,888] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[1]_f372111a-9923-4e01-925a-fcf880e3c5a7
INFO  [2023-01-06 20:44:49,888] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[1]_83ea48f8-8cbe-43bd-9132-a9c03b821422
INFO  [2023-01-06 20:44:49,888] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[1]
INFO  [2023-01-06 20:44:49,896] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[1]
INFO  [2023-01-06 20:44:49,896] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,019] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:50,020] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:50,020] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:50,020] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:50,021] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-06 20:44:50,021] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-06 20:44:50,021] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:50,021] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:50,022] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,023] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_e530fc69-7159-45b9-866a-1f6788896adc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:50,023] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_e530fc69-7159-45b9-866a-1f6788896adc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:50,023] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:50,023] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_80c9ff65-e598-4e10-b961-b0f0c180f374 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:50,023] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_80c9ff65-e598-4e10-b961-b0f0c180f374 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:50,023] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:50,127] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,135] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,135] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_stamm
INFO  [2023-01-06 20:44:50,135] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_stamm
INFO  [2023-01-06 20:44:50,135] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_tage_range
INFO  [2023-01-06 20:44:50,136] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_tage_range
INFO  [2023-01-06 20:44:50,240] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-06 20:44:50,257] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,371] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:50,372] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:50,373] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:50,373] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:50,373] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-06 20:44:50,373] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[2]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.029886563sINFO  [2023-01-06 20:44:50,413] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:50,413] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@51205619)
INFO  [2023-01-06 20:44:50,413] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:50,413] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@79baf5f1)
INFO  [2023-01-06 20:44:50,413] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@208282fb)
INFO  [2023-01-06 20:44:50,413] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@349e01be), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@7b2ce378), dateReader=com.bakdata.conquery.util.DateReader@25efa443, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:50,416] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:50,416] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[2]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000610029sINFO  [2023-01-06 20:44:50,435] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:50,435] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@52858bf6)
INFO  [2023-01-06 20:44:50,435] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@153b5930)
INFO  [2023-01-06 20:44:50,435] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@32e5c371), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@6aafd71a), dateReader=com.bakdata.conquery.util.DateReader@21122e5a, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-06 20:44:50,437] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[2]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:50,437] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:50,437] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:50,437] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[2]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:50,466] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[2].vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:50 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B2%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:44:50,467] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:50,469] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:50,469] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:50,472] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:50,472] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:50,472] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:50,474] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:50,474] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:50,475] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:50,475] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:50,476] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:50,477] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:50,477] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.6
WARN  [2023-01-06 20:44:50,477] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:50,477] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:50,483] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[2].vers_tage_range
INFO  [2023-01-06 20:44:50,483] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:50 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B2%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:50,483] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:50,483] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:50,484] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,484] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:50,484] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:50,485] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:50,485] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.0
INFO  [2023-01-06 20:44:50,485] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.1
INFO  [2023-01-06 20:44:50,485] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.2
INFO  [2023-01-06 20:44:50,485] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.3
WARN  [2023-01-06 20:44:50,486] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:50,528] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.4
INFO  [2023-01-06 20:44:50,528] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.5
INFO  [2023-01-06 20:44:50,528] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.6
INFO  [2023-01-06 20:44:50,528] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.7
INFO  [2023-01-06 20:44:50,634] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,639] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:50,682] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,687] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,693] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:50,705] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:50,705] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:50,705] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:50,817] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f76a701f-3f2e-4e71-baee-2ec7ee574e3d] in Datasets[[]]
INFO  [2023-01-06 20:44:50,825] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[2].f76a701f-3f2e-4e71-baee-2ec7ee574e3d
INFO  [2023-01-06 20:44:50,825] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[2].f76a701f-3f2e-4e71-baee-2ec7ee574e3d
INFO  [2023-01-06 20:44:50,827] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[2].f76a701f-3f2e-4e71-baee-2ec7ee574e3d] with 1 results within PT0.002423S
INFO  [2023-01-06 20:44:50,828] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[2].f76a701f-3f2e-4e71-baee-2ec7ee574e3d, workerId=REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_e530fc69-7159-45b9-866a-1f6788896adc, startTime=2023-01-06T20:44:50.825061, finishTime=2023-01-06T20:44:50.827484) of size 1
INFO  [2023-01-06 20:44:50,828] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[2].f76a701f-3f2e-4e71-baee-2ec7ee574e3d] with 1 results within PT0.002747S
INFO  [2023-01-06 20:44:50,829] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[2].f76a701f-3f2e-4e71-baee-2ec7ee574e3d, workerId=REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_80c9ff65-e598-4e10-b961-b0f0c180f374, startTime=2023-01-06T20:44:50.825661, finishTime=2023-01-06T20:44:50.828408) of size 1
INFO  [2023-01-06 20:44:50,829] com.bakdata.conquery.models.execution.ManagedExecution: DONE ff801741-c80c-4df8-90a0-89c9f3a2f3d1 ManagedQuery within PT0.011892S
INFO  [2023-01-06 20:44:50,830] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:50,830] com.bakdata.conquery.models.execution.ManagedExecution: DONE f76a701f-3f2e-4e71-baee-2ec7ee574e3d ManagedInternalForm within PT0.012761S
INFO  [2023-01-06 20:44:50,842] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-06 20:44:50,845] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[2]
INFO  [2023-01-06 20:44:50,845] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-06 20:44:50,846] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-06 20:44:50,846] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[2]_e530fc69-7159-45b9-866a-1f6788896adc
INFO  [2023-01-06 20:44:50,846] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[2]_80c9ff65-e598-4e10-b961-b0f0c180f374
INFO  [2023-01-06 20:44:50,921] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[2]
INFO  [2023-01-06 20:44:50,923] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[2]_80c9ff65-e598-4e10-b961-b0f0c180f374
INFO  [2023-01-06 20:44:50,923] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[2]_e530fc69-7159-45b9-866a-1f6788896adc
INFO  [2023-01-06 20:44:50,986] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[2]
INFO  [2023-01-06 20:44:50,986] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,111] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:51,112] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:51,112] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:51,112] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:51,113] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-06 20:44:51,113] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-06 20:44:51,113] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:51,113] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:51,115] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_f15a2a1c-d5c7-42fc-8c33-1e15dec05edc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:51,115] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_f15a2a1c-d5c7-42fc-8c33-1e15dec05edc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:51,115] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:51,115] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_3e91bbe8-e870-4c52-b688-98e987270f55 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:51,115] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_3e91bbe8-e870-4c52-b688-98e987270f55 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:51,115] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:51,120] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,219] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,227] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,227] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_stamm
INFO  [2023-01-06 20:44:51,227] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_stamm
INFO  [2023-01-06 20:44:51,227] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_tage_range
INFO  [2023-01-06 20:44:51,227] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_tage_range
INFO  [2023-01-06 20:44:51,332] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-06 20:44:51,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,476] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:51,477] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:51,477] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:51,477] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:51,477] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-06 20:44:51,478] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[3]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.03610902sINFO  [2023-01-06 20:44:51,525] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:51,526] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@6b1c2cf0)
INFO  [2023-01-06 20:44:51,526] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@11ce7f5f)
INFO  [2023-01-06 20:44:51,526] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3fbb92fd)
INFO  [2023-01-06 20:44:51,526] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:51,526] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@141830a9), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@7832431a), dateReader=com.bakdata.conquery.util.DateReader@16057937, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:51,529] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[3]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:51,529] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[3]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000686831sINFO  [2023-01-06 20:44:51,547] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:51,547] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@5e37b950)
INFO  [2023-01-06 20:44:51,547] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@6b046dbd)
INFO  [2023-01-06 20:44:51,547] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@71c2042a), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@6d20af28), dateReader=com.bakdata.conquery.util.DateReader@7923a291, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-06 20:44:51,549] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[3]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:51,549] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:51,549] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[3]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:51,549] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[3]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:51,564] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[3].vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:51 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B3%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B3%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:51,566] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:51,567] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:51,567] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:51,571] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:51,571] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:51,571] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:51,573] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:51,573] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:51,574] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:51,574] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:51,575] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:51,576] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:51,576] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.6
WARN  [2023-01-06 20:44:51,576] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:51,576] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:51,589] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[3].vers_tage_range
INFO  [2023-01-06 20:44:51,589] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:51,589] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:51,589] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:51 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B3%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B3%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:44:51,590] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,590] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:51,590] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:51,591] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:51,591] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.0
INFO  [2023-01-06 20:44:51,591] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.1
INFO  [2023-01-06 20:44:51,591] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.2
INFO  [2023-01-06 20:44:51,591] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.3
WARN  [2023-01-06 20:44:51,592] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.5
INFO  [2023-01-06 20:44:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.7
INFO  [2023-01-06 20:44:51,636] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.4
INFO  [2023-01-06 20:44:51,636] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.6
INFO  [2023-01-06 20:44:51,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,746] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:51,789] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,795] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,800] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:51,808] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:51,808] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:51,808] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:51,918] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[64cc0723-7948-4d72-998d-063f81e9cfdd] in Datasets[[]]
INFO  [2023-01-06 20:44:51,923] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[3].64cc0723-7948-4d72-998d-063f81e9cfdd
INFO  [2023-01-06 20:44:51,923] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[3].64cc0723-7948-4d72-998d-063f81e9cfdd
INFO  [2023-01-06 20:44:51,928] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[3].64cc0723-7948-4d72-998d-063f81e9cfdd] with 1 results within PT0.005172S
INFO  [2023-01-06 20:44:51,929] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[3].64cc0723-7948-4d72-998d-063f81e9cfdd] with 1 results within PT0.005231S
INFO  [2023-01-06 20:44:51,929] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[3].64cc0723-7948-4d72-998d-063f81e9cfdd, workerId=REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_3e91bbe8-e870-4c52-b688-98e987270f55, startTime=2023-01-06T20:44:51.923729, finishTime=2023-01-06T20:44:51.928901) of size 1
INFO  [2023-01-06 20:44:51,929] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[3].64cc0723-7948-4d72-998d-063f81e9cfdd, workerId=REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_f15a2a1c-d5c7-42fc-8c33-1e15dec05edc, startTime=2023-01-06T20:44:51.923917, finishTime=2023-01-06T20:44:51.929148) of size 1
INFO  [2023-01-06 20:44:51,929] com.bakdata.conquery.models.execution.ManagedExecution: DONE 32064c0f-9bf5-4100-b4c3-fcfcfc260324 ManagedQuery within PT0.011097S
INFO  [2023-01-06 20:44:51,930] com.bakdata.conquery.models.execution.ManagedExecution: DONE 64cc0723-7948-4d72-998d-063f81e9cfdd ManagedInternalForm within PT0.01196S
INFO  [2023-01-06 20:44:51,930] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:51,941] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-06 20:44:51,942] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[3]
INFO  [2023-01-06 20:44:51,942] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-06 20:44:51,942] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-06 20:44:51,942] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[3]_f15a2a1c-d5c7-42fc-8c33-1e15dec05edc
INFO  [2023-01-06 20:44:51,942] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[3]_3e91bbe8-e870-4c52-b688-98e987270f55
INFO  [2023-01-06 20:44:52,014] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[3]
INFO  [2023-01-06 20:44:52,015] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[3]_f15a2a1c-d5c7-42fc-8c33-1e15dec05edc
INFO  [2023-01-06 20:44:52,015] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[3]_3e91bbe8-e870-4c52-b688-98e987270f55
INFO  [2023-01-06 20:44:52,093] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[3]
INFO  [2023-01-06 20:44:52,093] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,214] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:52,215] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:52,215] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:52,215] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:52,216] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:52,216] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:52,216] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:52,216] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:52,219] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_7e191838-9987-4435-a3fe-d14ce2eaf5a3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:52,219] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_7e191838-9987-4435-a3fe-d14ce2eaf5a3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:52,219] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:52,219] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_f500badd-3b61-4120-ac1c-c46bd574ed0e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:52,219] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_f500badd-3b61-4120-ac1c-c46bd574ed0e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:52,219] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:52,223] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,323] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[REL-EXPORT-FORM$20SECONDARY_ID.secondary]
INFO  [2023-01-06 20:44:52,323] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId REL-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-06 20:44:52,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId REL-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-06 20:44:52,431] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-06 20:44:52,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-06 20:44:52,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:52,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:52,537] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID IMPORT TABLES
INFO  [2023-01-06 20:44:52,552] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,670] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID IMPORT CONCEPTS
INFO  [2023-01-06 20:44:52,671] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:52,672] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:52,672] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:52,672] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:44:52,672] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.004915165sINFO  [2023-01-06 20:44:52,713] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:52,713] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@29b563ad)
INFO  [2023-01-06 20:44:52,713] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:52,713] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3e4c950c)
INFO  [2023-01-06 20:44:52,713] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@4ec2062e)
INFO  [2023-01-06 20:44:52,713] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@55991f29), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@493211c1), dateReader=com.bakdata.conquery.util.DateReader@11276918, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:52,716] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:52,716] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000622212sINFO  [2023-01-06 20:44:52,735] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-06 20:44:52,735] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@655ca5b8)
INFO  [2023-01-06 20:44:52,735] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:52,735] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:52,738] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:52,738] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:52,738] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:52,738] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:52,753] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:52 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+SECONDARY_ID%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:52,755] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:52,755] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:52,755] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:52,757] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:52,758] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:52,758] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:52,759] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:52,759] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:52,760] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:52,761] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:52,761] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:52,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.5
WARN  [2023-01-06 20:44:52,762] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:52,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:52,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:52,771] com.bakdata.conquery.models.jobs.ImportJob: Importing table into REL-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-06 20:44:52,779] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:52 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+SECONDARY_ID%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:44:52,779] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,780] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:52,780] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:52,780] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:52,781] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:52,781] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:52,781] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:52,781] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.table.table.0
INFO  [2023-01-06 20:44:52,887] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,892] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:52,919] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,924] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:52,933] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:52,933] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:52,933] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:53,040] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3a6492d6-5008-4005-b78a-21968f37dda1] in Datasets[[]]
INFO  [2023-01-06 20:44:53,049] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20SECONDARY_ID.3a6492d6-5008-4005-b78a-21968f37dda1
INFO  [2023-01-06 20:44:53,049] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20SECONDARY_ID.3a6492d6-5008-4005-b78a-21968f37dda1
INFO  [2023-01-06 20:44:53,054] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20SECONDARY_ID.3a6492d6-5008-4005-b78a-21968f37dda1] with 1 results within PT0.004839S
INFO  [2023-01-06 20:44:53,055] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20SECONDARY_ID.3a6492d6-5008-4005-b78a-21968f37dda1, workerId=REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_7e191838-9987-4435-a3fe-d14ce2eaf5a3, startTime=2023-01-06T20:44:53.049488, finishTime=2023-01-06T20:44:53.054327) of size 1
INFO  [2023-01-06 20:44:53,055] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20SECONDARY_ID.3a6492d6-5008-4005-b78a-21968f37dda1] with 0 results within PT0.005961S
INFO  [2023-01-06 20:44:53,056] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20SECONDARY_ID.3a6492d6-5008-4005-b78a-21968f37dda1, workerId=REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_f500badd-3b61-4120-ac1c-c46bd574ed0e, startTime=2023-01-06T20:44:53.049793, finishTime=2023-01-06T20:44:53.055754) of size 0
INFO  [2023-01-06 20:44:53,056] com.bakdata.conquery.models.execution.ManagedExecution: DONE 74cd8bdb-4352-4684-a5c1-724b8fd6a5ba ManagedQuery within PT0.01554S
INFO  [2023-01-06 20:44:53,057] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3a6492d6-5008-4005-b78a-21968f37dda1 ManagedInternalForm within PT0.016573S
INFO  [2023-01-06 20:44:53,057] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID QUERIES EXECUTED
INFO  [2023-01-06 20:44:53,074] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID CSV TESTING: results
INFO  [2023-01-06 20:44:53,076] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:53,076] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:53,076] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-06 20:44:53,076] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM SECONDARY_ID_f500badd-3b61-4120-ac1c-c46bd574ed0e
INFO  [2023-01-06 20:44:53,077] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM SECONDARY_ID_7e191838-9987-4435-a3fe-d14ce2eaf5a3
INFO  [2023-01-06 20:44:53,116] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:53,118] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM SECONDARY_ID_7e191838-9987-4435-a3fe-d14ce2eaf5a3
INFO  [2023-01-06 20:44:53,118] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM SECONDARY_ID_f500badd-3b61-4120-ac1c-c46bd574ed0e
INFO  [2023-01-06 20:44:53,181] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20SECONDARY_ID
INFO  [2023-01-06 20:44:53,181] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,239] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-06 20:44:53,239] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:53,239] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:53,239] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:53,241] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-06 20:44:53,241] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-06 20:44:53,241] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:53,241] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:53,243] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_6398d2c6-ff56-448a-96f0-13468b2743cc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:53,243] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_6398d2c6-ff56-448a-96f0-13468b2743cc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:53,243] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:53,251] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_47555195-5837-4c21-863a-fd5873a3806d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:53,251] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_47555195-5837-4c21-863a-fd5873a3806d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:53,251] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:53,252] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,361] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,368] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,368] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_stamm
INFO  [2023-01-06 20:44:53,368] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_stamm
INFO  [2023-01-06 20:44:53,368] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_tage_range
INFO  [2023-01-06 20:44:53,368] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_tage_range
INFO  [2023-01-06 20:44:53,473] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-06 20:44:53,492] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,612] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-06 20:44:53,612] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:53,613] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:53,613] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:53,613] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-06 20:44:53,613] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[4]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.066801746sINFO  [2023-01-06 20:44:53,701] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:53,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@473832c9)
INFO  [2023-01-06 20:44:53,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@7371c8e4)
INFO  [2023-01-06 20:44:53,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@49f32311)
INFO  [2023-01-06 20:44:53,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@6e0ce1f4), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@376959ed), dateReader=com.bakdata.conquery.util.DateReader@1dac99b1, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:53,703] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:53,706] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[4]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:53,706] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[4]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.001176537sINFO  [2023-01-06 20:44:53,732] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:53,732] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@408a451e)
INFO  [2023-01-06 20:44:53,732] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@540c2b4c)
INFO  [2023-01-06 20:44:53,732] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@dea0be4), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@32de429), dateReader=com.bakdata.conquery.util.DateReader@7126094, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-06 20:44:53,735] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[4]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:53,735] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:53,735] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[4]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:53,735] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL-EXPORT-FORM Test[4]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:53,753] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[4].vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:53 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B4%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B4%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:53,755] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:53,755] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:53,755] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:53,757] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:53,758] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:53,759] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:53,759] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:53,761] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.5
INFO  [2023-01-06 20:44:53,762] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:53,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:53,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.1
WARN  [2023-01-06 20:44:53,764] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:53,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:53,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:53,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:53,774] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[4].vers_tage_range
INFO  [2023-01-06 20:44:53,774] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:53,774] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:53,774] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:44:53 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B4%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL-EXPORT-FORM+Test%5B4%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:53,774] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,775] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:53,775] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:53,775] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-06 20:44:53,775] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.2
INFO  [2023-01-06 20:44:53,775] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.0
INFO  [2023-01-06 20:44:53,775] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.3
WARN  [2023-01-06 20:44:53,776] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:53,776] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.1
INFO  [2023-01-06 20:44:53,776] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.5
INFO  [2023-01-06 20:44:53,776] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.4
INFO  [2023-01-06 20:44:53,776] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.7
INFO  [2023-01-06 20:44:53,776] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.6
INFO  [2023-01-06 20:44:53,884] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,889] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:53,936] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,947] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:53,955] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:53,955] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:53,955] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-06 20:44:54,064] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4ef63e04-8099-4c93-a8ea-f7a5a66b63eb] in Datasets[[]]
INFO  [2023-01-06 20:44:54,071] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[4].4ef63e04-8099-4c93-a8ea-f7a5a66b63eb
INFO  [2023-01-06 20:44:54,072] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[4].4ef63e04-8099-4c93-a8ea-f7a5a66b63eb
INFO  [2023-01-06 20:44:54,108] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[4].4ef63e04-8099-4c93-a8ea-f7a5a66b63eb] with 1 results within PT0.036035S
INFO  [2023-01-06 20:44:54,109] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[4].4ef63e04-8099-4c93-a8ea-f7a5a66b63eb, workerId=REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_6398d2c6-ff56-448a-96f0-13468b2743cc, startTime=2023-01-06T20:44:54.072729, finishTime=2023-01-06T20:44:54.108764) of size 1
INFO  [2023-01-06 20:44:54,109] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[4].4ef63e04-8099-4c93-a8ea-f7a5a66b63eb] with 1 results within PT0.038519S
INFO  [2023-01-06 20:44:54,110] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[4].4ef63e04-8099-4c93-a8ea-f7a5a66b63eb, workerId=REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_47555195-5837-4c21-863a-fd5873a3806d, startTime=2023-01-06T20:44:54.071106, finishTime=2023-01-06T20:44:54.109625) of size 1
INFO  [2023-01-06 20:44:54,110] com.bakdata.conquery.models.execution.ManagedExecution: DONE f3f2efaf-654b-49a1-9bcb-9d39daf2bcf9 ManagedQuery within PT0.045731S
INFO  [2023-01-06 20:44:54,111] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-06 20:44:54,111] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4ef63e04-8099-4c93-a8ea-f7a5a66b63eb ManagedInternalForm within PT0.046406S
INFO  [2023-01-06 20:44:54,151] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-06 20:44:54,154] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[4]
INFO  [2023-01-06 20:44:54,154] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-06 20:44:54,154] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-06 20:44:54,159] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[4]_6398d2c6-ff56-448a-96f0-13468b2743cc
INFO  [2023-01-06 20:44:54,167] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[4]_47555195-5837-4c21-863a-fd5873a3806d
INFO  [2023-01-06 20:44:54,181] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[4]
INFO  [2023-01-06 20:44:54,184] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[4]_6398d2c6-ff56-448a-96f0-13468b2743cc
INFO  [2023-01-06 20:44:54,187] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[4]_47555195-5837-4c21-863a-fd5873a3806d
INFO  [2023-01-06 20:44:54,279] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[4]
INFO  [2023-01-06 20:44:54,279] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:54,384] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-06 20:44:54,384] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FULL_EXPORT_FORM
INFO  [2023-01-06 20:44:54,384] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:54,385] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:54,388] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-06 20:44:54,388] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-06 20:44:54,388] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:54,388] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:54,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_2dfb19a4-c6c5-4a4e-8cbf-1f9a4e70bbc6 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:54,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_2dfb19a4-c6c5-4a4e-8cbf-1f9a4e70bbc6 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:54,397] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:54,400] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_e57ccbff-90e3-4cc5-a044-f845e7dcf5d1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:54,401] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_e57ccbff-90e3-4cc5-a044-f845e7dcf5d1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:54,401] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:54,405] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:54,532] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[FULL_EXPORT_FORM.secondary]
INFO  [2023-01-06 20:44:54,533] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:54,533] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId FULL_EXPORT_FORM.secondary
INFO  [2023-01-06 20:44:54,533] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId FULL_EXPORT_FORM.secondary
INFO  [2023-01-06 20:44:54,641] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.vers_stamm
INFO  [2023-01-06 20:44:54,641] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.vers_stamm
INFO  [2023-01-06 20:44:54,641] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:54,641] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.table
INFO  [2023-01-06 20:44:54,642] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.table
INFO  [2023-01-06 20:44:54,746] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM IMPORT TABLES
INFO  [2023-01-06 20:44:54,763] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:54,872] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM IMPORT CONCEPTS
INFO  [2023-01-06 20:44:54,872] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:54,873] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:54,873] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:54,873] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:44:54,873] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FULL_EXPORT_FORM/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.002097091sINFO  [2023-01-06 20:44:54,891] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:54,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@48e407e9)
INFO  [2023-01-06 20:44:54,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:54,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@4210ee12)
INFO  [2023-01-06 20:44:54,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@69fd2c3b)
INFO  [2023-01-06 20:44:54,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@28f19d51), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@40403e88), dateReader=com.bakdata.conquery.util.DateReader@24932b3b, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-06 20:44:54,895] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FULL_EXPORT_FORM/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:54,895] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FULL_EXPORT_FORM/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000385256sINFO  [2023-01-06 20:44:54,912] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-06 20:44:54,912] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@45205e3f)
INFO  [2023-01-06 20:44:54,912] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:54,912] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:54,916] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FULL_EXPORT_FORM/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:54,916] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:44:54,916] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FULL_EXPORT_FORM/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:54,916] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FULL_EXPORT_FORM/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:54,936] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into FULL_EXPORT_FORM.vers_stamm
127.0.0.1 - - [06/Jan/2023:20:44:54 +0000] "POST /admin/datasets/FULL_EXPORT_FORM/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_FULL_EXPORT_FORM%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:44:54,938] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:54,940] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:54,940] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:54,944] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:44:54,944] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:54,945] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-06 20:44:54,947] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.0
INFO  [2023-01-06 20:44:54,947] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.1
INFO  [2023-01-06 20:44:54,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.2
INFO  [2023-01-06 20:44:54,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.3
INFO  [2023-01-06 20:44:54,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.4
INFO  [2023-01-06 20:44:54,948] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.5
WARN  [2023-01-06 20:44:54,948] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:54,949] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.6
INFO  [2023-01-06 20:44:54,949] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.7
INFO  [2023-01-06 20:44:54,952] com.bakdata.conquery.models.jobs.ImportJob: Importing table into FULL_EXPORT_FORM.table
INFO  [2023-01-06 20:44:54,952] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:44:54 +0000] "POST /admin/datasets/FULL_EXPORT_FORM/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_FULL_EXPORT_FORM%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:44:54,953] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:54,953] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:54,953] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:54,953] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:54,954] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.table.table], containing 6 entries.
WARN  [2023-01-06 20:44:54,954] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:54,954] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.table.table], containing 6 entries.
INFO  [2023-01-06 20:44:54,954] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.table.table.0
INFO  [2023-01-06 20:44:55,059] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,064] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM IMPORT TABLE CONTENTS
INFO  [2023-01-06 20:44:55,069] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,075] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM PARSE JSON FORM DESCRIPTION
INFO  [2023-01-06 20:44:55,084] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,085] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:55,085] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:44:55,197] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b699ec4a-17e9-41b1-8899-d3941eb916d2] in Datasets[[]]
INFO  [2023-01-06 20:44:55,209] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form FULL_EXPORT_FORM.b699ec4a-17e9-41b1-8899-d3941eb916d2
INFO  [2023-01-06 20:44:55,209] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form FULL_EXPORT_FORM.b699ec4a-17e9-41b1-8899-d3941eb916d2
INFO  [2023-01-06 20:44:55,218] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FULL_EXPORT_FORM.b699ec4a-17e9-41b1-8899-d3941eb916d2] with 0 results within PT0.009022S
INFO  [2023-01-06 20:44:55,219] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FULL_EXPORT_FORM.b699ec4a-17e9-41b1-8899-d3941eb916d2] with 1 results within PT0.009282S
INFO  [2023-01-06 20:44:55,219] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FULL_EXPORT_FORM.b699ec4a-17e9-41b1-8899-d3941eb916d2, workerId=FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_2dfb19a4-c6c5-4a4e-8cbf-1f9a4e70bbc6, startTime=2023-01-06T20:44:55.209868, finishTime=2023-01-06T20:44:55.218890) of size 0
INFO  [2023-01-06 20:44:55,220] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FULL_EXPORT_FORM.b699ec4a-17e9-41b1-8899-d3941eb916d2, workerId=FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_e57ccbff-90e3-4cc5-a044-f845e7dcf5d1, startTime=2023-01-06T20:44:55.209865, finishTime=2023-01-06T20:44:55.219147) of size 1
INFO  [2023-01-06 20:44:55,220] com.bakdata.conquery.models.execution.ManagedExecution: DONE 404b3802-4a29-4e5b-918b-21c2ef9d5076 ManagedQuery within PT0.02305S
INFO  [2023-01-06 20:44:55,221] com.bakdata.conquery.models.execution.ManagedExecution: DONE b699ec4a-17e9-41b1-8899-d3941eb916d2 ManagedInternalForm within PT0.024117S
INFO  [2023-01-06 20:44:55,221] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM QUERIES EXECUTED
INFO  [2023-01-06 20:44:55,242] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM CSV TESTING: results
INFO  [2023-01-06 20:44:55,243] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FULL_EXPORT_FORM
INFO  [2023-01-06 20:44:55,243] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-06 20:44:55,243] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-06 20:44:55,244] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FULL_EXPORT_FORM_2dfb19a4-c6c5-4a4e-8cbf-1f9a4e70bbc6
INFO  [2023-01-06 20:44:55,244] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FULL_EXPORT_FORM_e57ccbff-90e3-4cc5-a044-f845e7dcf5d1
INFO  [2023-01-06 20:44:55,341] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FULL_EXPORT_FORM_e57ccbff-90e3-4cc5-a044-f845e7dcf5d1
INFO  [2023-01-06 20:44:55,341] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FULL_EXPORT_FORM
INFO  [2023-01-06 20:44:55,341] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FULL_EXPORT_FORM_2dfb19a4-c6c5-4a4e-8cbf-1f9a4e70bbc6
INFO  [2023-01-06 20:44:55,354] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FULL_EXPORT_FORM
INFO  [2023-01-06 20:44:55,354] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,490] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FULL_EXPORT_FORM
INFO  [2023-01-06 20:44:55,491] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS_EXPORT Test
INFO  [2023-01-06 20:44:55,491] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:55,491] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:55,492] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-06 20:44:55,492] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-06 20:44:55,492] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:55,492] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:55,495] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_3c18c06f-9baf-46f4-9840-d549498f3ac0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:55,495] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_3c18c06f-9baf-46f4-9840-d549498f3ac0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:55,495] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:55,495] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_b84ab7b4-551c-4bec-909a-7676cd5929dd are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:55,495] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_b84ab7b4-551c-4bec-909a-7676cd5929dd are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:55,495] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:55,499] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,600] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,607] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,607] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS_EXPORT$20Test.test_table
INFO  [2023-01-06 20:44:55,607] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS_EXPORT$20Test.test_table
INFO  [2023-01-06 20:44:55,724] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,834] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:55,835] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:55,835] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 183 B in total
INFO  [2023-01-06 20:44:55,835] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS_EXPORT Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00021375sINFO  [2023-01-06 20:44:55,857] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=9, min=1, average=4.500000, max=8}
INFO  [2023-01-06 20:44:55,857] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=A, suffix=)
INFO  [2023-01-06 20:44:55,857] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=11109, maxValue=12935), dateReader=com.bakdata.conquery.util.DateReader@215ccd3c)
INFO  [2023-01-06 20:44:55,860] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS_EXPORT Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:55,860] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:55,860] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ABS_EXPORT Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:55,882] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into ABS_EXPORT$20Test.test_table
127.0.0.1 - - [06/Jan/2023:20:44:55 +0000] "POST /admin/datasets/ABS_EXPORT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ABS_EXPORT+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:55,883] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:55,884] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:55,885] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:55,885] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:55,887] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:55,888] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS_EXPORT$20Test.test_table.test_table], containing 9 entries.
INFO  [2023-01-06 20:44:55,888] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS_EXPORT$20Test.test_table.test_table], containing 9 entries.
WARN  [2023-01-06 20:44:55,889] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:55,889] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS_EXPORT$20Test.test_table.test_table.0
INFO  [2023-01-06 20:44:55,994] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,000] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,021] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,022] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:56,128] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: ABS_EXPORT Test QUERY INIT
INFO  [2023-01-06 20:44:56,150] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ABS_EXPORT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:56,151] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[01941688-fa73-45e4-a244-7c6b01972c9d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ABS_EXPORT Test))]]
INFO  [2023-01-06 20:44:56,157] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started AbsoluteFormQuery ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d
INFO  [2023-01-06 20:44:56,157] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started AbsoluteFormQuery ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d
WARN  [2023-01-06 20:44:56,158] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:44:56,158] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d] with 0 results within PT0.001277S
INFO  [2023-01-06 20:44:56,159] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d, workerId=ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_b84ab7b4-551c-4bec-909a-7676cd5929dd, startTime=2023-01-06T20:44:56.157475, finishTime=2023-01-06T20:44:56.158752) of size 0
INFO  [2023-01-06 20:44:56,160] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d] with 1 results within PT0.002617S
127.0.0.1 - - [06/Jan/2023:20:44:56 +0000] "POST /api/datasets/ABS_EXPORT$20Test/queries HTTP/1.1" 201 2252 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:44:56,160] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d, workerId=ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_3c18c06f-9baf-46f4-9840-d549498f3ac0, startTime=2023-01-06T20:44:56.157418, finishTime=2023-01-06T20:44:56.160035) of size 1
INFO  [2023-01-06 20:44:56,160] com.bakdata.conquery.models.execution.ManagedExecution: DONE 01941688-fa73-45e4-a244-7c6b01972c9d ManagedQuery within PT0.009353S
127.0.0.1 - - [06/Jan/2023:20:44:56 +0000] "GET /api/datasets/ABS_EXPORT$20Test/queries/ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d HTTP/1.1" 200 2511 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:44:56,198] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ABS_EXPORT Test], queryId=01941688-fa73-45e4-a244-7c6b01972c9d, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:44:56.151265, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7f0bc8b3[Count = 0], startTime=2023-01-06T20:44:56.151572, finishTime=2023-01-06T20:44:56.160925, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@664bd32b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ABS_EXPORT Test)), query=com.bakdata.conquery.models.forms.managed.AbsoluteFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4eea0c7a, com.bakdata.conquery.models.query.ColumnDescriptor@614963a1, com.bakdata.conquery.models.query.ColumnDescriptor@4026616b, com.bakdata.conquery.models.query.ColumnDescriptor@78cc20cf, com.bakdata.conquery.models.query.ColumnDescriptor@66db6a70]) download on dataset Dataset[label=null, name=ABS_EXPORT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:56,198] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ABS_EXPORT Test], queryId=01941688-fa73-45e4-a244-7c6b01972c9d, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:44:56.151265, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7f0bc8b3[Count = 0], startTime=2023-01-06T20:44:56.151572, finishTime=2023-01-06T20:44:56.160925, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@664bd32b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ABS_EXPORT Test)), query=com.bakdata.conquery.models.forms.managed.AbsoluteFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4eea0c7a, com.bakdata.conquery.models.query.ColumnDescriptor@614963a1, com.bakdata.conquery.models.query.ColumnDescriptor@4026616b, com.bakdata.conquery.models.query.ColumnDescriptor@78cc20cf, com.bakdata.conquery.models.query.ColumnDescriptor@66db6a70]) on dataset Dataset[label=null, name=ABS_EXPORT Test]
127.0.0.1 - - [06/Jan/2023:20:44:56 +0000] "GET /api/datasets/ABS_EXPORT%20Test/result/ABS_EXPORT$20Test.01941688-fa73-45e4-a244-7c6b01972c9d.csv?pretty=false HTTP/1.1" 200 199 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:44:56,219] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest ABS_EXPORT Test on 5 rows
INFO  [2023-01-06 20:44:56,220] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS_EXPORT Test
INFO  [2023-01-06 20:44:56,220] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-06 20:44:56,220] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS_EXPORT Test_b84ab7b4-551c-4bec-909a-7676cd5929dd
INFO  [2023-01-06 20:44:56,220] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-06 20:44:56,220] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS_EXPORT Test_3c18c06f-9baf-46f4-9840-d549498f3ac0
INFO  [2023-01-06 20:44:56,292] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS_EXPORT Test
INFO  [2023-01-06 20:44:56,293] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS_EXPORT Test_3c18c06f-9baf-46f4-9840-d549498f3ac0
INFO  [2023-01-06 20:44:56,293] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS_EXPORT Test_b84ab7b4-551c-4bec-909a-7676cd5929dd
INFO  [2023-01-06 20:44:56,389] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS_EXPORT$20Test
INFO  [2023-01-06 20:44:56,389] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,428] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS_EXPORT Test
INFO  [2023-01-06 20:44:56,428] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:56,428] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:56,428] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:56,429] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:56,429] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:56,429] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:56,429] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:56,431] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,431] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_dcf809ab-974f-46a0-804d-c14d6aa85e6a are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:56,431] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_dcf809ab-974f-46a0-804d-c14d6aa85e6a are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:56,431] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:56,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_a63cee29-438a-4803-9cff-bc334e2ee0a1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:56,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_a63cee29-438a-4803-9cff-bc334e2ee0a1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:56,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:56,535] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,542] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,542] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ARRAY_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:56,542] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ARRAY_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:56,664] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:56,783] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:56,783] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:56,783] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 137 B in total
INFO  [2023-01-06 20:44:56,783] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ARRAY_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00085315sINFO  [2023-01-06 20:44:56,869] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=7, min=1, average=1.750000, max=4}
INFO  [2023-01-06 20:44:56,869] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:56,869] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7be2c8ee)
INFO  [2023-01-06 20:44:56,872] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ARRAY_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:56,872] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:56,872] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_ARRAY_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:56,898] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into ARRAY_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:56,898] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:44:56 +0000] "POST /admin/datasets/ARRAY_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_ARRAY_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:56,899] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:56,900] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:56,900] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:56,903] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:44:56,903] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ARRAY_CONCEPT_QUERY$20Test.table1.table1], containing 7 entries.
INFO  [2023-01-06 20:44:56,904] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ARRAY_CONCEPT_QUERY$20Test.table1.table1], containing 7 entries.
WARN  [2023-01-06 20:44:56,905] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:56,905] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ARRAY_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:44:56,905] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ARRAY_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:44:57,010] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,015] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,026] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,027] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:57,027] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:57,132] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: ARRAY_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:44:57,150] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ARRAY_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:57,150] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a432176c-1447-4f29-af48-9e9d1f4f1f5b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ARRAY_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:44:57,155] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b
INFO  [2023-01-06 20:44:57,155] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b
INFO  [2023-01-06 20:44:57,156] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b] with 1 results within PT0.001496S
INFO  [2023-01-06 20:44:57,157] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b] with 3 results within PT0.001647S
INFO  [2023-01-06 20:44:57,157] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b, workerId=ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_a63cee29-438a-4803-9cff-bc334e2ee0a1, startTime=2023-01-06T20:44:57.155212, finishTime=2023-01-06T20:44:57.156708) of size 1
127.0.0.1 - - [06/Jan/2023:20:44:57 +0000] "POST /api/datasets/ARRAY_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 2161 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:44:57,157] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b, workerId=ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_dcf809ab-974f-46a0-804d-c14d6aa85e6a, startTime=2023-01-06T20:44:57.155373, finishTime=2023-01-06T20:44:57.157020) of size 3
INFO  [2023-01-06 20:44:57,157] com.bakdata.conquery.models.execution.ManagedExecution: DONE a432176c-1447-4f29-af48-9e9d1f4f1f5b ManagedQuery within PT0.006986S
127.0.0.1 - - [06/Jan/2023:20:44:57 +0000] "GET /api/datasets/ARRAY_CONCEPT_QUERY$20Test/queries/ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b HTTP/1.1" 200 2456 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:57,187] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test], queryId=a432176c-1447-4f29-af48-9e9d1f4f1f5b, label=select	@§$, creationTime=2023-01-06T20:44:57.150676, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7540314d[Count = 0], startTime=2023-01-06T20:44:57.150928, finishTime=2023-01-06T20:44:57.157914, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@29fb829e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ARRAY_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6667fa8, com.bakdata.conquery.models.query.ColumnDescriptor@1fd2cf8d, com.bakdata.conquery.models.query.ColumnDescriptor@6bec5f63, com.bakdata.conquery.models.query.ColumnDescriptor@1ef6dff2]) download on dataset Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:57,187] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test], queryId=a432176c-1447-4f29-af48-9e9d1f4f1f5b, label=select	@§$, creationTime=2023-01-06T20:44:57.150676, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7540314d[Count = 0], startTime=2023-01-06T20:44:57.150928, finishTime=2023-01-06T20:44:57.157914, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@29fb829e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ARRAY_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6667fa8, com.bakdata.conquery.models.query.ColumnDescriptor@1fd2cf8d, com.bakdata.conquery.models.query.ColumnDescriptor@6bec5f63, com.bakdata.conquery.models.query.ColumnDescriptor@1ef6dff2]) on dataset Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:44:57 +0000] "GET /api/datasets/ARRAY_CONCEPT_QUERY%20Test/result/ARRAY_CONCEPT_QUERY$20Test.a432176c-1447-4f29-af48-9e9d1f4f1f5b.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:44:57,205] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest ARRAY_CONCEPT_QUERY Test on 5 rows
INFO  [2023-01-06 20:44:57,205] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:57,205] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:57,205] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:57,205] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ARRAY_CONCEPT_QUERY Test_a63cee29-438a-4803-9cff-bc334e2ee0a1
INFO  [2023-01-06 20:44:57,206] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ARRAY_CONCEPT_QUERY Test_dcf809ab-974f-46a0-804d-c14d6aa85e6a
INFO  [2023-01-06 20:44:57,230] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:57,231] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ARRAY_CONCEPT_QUERY Test_dcf809ab-974f-46a0-804d-c14d6aa85e6a
INFO  [2023-01-06 20:44:57,231] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ARRAY_CONCEPT_QUERY Test_a63cee29-438a-4803-9cff-bc334e2ee0a1
INFO  [2023-01-06 20:44:57,305] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ARRAY_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:44:57,305] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,432] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:57,432] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:57,432] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:57,433] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:57,433] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:57,433] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:57,433] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:57,434] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:57,435] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6df281f6-4937-48c0-8108-de8c6db7841e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:57,435] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6df281f6-4937-48c0-8108-de8c6db7841e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:57,436] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:57,436] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e3051f9d-f8c3-4e3e-beb5-309d09e669df are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:57,436] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e3051f9d-f8c3-4e3e-beb5-309d09e669df are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:57,436] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:57,439] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,539] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,546] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,547] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:57,547] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:57,664] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,774] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:57,775] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:57,775] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:44:57,775] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000291427sINFO  [2023-01-06 20:44:57,804] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:57,804] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:57,804] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4e81ca95)
INFO  [2023-01-06 20:44:57,807] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:57,807] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:57,807] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:57,830] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:57,831] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:44:57 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:44:57,832] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:57,833] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:57,833] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:57,836] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:44:57,836] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:44:57,837] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:44:57,838] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
WARN  [2023-01-06 20:44:57,838] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:57,838] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:44:57,838] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:44:57,943] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,949] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,969] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:57,969] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:57,969] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:58,075] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:44:58,094] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:58,095] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[67c4c27a-13fa-4aea-aeaa-816352141d1a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:44:58,101] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a
INFO  [2023-01-06 20:44:58,101] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a
127.0.0.1 - - [06/Jan/2023:20:44:58 +0000] "POST /api/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1579 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:44:58,103] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a] with 0 results within PT0.001785S
INFO  [2023-01-06 20:44:58,103] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a] with 1 results within PT0.002148S
INFO  [2023-01-06 20:44:58,104] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a, workerId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6df281f6-4937-48c0-8108-de8c6db7841e, startTime=2023-01-06T20:44:58.101566, finishTime=2023-01-06T20:44:58.103351) of size 0
INFO  [2023-01-06 20:44:58,104] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a, workerId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e3051f9d-f8c3-4e3e-beb5-309d09e669df, startTime=2023-01-06T20:44:58.101506, finishTime=2023-01-06T20:44:58.103654) of size 1
INFO  [2023-01-06 20:44:58,104] com.bakdata.conquery.models.execution.ManagedExecution: DONE 67c4c27a-13fa-4aea-aeaa-816352141d1a ManagedQuery within PT0.009186S
127.0.0.1 - - [06/Jan/2023:20:44:58 +0000] "GET /api/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a HTTP/1.1" 200 1998 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:58,135] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=67c4c27a-13fa-4aea-aeaa-816352141d1a, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:44:58.095094, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@263c139d[Count = 0], startTime=2023-01-06T20:44:58.095341, finishTime=2023-01-06T20:44:58.104527, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@148eb09d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3bc7c84b, com.bakdata.conquery.models.query.ColumnDescriptor@2c661746]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:58,136] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=67c4c27a-13fa-4aea-aeaa-816352141d1a, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:44:58.095094, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@263c139d[Count = 0], startTime=2023-01-06T20:44:58.095341, finishTime=2023-01-06T20:44:58.104527, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@148eb09d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3bc7c84b, com.bakdata.conquery.models.query.ColumnDescriptor@2c661746]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:44:58 +0000] "GET /api/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.67c4c27a-13fa-4aea-aeaa-816352141d1a.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:44:58,154] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 2 rows
INFO  [2023-01-06 20:44:58,154] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:58,155] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:58,155] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:58,155] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_6df281f6-4937-48c0-8108-de8c6db7841e
INFO  [2023-01-06 20:44:58,155] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_e3051f9d-f8c3-4e3e-beb5-309d09e669df
INFO  [2023-01-06 20:44:58,234] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:58,235] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_6df281f6-4937-48c0-8108-de8c6db7841e
INFO  [2023-01-06 20:44:58,235] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_e3051f9d-f8c3-4e3e-beb5-309d09e669df
INFO  [2023-01-06 20:44:58,253] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:44:58,253] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,375] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:58,376] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:58,376] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:58,376] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:58,377] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:58,377] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:58,377] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:58,377] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:58,379] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_b45da2b4-2908-4c77-975e-c224dca5c2fc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:58,379] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_b45da2b4-2908-4c77-975e-c224dca5c2fc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:58,379] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:58,379] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e2a2bd27-9146-4c8e-aff8-a99ff3dc03cc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:58,379] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e2a2bd27-9146-4c8e-aff8-a99ff3dc03cc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:58,379] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:58,383] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,483] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,490] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,491] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:58,491] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:44:58,603] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,715] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:58,715] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:58,715] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:44:58,715] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000376263sINFO  [2023-01-06 20:44:58,754] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:58,754] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:44:58,754] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4e9dbb8a)
INFO  [2023-01-06 20:44:58,756] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:58,757] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:58,757] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:58,770] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:44:58 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:44:58,771] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,772] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:58,774] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:58,774] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:58,777] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:44:58,777] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:44:58,777] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:44:58,778] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
WARN  [2023-01-06 20:44:58,779] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:58,779] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:44:58,779] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:44:58,884] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,889] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,906] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:58,906] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:58,906] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:59,012] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:44:59,043] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:59,044] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[8c00a590-212b-4e0e-83bb-b2fd3c5165c6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:44:59,050] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6
INFO  [2023-01-06 20:44:59,050] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6
INFO  [2023-01-06 20:44:59,051] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6] with 3 results within PT0.000955S
INFO  [2023-01-06 20:44:59,051] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6] with 4 results within PT0.00122S
INFO  [2023-01-06 20:44:59,052] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6, workerId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e2a2bd27-9146-4c8e-aff8-a99ff3dc03cc, startTime=2023-01-06T20:44:59.050444, finishTime=2023-01-06T20:44:59.051399) of size 3
INFO  [2023-01-06 20:44:59,052] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6, workerId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_b45da2b4-2908-4c77-975e-c224dca5c2fc, startTime=2023-01-06T20:44:59.050315, finishTime=2023-01-06T20:44:59.051535) of size 4
INFO  [2023-01-06 20:44:59,052] com.bakdata.conquery.models.execution.ManagedExecution: DONE 8c00a590-212b-4e0e-83bb-b2fd3c5165c6 ManagedQuery within PT0.007345S
127.0.0.1 - - [06/Jan/2023:20:44:59 +0000] "POST /api/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1655 "-" "Conquery (test client)" 12
127.0.0.1 - - [06/Jan/2023:20:44:59 +0000] "GET /api/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6 HTTP/1.1" 200 2110 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:44:59,073] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=8c00a590-212b-4e0e-83bb-b2fd3c5165c6, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:44:59.044159, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@91ef8d7[Count = 0], startTime=2023-01-06T20:44:59.044850, finishTime=2023-01-06T20:44:59.052195, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1a4a36dd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@632f1b7e, com.bakdata.conquery.models.query.ColumnDescriptor@3ef58dd1]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:44:59,073] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=8c00a590-212b-4e0e-83bb-b2fd3c5165c6, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:44:59.044159, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@91ef8d7[Count = 0], startTime=2023-01-06T20:44:59.044850, finishTime=2023-01-06T20:44:59.052195, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1a4a36dd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@632f1b7e, com.bakdata.conquery.models.query.ColumnDescriptor@3ef58dd1]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:44:59 +0000] "GET /api/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.8c00a590-212b-4e0e-83bb-b2fd3c5165c6.csv?pretty=false HTTP/1.1" 200 48 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:44:59,090] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 8 rows
INFO  [2023-01-06 20:44:59,090] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:59,091] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:59,091] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:44:59,091] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_e2a2bd27-9146-4c8e-aff8-a99ff3dc03cc
INFO  [2023-01-06 20:44:59,091] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_b45da2b4-2908-4c77-975e-c224dca5c2fc
INFO  [2023-01-06 20:44:59,189] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:59,189] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_b45da2b4-2908-4c77-975e-c224dca5c2fc
INFO  [2023-01-06 20:44:59,189] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_e2a2bd27-9146-4c8e-aff8-a99ff3dc03cc
INFO  [2023-01-06 20:44:59,289] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:44:59,289] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,312] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:44:59,312] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-06 20:44:59,313] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:44:59,313] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:44:59,314] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-06 20:44:59,314] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-06 20:44:59,314] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:59,314] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:44:59,315] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,316] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_826a3e3e-a679-4623-84b5-aaa835a63b96 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:59,316] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_826a3e3e-a679-4623-84b5-aaa835a63b96 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:59,316] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:59,316] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_a999a46c-88a4-4825-8958-5d6e653b3050 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:44:59,316] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_a999a46c-88a4-4825-8958-5d6e653b3050 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:44:59,316] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:44:59,419] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,426] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,427] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose
INFO  [2023-01-06 20:44:59,427] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose
INFO  [2023-01-06 20:44:59,538] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,652] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:44:59,652] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:44:59,653] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 136 B in total
INFO  [2023-01-06 20:44:59,653] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COMMON_CONCEPT_ICD_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000346158sINFO  [2023-01-06 20:44:59,688] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=3, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:44:59,688] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17256, maxValue=17347), dateReader=com.bakdata.conquery.util.DateReader@324bc46a)
INFO  [2023-01-06 20:44:59,688] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[aufnahmedatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17167, maxValue=17257), dateReader=com.bakdata.conquery.util.DateReader@60f9777a)
INFO  [2023-01-06 20:44:59,688] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=3, nullLines=0), encoding=null, prefix=F2, suffix=)
INFO  [2023-01-06 20:44:59,691] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COMMON_CONCEPT_ICD_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:59,691] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:44:59,691] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COMMON_CONCEPT_ICD_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:44:59,707] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose
127.0.0.1 - - [06/Jan/2023:20:44:59 +0000] "POST /admin/datasets/COMMON_CONCEPT_ICD_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COMMON_CONCEPT_ICD_QUERY+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:44:59,707] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,709] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:44:59,710] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:44:59,710] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:44:59,715] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:44:59,715] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
INFO  [2023-01-06 20:44:59,715] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
WARN  [2023-01-06 20:44:59,716] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:44:59,717] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-06 20:44:59,822] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,827] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,839] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:44:59,839] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:44:59,980] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COMMON_CONCEPT_ICD_QUERY Test QUERY INIT
INFO  [2023-01-06 20:44:59,990] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COMMON_CONCEPT_ICD_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:44:59,990] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e87b180f-06bf-4be9-bda4-c9dd752c0f8e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COMMON_CONCEPT_ICD_QUERY Test))]]
INFO  [2023-01-06 20:44:59,993] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e
INFO  [2023-01-06 20:44:59,993] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e
WARN  [2023-01-06 20:44:59,993] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:44:59,993] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e] with 0 results within PT0.000116S
INFO  [2023-01-06 20:44:59,993] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e, workerId=COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_a999a46c-88a4-4825-8958-5d6e653b3050, startTime=2023-01-06T20:44:59.993159, finishTime=2023-01-06T20:44:59.993275) of size 0
127.0.0.1 - - [06/Jan/2023:20:44:59 +0000] "POST /api/datasets/COMMON_CONCEPT_ICD_QUERY$20Test/queries HTTP/1.1" 201 1306 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:44:59,993] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e] with 2 results within PT0.000833S
INFO  [2023-01-06 20:44:59,994] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e, workerId=COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_826a3e3e-a679-4623-84b5-aaa835a63b96, startTime=2023-01-06T20:44:59.993159, finishTime=2023-01-06T20:44:59.993992) of size 2
INFO  [2023-01-06 20:44:59,994] com.bakdata.conquery.models.execution.ManagedExecution: DONE e87b180f-06bf-4be9-bda4-c9dd752c0f8e ManagedQuery within PT0.0035S
127.0.0.1 - - [06/Jan/2023:20:45:00 +0000] "GET /api/datasets/COMMON_CONCEPT_ICD_QUERY$20Test/queries/COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e HTTP/1.1" 200 1621 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:00,012] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test], queryId=e87b180f-06bf-4be9-bda4-c9dd752c0f8e, label=F20	@§$, creationTime=2023-01-06T20:44:59.990745, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7032868c[Count = 0], startTime=2023-01-06T20:44:59.990913, finishTime=2023-01-06T20:44:59.994413, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b1c0425), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMMON_CONCEPT_ICD_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@f8b0b86, com.bakdata.conquery.models.query.ColumnDescriptor@3afae87b]) download on dataset Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:00,012] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test], queryId=e87b180f-06bf-4be9-bda4-c9dd752c0f8e, label=F20	@§$, creationTime=2023-01-06T20:44:59.990745, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7032868c[Count = 0], startTime=2023-01-06T20:44:59.990913, finishTime=2023-01-06T20:44:59.994413, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b1c0425), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMMON_CONCEPT_ICD_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@f8b0b86, com.bakdata.conquery.models.query.ColumnDescriptor@3afae87b]) on dataset Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:00 +0000] "GET /api/datasets/COMMON_CONCEPT_ICD_QUERY%20Test/result/COMMON_CONCEPT_ICD_QUERY$20Test.e87b180f-06bf-4be9-bda4-c9dd752c0f8e.csv?pretty=false HTTP/1.1" 200 66 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:00,028] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COMMON_CONCEPT_ICD_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:00,028] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-06 20:45:00,028] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-06 20:45:00,028] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-06 20:45:00,028] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMMON_CONCEPT_ICD_QUERY Test_a999a46c-88a4-4825-8958-5d6e653b3050
INFO  [2023-01-06 20:45:00,028] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMMON_CONCEPT_ICD_QUERY Test_826a3e3e-a679-4623-84b5-aaa835a63b96
INFO  [2023-01-06 20:45:00,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-06 20:45:00,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMMON_CONCEPT_ICD_QUERY Test_a999a46c-88a4-4825-8958-5d6e653b3050
INFO  [2023-01-06 20:45:00,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMMON_CONCEPT_ICD_QUERY Test_826a3e3e-a679-4623-84b5-aaa835a63b96
INFO  [2023-01-06 20:45:00,226] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COMMON_CONCEPT_ICD_QUERY$20Test
INFO  [2023-01-06 20:45:00,227] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,248] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-06 20:45:00,249] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COMPOUND_DATERANGE Test
INFO  [2023-01-06 20:45:00,249] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:00,249] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:00,251] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-06 20:45:00,251] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-06 20:45:00,251] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:00,251] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_3fef357f-d680-4bea-a9e1-67136dd60bd2 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_3fef357f-d680-4bea-a9e1-67136dd60bd2 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_cf22d8a6-45e7-487e-8433-06360d846151 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_cf22d8a6-45e7-487e-8433-06360d846151 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:00,255] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:00,358] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,366] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,367] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMPOUND_DATERANGE$20Test.test_table
INFO  [2023-01-06 20:45:00,367] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMPOUND_DATERANGE$20Test.test_table
INFO  [2023-01-06 20:45:00,484] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,595] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:00,595] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:00,596] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 213 B in total
INFO  [2023-01-06 20:45:00,596] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COMPOUND_DATERANGE Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000396505sINFO  [2023-01-06 20:45:00,636] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=9, sum=9, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:00,636] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlung_ende] with DateParser(super=Parser(lines=9, nullLines=2), subType=IntegerParser(super=Parser(lines=9, nullLines=2), minValue=14958, maxValue=16139), dateReader=com.bakdata.conquery.util.DateReader@5cd7ae0c)
INFO  [2023-01-06 20:45:00,636] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlung_start] with DateParser(super=Parser(lines=9, nullLines=2), subType=IntegerParser(super=Parser(lines=9, nullLines=2), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@9efaa48)
INFO  [2023-01-06 20:45:00,636] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlung] with CompoundDateRangeParser(super=Parser(lines=9, nullLines=0), startColumn=behandlung_start, endColumn=behandlung_ende)
INFO  [2023-01-06 20:45:00,639] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COMPOUND_DATERANGE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:00,639] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:00,639] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COMPOUND_DATERANGE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:00,662] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into COMPOUND_DATERANGE$20Test.test_table
INFO  [2023-01-06 20:45:00,663] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:00 +0000] "POST /admin/datasets/COMPOUND_DATERANGE%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COMPOUND_DATERANGE+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:00,664] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:00,664] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:00,664] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:00,667] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:00,669] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMPOUND_DATERANGE$20Test.test_table.test_table], containing 9 entries.
INFO  [2023-01-06 20:45:00,669] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMPOUND_DATERANGE$20Test.test_table.test_table], containing 9 entries.
WARN  [2023-01-06 20:45:00,669] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:00,669] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMPOUND_DATERANGE$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:00,670] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMPOUND_DATERANGE$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:00,670] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMPOUND_DATERANGE$20Test.test_table.test_table.2
INFO  [2023-01-06 20:45:00,775] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,780] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,793] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:00,794] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:00,794] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:00,900] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COMPOUND_DATERANGE Test QUERY INIT
INFO  [2023-01-06 20:45:00,917] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COMPOUND_DATERANGE$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:00,918] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5c93515b-b46b-4712-abf9-1708180d9b4b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COMPOUND_DATERANGE Test))]]
INFO  [2023-01-06 20:45:00,921] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b
INFO  [2023-01-06 20:45:00,921] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b
INFO  [2023-01-06 20:45:00,922] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b] with 3 results within PT0.001415S
127.0.0.1 - - [06/Jan/2023:20:45:00 +0000] "POST /api/datasets/COMPOUND_DATERANGE$20Test/queries HTTP/1.1" 201 1113 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:00,923] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b] with 4 results within PT0.001894S
INFO  [2023-01-06 20:45:00,923] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b, workerId=COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_3fef357f-d680-4bea-a9e1-67136dd60bd2, startTime=2023-01-06T20:45:00.921278, finishTime=2023-01-06T20:45:00.922693) of size 3
INFO  [2023-01-06 20:45:00,923] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b, workerId=COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_cf22d8a6-45e7-487e-8433-06360d846151, startTime=2023-01-06T20:45:00.921300, finishTime=2023-01-06T20:45:00.923194) of size 4
INFO  [2023-01-06 20:45:00,923] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5c93515b-b46b-4712-abf9-1708180d9b4b ManagedQuery within PT0.005902S
127.0.0.1 - - [06/Jan/2023:20:45:00 +0000] "GET /api/datasets/COMPOUND_DATERANGE$20Test/queries/COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b HTTP/1.1" 200 1404 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:00,952] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMPOUND_DATERANGE Test], queryId=5c93515b-b46b-4712-abf9-1708180d9b4b, label=test_tree	@§$, creationTime=2023-01-06T20:45:00.917776, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7115582a[Count = 0], startTime=2023-01-06T20:45:00.918060, finishTime=2023-01-06T20:45:00.923962, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@40b9de3b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMPOUND_DATERANGE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1e1160b6, com.bakdata.conquery.models.query.ColumnDescriptor@755b61df]) download on dataset Dataset[label=null, name=COMPOUND_DATERANGE Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:00,952] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMPOUND_DATERANGE Test], queryId=5c93515b-b46b-4712-abf9-1708180d9b4b, label=test_tree	@§$, creationTime=2023-01-06T20:45:00.917776, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7115582a[Count = 0], startTime=2023-01-06T20:45:00.918060, finishTime=2023-01-06T20:45:00.923962, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@40b9de3b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMPOUND_DATERANGE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1e1160b6, com.bakdata.conquery.models.query.ColumnDescriptor@755b61df]) on dataset Dataset[label=null, name=COMPOUND_DATERANGE Test]
127.0.0.1 - - [06/Jan/2023:20:45:00 +0000] "GET /api/datasets/COMPOUND_DATERANGE%20Test/result/COMPOUND_DATERANGE$20Test.5c93515b-b46b-4712-abf9-1708180d9b4b.csv?pretty=false HTTP/1.1" 200 183 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:00,973] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COMPOUND_DATERANGE Test on 8 rows
INFO  [2023-01-06 20:45:00,973] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COMPOUND_DATERANGE Test
INFO  [2023-01-06 20:45:00,973] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-06 20:45:00,973] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-06 20:45:00,973] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMPOUND_DATERANGE Test_cf22d8a6-45e7-487e-8433-06360d846151
INFO  [2023-01-06 20:45:00,973] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMPOUND_DATERANGE Test_3fef357f-d680-4bea-a9e1-67136dd60bd2
INFO  [2023-01-06 20:45:01,053] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COMPOUND_DATERANGE Test
INFO  [2023-01-06 20:45:01,053] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMPOUND_DATERANGE Test_cf22d8a6-45e7-487e-8433-06360d846151
INFO  [2023-01-06 20:45:01,080] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMPOUND_DATERANGE Test_3fef357f-d680-4bea-a9e1-67136dd60bd2
INFO  [2023-01-06 20:45:01,080] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COMPOUND_DATERANGE$20Test
INFO  [2023-01-06 20:45:01,080] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,204] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COMPOUND_DATERANGE Test
INFO  [2023-01-06 20:45:01,204] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:01,204] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:01,204] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:01,206] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:01,206] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:01,206] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:01,206] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:01,208] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_b36de859-902c-4529-ad60-32be8a743190 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:01,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_b36de859-902c-4529-ad60-32be8a743190 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:01,208] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:01,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_eae378d5-66ba-426e-9348-bb0114377876 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:01,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_eae378d5-66ba-426e-9348-bb0114377876 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:01,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:01,312] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,320] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,320] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-06 20:45:01,320] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-06 20:45:01,441] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,552] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:01,553] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:01,553] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 44 B in total
INFO  [2023-01-06 20:45:01,553] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00034339sINFO  [2023-01-06 20:45:01,588] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:01,588] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:01,591] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:01,591] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:01,591] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:01,610] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-06 20:45:01,611] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:01 +0000] "POST /admin/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:01,611] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:01,612] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:01,612] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:01,613] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:01,614] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:45:01,614] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
WARN  [2023-01-06 20:45:01,614] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:01,614] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:01,614] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:01,719] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,725] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,735] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:01,736] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:01,736] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:01,842] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test QUERY INIT
INFO  [2023-01-06 20:45:01,875] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:01,876] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b02f4fe7-7d52-4d49-805d-6d990a2d3ec7] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test))]]
INFO  [2023-01-06 20:45:01,879] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7
INFO  [2023-01-06 20:45:01,879] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7
INFO  [2023-01-06 20:45:01,880] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7] with 0 results within PT0.000674S
INFO  [2023-01-06 20:45:01,881] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7, workerId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_eae378d5-66ba-426e-9348-bb0114377876, startTime=2023-01-06T20:45:01.879961, finishTime=2023-01-06T20:45:01.880635) of size 0
INFO  [2023-01-06 20:45:01,881] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7] with 2 results within PT0.001317S
127.0.0.1 - - [06/Jan/2023:20:45:01 +0000] "POST /api/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test/queries HTTP/1.1" 201 1647 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:01,881] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7, workerId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_b36de859-902c-4529-ad60-32be8a743190, startTime=2023-01-06T20:45:01.879964, finishTime=2023-01-06T20:45:01.881281) of size 2
INFO  [2023-01-06 20:45:01,881] com.bakdata.conquery.models.execution.ManagedExecution: DONE b02f4fe7-7d52-4d49-805d-6d990a2d3ec7 ManagedQuery within PT0.005259S
127.0.0.1 - - [06/Jan/2023:20:45:01 +0000] "GET /api/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test/queries/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7 HTTP/1.1" 200 2046 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:01,905] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test], queryId=b02f4fe7-7d52-4d49-805d-6d990a2d3ec7, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:01.876302, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2d210db9[Count = 0], startTime=2023-01-06T20:45:01.876549, finishTime=2023-01-06T20:45:01.881808, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6e6da5d0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@31aee83b, com.bakdata.conquery.models.query.ColumnDescriptor@70d657d8, com.bakdata.conquery.models.query.ColumnDescriptor@55bfd0be]) download on dataset Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:01,905] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test], queryId=b02f4fe7-7d52-4d49-805d-6d990a2d3ec7, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:01.876302, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2d210db9[Count = 0], startTime=2023-01-06T20:45:01.876549, finishTime=2023-01-06T20:45:01.881808, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6e6da5d0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@31aee83b, com.bakdata.conquery.models.query.ColumnDescriptor@70d657d8, com.bakdata.conquery.models.query.ColumnDescriptor@55bfd0be]) on dataset Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
127.0.0.1 - - [06/Jan/2023:20:45:01 +0000] "GET /api/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE%20Test/result/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.b02f4fe7-7d52-4d49-805d-6d990a2d3ec7.csv?pretty=false HTTP/1.1" 200 106 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:01,924] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test on 3 rows
INFO  [2023-01-06 20:45:01,924] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:01,924] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:01,924] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:01,924] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_eae378d5-66ba-426e-9348-bb0114377876
INFO  [2023-01-06 20:45:01,924] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_b36de859-902c-4529-ad60-32be8a743190
INFO  [2023-01-06 20:45:02,034] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_b36de859-902c-4529-ad60-32be8a743190
INFO  [2023-01-06 20:45:02,034] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_eae378d5-66ba-426e-9348-bb0114377876
INFO  [2023-01-06 20:45:02,034] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:02,134] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test
INFO  [2023-01-06 20:45:02,134] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,142] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:02,142] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CONCEPT_RESTRICTION Test
INFO  [2023-01-06 20:45:02,142] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:02,142] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:02,144] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-06 20:45:02,144] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-06 20:45:02,144] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:02,144] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:02,146] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_41318bbd-5993-4237-b8a7-1b8f098acf84 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:02,146] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_41318bbd-5993-4237-b8a7-1b8f098acf84 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:02,146] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:02,148] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_65154afc-b17a-490d-97a6-6a66504f2405 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:02,148] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_65154afc-b17a-490d-97a6-6a66504f2405 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:02,148] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:02,150] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,258] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,265] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_RESTRICTION$20Test.kh_diagnose
INFO  [2023-01-06 20:45:02,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_RESTRICTION$20Test.kh_diagnose
INFO  [2023-01-06 20:45:02,380] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,491] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:02,492] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:02,492] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 137 B in total
INFO  [2023-01-06 20:45:02,492] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_RESTRICTION Test/kh_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000400675sINFO  [2023-01-06 20:45:02,532] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=3, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:02,533] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17256, maxValue=17347), dateReader=com.bakdata.conquery.util.DateReader@61c75836)
INFO  [2023-01-06 20:45:02,533] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=3, nullLines=0), encoding=null, prefix=F2, suffix=)
INFO  [2023-01-06 20:45:02,533] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[aufnahmedatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17167, maxValue=17257), dateReader=com.bakdata.conquery.util.DateReader@2fc4408c)
INFO  [2023-01-06 20:45:02,536] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_RESTRICTION Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:02,536] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:02,536] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_RESTRICTION Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:02,558] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into CONCEPT_RESTRICTION$20Test.kh_diagnose
127.0.0.1 - - [06/Jan/2023:20:45:02 +0000] "POST /admin/datasets/CONCEPT_RESTRICTION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_CONCEPT_RESTRICTION+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:02,559] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,561] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:02,562] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:02,562] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:02,566] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:02,567] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_RESTRICTION$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
INFO  [2023-01-06 20:45:02,567] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_RESTRICTION$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
WARN  [2023-01-06 20:45:02,568] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:02,568] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_RESTRICTION$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-06 20:45:02,674] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,680] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,690] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:02,691] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:02,804] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CONCEPT_RESTRICTION Test QUERY INIT
INFO  [2023-01-06 20:45:02,815] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CONCEPT_RESTRICTION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:02,816] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[27a3db02-2d6e-4a8e-9415-a78bcc6bbddd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_RESTRICTION Test))]]
INFO  [2023-01-06 20:45:02,820] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd
INFO  [2023-01-06 20:45:02,820] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd
WARN  [2023-01-06 20:45:02,820] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:02,820] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd] with 0 results within PT0.000178S
INFO  [2023-01-06 20:45:02,821] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd, workerId=CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_41318bbd-5993-4237-b8a7-1b8f098acf84, startTime=2023-01-06T20:45:02.820748, finishTime=2023-01-06T20:45:02.820926) of size 0
INFO  [2023-01-06 20:45:02,821] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd] with 1 results within PT0.001015S
127.0.0.1 - - [06/Jan/2023:20:45:02 +0000] "POST /api/datasets/CONCEPT_RESTRICTION$20Test/queries HTTP/1.1" 201 1291 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:02,822] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd, workerId=CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_65154afc-b17a-490d-97a6-6a66504f2405, startTime=2023-01-06T20:45:02.820763, finishTime=2023-01-06T20:45:02.821778) of size 1
INFO  [2023-01-06 20:45:02,822] com.bakdata.conquery.models.execution.ManagedExecution: DONE 27a3db02-2d6e-4a8e-9415-a78bcc6bbddd ManagedQuery within PT0.006049S
127.0.0.1 - - [06/Jan/2023:20:45:02 +0000] "GET /api/datasets/CONCEPT_RESTRICTION$20Test/queries/CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd HTTP/1.1" 200 1586 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:02,853] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_RESTRICTION Test], queryId=27a3db02-2d6e-4a8e-9415-a78bcc6bbddd, label=F20	@§$, creationTime=2023-01-06T20:45:02.816154, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b9db790[Count = 0], startTime=2023-01-06T20:45:02.816412, finishTime=2023-01-06T20:45:02.822461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7d950bd3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@182ea63a, com.bakdata.conquery.models.query.ColumnDescriptor@78683ad9]) download on dataset Dataset[label=null, name=CONCEPT_RESTRICTION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:02,853] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_RESTRICTION Test], queryId=27a3db02-2d6e-4a8e-9415-a78bcc6bbddd, label=F20	@§$, creationTime=2023-01-06T20:45:02.816154, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b9db790[Count = 0], startTime=2023-01-06T20:45:02.816412, finishTime=2023-01-06T20:45:02.822461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7d950bd3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@182ea63a, com.bakdata.conquery.models.query.ColumnDescriptor@78683ad9]) on dataset Dataset[label=null, name=CONCEPT_RESTRICTION Test]
127.0.0.1 - - [06/Jan/2023:20:45:02 +0000] "GET /api/datasets/CONCEPT_RESTRICTION%20Test/result/CONCEPT_RESTRICTION$20Test.27a3db02-2d6e-4a8e-9415-a78bcc6bbddd.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:02,872] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CONCEPT_RESTRICTION Test on 2 rows
INFO  [2023-01-06 20:45:02,873] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CONCEPT_RESTRICTION Test
INFO  [2023-01-06 20:45:02,873] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-06 20:45:02,873] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_RESTRICTION Test_41318bbd-5993-4237-b8a7-1b8f098acf84
INFO  [2023-01-06 20:45:02,873] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-06 20:45:02,873] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_RESTRICTION Test_65154afc-b17a-490d-97a6-6a66504f2405
INFO  [2023-01-06 20:45:02,944] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CONCEPT_RESTRICTION Test
INFO  [2023-01-06 20:45:02,946] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_RESTRICTION Test_41318bbd-5993-4237-b8a7-1b8f098acf84
INFO  [2023-01-06 20:45:02,948] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_RESTRICTION Test_65154afc-b17a-490d-97a6-6a66504f2405
INFO  [2023-01-06 20:45:02,970] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CONCEPT_RESTRICTION$20Test
INFO  [2023-01-06 20:45:02,971] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,104] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CONCEPT_RESTRICTION Test
INFO  [2023-01-06 20:45:03,105] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:03,105] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:03,105] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:03,106] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:03,106] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:03,106] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:03,106] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_d4c61b2a-b1fc-473e-af6e-9291084728fe are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_d4c61b2a-b1fc-473e-af6e-9291084728fe are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_6315a1ce-f654-4154-b453-4df6ff19bb1d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_6315a1ce-f654-4154-b453-4df6ff19bb1d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:03,108] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:03,218] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,225] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,225] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-06 20:45:03,225] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-06 20:45:03,343] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,454] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:03,454] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:03,454] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 44 B in total
INFO  [2023-01-06 20:45:03,455] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000434784sINFO  [2023-01-06 20:45:03,499] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:03,499] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:03,501] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:03,501] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:03,501] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CONCEPT_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:03,522] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table
127.0.0.1 - - [06/Jan/2023:20:45:03 +0000] "POST /admin/datasets/CONCEPT_WITHOUT_VALIDITYDATE%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_CONCEPT_WITHOUT_VALIDITYDATE+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:03,523] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,524] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:03,525] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:03,525] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:03,527] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:03,527] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:45:03,527] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
WARN  [2023-01-06 20:45:03,528] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:03,529] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:03,529] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:03,633] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,639] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,653] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:03,653] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:03,653] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:03,768] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CONCEPT_WITHOUT_VALIDITYDATE Test QUERY INIT
INFO  [2023-01-06 20:45:03,785] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CONCEPT_WITHOUT_VALIDITYDATE$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:03,786] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[22cc6411-79c2-4e7c-bd6a-62800b8becde] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_WITHOUT_VALIDITYDATE Test))]]
INFO  [2023-01-06 20:45:03,789] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde
INFO  [2023-01-06 20:45:03,789] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde
127.0.0.1 - - [06/Jan/2023:20:45:03 +0000] "POST /api/datasets/CONCEPT_WITHOUT_VALIDITYDATE$20Test/queries HTTP/1.1" 201 1461 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:03,816] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde] with 0 results within PT0.026749S
INFO  [2023-01-06 20:45:03,816] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde] with 2 results within PT0.027033S
INFO  [2023-01-06 20:45:03,816] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde, workerId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_d4c61b2a-b1fc-473e-af6e-9291084728fe, startTime=2023-01-06T20:45:03.789539, finishTime=2023-01-06T20:45:03.816288) of size 0
INFO  [2023-01-06 20:45:03,817] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde, workerId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_6315a1ce-f654-4154-b453-4df6ff19bb1d, startTime=2023-01-06T20:45:03.789551, finishTime=2023-01-06T20:45:03.816584) of size 2
INFO  [2023-01-06 20:45:03,817] com.bakdata.conquery.models.execution.ManagedExecution: DONE 22cc6411-79c2-4e7c-bd6a-62800b8becde ManagedQuery within PT0.031256S
127.0.0.1 - - [06/Jan/2023:20:45:03 +0000] "GET /api/datasets/CONCEPT_WITHOUT_VALIDITYDATE$20Test/queries/CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde HTTP/1.1" 200 1793 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:45:03,834] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test], queryId=22cc6411-79c2-4e7c-bd6a-62800b8becde, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:03.785815, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3407350[Count = 0], startTime=2023-01-06T20:45:03.786125, finishTime=2023-01-06T20:45:03.817381, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6b355e23), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@32b42f1b, com.bakdata.conquery.models.query.ColumnDescriptor@10f3f464, com.bakdata.conquery.models.query.ColumnDescriptor@10a9dfd9]) download on dataset Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:03,834] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test], queryId=22cc6411-79c2-4e7c-bd6a-62800b8becde, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:03.785815, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3407350[Count = 0], startTime=2023-01-06T20:45:03.786125, finishTime=2023-01-06T20:45:03.817381, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6b355e23), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@32b42f1b, com.bakdata.conquery.models.query.ColumnDescriptor@10f3f464, com.bakdata.conquery.models.query.ColumnDescriptor@10a9dfd9]) on dataset Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
127.0.0.1 - - [06/Jan/2023:20:45:03 +0000] "GET /api/datasets/CONCEPT_WITHOUT_VALIDITYDATE%20Test/result/CONCEPT_WITHOUT_VALIDITYDATE$20Test.22cc6411-79c2-4e7c-bd6a-62800b8becde.csv?pretty=false HTTP/1.1" 200 82 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:03,853] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CONCEPT_WITHOUT_VALIDITYDATE Test on 3 rows
INFO  [2023-01-06 20:45:03,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:03,853] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:03,853] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-06 20:45:03,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_WITHOUT_VALIDITYDATE Test_d4c61b2a-b1fc-473e-af6e-9291084728fe
INFO  [2023-01-06 20:45:03,854] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_WITHOUT_VALIDITYDATE Test_6315a1ce-f654-4154-b453-4df6ff19bb1d
INFO  [2023-01-06 20:45:03,906] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:03,907] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_WITHOUT_VALIDITYDATE Test_d4c61b2a-b1fc-473e-af6e-9291084728fe
INFO  [2023-01-06 20:45:03,908] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_WITHOUT_VALIDITYDATE Test_6315a1ce-f654-4154-b453-4df6ff19bb1d
INFO  [2023-01-06 20:45:03,929] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CONCEPT_WITHOUT_VALIDITYDATE$20Test
INFO  [2023-01-06 20:45:03,929] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,068] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-06 20:45:04,069] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:04,069] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:04,069] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:04,071] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:04,071] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:04,071] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:04,071] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:04,074] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_d08001a6-1ad6-4426-a985-5c30685d1fc1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:04,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_d08001a6-1ad6-4426-a985-5c30685d1fc1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:04,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:04,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_f2d7c487-be0d-4f60-b04d-88fe6065dbd1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:04,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_f2d7c487-be0d-4f60-b04d-88fe6065dbd1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:04,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:04,177] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,185] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,186] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test.table
INFO  [2023-01-06 20:45:04,186] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test.table
INFO  [2023-01-06 20:45:04,310] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,425] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:04,426] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:04,426] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 175 B in total
INFO  [2023-01-06 20:45:04,426] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00032242sINFO  [2023-01-06 20:45:04,458] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=7, min=1, average=1.166667, max=2}
INFO  [2023-01-06 20:45:04,459] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:04,459] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[include] with StringParser(super=Parser(lines=7, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:04,459] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@64a7b623)
INFO  [2023-01-06 20:45:04,462] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:04,462] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:04,462] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:04,480] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SIMPLE_TREECONCEPT_QUERY$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:04 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_TREECONCEPT_QUERY+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:04,481] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,482] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:04,484] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:04,484] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:04,487] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:04,487] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test.table.table], containing 7 entries.
INFO  [2023-01-06 20:45:04,487] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test.table.table], containing 7 entries.
INFO  [2023-01-06 20:45:04,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test.table.table.0
WARN  [2023-01-06 20:45:04,489] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:04,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test.table.table.1
INFO  [2023-01-06 20:45:04,594] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,599] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,613] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:04,613] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:04,614] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:04,719] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:04,736] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:04,736] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b607b276-5921-4dda-82ef-b61539dd7dcc] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:04,740] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc
INFO  [2023-01-06 20:45:04,740] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc
INFO  [2023-01-06 20:45:04,741] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc] with 2 results within PT0.001231S
INFO  [2023-01-06 20:45:04,741] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc] with 0 results within PT0.001249S
127.0.0.1 - - [06/Jan/2023:20:45:04 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1121 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:04,741] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc, workerId=SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_d08001a6-1ad6-4426-a985-5c30685d1fc1, startTime=2023-01-06T20:45:04.740080, finishTime=2023-01-06T20:45:04.741311) of size 2
INFO  [2023-01-06 20:45:04,742] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc, workerId=SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_f2d7c487-be0d-4f60-b04d-88fe6065dbd1, startTime=2023-01-06T20:45:04.740094, finishTime=2023-01-06T20:45:04.741343) of size 0
INFO  [2023-01-06 20:45:04,742] com.bakdata.conquery.models.execution.ManagedExecution: DONE b607b276-5921-4dda-82ef-b61539dd7dcc ManagedQuery within PT0.005354S
127.0.0.1 - - [06/Jan/2023:20:45:04 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test/queries/SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc HTTP/1.1" 200 1436 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:04,767] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test], queryId=b607b276-5921-4dda-82ef-b61539dd7dcc, label=tree-a1	@§$, creationTime=2023-01-06T20:45:04.736575, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@122be823[Count = 0], startTime=2023-01-06T20:45:04.736882, finishTime=2023-01-06T20:45:04.742236, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7a8e9ee3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@290785d5, com.bakdata.conquery.models.query.ColumnDescriptor@7f2e9590]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:04,767] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test], queryId=b607b276-5921-4dda-82ef-b61539dd7dcc, label=tree-a1	@§$, creationTime=2023-01-06T20:45:04.736575, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@122be823[Count = 0], startTime=2023-01-06T20:45:04.736882, finishTime=2023-01-06T20:45:04.742236, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7a8e9ee3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@290785d5, com.bakdata.conquery.models.query.ColumnDescriptor@7f2e9590]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:04 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY%20Test/result/SIMPLE_TREECONCEPT_QUERY$20Test.b607b276-5921-4dda-82ef-b61539dd7dcc.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:04,785] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:04,785] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:04,785] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:04,785] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:04,785] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test_d08001a6-1ad6-4426-a985-5c30685d1fc1
INFO  [2023-01-06 20:45:04,785] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test_f2d7c487-be0d-4f60-b04d-88fe6065dbd1
INFO  [2023-01-06 20:45:04,890] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:04,890] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test_f2d7c487-be0d-4f60-b04d-88fe6065dbd1
INFO  [2023-01-06 20:45:04,890] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test_d08001a6-1ad6-4426-a985-5c30685d1fc1
INFO  [2023-01-06 20:45:04,898] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:04,898] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,019] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:05,020] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CQExternal Extra Data Test
INFO  [2023-01-06 20:45:05,020] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:05,020] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:05,022] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-06 20:45:05,022] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-06 20:45:05,022] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:05,022] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_b53b01db-66d3-46d3-b2fb-f867673b2f3a are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_b53b01db-66d3-46d3-b2fb-f867673b2f3a are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_5fa1fb0c-1985-41d2-a628-233e859b1700 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_5fa1fb0c-1985-41d2-a628-233e859b1700 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:05,024] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:05,128] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,136] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,137] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CQExternal$20Extra$20Data$20Test.test_table
INFO  [2023-01-06 20:45:05,137] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CQExternal$20Extra$20Data$20Test.test_table
INFO  [2023-01-06 20:45:05,258] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,369] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:05,369] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:05,370] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 88 B in total
INFO  [2023-01-06 20:45:05,370] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CQExternal Extra Data Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000328741sINFO  [2023-01-06 20:45:05,403] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=6, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:05,403] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@57c6cb71)
INFO  [2023-01-06 20:45:05,405] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CQExternal Extra Data Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:05,405] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:05,405] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_CQExternal Extra Data Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:05,425] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into CQExternal$20Extra$20Data$20Test.test_table
INFO  [2023-01-06 20:45:05,425] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:05 +0000] "POST /admin/datasets/CQExternal%20Extra%20Data%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_CQExternal+Extra+Data+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:05,426] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:05,426] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:05,426] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:05,426] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:05,427] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CQExternal$20Extra$20Data$20Test.test_table.test_table], containing 6 entries.
INFO  [2023-01-06 20:45:05,427] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CQExternal$20Extra$20Data$20Test.test_table.test_table], containing 6 entries.
WARN  [2023-01-06 20:45:05,427] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:05,427] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CQExternal$20Extra$20Data$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:05,427] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CQExternal$20Extra$20Data$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:05,532] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,537] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,550] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,551] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:05,551] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:05,657] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CQExternal Extra Data Test QUERY INIT
INFO  [2023-01-06 20:45:05,677] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CQExternal$20Extra$20Data$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:05,679] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a7a35681-6962-47b0-8263-c9de30c7e055] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CQExternal Extra Data Test))]]
INFO  [2023-01-06 20:45:05,684] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055
INFO  [2023-01-06 20:45:05,684] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055
INFO  [2023-01-06 20:45:05,684] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055] with 1 results within PT0.000875S
INFO  [2023-01-06 20:45:05,685] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055] with 2 results within PT0.00081S
127.0.0.1 - - [06/Jan/2023:20:45:05 +0000] "POST /api/datasets/CQExternal$20Extra$20Data$20Test/queries HTTP/1.1" 201 1186 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:05,685] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055, workerId=CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_5fa1fb0c-1985-41d2-a628-233e859b1700, startTime=2023-01-06T20:45:05.684062, finishTime=2023-01-06T20:45:05.684937) of size 1
INFO  [2023-01-06 20:45:05,685] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055, workerId=CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_b53b01db-66d3-46d3-b2fb-f867673b2f3a, startTime=2023-01-06T20:45:05.684281, finishTime=2023-01-06T20:45:05.685091) of size 2
INFO  [2023-01-06 20:45:05,685] com.bakdata.conquery.models.execution.ManagedExecution: DONE a7a35681-6962-47b0-8263-c9de30c7e055 ManagedQuery within PT0.006248S
127.0.0.1 - - [06/Jan/2023:20:45:05 +0000] "GET /api/datasets/CQExternal$20Extra$20Data$20Test/queries/CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055 HTTP/1.1" 200 1504 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:05,713] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CQExternal Extra Data Test], queryId=a7a35681-6962-47b0-8263-c9de30c7e055, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:05.677958, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@202aabd[Count = 0], startTime=2023-01-06T20:45:05.679722, finishTime=2023-01-06T20:45:05.685970, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5a1d2fb7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CQExternal Extra Data Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7097b455, com.bakdata.conquery.models.query.ColumnDescriptor@39666459, com.bakdata.conquery.models.query.ColumnDescriptor@2ea6668a]) download on dataset Dataset[label=null, name=CQExternal Extra Data Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:05,713] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CQExternal Extra Data Test], queryId=a7a35681-6962-47b0-8263-c9de30c7e055, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:05.677958, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@202aabd[Count = 0], startTime=2023-01-06T20:45:05.679722, finishTime=2023-01-06T20:45:05.685970, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5a1d2fb7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CQExternal Extra Data Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7097b455, com.bakdata.conquery.models.query.ColumnDescriptor@39666459, com.bakdata.conquery.models.query.ColumnDescriptor@2ea6668a]) on dataset Dataset[label=null, name=CQExternal Extra Data Test]
127.0.0.1 - - [06/Jan/2023:20:45:05 +0000] "GET /api/datasets/CQExternal%20Extra%20Data%20Test/result/CQExternal$20Extra$20Data$20Test.a7a35681-6962-47b0-8263-c9de30c7e055.csv?pretty=false HTTP/1.1" 200 132 "-" "Conquery (test client)" 25
INFO  [2023-01-06 20:45:05,736] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CQExternal Extra Data Test on 4 rows
INFO  [2023-01-06 20:45:05,736] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CQExternal Extra Data Test
INFO  [2023-01-06 20:45:05,736] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-06 20:45:05,737] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-06 20:45:05,737] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CQExternal Extra Data Test_5fa1fb0c-1985-41d2-a628-233e859b1700
INFO  [2023-01-06 20:45:05,737] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CQExternal Extra Data Test_b53b01db-66d3-46d3-b2fb-f867673b2f3a
INFO  [2023-01-06 20:45:05,834] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CQExternal Extra Data Test_5fa1fb0c-1985-41d2-a628-233e859b1700
INFO  [2023-01-06 20:45:05,834] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CQExternal Extra Data Test_b53b01db-66d3-46d3-b2fb-f867673b2f3a
INFO  [2023-01-06 20:45:05,834] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CQExternal Extra Data Test
INFO  [2023-01-06 20:45:05,934] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CQExternal$20Extra$20Data$20Test
INFO  [2023-01-06 20:45:05,934] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:05,957] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CQExternal Extra Data Test
INFO  [2023-01-06 20:45:05,957] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_ERSTER Test
INFO  [2023-01-06 20:45:05,957] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:05,957] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:05,958] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-06 20:45:05,958] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-06 20:45:05,958] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:05,959] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_df381581-a1e5-43af-a618-4b8ccf9d63e4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_df381581-a1e5-43af-a618-4b8ccf9d63e4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_78eff8df-8df0-4d32-b110-7d637559afd0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_78eff8df-8df0-4d32-b110-7d637559afd0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:05,965] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,064] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,071] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,072] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:06,072] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:06,189] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,300] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:06,300] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:06,300] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:06,300] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_ERSTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000373319sINFO  [2023-01-06 20:45:06,338] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:06,338] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@3aeda973)
INFO  [2023-01-06 20:45:06,338] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@4c8b13f3)
INFO  [2023-01-06 20:45:06,338] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@24b21d40), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@750b3b7), dateReader=com.bakdata.conquery.util.DateReader@7399f4ff, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:06,338] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@74a87f13)
INFO  [2023-01-06 20:45:06,340] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:06,340] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:06,340] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:06,354] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:06,354] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:06 +0000] "POST /admin/datasets/DATE_DISTANCE_ERSTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_ERSTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:06,355] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:06,355] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:06,355] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:06,356] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:06,356] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:06,356] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.0
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.1
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.2
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.4
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.3
WARN  [2023-01-06 20:45:06,357] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.5
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.6
INFO  [2023-01-06 20:45:06,357] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.7
INFO  [2023-01-06 20:45:06,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,468] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,484] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,485] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:06,485] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:06,590] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_ERSTER Test QUERY INIT
INFO  [2023-01-06 20:45:06,607] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_ERSTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:06,608] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2b8f08f1-99c8-42d8-9558-f715ccfd3e0f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_ERSTER Test))]]
INFO  [2023-01-06 20:45:06,613] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f
INFO  [2023-01-06 20:45:06,613] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f
127.0.0.1 - - [06/Jan/2023:20:45:06 +0000] "POST /api/datasets/DATE_DISTANCE_ERSTER$20Test/queries HTTP/1.1" 201 1393 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:06,616] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f] with 2 results within PT0.003505S
INFO  [2023-01-06 20:45:06,616] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f] with 0 results within PT0.003144S
INFO  [2023-01-06 20:45:06,617] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f, workerId=DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_78eff8df-8df0-4d32-b110-7d637559afd0, startTime=2023-01-06T20:45:06.613573, finishTime=2023-01-06T20:45:06.616717) of size 0
INFO  [2023-01-06 20:45:06,617] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f, workerId=DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_df381581-a1e5-43af-a618-4b8ccf9d63e4, startTime=2023-01-06T20:45:06.613084, finishTime=2023-01-06T20:45:06.616589) of size 2
INFO  [2023-01-06 20:45:06,617] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2b8f08f1-99c8-42d8-9558-f715ccfd3e0f ManagedQuery within PT0.009465S
127.0.0.1 - - [06/Jan/2023:20:45:06 +0000] "GET /api/datasets/DATE_DISTANCE_ERSTER$20Test/queries/DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f HTTP/1.1" 200 1692 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:06,641] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_ERSTER Test], queryId=2b8f08f1-99c8-42d8-9558-f715ccfd3e0f, label=Alter	@§$, creationTime=2023-01-06T20:45:06.607866, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5fca6a82[Count = 0], startTime=2023-01-06T20:45:06.608101, finishTime=2023-01-06T20:45:06.617566, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@41b6fc5a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@541163cd, com.bakdata.conquery.models.query.ColumnDescriptor@11e67e60]) download on dataset Dataset[label=null, name=DATE_DISTANCE_ERSTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:06,641] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_ERSTER Test], queryId=2b8f08f1-99c8-42d8-9558-f715ccfd3e0f, label=Alter	@§$, creationTime=2023-01-06T20:45:06.607866, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5fca6a82[Count = 0], startTime=2023-01-06T20:45:06.608101, finishTime=2023-01-06T20:45:06.617566, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@41b6fc5a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@541163cd, com.bakdata.conquery.models.query.ColumnDescriptor@11e67e60]) on dataset Dataset[label=null, name=DATE_DISTANCE_ERSTER Test]
127.0.0.1 - - [06/Jan/2023:20:45:06 +0000] "GET /api/datasets/DATE_DISTANCE_ERSTER%20Test/result/DATE_DISTANCE_ERSTER$20Test.2b8f08f1-99c8-42d8-9558-f715ccfd3e0f.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:06,662] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_ERSTER Test on 3 rows
INFO  [2023-01-06 20:45:06,662] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_ERSTER Test
INFO  [2023-01-06 20:45:06,662] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-06 20:45:06,663] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-06 20:45:06,663] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_ERSTER Test_df381581-a1e5-43af-a618-4b8ccf9d63e4
INFO  [2023-01-06 20:45:06,663] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_ERSTER Test_78eff8df-8df0-4d32-b110-7d637559afd0
INFO  [2023-01-06 20:45:06,762] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_ERSTER Test
INFO  [2023-01-06 20:45:06,762] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_ERSTER Test_df381581-a1e5-43af-a618-4b8ccf9d63e4
INFO  [2023-01-06 20:45:06,762] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_ERSTER Test_78eff8df-8df0-4d32-b110-7d637559afd0
INFO  [2023-01-06 20:45:06,862] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_ERSTER$20Test
INFO  [2023-01-06 20:45:06,862] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,891] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_ERSTER Test
INFO  [2023-01-06 20:45:06,891] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_LETZTER Test
INFO  [2023-01-06 20:45:06,891] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:06,891] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:06,892] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-06 20:45:06,892] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-06 20:45:06,892] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:06,892] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_31a407b4-d359-40ed-9f2f-fbafbd0d6c2b are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_31a407b4-d359-40ed-9f2f-fbafbd0d6c2b are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_b2d7838c-c6c4-4a67-82ca-4109a40050ae are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_b2d7838c-c6c4-4a67-82ca-4109a40050ae are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:06,894] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:06,998] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,004] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,005] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:07,005] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:07,121] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,230] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:07,230] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:07,230] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:07,230] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_LETZTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000261573sINFO  [2023-01-06 20:45:07,257] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:07,257] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@1dfbc7fe)
INFO  [2023-01-06 20:45:07,257] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@2a565201), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@1eba1216), dateReader=com.bakdata.conquery.util.DateReader@1c75c77d, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:07,257] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@12a69f3a)
INFO  [2023-01-06 20:45:07,257] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@50977fc2)
INFO  [2023-01-06 20:45:07,260] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:07,260] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:07,260] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:07,283] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:07,284] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:07 +0000] "POST /admin/datasets/DATE_DISTANCE_LETZTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_LETZTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:07,285] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:07,285] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:07,285] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:07,288] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:07,288] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:07,288] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:07,290] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.0
INFO  [2023-01-06 20:45:07,290] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.1
INFO  [2023-01-06 20:45:07,291] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.2
INFO  [2023-01-06 20:45:07,291] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.4
WARN  [2023-01-06 20:45:07,291] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:07,291] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.3
INFO  [2023-01-06 20:45:07,291] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.6
INFO  [2023-01-06 20:45:07,291] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.5
INFO  [2023-01-06 20:45:07,292] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.7
INFO  [2023-01-06 20:45:07,396] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,402] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,413] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,414] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:07,414] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:07,519] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_LETZTER Test QUERY INIT
INFO  [2023-01-06 20:45:07,536] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_LETZTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:07,536] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[01062112-bfd5-4ded-8495-c64f01c181d2] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_LETZTER Test))]]
INFO  [2023-01-06 20:45:07,541] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2
INFO  [2023-01-06 20:45:07,541] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2
127.0.0.1 - - [06/Jan/2023:20:45:07 +0000] "POST /api/datasets/DATE_DISTANCE_LETZTER$20Test/queries HTTP/1.1" 201 1397 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:07,544] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2] with 0 results within PT0.002912S
INFO  [2023-01-06 20:45:07,545] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2, workerId=DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_31a407b4-d359-40ed-9f2f-fbafbd0d6c2b, startTime=2023-01-06T20:45:07.541731, finishTime=2023-01-06T20:45:07.544643) of size 0
INFO  [2023-01-06 20:45:07,545] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2] with 2 results within PT0.003679S
INFO  [2023-01-06 20:45:07,546] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2, workerId=DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_b2d7838c-c6c4-4a67-82ca-4109a40050ae, startTime=2023-01-06T20:45:07.541753, finishTime=2023-01-06T20:45:07.545432) of size 2
INFO  [2023-01-06 20:45:07,546] com.bakdata.conquery.models.execution.ManagedExecution: DONE 01062112-bfd5-4ded-8495-c64f01c181d2 ManagedQuery within PT0.009345S
127.0.0.1 - - [06/Jan/2023:20:45:07 +0000] "GET /api/datasets/DATE_DISTANCE_LETZTER$20Test/queries/DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2 HTTP/1.1" 200 1700 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:07,572] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_LETZTER Test], queryId=01062112-bfd5-4ded-8495-c64f01c181d2, label=Alter	@§$, creationTime=2023-01-06T20:45:07.536604, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b68e221[Count = 0], startTime=2023-01-06T20:45:07.536832, finishTime=2023-01-06T20:45:07.546177, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@10adbc09), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@274d195d, com.bakdata.conquery.models.query.ColumnDescriptor@bb44871]) download on dataset Dataset[label=null, name=DATE_DISTANCE_LETZTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:07,573] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_LETZTER Test], queryId=01062112-bfd5-4ded-8495-c64f01c181d2, label=Alter	@§$, creationTime=2023-01-06T20:45:07.536604, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b68e221[Count = 0], startTime=2023-01-06T20:45:07.536832, finishTime=2023-01-06T20:45:07.546177, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@10adbc09), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@274d195d, com.bakdata.conquery.models.query.ColumnDescriptor@bb44871]) on dataset Dataset[label=null, name=DATE_DISTANCE_LETZTER Test]
127.0.0.1 - - [06/Jan/2023:20:45:07 +0000] "GET /api/datasets/DATE_DISTANCE_LETZTER%20Test/result/DATE_DISTANCE_LETZTER$20Test.01062112-bfd5-4ded-8495-c64f01c181d2.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:07,593] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_LETZTER Test on 3 rows
INFO  [2023-01-06 20:45:07,594] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_LETZTER Test
INFO  [2023-01-06 20:45:07,594] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-06 20:45:07,594] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-06 20:45:07,594] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_LETZTER Test_b2d7838c-c6c4-4a67-82ca-4109a40050ae
INFO  [2023-01-06 20:45:07,594] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_LETZTER Test_31a407b4-d359-40ed-9f2f-fbafbd0d6c2b
INFO  [2023-01-06 20:45:07,693] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_LETZTER Test_b2d7838c-c6c4-4a67-82ca-4109a40050ae
INFO  [2023-01-06 20:45:07,693] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_LETZTER Test_31a407b4-d359-40ed-9f2f-fbafbd0d6c2b
INFO  [2023-01-06 20:45:07,693] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_LETZTER Test
INFO  [2023-01-06 20:45:07,694] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_LETZTER$20Test
INFO  [2023-01-06 20:45:07,694] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,820] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_LETZTER Test
INFO  [2023-01-06 20:45:07,820] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:07,820] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:07,820] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:07,821] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:07,821] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:07,821] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:07,822] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:07,823] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_be7c2fe8-d43c-47dc-a36d-8e4c47249083 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:07,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_be7c2fe8-d43c-47dc-a36d-8e4c47249083 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:07,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:07,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_e993ea88-2911-4824-a92e-8067d74add17 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:07,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_e993ea88-2911-4824-a92e-8067d74add17 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:07,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:07,828] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,928] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,935] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:07,935] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:07,935] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:08,052] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,160] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:08,161] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:08,161] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:08,161] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00036097sINFO  [2023-01-06 20:45:08,198] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:08,198] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@29a751d7)
INFO  [2023-01-06 20:45:08,198] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1162bf26)
INFO  [2023-01-06 20:45:08,198] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@2077e600), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@38296e55), dateReader=com.bakdata.conquery.util.DateReader@764de1e4, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:08,198] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@39cf68b3)
INFO  [2023-01-06 20:45:08,200] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:08,200] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:08,200] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:08,221] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:08,222] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:08 +0000] "POST /admin/datasets/DATE_DISTANCE_VERSICHERTENZEIT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_VERSICHERTENZEIT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:08,222] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:08,222] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:08,222] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:08,224] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:08,224] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:08,224] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.0
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.2
WARN  [2023-01-06 20:45:08,225] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.1
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.3
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.4
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.5
INFO  [2023-01-06 20:45:08,225] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.6
INFO  [2023-01-06 20:45:08,226] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.7
INFO  [2023-01-06 20:45:08,331] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,336] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,349] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,350] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:08,350] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:08,456] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_VERSICHERTENZEIT Test QUERY INIT
INFO  [2023-01-06 20:45:08,473] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_VERSICHERTENZEIT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:08,473] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a6e61ede-0933-46bb-9bda-19139d0a92d3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_VERSICHERTENZEIT Test))]]
INFO  [2023-01-06 20:45:08,479] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3
INFO  [2023-01-06 20:45:08,480] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3
127.0.0.1 - - [06/Jan/2023:20:45:08 +0000] "POST /api/datasets/DATE_DISTANCE_VERSICHERTENZEIT$20Test/queries HTTP/1.1" 201 1433 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:08,484] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3] with 4 results within PT0.00431S
INFO  [2023-01-06 20:45:08,484] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3, workerId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_e993ea88-2911-4824-a92e-8067d74add17, startTime=2023-01-06T20:45:08.479934, finishTime=2023-01-06T20:45:08.484244) of size 4
INFO  [2023-01-06 20:45:08,485] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3] with 0 results within PT0.004905S
INFO  [2023-01-06 20:45:08,486] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3, workerId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_be7c2fe8-d43c-47dc-a36d-8e4c47249083, startTime=2023-01-06T20:45:08.480609, finishTime=2023-01-06T20:45:08.485514) of size 0
INFO  [2023-01-06 20:45:08,486] com.bakdata.conquery.models.execution.ManagedExecution: DONE a6e61ede-0933-46bb-9bda-19139d0a92d3 ManagedQuery within PT0.01214S
127.0.0.1 - - [06/Jan/2023:20:45:08 +0000] "GET /api/datasets/DATE_DISTANCE_VERSICHERTENZEIT$20Test/queries/DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3 HTTP/1.1" 200 1773 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:08,510] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test], queryId=a6e61ede-0933-46bb-9bda-19139d0a92d3, label=Alter	@§$, creationTime=2023-01-06T20:45:08.473779, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@796d94ac[Count = 0], startTime=2023-01-06T20:45:08.474044, finishTime=2023-01-06T20:45:08.486184, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@52dedf21), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7162f37b, com.bakdata.conquery.models.query.ColumnDescriptor@470b5ec6]) download on dataset Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:08,510] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test], queryId=a6e61ede-0933-46bb-9bda-19139d0a92d3, label=Alter	@§$, creationTime=2023-01-06T20:45:08.473779, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@796d94ac[Count = 0], startTime=2023-01-06T20:45:08.474044, finishTime=2023-01-06T20:45:08.486184, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@52dedf21), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7162f37b, com.bakdata.conquery.models.query.ColumnDescriptor@470b5ec6]) on dataset Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
127.0.0.1 - - [06/Jan/2023:20:45:08 +0000] "GET /api/datasets/DATE_DISTANCE_VERSICHERTENZEIT%20Test/result/DATE_DISTANCE_VERSICHERTENZEIT$20Test.a6e61ede-0933-46bb-9bda-19139d0a92d3.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 24
INFO  [2023-01-06 20:45:08,532] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_VERSICHERTENZEIT Test on 5 rows
INFO  [2023-01-06 20:45:08,533] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:08,533] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:08,533] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:08,533] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_VERSICHERTENZEIT Test_be7c2fe8-d43c-47dc-a36d-8e4c47249083
INFO  [2023-01-06 20:45:08,533] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_VERSICHERTENZEIT Test_e993ea88-2911-4824-a92e-8067d74add17
INFO  [2023-01-06 20:45:08,640] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_VERSICHERTENZEIT Test_e993ea88-2911-4824-a92e-8067d74add17
INFO  [2023-01-06 20:45:08,640] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:08,640] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_VERSICHERTENZEIT Test_be7c2fe8-d43c-47dc-a36d-8e4c47249083
INFO  [2023-01-06 20:45:08,740] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_VERSICHERTENZEIT$20Test
INFO  [2023-01-06 20:45:08,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,755] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:08,756] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-06 20:45:08,756] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:08,756] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:08,757] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-06 20:45:08,757] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-06 20:45:08,757] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:08,757] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_d948048a-8b9c-4c5b-8d69-942f399857bb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_d948048a-8b9c-4c5b-8d69-942f399857bb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_ba6d3a77-dbc2-46a2-91b0-d1f9fb9b31c4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_ba6d3a77-dbc2-46a2-91b0-d1f9fb9b31c4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:08,759] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:08,863] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,870] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:08,870] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:08,871] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:08,988] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,099] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:09,099] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:09,099] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:09,099] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_ERSTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000299766sINFO  [2023-01-06 20:45:09,130] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:09,130] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5acb1e8f)
INFO  [2023-01-06 20:45:09,130] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@44318e7f)
INFO  [2023-01-06 20:45:09,130] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@2c9584c2)
INFO  [2023-01-06 20:45:09,130] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@34029f26), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@200216de), dateReader=com.bakdata.conquery.util.DateReader@6fb12ab, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:09,132] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:09,132] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:09,132] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:09,152] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:09 +0000] "POST /admin/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_AGE_SPAN_ERSTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:45:09,153] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,154] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:09,154] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:09,154] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:09,156] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:09,156] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:09,156] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:09,158] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.0
INFO  [2023-01-06 20:45:09,158] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.1
INFO  [2023-01-06 20:45:09,158] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.2
INFO  [2023-01-06 20:45:09,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.3
INFO  [2023-01-06 20:45:09,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.4
INFO  [2023-01-06 20:45:09,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.5
WARN  [2023-01-06 20:45:09,159] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:09,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.6
INFO  [2023-01-06 20:45:09,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.7
INFO  [2023-01-06 20:45:09,264] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,270] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,287] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,287] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:09,287] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:09,393] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGE_SPAN_ERSTER Test QUERY INIT
INFO  [2023-01-06 20:45:09,411] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:09,412] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b2fd5e4e-bfec-4b87-85ba-0c90900242ad] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_ERSTER Test))]]
INFO  [2023-01-06 20:45:09,417] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad
INFO  [2023-01-06 20:45:09,417] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad
127.0.0.1 - - [06/Jan/2023:20:45:09 +0000] "POST /api/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test/queries HTTP/1.1" 201 1428 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:09,419] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad] with 1 results within PT0.002399S
INFO  [2023-01-06 20:45:09,419] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad] with 3 results within PT0.002774S
INFO  [2023-01-06 20:45:09,420] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad, workerId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_ba6d3a77-dbc2-46a2-91b0-d1f9fb9b31c4, startTime=2023-01-06T20:45:09.417054, finishTime=2023-01-06T20:45:09.419453) of size 1
INFO  [2023-01-06 20:45:09,420] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad, workerId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_d948048a-8b9c-4c5b-8d69-942f399857bb, startTime=2023-01-06T20:45:09.417087, finishTime=2023-01-06T20:45:09.419861) of size 3
INFO  [2023-01-06 20:45:09,420] com.bakdata.conquery.models.execution.ManagedExecution: DONE b2fd5e4e-bfec-4b87-85ba-0c90900242ad ManagedQuery within PT0.008262S
127.0.0.1 - - [06/Jan/2023:20:45:09 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test/queries/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad HTTP/1.1" 200 1763 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:09,446] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test], queryId=b2fd5e4e-bfec-4b87-85ba-0c90900242ad, label=Alter	@§$, creationTime=2023-01-06T20:45:09.412129, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f54422d[Count = 0], startTime=2023-01-06T20:45:09.412360, finishTime=2023-01-06T20:45:09.420622, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7887dc1f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@28a1efa1, com.bakdata.conquery.models.query.ColumnDescriptor@61a9711]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:09,446] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test], queryId=b2fd5e4e-bfec-4b87-85ba-0c90900242ad, label=Alter	@§$, creationTime=2023-01-06T20:45:09.412129, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f54422d[Count = 0], startTime=2023-01-06T20:45:09.412360, finishTime=2023-01-06T20:45:09.420622, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7887dc1f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@28a1efa1, com.bakdata.conquery.models.query.ColumnDescriptor@61a9711]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
127.0.0.1 - - [06/Jan/2023:20:45:09 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER%20Test/result/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.b2fd5e4e-bfec-4b87-85ba-0c90900242ad.csv?pretty=false HTTP/1.1" 200 119 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:09,463] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGE_SPAN_ERSTER Test on 5 rows
INFO  [2023-01-06 20:45:09,463] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-06 20:45:09,464] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-06 20:45:09,464] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-06 20:45:09,464] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_ba6d3a77-dbc2-46a2-91b0-d1f9fb9b31c4
INFO  [2023-01-06 20:45:09,464] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_d948048a-8b9c-4c5b-8d69-942f399857bb
INFO  [2023-01-06 20:45:09,561] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_d948048a-8b9c-4c5b-8d69-942f399857bb
INFO  [2023-01-06 20:45:09,561] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_ba6d3a77-dbc2-46a2-91b0-d1f9fb9b31c4
INFO  [2023-01-06 20:45:09,561] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-06 20:45:09,662] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGE_SPAN_ERSTER$20Test
INFO  [2023-01-06 20:45:09,662] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,693] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-06 20:45:09,693] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-06 20:45:09,693] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:09,693] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:09,694] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-06 20:45:09,694] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-06 20:45:09,695] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:09,695] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:09,697] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_2bbe1a20-9c8b-4357-b4d6-48e4e00639fe are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:09,697] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_2bbe1a20-9c8b-4357-b4d6-48e4e00639fe are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:09,697] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:09,697] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_89614520-726c-4a25-952b-c4a748ce1ed9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:09,697] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_89614520-726c-4a25-952b-c4a748ce1ed9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:09,697] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:09,701] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,801] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,808] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:09,808] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:09,808] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:09,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,037] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:10,037] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:10,037] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:10,037] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_LETZTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000357611sINFO  [2023-01-06 20:45:10,074] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:10,074] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@76b7af4e)
INFO  [2023-01-06 20:45:10,074] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@5b69271f)
INFO  [2023-01-06 20:45:10,074] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@1bf236a4)
INFO  [2023-01-06 20:45:10,074] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@44b36837), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@64e82428), dateReader=com.bakdata.conquery.util.DateReader@54ec7a8d, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:10,083] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:10,083] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:10,083] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:10,107] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:10 +0000] "POST /admin/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_AGE_SPAN_LETZTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:10,108] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,109] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:10,109] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:10,109] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:10,112] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:10,112] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:10,112] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.0
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.1
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.2
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.4
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.3
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.6
WARN  [2023-01-06 20:45:10,114] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.5
INFO  [2023-01-06 20:45:10,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.7
INFO  [2023-01-06 20:45:10,220] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,225] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,241] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,242] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:10,242] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:10,348] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGE_SPAN_LETZTER Test QUERY INIT
INFO  [2023-01-06 20:45:10,366] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:10,366] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[43206266-8b0e-4da8-a21b-361150655893] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_LETZTER Test))]]
INFO  [2023-01-06 20:45:10,371] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893
INFO  [2023-01-06 20:45:10,371] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893
127.0.0.1 - - [06/Jan/2023:20:45:10 +0000] "POST /api/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test/queries HTTP/1.1" 201 1433 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:10,374] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893] with 0 results within PT0.002717S
INFO  [2023-01-06 20:45:10,374] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893] with 4 results within PT0.002983S
INFO  [2023-01-06 20:45:10,375] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893, workerId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_2bbe1a20-9c8b-4357-b4d6-48e4e00639fe, startTime=2023-01-06T20:45:10.371832, finishTime=2023-01-06T20:45:10.374549) of size 0
INFO  [2023-01-06 20:45:10,375] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893, workerId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_89614520-726c-4a25-952b-c4a748ce1ed9, startTime=2023-01-06T20:45:10.371842, finishTime=2023-01-06T20:45:10.374825) of size 4
INFO  [2023-01-06 20:45:10,375] com.bakdata.conquery.models.execution.ManagedExecution: DONE 43206266-8b0e-4da8-a21b-361150655893 ManagedQuery within PT0.009023S
127.0.0.1 - - [06/Jan/2023:20:45:10 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test/queries/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893 HTTP/1.1" 200 1772 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:10,399] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test], queryId=43206266-8b0e-4da8-a21b-361150655893, label=Alter	@§$, creationTime=2023-01-06T20:45:10.366458, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@13a5abf5[Count = 0], startTime=2023-01-06T20:45:10.366738, finishTime=2023-01-06T20:45:10.375761, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@20476f2f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@791e85a9, com.bakdata.conquery.models.query.ColumnDescriptor@530cb006]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:10,400] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test], queryId=43206266-8b0e-4da8-a21b-361150655893, label=Alter	@§$, creationTime=2023-01-06T20:45:10.366458, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@13a5abf5[Count = 0], startTime=2023-01-06T20:45:10.366738, finishTime=2023-01-06T20:45:10.375761, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@20476f2f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@791e85a9, com.bakdata.conquery.models.query.ColumnDescriptor@530cb006]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
127.0.0.1 - - [06/Jan/2023:20:45:10 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER%20Test/result/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.43206266-8b0e-4da8-a21b-361150655893.csv?pretty=false HTTP/1.1" 200 119 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:45:10,421] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGE_SPAN_LETZTER Test on 5 rows
INFO  [2023-01-06 20:45:10,422] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-06 20:45:10,422] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-06 20:45:10,422] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-06 20:45:10,423] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_89614520-726c-4a25-952b-c4a748ce1ed9
INFO  [2023-01-06 20:45:10,423] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_2bbe1a20-9c8b-4357-b4d6-48e4e00639fe
INFO  [2023-01-06 20:45:10,495] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-06 20:45:10,496] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_2bbe1a20-9c8b-4357-b4d6-48e4e00639fe
INFO  [2023-01-06 20:45:10,496] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_89614520-726c-4a25-952b-c4a748ce1ed9
INFO  [2023-01-06 20:45:10,522] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGE_SPAN_LETZTER$20Test
INFO  [2023-01-06 20:45:10,522] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,648] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-06 20:45:10,648] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:10,648] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:10,649] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:10,650] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:10,650] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:10,650] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:10,650] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_49b8f7b4-1adf-456d-975c-fd58e19e2084 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_49b8f7b4-1adf-456d-975c-fd58e19e2084 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_7d0bd3e6-1d87-4daa-95e6-60176cd3835b are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_7d0bd3e6-1d87-4daa-95e6-60176cd3835b are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:10,652] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:10,756] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,764] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,766] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:10,766] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:10,883] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:10,993] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:10,993] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:10,994] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:10,994] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000333309sINFO  [2023-01-06 20:45:11,028] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:11,028] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@749826ee)
INFO  [2023-01-06 20:45:11,028] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@2841b1e1), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@311a37cc), dateReader=com.bakdata.conquery.util.DateReader@799005e6, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:11,028] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@70703377)
INFO  [2023-01-06 20:45:11,028] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@a5c44f1)
INFO  [2023-01-06 20:45:11,032] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:11,032] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:11,032] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:11,063] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:11 +0000] "POST /admin/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:11,064] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,064] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:11,064] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:11,064] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:11,065] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:11,066] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:11,066] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:11,066] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.0
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.1
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.2
WARN  [2023-01-06 20:45:11,067] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.4
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.3
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.5
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.6
INFO  [2023-01-06 20:45:11,067] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.7
INFO  [2023-01-06 20:45:11,172] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,178] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,194] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:11,194] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:11,300] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test QUERY INIT
INFO  [2023-01-06 20:45:11,319] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:11,320] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3e679afe-de17-4b0d-97c7-d225913c68b4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test))]]
INFO  [2023-01-06 20:45:11,325] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4
INFO  [2023-01-06 20:45:11,325] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4
127.0.0.1 - - [06/Jan/2023:20:45:11 +0000] "POST /api/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test/queries HTTP/1.1" 201 1469 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:11,328] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4] with 2 results within PT0.002747S
INFO  [2023-01-06 20:45:11,328] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4] with 6 results within PT0.00327S
INFO  [2023-01-06 20:45:11,328] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4, workerId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_49b8f7b4-1adf-456d-975c-fd58e19e2084, startTime=2023-01-06T20:45:11.325308, finishTime=2023-01-06T20:45:11.328055) of size 2
INFO  [2023-01-06 20:45:11,329] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4, workerId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_7d0bd3e6-1d87-4daa-95e6-60176cd3835b, startTime=2023-01-06T20:45:11.325185, finishTime=2023-01-06T20:45:11.328455) of size 6
INFO  [2023-01-06 20:45:11,329] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3e679afe-de17-4b0d-97c7-d225913c68b4 ManagedQuery within PT0.009347S
127.0.0.1 - - [06/Jan/2023:20:45:11 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test/queries/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4 HTTP/1.1" 200 1844 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:11,354] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test], queryId=3e679afe-de17-4b0d-97c7-d225913c68b4, label=Alter	@§$, creationTime=2023-01-06T20:45:11.319832, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22cb6858[Count = 0], startTime=2023-01-06T20:45:11.320067, finishTime=2023-01-06T20:45:11.329414, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@58bdd097), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=8, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1344006b, com.bakdata.conquery.models.query.ColumnDescriptor@2defd5a4]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:11,355] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test], queryId=3e679afe-de17-4b0d-97c7-d225913c68b4, label=Alter	@§$, creationTime=2023-01-06T20:45:11.319832, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22cb6858[Count = 0], startTime=2023-01-06T20:45:11.320067, finishTime=2023-01-06T20:45:11.329414, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@58bdd097), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=8, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1344006b, com.bakdata.conquery.models.query.ColumnDescriptor@2defd5a4]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
127.0.0.1 - - [06/Jan/2023:20:45:11 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT%20Test/result/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.3e679afe-de17-4b0d-97c7-d225913c68b4.csv?pretty=false HTTP/1.1" 200 225 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:11,372] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test on 9 rows
INFO  [2023-01-06 20:45:11,372] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:11,372] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:11,372] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:11,373] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_49b8f7b4-1adf-456d-975c-fd58e19e2084
INFO  [2023-01-06 20:45:11,373] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_7d0bd3e6-1d87-4daa-95e6-60176cd3835b
INFO  [2023-01-06 20:45:11,470] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:11,470] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_49b8f7b4-1adf-456d-975c-fd58e19e2084
INFO  [2023-01-06 20:45:11,470] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_7d0bd3e6-1d87-4daa-95e6-60176cd3835b
INFO  [2023-01-06 20:45:11,570] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test
INFO  [2023-01-06 20:45:11,570] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,599] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:11,600] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-06 20:45:11,600] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:11,600] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:11,601] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-06 20:45:11,601] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-06 20:45:11,601] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:11,601] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_1b0a604e-6a38-477c-b1c9-65cd11b2578f are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_1b0a604e-6a38-477c-b1c9-65cd11b2578f are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_addc6f09-54ed-44f7-8430-0d50f20b4f64 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_addc6f09-54ed-44f7-8430-0d50f20b4f64 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:11,603] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:11,707] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,714] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,714] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:11,715] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:11,832] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:11,942] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:11,942] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:11,942] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:11,942] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_ERSTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000326079sINFO  [2023-01-06 20:45:11,976] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:11,976] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@156ef106)
INFO  [2023-01-06 20:45:11,976] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@74ef39d0), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@74dc001b), dateReader=com.bakdata.conquery.util.DateReader@7e115f32, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:11,976] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@55f1675d)
INFO  [2023-01-06 20:45:11,976] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@6bd39414)
INFO  [2023-01-06 20:45:11,978] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:11,978] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:11,978] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:11,997] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_NEGATION_ERSTER$20Test.table1
INFO  [2023-01-06 20:45:11,998] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:11 +0000] "POST /admin/datasets/DATE_DISTANCE_NEGATION_ERSTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_NEGATION_ERSTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:11,998] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:11,998] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:11,998] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:11,999] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:12,000] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:12,000] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:12,000] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.0
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.2
WARN  [2023-01-06 20:45:12,001] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.1
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.3
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.5
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.4
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.7
INFO  [2023-01-06 20:45:12,001] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.6
INFO  [2023-01-06 20:45:12,106] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,111] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,123] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,124] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:12,124] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:12,230] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_NEGATION_ERSTER Test QUERY INIT
INFO  [2023-01-06 20:45:12,246] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_NEGATION_ERSTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:12,247] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7615d358-3a09-48d8-8fd7-330136a579cb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_ERSTER Test))]]
INFO  [2023-01-06 20:45:12,252] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb
INFO  [2023-01-06 20:45:12,253] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb
127.0.0.1 - - [06/Jan/2023:20:45:12 +0000] "POST /api/datasets/DATE_DISTANCE_NEGATION_ERSTER$20Test/queries HTTP/1.1" 201 1468 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:12,257] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb] with 10 results within PT0.004097S
INFO  [2023-01-06 20:45:12,257] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb] with 10 results within PT0.004315S
INFO  [2023-01-06 20:45:12,257] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb, workerId=DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_addc6f09-54ed-44f7-8430-0d50f20b4f64, startTime=2023-01-06T20:45:12.253136, finishTime=2023-01-06T20:45:12.257233) of size 10
INFO  [2023-01-06 20:45:12,258] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb, workerId=DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_1b0a604e-6a38-477c-b1c9-65cd11b2578f, startTime=2023-01-06T20:45:12.253027, finishTime=2023-01-06T20:45:12.257342) of size 10
INFO  [2023-01-06 20:45:12,258] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7615d358-3a09-48d8-8fd7-330136a579cb ManagedQuery within PT0.010546S
127.0.0.1 - - [06/Jan/2023:20:45:12 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_ERSTER$20Test/queries/DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb HTTP/1.1" 200 1805 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:12,287] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test], queryId=7615d358-3a09-48d8-8fd7-330136a579cb, label=Alter	@§$, creationTime=2023-01-06T20:45:12.247255, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@47cce399[Count = 0], startTime=2023-01-06T20:45:12.247500, finishTime=2023-01-06T20:45:12.258046, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@762bc97a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4b6f48c0, com.bakdata.conquery.models.query.ColumnDescriptor@b0fd5f4]) download on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:12,287] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test], queryId=7615d358-3a09-48d8-8fd7-330136a579cb, label=Alter	@§$, creationTime=2023-01-06T20:45:12.247255, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@47cce399[Count = 0], startTime=2023-01-06T20:45:12.247500, finishTime=2023-01-06T20:45:12.258046, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@762bc97a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4b6f48c0, com.bakdata.conquery.models.query.ColumnDescriptor@b0fd5f4]) on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test]
127.0.0.1 - - [06/Jan/2023:20:45:12 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_ERSTER%20Test/result/DATE_DISTANCE_NEGATION_ERSTER$20Test.7615d358-3a09-48d8-8fd7-330136a579cb.csv?pretty=false HTTP/1.1" 200 130 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:12,305] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_NEGATION_ERSTER Test on 21 rows
INFO  [2023-01-06 20:45:12,305] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-06 20:45:12,306] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-06 20:45:12,306] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-06 20:45:12,306] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_ERSTER Test_addc6f09-54ed-44f7-8430-0d50f20b4f64
INFO  [2023-01-06 20:45:12,306] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_ERSTER Test_1b0a604e-6a38-477c-b1c9-65cd11b2578f
INFO  [2023-01-06 20:45:12,308] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_ERSTER Test_addc6f09-54ed-44f7-8430-0d50f20b4f64
INFO  [2023-01-06 20:45:12,308] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_ERSTER Test_1b0a604e-6a38-477c-b1c9-65cd11b2578f
INFO  [2023-01-06 20:45:12,308] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-06 20:45:12,408] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_NEGATION_ERSTER$20Test
INFO  [2023-01-06 20:45:12,408] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,430] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-06 20:45:12,430] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-06 20:45:12,430] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:12,430] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:12,431] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-06 20:45:12,431] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-06 20:45:12,431] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:12,431] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:12,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_602493ab-7341-4044-9956-54a9a9dbfd79 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:12,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_602493ab-7341-4044-9956-54a9a9dbfd79 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:12,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:12,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_8cc87952-e195-47af-8357-3285a7d37522 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:12,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_8cc87952-e195-47af-8357-3285a7d37522 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:12,432] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:12,437] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,536] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,543] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,543] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:12,543] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_LETZTER$20Test.table1
INFO  [2023-01-06 20:45:12,661] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,770] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:12,771] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:12,771] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:12,771] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_LETZTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000424207sINFO  [2023-01-06 20:45:12,814] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:12,814] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@f8b200c), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@227a994), dateReader=com.bakdata.conquery.util.DateReader@166fad13, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:12,814] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@701dc0af)
INFO  [2023-01-06 20:45:12,814] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@4792e433)
INFO  [2023-01-06 20:45:12,815] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@56e926b4)
INFO  [2023-01-06 20:45:12,817] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:12,817] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:12,817] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:12,830] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_NEGATION_LETZTER$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:12 +0000] "POST /admin/datasets/DATE_DISTANCE_NEGATION_LETZTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_NEGATION_LETZTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:12,831] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,831] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:12,831] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:12,831] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:12,832] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:12,833] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:12,833] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:12,833] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.0
INFO  [2023-01-06 20:45:12,834] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.1
WARN  [2023-01-06 20:45:12,834] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:12,834] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.4
INFO  [2023-01-06 20:45:12,834] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.2
INFO  [2023-01-06 20:45:12,834] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.3
INFO  [2023-01-06 20:45:12,834] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.5
INFO  [2023-01-06 20:45:12,834] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.7
INFO  [2023-01-06 20:45:12,880] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.6
INFO  [2023-01-06 20:45:12,986] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:12,991] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,008] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,008] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:13,008] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:13,114] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_NEGATION_LETZTER Test QUERY INIT
INFO  [2023-01-06 20:45:13,131] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_NEGATION_LETZTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:13,131] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[8b1a6734-8191-460d-ae00-720320eef57b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_LETZTER Test))]]
INFO  [2023-01-06 20:45:13,137] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b
INFO  [2023-01-06 20:45:13,138] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b
127.0.0.1 - - [06/Jan/2023:20:45:13 +0000] "POST /api/datasets/DATE_DISTANCE_NEGATION_LETZTER$20Test/queries HTTP/1.1" 201 1474 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:13,140] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b] with 10 results within PT0.002879S
INFO  [2023-01-06 20:45:13,141] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b] with 10 results within PT0.003252S
INFO  [2023-01-06 20:45:13,141] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b, workerId=DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_8cc87952-e195-47af-8357-3285a7d37522, startTime=2023-01-06T20:45:13.138036, finishTime=2023-01-06T20:45:13.140915) of size 10
INFO  [2023-01-06 20:45:13,141] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b, workerId=DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_602493ab-7341-4044-9956-54a9a9dbfd79, startTime=2023-01-06T20:45:13.137920, finishTime=2023-01-06T20:45:13.141172) of size 10
INFO  [2023-01-06 20:45:13,142] com.bakdata.conquery.models.execution.ManagedExecution: DONE 8b1a6734-8191-460d-ae00-720320eef57b ManagedQuery within PT0.010037S
127.0.0.1 - - [06/Jan/2023:20:45:13 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_LETZTER$20Test/queries/DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b HTTP/1.1" 200 1815 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:13,169] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test], queryId=8b1a6734-8191-460d-ae00-720320eef57b, label=Alter	@§$, creationTime=2023-01-06T20:45:13.131776, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@16b1874e[Count = 0], startTime=2023-01-06T20:45:13.132032, finishTime=2023-01-06T20:45:13.142069, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2fb91c84), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@73a8578c, com.bakdata.conquery.models.query.ColumnDescriptor@707bf3c3]) download on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:13,169] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test], queryId=8b1a6734-8191-460d-ae00-720320eef57b, label=Alter	@§$, creationTime=2023-01-06T20:45:13.131776, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@16b1874e[Count = 0], startTime=2023-01-06T20:45:13.132032, finishTime=2023-01-06T20:45:13.142069, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2fb91c84), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@73a8578c, com.bakdata.conquery.models.query.ColumnDescriptor@707bf3c3]) on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test]
127.0.0.1 - - [06/Jan/2023:20:45:13 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_LETZTER%20Test/result/DATE_DISTANCE_NEGATION_LETZTER$20Test.8b1a6734-8191-460d-ae00-720320eef57b.csv?pretty=false HTTP/1.1" 200 130 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:13,189] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_NEGATION_LETZTER Test on 21 rows
INFO  [2023-01-06 20:45:13,190] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-06 20:45:13,190] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-06 20:45:13,190] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-06 20:45:13,190] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_LETZTER Test_8cc87952-e195-47af-8357-3285a7d37522
INFO  [2023-01-06 20:45:13,191] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_LETZTER Test_602493ab-7341-4044-9956-54a9a9dbfd79
INFO  [2023-01-06 20:45:13,231] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-06 20:45:13,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_LETZTER Test_602493ab-7341-4044-9956-54a9a9dbfd79
INFO  [2023-01-06 20:45:13,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_LETZTER Test_8cc87952-e195-47af-8357-3285a7d37522
INFO  [2023-01-06 20:45:13,234] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_NEGATION_LETZTER$20Test
INFO  [2023-01-06 20:45:13,234] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,414] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-06 20:45:13,414] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:13,414] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:13,414] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:13,415] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:13,415] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:13,415] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:13,415] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:13,417] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_bb4a3dd2-49bc-4cc0-a152-a5a75b6e1df8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:13,417] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_bb4a3dd2-49bc-4cc0-a152-a5a75b6e1df8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:13,417] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:13,418] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_447f9b37-eda8-46de-acb6-118ade35e294 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:13,418] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_447f9b37-eda8-46de-acb6-118ade35e294 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:13,418] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:13,422] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,521] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,529] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,530] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:13,530] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-06 20:45:13,640] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,750] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:13,750] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:13,750] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-06 20:45:13,750] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000401099sINFO  [2023-01-06 20:45:13,791] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-06 20:45:13,791] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@64b42f6)
INFO  [2023-01-06 20:45:13,791] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6cc0076c), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@249d0bfd), dateReader=com.bakdata.conquery.util.DateReader@150b190e, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:45:13,791] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@62f39823)
INFO  [2023-01-06 20:45:13,791] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@1f78789f)
INFO  [2023-01-06 20:45:13,794] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:13,794] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:13,794] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:13,810] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:13 +0000] "POST /admin/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:13,810] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,811] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:13,811] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:13,811] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:13,813] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:45:13,813] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:13,813] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.0
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.1
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.2
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.3
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.4
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.5
WARN  [2023-01-06 20:45:13,815] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:13,815] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.6
INFO  [2023-01-06 20:45:13,816] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.7
INFO  [2023-01-06 20:45:13,920] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,942] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:13,942] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:13,942] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:14,048] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test QUERY INIT
INFO  [2023-01-06 20:45:14,079] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:14,080] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2113f06b-8cd9-471d-a702-35a45f7e2795] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test))]]
INFO  [2023-01-06 20:45:14,085] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795
INFO  [2023-01-06 20:45:14,086] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795
127.0.0.1 - - [06/Jan/2023:20:45:14 +0000] "POST /api/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test/queries HTTP/1.1" 201 1510 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:45:14,087] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795] with 8 results within PT0.002176S
INFO  [2023-01-06 20:45:14,088] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795] with 10 results within PT0.00202S
INFO  [2023-01-06 20:45:14,088] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795, workerId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_bb4a3dd2-49bc-4cc0-a152-a5a75b6e1df8, startTime=2023-01-06T20:45:14.085761, finishTime=2023-01-06T20:45:14.087937) of size 8
INFO  [2023-01-06 20:45:14,088] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795, workerId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_447f9b37-eda8-46de-acb6-118ade35e294, startTime=2023-01-06T20:45:14.086221, finishTime=2023-01-06T20:45:14.088241) of size 10
INFO  [2023-01-06 20:45:14,089] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2113f06b-8cd9-471d-a702-35a45f7e2795 ManagedQuery within PT0.008531S
127.0.0.1 - - [06/Jan/2023:20:45:14 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test/queries/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795 HTTP/1.1" 200 1886 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:14,115] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test], queryId=2113f06b-8cd9-471d-a702-35a45f7e2795, label=Alter	@§$, creationTime=2023-01-06T20:45:14.080256, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4c372127[Count = 0], startTime=2023-01-06T20:45:14.080484, finishTime=2023-01-06T20:45:14.089015, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@15a945c7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=18, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@34e759c0, com.bakdata.conquery.models.query.ColumnDescriptor@19f22535]) download on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:14,115] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test], queryId=2113f06b-8cd9-471d-a702-35a45f7e2795, label=Alter	@§$, creationTime=2023-01-06T20:45:14.080256, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4c372127[Count = 0], startTime=2023-01-06T20:45:14.080484, finishTime=2023-01-06T20:45:14.089015, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@15a945c7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=18, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@34e759c0, com.bakdata.conquery.models.query.ColumnDescriptor@19f22535]) on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
127.0.0.1 - - [06/Jan/2023:20:45:14 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT%20Test/result/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.2113f06b-8cd9-471d-a702-35a45f7e2795.csv?pretty=false HTTP/1.1" 200 120 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:14,132] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test on 19 rows
INFO  [2023-01-06 20:45:14,132] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:14,133] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:14,133] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-06 20:45:14,133] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_447f9b37-eda8-46de-acb6-118ade35e294
INFO  [2023-01-06 20:45:14,133] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_bb4a3dd2-49bc-4cc0-a152-a5a75b6e1df8
INFO  [2023-01-06 20:45:14,216] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:14,230] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_447f9b37-eda8-46de-acb6-118ade35e294
INFO  [2023-01-06 20:45:14,230] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_bb4a3dd2-49bc-4cc0-a152-a5a75b6e1df8
INFO  [2023-01-06 20:45:14,230] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test
INFO  [2023-01-06 20:45:14,231] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,348] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-06 20:45:14,348] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DELETE_IMPORT_TESTS Test
INFO  [2023-01-06 20:45:14,348] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:14,348] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:14,349] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-06 20:45:14,349] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-06 20:45:14,349] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:14,349] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_f5730c89-7217-42c6-b070-8c5d3e1e2349 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_f5730c89-7217-42c6-b070-8c5d3e1e2349 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_89149f22-a96f-4aa8-bffb-e8540a20f836 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_89149f22-a96f-4aa8-bffb-e8540a20f836 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:14,351] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:14,455] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,462] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table
INFO  [2023-01-06 20:45:14,462] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table
INFO  [2023-01-06 20:45:14,462] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table2
INFO  [2023-01-06 20:45:14,462] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table2
INFO  [2023-01-06 20:45:14,578] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,696] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:14,697] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:14,697] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:14,697] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 115 B in total
INFO  [2023-01-06 20:45:14,697] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DELETE_IMPORT_TESTS Test/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.029200848sINFO  [2023-01-06 20:45:14,726] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:14,726] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:14,726] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7318cbf5)
INFO  [2023-01-06 20:45:14,729] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DELETE_IMPORT_TESTS Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:14,729] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DELETE_IMPORT_TESTS Test/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000470023sINFO  [2023-01-06 20:45:14,744] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:14,745] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:14,745] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@679a03c2)
INFO  [2023-01-06 20:45:14,749] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DELETE_IMPORT_TESTS Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:14,749] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:14,749] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DELETE_IMPORT_TESTS Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:14,749] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DELETE_IMPORT_TESTS Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:14,762] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into DELETE_IMPORT_TESTS$20Test.test_table
127.0.0.1 - - [06/Jan/2023:20:45:14 +0000] "POST /admin/datasets/DELETE_IMPORT_TESTS%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DELETE_IMPORT_TESTS+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:14,764] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:14,765] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:14,765] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:14,768] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:14,768] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table.test_table], containing 2 entries.
INFO  [2023-01-06 20:45:14,769] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table.test_table], containing 2 entries.
WARN  [2023-01-06 20:45:14,770] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:14,770] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DELETE_IMPORT_TESTS$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:14,779] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table2 into DELETE_IMPORT_TESTS$20Test.test_table2
INFO  [2023-01-06 20:45:14,779] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:14,779] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:14,779] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:45:14 +0000] "POST /admin/datasets/DELETE_IMPORT_TESTS%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DELETE_IMPORT_TESTS+Test%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:14,779] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,780] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:14,780] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table2.test_table2], containing 2 entries.
WARN  [2023-01-06 20:45:14,780] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:14,780] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table2.test_table2], containing 2 entries.
INFO  [2023-01-06 20:45:14,780] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DELETE_IMPORT_TESTS$20Test.test_table2.test_table2.0
INFO  [2023-01-06 20:45:14,780] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DELETE_IMPORT_TESTS$20Test.test_table2.test_table2.1
INFO  [2023-01-06 20:45:14,885] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,904] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:14,904] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:14,904] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:15,011] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DELETE_IMPORT_TESTS Test QUERY INIT
INFO  [2023-01-06 20:45:15,029] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DELETE_IMPORT_TESTS$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:15,029] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[46e775a5-3108-4826-a2b8-3926de8af4dd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DELETE_IMPORT_TESTS Test))]]
INFO  [2023-01-06 20:45:15,033] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd
INFO  [2023-01-06 20:45:15,033] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd
INFO  [2023-01-06 20:45:15,034] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd] with 0 results within PT0.000741S
INFO  [2023-01-06 20:45:15,034] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd, workerId=DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_89149f22-a96f-4aa8-bffb-e8540a20f836, startTime=2023-01-06T20:45:15.033711, finishTime=2023-01-06T20:45:15.034452) of size 0
INFO  [2023-01-06 20:45:15,035] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd] with 2 results within PT0.001311S
127.0.0.1 - - [06/Jan/2023:20:45:15 +0000] "POST /api/datasets/DELETE_IMPORT_TESTS$20Test/queries HTTP/1.1" 201 1243 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:15,035] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd, workerId=DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_f5730c89-7217-42c6-b070-8c5d3e1e2349, startTime=2023-01-06T20:45:15.033720, finishTime=2023-01-06T20:45:15.035031) of size 2
INFO  [2023-01-06 20:45:15,035] com.bakdata.conquery.models.execution.ManagedExecution: DONE 46e775a5-3108-4826-a2b8-3926de8af4dd ManagedQuery within PT0.006079S
127.0.0.1 - - [06/Jan/2023:20:45:15 +0000] "GET /api/datasets/DELETE_IMPORT_TESTS$20Test/queries/DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd HTTP/1.1" 200 1538 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:15,063] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DELETE_IMPORT_TESTS Test], queryId=46e775a5-3108-4826-a2b8-3926de8af4dd, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:15.029550, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@daaddf8[Count = 0], startTime=2023-01-06T20:45:15.029846, finishTime=2023-01-06T20:45:15.035925, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6ccecba0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DELETE_IMPORT_TESTS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@36478de2, com.bakdata.conquery.models.query.ColumnDescriptor@742d27f7]) download on dataset Dataset[label=null, name=DELETE_IMPORT_TESTS Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:15,063] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DELETE_IMPORT_TESTS Test], queryId=46e775a5-3108-4826-a2b8-3926de8af4dd, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:15.029550, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@daaddf8[Count = 0], startTime=2023-01-06T20:45:15.029846, finishTime=2023-01-06T20:45:15.035925, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6ccecba0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DELETE_IMPORT_TESTS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@36478de2, com.bakdata.conquery.models.query.ColumnDescriptor@742d27f7]) on dataset Dataset[label=null, name=DELETE_IMPORT_TESTS Test]
127.0.0.1 - - [06/Jan/2023:20:45:15 +0000] "GET /api/datasets/DELETE_IMPORT_TESTS%20Test/result/DELETE_IMPORT_TESTS$20Test.46e775a5-3108-4826-a2b8-3926de8af4dd.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 27
INFO  [2023-01-06 20:45:15,088] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DELETE_IMPORT_TESTS Test on 3 rows
INFO  [2023-01-06 20:45:15,088] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DELETE_IMPORT_TESTS Test
INFO  [2023-01-06 20:45:15,088] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-06 20:45:15,088] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-06 20:45:15,088] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DELETE_IMPORT_TESTS Test_89149f22-a96f-4aa8-bffb-e8540a20f836
INFO  [2023-01-06 20:45:15,088] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DELETE_IMPORT_TESTS Test_f5730c89-7217-42c6-b070-8c5d3e1e2349
INFO  [2023-01-06 20:45:15,150] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DELETE_IMPORT_TESTS Test
INFO  [2023-01-06 20:45:15,150] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DELETE_IMPORT_TESTS Test_f5730c89-7217-42c6-b070-8c5d3e1e2349
INFO  [2023-01-06 20:45:15,151] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DELETE_IMPORT_TESTS Test_89149f22-a96f-4aa8-bffb-e8540a20f836
INFO  [2023-01-06 20:45:15,186] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DELETE_IMPORT_TESTS$20Test
INFO  [2023-01-06 20:45:15,186] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,309] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DELETE_IMPORT_TESTS Test
INFO  [2023-01-06 20:45:15,310] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:15,310] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:15,310] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:15,311] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:15,311] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:15,311] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:15,311] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_6a82ddef-0232-41dc-a22d-4e0fa1c52ea0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_6a82ddef-0232-41dc-a22d-4e0fa1c52ea0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_4b1fbc0f-8d3f-45da-b206-7873f6e15991 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_4b1fbc0f-8d3f-45da-b206-7873f6e15991 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:15,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:15,417] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,424] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,424] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:15,424] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:15,544] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,654] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:15,655] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:15,655] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 221 B in total
INFO  [2023-01-06 20:45:15,655] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000421968sINFO  [2023-01-06 20:45:15,698] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=5, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:15,698] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[ende] with DateParser(super=Parser(lines=5, nullLines=1), subType=IntegerParser(super=Parser(lines=5, nullLines=1), minValue=14608, maxValue=14644), dateReader=com.bakdata.conquery.util.DateReader@2e82502f)
INFO  [2023-01-06 20:45:15,698] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[anfang] with DateParser(super=Parser(lines=5, nullLines=1), subType=IntegerParser(super=Parser(lines=5, nullLines=1), minValue=14608, maxValue=14642), dateReader=com.bakdata.conquery.util.DateReader@223f115d)
INFO  [2023-01-06 20:45:15,698] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=5, nullLines=1), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14608, maxValue=14642), dateReader=com.bakdata.conquery.util.DateReader@34ae4f31), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14608, maxValue=14644), dateReader=com.bakdata.conquery.util.DateReader@bfa2b39), dateReader=com.bakdata.conquery.util.DateReader@60161860, onlyQuarters=false, maxValue=14644, minValue=14608, anyOpen=false)
INFO  [2023-01-06 20:45:15,701] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:15,701] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:15,701] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:15,716] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:15,717] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:15 +0000] "POST /admin/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:15,718] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:15,718] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:15,718] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:15,727] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:15,727] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 5 entries.
INFO  [2023-01-06 20:45:15,727] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 5 entries.
WARN  [2023-01-06 20:45:15,728] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:15,728] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:15,728] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:15,833] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,839] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,851] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:15,851] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:15,851] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:15,957] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:15,975] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:15,976] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[68364693-b26e-453c-9ce3-12d3a7b61640] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:15,981] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640
INFO  [2023-01-06 20:45:15,981] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640
127.0.0.1 - - [06/Jan/2023:20:45:15 +0000] "POST /api/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1483 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:15,983] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640] with 0 results within PT0.002431S
INFO  [2023-01-06 20:45:15,984] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640] with 2 results within PT0.002717S
INFO  [2023-01-06 20:45:15,984] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640, workerId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_4b1fbc0f-8d3f-45da-b206-7873f6e15991, startTime=2023-01-06T20:45:15.981426, finishTime=2023-01-06T20:45:15.983857) of size 0
INFO  [2023-01-06 20:45:15,984] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640, workerId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_6a82ddef-0232-41dc-a22d-4e0fa1c52ea0, startTime=2023-01-06T20:45:15.981426, finishTime=2023-01-06T20:45:15.984143) of size 2
INFO  [2023-01-06 20:45:15,985] com.bakdata.conquery.models.execution.ManagedExecution: DONE 68364693-b26e-453c-9ce3-12d3a7b61640 ManagedQuery within PT0.008785S
127.0.0.1 - - [06/Jan/2023:20:45:16 +0000] "GET /api/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640 HTTP/1.1" 200 1850 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:16,013] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=68364693-b26e-453c-9ce3-12d3a7b61640, label=KG-Tage-DURATION_SUM	@§$, creationTime=2023-01-06T20:45:15.975976, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@18583c37[Count = 0], startTime=2023-01-06T20:45:15.976236, finishTime=2023-01-06T20:45:15.985021, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@24505ba9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5487561d, com.bakdata.conquery.models.query.ColumnDescriptor@5a62df19]) download on dataset Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:16,013] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=68364693-b26e-453c-9ce3-12d3a7b61640, label=KG-Tage-DURATION_SUM	@§$, creationTime=2023-01-06T20:45:15.975976, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@18583c37[Count = 0], startTime=2023-01-06T20:45:15.976236, finishTime=2023-01-06T20:45:15.985021, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@24505ba9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5487561d, com.bakdata.conquery.models.query.ColumnDescriptor@5a62df19]) on dataset Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:16 +0000] "GET /api/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/result/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.68364693-b26e-453c-9ce3-12d3a7b61640.csv?pretty=false HTTP/1.1" 200 66 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:16,032] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:16,032] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:16,032] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:16,032] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:16,032] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_6a82ddef-0232-41dc-a22d-4e0fa1c52ea0
INFO  [2023-01-06 20:45:16,032] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_4b1fbc0f-8d3f-45da-b206-7873f6e15991
INFO  [2023-01-06 20:45:16,111] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:16,112] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_6a82ddef-0232-41dc-a22d-4e0fa1c52ea0
INFO  [2023-01-06 20:45:16,112] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_4b1fbc0f-8d3f-45da-b206-7873f6e15991
INFO  [2023-01-06 20:45:16,130] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:16,130] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,257] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:16,257] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND Test
INFO  [2023-01-06 20:45:16,257] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:16,258] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:16,259] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-06 20:45:16,259] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-06 20:45:16,259] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:16,259] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:16,260] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,261] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test.worker_AND$20Test_3db405a4-37d9-4565-8336-7404b8496a93 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:16,261] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test.worker_AND$20Test_3db405a4-37d9-4565-8336-7404b8496a93 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:16,261] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:16,261] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test.worker_AND$20Test_6f2a8f9f-2261-4bc3-aed5-0bfd8dc23696 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:16,261] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test.worker_AND$20Test_6f2a8f9f-2261-4bc3-aed5-0bfd8dc23696 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:16,261] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:16,364] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,371] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,372] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test.table
INFO  [2023-01-06 20:45:16,372] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test.table
INFO  [2023-01-06 20:45:16,483] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,593] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:16,593] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:16,593] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 152 B in total
INFO  [2023-01-06 20:45:16,593] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000333655sINFO  [2023-01-06 20:45:16,627] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=1, average=2.000000, max=3}
INFO  [2023-01-06 20:45:16,627] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:16,627] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1e5ab6c1)
INFO  [2023-01-06 20:45:16,630] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:16,630] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:16,630] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:16,653] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:16 +0000] "POST /admin/datasets/AND%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:45:16,654] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,654] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:16,655] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:16,655] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:16,656] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:16,656] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test.table.table], containing 8 entries.
INFO  [2023-01-06 20:45:16,656] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test.table.table], containing 8 entries.
WARN  [2023-01-06 20:45:16,657] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:16,657] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test.table.table.0
INFO  [2023-01-06 20:45:16,657] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test.table.table.1
INFO  [2023-01-06 20:45:16,762] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,768] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,778] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:16,779] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:16,779] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:16,886] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND Test QUERY INIT
127.0.0.1 - - [06/Jan/2023:20:45:16 +0000] "POST /api/datasets/AND$20Test/queries HTTP/1.1" 201 1490 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:16,901] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:16,901] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d4817925-139a-4406-b661-a5a309d5cd23] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND Test))]]
INFO  [2023-01-06 20:45:16,904] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23
INFO  [2023-01-06 20:45:16,904] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23
INFO  [2023-01-06 20:45:16,907] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23] with 1 results within PT0.00371S
INFO  [2023-01-06 20:45:16,908] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23, workerId=AND$20Test.worker_AND$20Test_3db405a4-37d9-4565-8336-7404b8496a93, startTime=2023-01-06T20:45:16.904212, finishTime=2023-01-06T20:45:16.907922) of size 1
INFO  [2023-01-06 20:45:16,915] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23] with 1 results within PT0.009394S
INFO  [2023-01-06 20:45:16,916] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23, workerId=AND$20Test.worker_AND$20Test_6f2a8f9f-2261-4bc3-aed5-0bfd8dc23696, startTime=2023-01-06T20:45:16.906577, finishTime=2023-01-06T20:45:16.915971) of size 1
INFO  [2023-01-06 20:45:16,916] com.bakdata.conquery.models.execution.ManagedExecution: DONE d4817925-139a-4406-b661-a5a309d5cd23 ManagedQuery within PT0.014778S
127.0.0.1 - - [06/Jan/2023:20:45:16 +0000] "GET /api/datasets/AND$20Test/queries/AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23 HTTP/1.1" 200 1722 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:16,929] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test], queryId=d4817925-139a-4406-b661-a5a309d5cd23, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:16.901826, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4de98445[Count = 0], startTime=2023-01-06T20:45:16.901996, finishTime=2023-01-06T20:45:16.916774, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@621282b2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7fdaac87, com.bakdata.conquery.models.query.ColumnDescriptor@18d3829a, com.bakdata.conquery.models.query.ColumnDescriptor@1919f669]) download on dataset Dataset[label=null, name=AND Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:16,929] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test], queryId=d4817925-139a-4406-b661-a5a309d5cd23, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:16.901826, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4de98445[Count = 0], startTime=2023-01-06T20:45:16.901996, finishTime=2023-01-06T20:45:16.916774, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@621282b2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7fdaac87, com.bakdata.conquery.models.query.ColumnDescriptor@18d3829a, com.bakdata.conquery.models.query.ColumnDescriptor@1919f669]) on dataset Dataset[label=null, name=AND Test]
127.0.0.1 - - [06/Jan/2023:20:45:16 +0000] "GET /api/datasets/AND%20Test/result/AND$20Test.d4817925-139a-4406-b661-a5a309d5cd23.csv?pretty=false HTTP/1.1" 200 87 "-" "Conquery (test client)" 28
INFO  [2023-01-06 20:45:16,955] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND Test on 3 rows
INFO  [2023-01-06 20:45:16,955] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND Test
INFO  [2023-01-06 20:45:16,956] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-06 20:45:16,956] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-06 20:45:16,956] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test_3db405a4-37d9-4565-8336-7404b8496a93
INFO  [2023-01-06 20:45:16,956] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test_6f2a8f9f-2261-4bc3-aed5-0bfd8dc23696
INFO  [2023-01-06 20:45:16,959] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND Test
INFO  [2023-01-06 20:45:16,960] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test_3db405a4-37d9-4565-8336-7404b8496a93
INFO  [2023-01-06 20:45:16,961] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test_6f2a8f9f-2261-4bc3-aed5-0bfd8dc23696
INFO  [2023-01-06 20:45:17,062] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20Test
INFO  [2023-01-06 20:45:17,062] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,085] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND Test
INFO  [2023-01-06 20:45:17,085] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DATE LOGIC Test
INFO  [2023-01-06 20:45:17,085] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:17,085] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:17,086] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-06 20:45:17,087] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:17,087] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-06 20:45:17,087] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:17,090] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_2ddfee4d-f6dc-4b5e-ae7e-579e76ac22b1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:17,090] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_2ddfee4d-f6dc-4b5e-ae7e-579e76ac22b1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:17,090] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:17,093] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_a22cb94f-df52-4efb-9118-409628313bfb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:17,093] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_a22cb94f-df52-4efb-9118-409628313bfb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:17,093] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:17,098] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,197] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,204] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,204] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DATE$20LOGIC$20Test.table
INFO  [2023-01-06 20:45:17,204] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DATE$20LOGIC$20Test.table
INFO  [2023-01-06 20:45:17,319] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,428] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:17,428] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:17,428] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 240 B in total
INFO  [2023-01-06 20:45:17,428] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DATE LOGIC Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000399943sINFO  [2023-01-06 20:45:17,469] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=13, min=2, average=2.600000, max=4}
INFO  [2023-01-06 20:45:17,469] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=13, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:17,469] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=0), subType=IntegerParser(super=Parser(lines=13, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@58cbb5fc)
INFO  [2023-01-06 20:45:17,471] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:17,471] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:17,471] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:17,493] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DATE$20LOGIC$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:17 +0000] "POST /admin/datasets/AND%20DATE%20LOGIC%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DATE+LOGIC+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:17,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,495] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:17,495] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:17,495] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:17,497] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:17,497] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
INFO  [2023-01-06 20:45:17,497] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
WARN  [2023-01-06 20:45:17,498] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:17,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DATE$20LOGIC$20Test.table.table.1
INFO  [2023-01-06 20:45:17,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DATE$20LOGIC$20Test.table.table.0
INFO  [2023-01-06 20:45:17,603] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,608] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,618] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:17,619] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:17,619] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:17,724] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DATE LOGIC Test QUERY INIT
INFO  [2023-01-06 20:45:17,740] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DATE$20LOGIC$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:17,741] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DATE LOGIC Test))]]
INFO  [2023-01-06 20:45:17,744] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc
INFO  [2023-01-06 20:45:17,744] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc
INFO  [2023-01-06 20:45:17,745] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc] with 2 results within PT0.001218S
INFO  [2023-01-06 20:45:17,745] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc] with 2 results within PT0.001303S
127.0.0.1 - - [06/Jan/2023:20:45:17 +0000] "POST /api/datasets/AND$20DATE$20LOGIC$20Test/queries HTTP/1.1" 201 1567 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:17,746] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc, workerId=AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_a22cb94f-df52-4efb-9118-409628313bfb, startTime=2023-01-06T20:45:17.744657, finishTime=2023-01-06T20:45:17.745875) of size 2
INFO  [2023-01-06 20:45:17,746] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc, workerId=AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_2ddfee4d-f6dc-4b5e-ae7e-579e76ac22b1, startTime=2023-01-06T20:45:17.744601, finishTime=2023-01-06T20:45:17.745904) of size 2
INFO  [2023-01-06 20:45:17,746] com.bakdata.conquery.models.execution.ManagedExecution: DONE 67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc ManagedQuery within PT0.00576S
127.0.0.1 - - [06/Jan/2023:20:45:17 +0000] "GET /api/datasets/AND$20DATE$20LOGIC$20Test/queries/AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc HTTP/1.1" 200 1858 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:17,776] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DATE LOGIC Test], queryId=67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:17.740761, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2751489[Count = 0], startTime=2023-01-06T20:45:17.741073, finishTime=2023-01-06T20:45:17.746833, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@294ebd87), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4e6dee39, com.bakdata.conquery.models.query.ColumnDescriptor@3ab2d599, com.bakdata.conquery.models.query.ColumnDescriptor@10494f1a]) download on dataset Dataset[label=null, name=AND DATE LOGIC Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:17,777] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DATE LOGIC Test], queryId=67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:17.740761, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2751489[Count = 0], startTime=2023-01-06T20:45:17.741073, finishTime=2023-01-06T20:45:17.746833, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@294ebd87), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4e6dee39, com.bakdata.conquery.models.query.ColumnDescriptor@3ab2d599, com.bakdata.conquery.models.query.ColumnDescriptor@10494f1a]) on dataset Dataset[label=null, name=AND DATE LOGIC Test]
127.0.0.1 - - [06/Jan/2023:20:45:17 +0000] "GET /api/datasets/AND%20DATE%20LOGIC%20Test/result/AND$20DATE$20LOGIC$20Test.67fa8faf-2a7e-4fa4-87ad-538fac3fd1cc.csv?pretty=false HTTP/1.1" 200 122 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:17,795] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DATE LOGIC Test on 5 rows
INFO  [2023-01-06 20:45:17,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DATE LOGIC Test
INFO  [2023-01-06 20:45:17,796] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-06 20:45:17,796] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-06 20:45:17,796] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DATE LOGIC Test_a22cb94f-df52-4efb-9118-409628313bfb
INFO  [2023-01-06 20:45:17,796] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DATE LOGIC Test_2ddfee4d-f6dc-4b5e-ae7e-579e76ac22b1
INFO  [2023-01-06 20:45:17,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DATE LOGIC Test_a22cb94f-df52-4efb-9118-409628313bfb
INFO  [2023-01-06 20:45:17,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DATE LOGIC Test
INFO  [2023-01-06 20:45:17,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DATE LOGIC Test_2ddfee4d-f6dc-4b5e-ae7e-579e76ac22b1
INFO  [2023-01-06 20:45:17,898] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DATE$20LOGIC$20Test
INFO  [2023-01-06 20:45:17,898] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,024] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DATE LOGIC Test
INFO  [2023-01-06 20:45:18,024] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:18,024] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:18,025] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:18,026] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-06 20:45:18,026] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:18,026] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-06 20:45:18,026] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:18,028] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_80332bfc-de57-4109-a2c6-56755f8af108 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:18,028] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_80332bfc-de57-4109-a2c6-56755f8af108 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:18,028] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:18,028] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_fe85dd47-deba-457d-97a6-77fff330a608 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:18,028] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_fe85dd47-deba-457d-97a6-77fff330a608 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:18,028] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:18,032] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,132] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,139] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,139] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test.table
INFO  [2023-01-06 20:45:18,139] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test.table
INFO  [2023-01-06 20:45:18,251] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,367] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:18,367] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:18,367] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 307 B in total
INFO  [2023-01-06 20:45:18,367] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000339144sINFO  [2023-01-06 20:45:18,402] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-06 20:45:18,402] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:18,402] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@2b2ee3f)
INFO  [2023-01-06 20:45:18,407] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:18,407] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:18,407] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:18,428] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:18 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DURATION+SUM+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:18,428] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,429] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:18,429] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:18,429] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:18,431] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:18,432] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test.table.table], containing 17 entries.
INFO  [2023-01-06 20:45:18,432] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test.table.table], containing 17 entries.
WARN  [2023-01-06 20:45:18,432] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:18,433] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test.table.table.0
INFO  [2023-01-06 20:45:18,433] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test.table.table.1
INFO  [2023-01-06 20:45:18,538] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,544] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,555] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,555] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:18,555] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:18,664] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-06 20:45:18,681] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:18,682] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[04334acb-eafc-4a31-808a-07cc5e50cb91] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test))]]
INFO  [2023-01-06 20:45:18,685] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91
INFO  [2023-01-06 20:45:18,686] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91
INFO  [2023-01-06 20:45:18,687] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91] with 2 results within PT0.001463S
INFO  [2023-01-06 20:45:18,687] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91] with 2 results within PT0.001826S
127.0.0.1 - - [06/Jan/2023:20:45:18 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test/queries HTTP/1.1" 201 1829 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:18,688] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91, workerId=AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_fe85dd47-deba-457d-97a6-77fff330a608, startTime=2023-01-06T20:45:18.686081, finishTime=2023-01-06T20:45:18.687544) of size 2
INFO  [2023-01-06 20:45:18,688] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91, workerId=AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_80332bfc-de57-4109-a2c6-56755f8af108, startTime=2023-01-06T20:45:18.685941, finishTime=2023-01-06T20:45:18.687767) of size 2
INFO  [2023-01-06 20:45:18,688] com.bakdata.conquery.models.execution.ManagedExecution: DONE 04334acb-eafc-4a31-808a-07cc5e50cb91 ManagedQuery within PT0.00621S
127.0.0.1 - - [06/Jan/2023:20:45:18 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test/queries/AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91 HTTP/1.1" 200 2128 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:18,729] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test], queryId=04334acb-eafc-4a31-808a-07cc5e50cb91, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:18.682075, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@477af101[Count = 0], startTime=2023-01-06T20:45:18.682314, finishTime=2023-01-06T20:45:18.688524, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@152e68ce), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2cb95f5b, com.bakdata.conquery.models.query.ColumnDescriptor@67cb7903, com.bakdata.conquery.models.query.ColumnDescriptor@77296e02, com.bakdata.conquery.models.query.ColumnDescriptor@11270031]) download on dataset Dataset[label=null, name=AND DURATION SUM Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:18,730] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test], queryId=04334acb-eafc-4a31-808a-07cc5e50cb91, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:18.682075, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@477af101[Count = 0], startTime=2023-01-06T20:45:18.682314, finishTime=2023-01-06T20:45:18.688524, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@152e68ce), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2cb95f5b, com.bakdata.conquery.models.query.ColumnDescriptor@67cb7903, com.bakdata.conquery.models.query.ColumnDescriptor@77296e02, com.bakdata.conquery.models.query.ColumnDescriptor@11270031]) on dataset Dataset[label=null, name=AND DURATION SUM Test]
127.0.0.1 - - [06/Jan/2023:20:45:18 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test/result/AND$20DURATION$20SUM$20Test.04334acb-eafc-4a31-808a-07cc5e50cb91.csv?pretty=false HTTP/1.1" 200 152 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:18,750] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 5 rows
INFO  [2023-01-06 20:45:18,750] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test
INFO  [2023-01-06 20:45:18,751] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-06 20:45:18,751] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-06 20:45:18,751] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test_80332bfc-de57-4109-a2c6-56755f8af108
INFO  [2023-01-06 20:45:18,751] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test_fe85dd47-deba-457d-97a6-77fff330a608
INFO  [2023-01-06 20:45:18,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test
INFO  [2023-01-06 20:45:18,827] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test_80332bfc-de57-4109-a2c6-56755f8af108
INFO  [2023-01-06 20:45:18,828] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test_fe85dd47-deba-457d-97a6-77fff330a608
INFO  [2023-01-06 20:45:18,847] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test
INFO  [2023-01-06 20:45:18,848] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:18,962] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:18,962] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:18,962] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:18,962] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:18,963] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-06 20:45:18,963] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:18,963] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-06 20:45:18,964] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:18,965] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_b18b4947-fd33-49fe-85a3-edba867244a4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:18,965] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_b18b4947-fd33-49fe-85a3-edba867244a4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:18,965] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:18,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_13aa58ab-a21e-46d2-9611-60aec48dcf80 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:18,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_13aa58ab-a21e-46d2-9611-60aec48dcf80 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:18,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:18,970] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,069] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[AND$20DURATION$20SUM$20Test[1].secondary]
INFO  [2023-01-06 20:45:19,070] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[AND$20DURATION$20SUM$20Test[1].ignored]
INFO  [2023-01-06 20:45:19,070] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,071] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].secondary
INFO  [2023-01-06 20:45:19,071] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].secondary
INFO  [2023-01-06 20:45:19,071] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].ignored
INFO  [2023-01-06 20:45:19,071] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].ignored
INFO  [2023-01-06 20:45:19,178] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,178] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table1
INFO  [2023-01-06 20:45:19,178] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table1
INFO  [2023-01-06 20:45:19,178] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table2
INFO  [2023-01-06 20:45:19,179] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table2
INFO  [2023-01-06 20:45:19,297] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,408] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:19,408] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:19,409] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:19,409] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 373 B in total
INFO  [2023-01-06 20:45:19,409] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[1]/table1.import.json, tag=Optional.empty)
██████████████████████████████████                ▌  68%	est. time remaining: 0.013514796sINFO  [2023-01-06 20:45:19,437] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=12, min=1, average=4.000000, max=10}
INFO  [2023-01-06 20:45:19,437] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=15340, maxValue=15343), dateReader=com.bakdata.conquery.util.DateReader@5bb48d69)
INFO  [2023-01-06 20:45:19,437] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[secondary] with StringParser(super=Parser(lines=12, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:19,437] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=12, nullLines=2), encoding=null, prefix=A, suffix=A)
INFO  [2023-01-06 20:45:19,441] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:19,441] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[1]/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000491257sINFO  [2023-01-06 20:45:19,460] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=6, min=1, average=2.000000, max=4}
INFO  [2023-01-06 20:45:19,461] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@52372311)
INFO  [2023-01-06 20:45:19,461] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=B, suffix=B)
INFO  [2023-01-06 20:45:19,463] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[1]/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:19,464] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:19,464] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:19,464] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[1]/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:19,482] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into AND$20DURATION$20SUM$20Test[1].table1
127.0.0.1 - - [06/Jan/2023:20:45:19 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DURATION+SUM+Test%5B1%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:19,483] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:19,483] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:19,483] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:19,485] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:19,485] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table1.table1], containing 12 entries.
INFO  [2023-01-06 20:45:19,485] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table1.table1], containing 12 entries.
WARN  [2023-01-06 20:45:19,486] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:19,486] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[1].table1.table1.0
INFO  [2023-01-06 20:45:19,499] com.bakdata.conquery.models.jobs.ImportJob: Importing table2 into AND$20DURATION$20SUM$20Test[1].table2
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:19,500] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
127.0.0.1 - - [06/Jan/2023:20:45:19 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DURATION+SUM+Test%5B1%5D%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
WARN  [2023-01-06 20:45:19,500] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table2.table2], containing 6 entries.
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table2.table2], containing 6 entries.
INFO  [2023-01-06 20:45:19,500] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[1].table2.table2.0
INFO  [2023-01-06 20:45:19,605] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,611] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,632] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:19,632] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:45:19,739] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-06 20:45:19,758] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:19,759] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[84011815-beb5-428b-8dc0-3f616e56f733] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[1]))]]
INFO  [2023-01-06 20:45:19,761] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery AND$20DURATION$20SUM$20Test[1].84011815-beb5-428b-8dc0-3f616e56f733
INFO  [2023-01-06 20:45:19,761] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery AND$20DURATION$20SUM$20Test[1].84011815-beb5-428b-8dc0-3f616e56f733
WARN  [2023-01-06 20:45:19,762] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
127.0.0.1 - - [06/Jan/2023:20:45:19 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B1%5D/queries HTTP/1.1" 201 2114 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:19,762] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[1].84011815-beb5-428b-8dc0-3f616e56f733] with 0 results within PT0.000292S
INFO  [2023-01-06 20:45:19,763] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[1].84011815-beb5-428b-8dc0-3f616e56f733, workerId=AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_b18b4947-fd33-49fe-85a3-edba867244a4, startTime=2023-01-06T20:45:19.761963, finishTime=2023-01-06T20:45:19.762255) of size 0
INFO  [2023-01-06 20:45:19,763] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[1].84011815-beb5-428b-8dc0-3f616e56f733] with 2 results within PT0.001482S
INFO  [2023-01-06 20:45:19,764] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[1].84011815-beb5-428b-8dc0-3f616e56f733, workerId=AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_13aa58ab-a21e-46d2-9611-60aec48dcf80, startTime=2023-01-06T20:45:19.762014, finishTime=2023-01-06T20:45:19.763496) of size 2
INFO  [2023-01-06 20:45:19,764] com.bakdata.conquery.models.execution.ManagedExecution: DONE 84011815-beb5-428b-8dc0-3f616e56f733 ManagedQuery within PT0.005135S
127.0.0.1 - - [06/Jan/2023:20:45:19 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B1%5D/queries/AND$20DURATION$20SUM$20Test%5B1%5D.84011815-beb5-428b-8dc0-3f616e56f733 HTTP/1.1" 200 2697 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:19,793] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[1]], queryId=84011815-beb5-428b-8dc0-3f616e56f733, label=tree1-a tree2-b	@§$, creationTime=2023-01-06T20:45:19.758949, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@133fc627[Count = 0], startTime=2023-01-06T20:45:19.759098, finishTime=2023-01-06T20:45:19.764233, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2de08c0c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2756584d, com.bakdata.conquery.models.query.ColumnDescriptor@354d0261, com.bakdata.conquery.models.query.ColumnDescriptor@582ef411, com.bakdata.conquery.models.query.ColumnDescriptor@718793f4, com.bakdata.conquery.models.query.ColumnDescriptor@648639b9]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:19,793] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[1]], queryId=84011815-beb5-428b-8dc0-3f616e56f733, label=tree1-a tree2-b	@§$, creationTime=2023-01-06T20:45:19.758949, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@133fc627[Count = 0], startTime=2023-01-06T20:45:19.759098, finishTime=2023-01-06T20:45:19.764233, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2de08c0c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2756584d, com.bakdata.conquery.models.query.ColumnDescriptor@354d0261, com.bakdata.conquery.models.query.ColumnDescriptor@582ef411, com.bakdata.conquery.models.query.ColumnDescriptor@718793f4, com.bakdata.conquery.models.query.ColumnDescriptor@648639b9]) on dataset Dataset[label=null, name=AND DURATION SUM Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:19 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B1%5D/result/AND$20DURATION$20SUM$20Test%5B1%5D.84011815-beb5-428b-8dc0-3f616e56f733.csv?pretty=false HTTP/1.1" 200 174 "-" "Conquery (test client)" 29
INFO  [2023-01-06 20:45:19,822] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 7 rows
INFO  [2023-01-06 20:45:19,822] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[1]
INFO  [2023-01-06 20:45:19,822] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-06 20:45:19,822] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-06 20:45:19,822] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[1]_b18b4947-fd33-49fe-85a3-edba867244a4
INFO  [2023-01-06 20:45:19,822] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[1]_13aa58ab-a21e-46d2-9611-60aec48dcf80
INFO  [2023-01-06 20:45:19,865] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[1]
INFO  [2023-01-06 20:45:19,870] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[1]_13aa58ab-a21e-46d2-9611-60aec48dcf80
INFO  [2023-01-06 20:45:19,870] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[1]_b18b4947-fd33-49fe-85a3-edba867244a4
INFO  [2023-01-06 20:45:19,920] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[1]
INFO  [2023-01-06 20:45:19,920] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,038] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:20,039] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND NEGATION Test
INFO  [2023-01-06 20:45:20,039] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:20,039] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:20,040] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-06 20:45:20,040] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-06 20:45:20,040] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:20,040] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:20,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_35358517-3c97-4490-b70d-b6f63db8e9c3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:20,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_35358517-3c97-4490-b70d-b6f63db8e9c3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:20,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:20,043] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_753df5aa-4a04-45c5-964d-48a35c780504 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:20,043] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_753df5aa-4a04-45c5-964d-48a35c780504 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:20,043] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:20,047] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,146] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,153] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,153] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20Test.table
INFO  [2023-01-06 20:45:20,153] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20Test.table
INFO  [2023-01-06 20:45:20,268] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,385] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:20,385] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:20,386] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 202 B in total
INFO  [2023-01-06 20:45:20,386] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000338126sINFO  [2023-01-06 20:45:20,420] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=11, min=1, average=2.200000, max=3}
INFO  [2023-01-06 20:45:20,420] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=11, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:20,420] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=11, nullLines=0), subType=IntegerParser(super=Parser(lines=11, nullLines=0), minValue=15340, maxValue=15342), dateReader=com.bakdata.conquery.util.DateReader@19907b28)
INFO  [2023-01-06 20:45:20,423] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:20,423] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:20,423] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:20,443] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20NEGATION$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:20 +0000] "POST /admin/datasets/AND%20NEGATION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+NEGATION+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:20,444] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,444] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:20,445] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:20,445] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:20,446] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:20,446] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20Test.table.table], containing 11 entries.
INFO  [2023-01-06 20:45:20,446] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20Test.table.table], containing 11 entries.
WARN  [2023-01-06 20:45:20,447] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:20,447] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20Test.table.table.0
INFO  [2023-01-06 20:45:20,447] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20Test.table.table.1
INFO  [2023-01-06 20:45:20,552] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,558] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,572] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,572] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:20,573] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:20,678] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND NEGATION Test QUERY INIT
INFO  [2023-01-06 20:45:20,704] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20NEGATION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:20,705] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[79caf4f1-727f-4d14-8621-9c494462d437] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION Test))]]
INFO  [2023-01-06 20:45:20,709] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437
INFO  [2023-01-06 20:45:20,709] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437
INFO  [2023-01-06 20:45:20,710] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437] with 1 results within PT0.000866S
INFO  [2023-01-06 20:45:20,710] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437] with 1 results within PT0.001111S
INFO  [2023-01-06 20:45:20,710] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437, workerId=AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_753df5aa-4a04-45c5-964d-48a35c780504, startTime=2023-01-06T20:45:20.709360, finishTime=2023-01-06T20:45:20.710226) of size 1
INFO  [2023-01-06 20:45:20,711] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437, workerId=AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_35358517-3c97-4490-b70d-b6f63db8e9c3, startTime=2023-01-06T20:45:20.709562, finishTime=2023-01-06T20:45:20.710673) of size 1
INFO  [2023-01-06 20:45:20,711] com.bakdata.conquery.models.execution.ManagedExecution: DONE 79caf4f1-727f-4d14-8621-9c494462d437 ManagedQuery within PT0.006252S
127.0.0.1 - - [06/Jan/2023:20:45:20 +0000] "POST /api/datasets/AND$20NEGATION$20Test/queries HTTP/1.1" 201 1611 "-" "Conquery (test client)" 9
127.0.0.1 - - [06/Jan/2023:20:45:20 +0000] "GET /api/datasets/AND$20NEGATION$20Test/queries/AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437 HTTP/1.1" 200 1886 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:20,742] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION Test], queryId=79caf4f1-727f-4d14-8621-9c494462d437, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:20.704996, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@401158d[Count = 0], startTime=2023-01-06T20:45:20.705230, finishTime=2023-01-06T20:45:20.711482, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6448bacc), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@26ac5816, com.bakdata.conquery.models.query.ColumnDescriptor@7f9cc10e, com.bakdata.conquery.models.query.ColumnDescriptor@733d0e0d]) download on dataset Dataset[label=null, name=AND NEGATION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:20,742] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION Test], queryId=79caf4f1-727f-4d14-8621-9c494462d437, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:20.704996, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@401158d[Count = 0], startTime=2023-01-06T20:45:20.705230, finishTime=2023-01-06T20:45:20.711482, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6448bacc), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@26ac5816, com.bakdata.conquery.models.query.ColumnDescriptor@7f9cc10e, com.bakdata.conquery.models.query.ColumnDescriptor@733d0e0d]) on dataset Dataset[label=null, name=AND NEGATION Test]
127.0.0.1 - - [06/Jan/2023:20:45:20 +0000] "GET /api/datasets/AND%20NEGATION%20Test/result/AND$20NEGATION$20Test.79caf4f1-727f-4d14-8621-9c494462d437.csv?pretty=false HTTP/1.1" 200 90 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:20,762] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND NEGATION Test on 3 rows
INFO  [2023-01-06 20:45:20,762] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND NEGATION Test
INFO  [2023-01-06 20:45:20,762] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-06 20:45:20,762] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-06 20:45:20,763] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION Test_753df5aa-4a04-45c5-964d-48a35c780504
INFO  [2023-01-06 20:45:20,763] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION Test_35358517-3c97-4490-b70d-b6f63db8e9c3
INFO  [2023-01-06 20:45:20,840] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND NEGATION Test
INFO  [2023-01-06 20:45:20,841] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION Test_35358517-3c97-4490-b70d-b6f63db8e9c3
INFO  [2023-01-06 20:45:20,842] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION Test_753df5aa-4a04-45c5-964d-48a35c780504
INFO  [2023-01-06 20:45:20,860] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20NEGATION$20Test
INFO  [2023-01-06 20:45:20,860] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:20,978] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND NEGATION Test
INFO  [2023-01-06 20:45:20,978] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-06 20:45:20,978] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:20,978] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:20,979] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-06 20:45:20,980] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-06 20:45:20,980] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:20,980] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:20,982] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_0fa361a1-9a31-4416-aa19-29af5a1aa180 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:20,982] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_0fa361a1-9a31-4416-aa19-29af5a1aa180 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:20,982] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:20,982] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_d9e0b971-77ae-41c2-ad17-3bee4f891af1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:20,982] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_d9e0b971-77ae-41c2-ad17-3bee4f891af1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:20,982] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:20,986] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,085] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,093] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test.table
INFO  [2023-01-06 20:45:21,093] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test.table
INFO  [2023-01-06 20:45:21,209] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,319] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:21,320] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:21,320] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 172 B in total
INFO  [2023-01-06 20:45:21,320] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION DATE LOGIC Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000335772sINFO  [2023-01-06 20:45:21,354] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=9, min=1, average=1.800000, max=2}
INFO  [2023-01-06 20:45:21,354] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=9, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:21,354] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@1c7149b3)
INFO  [2023-01-06 20:45:21,357] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:21,357] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:21,357] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:21,378] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20NEGATION$20DATE$20LOGIC$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:21 +0000] "POST /admin/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+NEGATION+DATE+LOGIC+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:21,378] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,379] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:21,379] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:21,379] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:21,381] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:21,381] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test.table.table], containing 9 entries.
INFO  [2023-01-06 20:45:21,381] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test.table.table], containing 9 entries.
WARN  [2023-01-06 20:45:21,382] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:21,382] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test.table.table.0
INFO  [2023-01-06 20:45:21,382] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test.table.table.1
INFO  [2023-01-06 20:45:21,487] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,492] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,506] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:21,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:21,623] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND NEGATION DATE LOGIC Test QUERY INIT
INFO  [2023-01-06 20:45:21,639] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20NEGATION$20DATE$20LOGIC$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:21,640] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test))]]
INFO  [2023-01-06 20:45:21,644] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4
INFO  [2023-01-06 20:45:21,644] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4
INFO  [2023-01-06 20:45:21,645] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4] with 1 results within PT0.001569S
INFO  [2023-01-06 20:45:21,645] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4] with 1 results within PT0.001656S
127.0.0.1 - - [06/Jan/2023:20:45:21 +0000] "POST /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test/queries HTTP/1.1" 201 1669 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:21,646] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_d9e0b971-77ae-41c2-ad17-3bee4f891af1, startTime=2023-01-06T20:45:21.644112, finishTime=2023-01-06T20:45:21.645681) of size 1
INFO  [2023-01-06 20:45:21,646] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_0fa361a1-9a31-4416-aa19-29af5a1aa180, startTime=2023-01-06T20:45:21.644239, finishTime=2023-01-06T20:45:21.645895) of size 1
INFO  [2023-01-06 20:45:21,646] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4 ManagedQuery within PT0.006369S
127.0.0.1 - - [06/Jan/2023:20:45:21 +0000] "GET /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test/queries/AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4 HTTP/1.1" 200 2004 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:21,676] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test], queryId=2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:21.640006, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22171549[Count = 0], startTime=2023-01-06T20:45:21.640294, finishTime=2023-01-06T20:45:21.646663, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@161c7755), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7222d370, com.bakdata.conquery.models.query.ColumnDescriptor@93f2469, com.bakdata.conquery.models.query.ColumnDescriptor@1398c55a]) download on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:21,676] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test], queryId=2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:21.640006, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22171549[Count = 0], startTime=2023-01-06T20:45:21.640294, finishTime=2023-01-06T20:45:21.646663, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@161c7755), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7222d370, com.bakdata.conquery.models.query.ColumnDescriptor@93f2469, com.bakdata.conquery.models.query.ColumnDescriptor@1398c55a]) on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test]
127.0.0.1 - - [06/Jan/2023:20:45:21 +0000] "GET /api/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test/result/AND$20NEGATION$20DATE$20LOGIC$20Test.2cff19ef-7c03-4fa7-bd0e-0fd7b40075d4.csv?pretty=false HTTP/1.1" 200 113 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:21,693] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND NEGATION DATE LOGIC Test on 3 rows
INFO  [2023-01-06 20:45:21,693] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND NEGATION DATE LOGIC Test
INFO  [2023-01-06 20:45:21,694] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-06 20:45:21,694] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-06 20:45:21,694] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test_0fa361a1-9a31-4416-aa19-29af5a1aa180
INFO  [2023-01-06 20:45:21,694] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test_d9e0b971-77ae-41c2-ad17-3bee4f891af1
INFO  [2023-01-06 20:45:21,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test_d9e0b971-77ae-41c2-ad17-3bee4f891af1
INFO  [2023-01-06 20:45:21,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND NEGATION DATE LOGIC Test
INFO  [2023-01-06 20:45:21,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test_0fa361a1-9a31-4416-aa19-29af5a1aa180
INFO  [2023-01-06 20:45:21,892] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20NEGATION$20DATE$20LOGIC$20Test
INFO  [2023-01-06 20:45:21,892] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,924] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-06 20:45:21,924] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-06 20:45:21,924] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:21,925] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:21,926] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-06 20:45:21,926] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-06 20:45:21,926] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:21,926] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:21,927] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:21,927] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_a5ea3c5b-fbab-4196-8e3b-acdadf2aefc1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:21,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_a5ea3c5b-fbab-4196-8e3b-acdadf2aefc1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:21,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:21,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_12a878a5-4a46-4cb5-8e4a-3c502a98f6b3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:21,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_12a878a5-4a46-4cb5-8e4a-3c502a98f6b3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:21,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:22,031] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,038] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,039] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test[1].table
INFO  [2023-01-06 20:45:22,039] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test[1].table
INFO  [2023-01-06 20:45:22,152] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,261] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:22,261] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:22,261] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 206 B in total
INFO  [2023-01-06 20:45:22,261] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION DATE LOGIC Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000223062sINFO  [2023-01-06 20:45:22,284] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=11, min=1, average=2.200000, max=4}
INFO  [2023-01-06 20:45:22,284] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=11, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:22,284] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=11, nullLines=0), subType=IntegerParser(super=Parser(lines=11, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@9278f14)
INFO  [2023-01-06 20:45:22,287] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION DATE LOGIC Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:22,288] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:22,288] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND NEGATION DATE LOGIC Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:22,305] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20NEGATION$20DATE$20LOGIC$20Test[1].table
127.0.0.1 - - [06/Jan/2023:20:45:22 +0000] "POST /admin/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+NEGATION+DATE+LOGIC+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:22,306] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,307] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:22,308] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:22,308] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:22,311] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:22,311] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table], containing 11 entries.
INFO  [2023-01-06 20:45:22,312] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table], containing 11 entries.
WARN  [2023-01-06 20:45:22,313] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:22,313] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table.0
INFO  [2023-01-06 20:45:22,314] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table.1
INFO  [2023-01-06 20:45:22,418] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,434] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,435] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:22,435] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:22,540] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND NEGATION DATE LOGIC Test QUERY INIT
INFO  [2023-01-06 20:45:22,568] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20NEGATION$20DATE$20LOGIC$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:22,568] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[978a9583-023f-4310-9cdc-c457069b1444] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test[1]))]]
INFO  [2023-01-06 20:45:22,571] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test[1].978a9583-023f-4310-9cdc-c457069b1444
INFO  [2023-01-06 20:45:22,571] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test[1].978a9583-023f-4310-9cdc-c457069b1444
127.0.0.1 - - [06/Jan/2023:20:45:22 +0000] "POST /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D/queries HTTP/1.1" 201 1785 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:22,572] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test[1].978a9583-023f-4310-9cdc-c457069b1444] with 1 results within PT0.001122S
INFO  [2023-01-06 20:45:22,573] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test[1].978a9583-023f-4310-9cdc-c457069b1444] with 1 results within PT0.001433S
INFO  [2023-01-06 20:45:22,573] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].978a9583-023f-4310-9cdc-c457069b1444, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_12a878a5-4a46-4cb5-8e4a-3c502a98f6b3, startTime=2023-01-06T20:45:22.571643, finishTime=2023-01-06T20:45:22.572765) of size 1
INFO  [2023-01-06 20:45:22,573] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].978a9583-023f-4310-9cdc-c457069b1444, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_a5ea3c5b-fbab-4196-8e3b-acdadf2aefc1, startTime=2023-01-06T20:45:22.571701, finishTime=2023-01-06T20:45:22.573134) of size 1
INFO  [2023-01-06 20:45:22,573] com.bakdata.conquery.models.execution.ManagedExecution: DONE 978a9583-023f-4310-9cdc-c457069b1444 ManagedQuery within PT0.005321S
127.0.0.1 - - [06/Jan/2023:20:45:22 +0000] "GET /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D/queries/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D.978a9583-023f-4310-9cdc-c457069b1444 HTTP/1.1" 200 2440 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:22,594] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]], queryId=978a9583-023f-4310-9cdc-c457069b1444, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:22.568292, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7266e5a[Count = 0], startTime=2023-01-06T20:45:22.568463, finishTime=2023-01-06T20:45:22.573784, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@13ff6a81), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7d01ffc5, com.bakdata.conquery.models.query.ColumnDescriptor@316c6be5, com.bakdata.conquery.models.query.ColumnDescriptor@58c972c]) download on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:22,594] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]], queryId=978a9583-023f-4310-9cdc-c457069b1444, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:22.568292, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7266e5a[Count = 0], startTime=2023-01-06T20:45:22.568463, finishTime=2023-01-06T20:45:22.573784, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@13ff6a81), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7d01ffc5, com.bakdata.conquery.models.query.ColumnDescriptor@316c6be5, com.bakdata.conquery.models.query.ColumnDescriptor@58c972c]) on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:22 +0000] "GET /api/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test%5B1%5D/result/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D.978a9583-023f-4310-9cdc-c457069b1444.csv?pretty=false HTTP/1.1" 200 89 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:22,614] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND NEGATION DATE LOGIC Test on 3 rows
INFO  [2023-01-06 20:45:22,614] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND NEGATION DATE LOGIC Test[1]
INFO  [2023-01-06 20:45:22,614] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-06 20:45:22,614] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-06 20:45:22,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test[1]_a5ea3c5b-fbab-4196-8e3b-acdadf2aefc1
INFO  [2023-01-06 20:45:22,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test[1]_12a878a5-4a46-4cb5-8e4a-3c502a98f6b3
INFO  [2023-01-06 20:45:22,626] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND NEGATION DATE LOGIC Test[1]
INFO  [2023-01-06 20:45:22,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test[1]_a5ea3c5b-fbab-4196-8e3b-acdadf2aefc1
INFO  [2023-01-06 20:45:22,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test[1]_12a878a5-4a46-4cb5-8e4a-3c502a98f6b3
INFO  [2023-01-06 20:45:22,714] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20NEGATION$20DATE$20LOGIC$20Test[1]
INFO  [2023-01-06 20:45:22,714] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,740] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-06 20:45:22,741] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND Test
INFO  [2023-01-06 20:45:22,741] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:22,741] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:22,743] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-06 20:45:22,743] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-06 20:45:22,743] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:22,743] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test[1].worker_AND$20Test[1]_96d03120-21e4-4d15-98fd-2c1907d6f575 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test[1].worker_AND$20Test[1]_96d03120-21e4-4d15-98fd-2c1907d6f575 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test[1].worker_AND$20Test[1]_b1863e52-316a-4785-b99e-5cb05a3e00ac are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test[1].worker_AND$20Test[1]_b1863e52-316a-4785-b99e-5cb05a3e00ac are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:22,747] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,849] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,857] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:22,858] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test[1].table
INFO  [2023-01-06 20:45:22,858] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test[1].table
INFO  [2023-01-06 20:45:22,975] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,086] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:23,086] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:23,087] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 152 B in total
INFO  [2023-01-06 20:45:23,087] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000426737sINFO  [2023-01-06 20:45:23,130] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=1, average=2.000000, max=3}
INFO  [2023-01-06 20:45:23,130] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:23,130] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=15341), dateReader=com.bakdata.conquery.util.DateReader@62e30a8a)
INFO  [2023-01-06 20:45:23,134] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:23,135] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:23,135] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:23,165] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20Test[1].table
INFO  [2023-01-06 20:45:23,166] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,166] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [06/Jan/2023:20:45:23 +0000] "POST /admin/datasets/AND%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:45:23,167] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:23,167] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:23,169] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:23,169] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test[1].table.table], containing 8 entries.
INFO  [2023-01-06 20:45:23,169] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test[1].table.table], containing 8 entries.
WARN  [2023-01-06 20:45:23,170] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:23,170] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test[1].table.table.0
INFO  [2023-01-06 20:45:23,170] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test[1].table.table.1
INFO  [2023-01-06 20:45:23,275] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,281] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,290] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,291] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:23,291] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:23,397] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND Test QUERY INIT
INFO  [2023-01-06 20:45:23,409] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:23,409] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[16740cf9-1477-4b46-9cee-4ec72dfec629] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND Test[1]))]]
INFO  [2023-01-06 20:45:23,424] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test[1].16740cf9-1477-4b46-9cee-4ec72dfec629
INFO  [2023-01-06 20:45:23,424] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test[1].16740cf9-1477-4b46-9cee-4ec72dfec629
INFO  [2023-01-06 20:45:23,425] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test[1].16740cf9-1477-4b46-9cee-4ec72dfec629] with 1 results within PT0.001275S
INFO  [2023-01-06 20:45:23,425] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test[1].16740cf9-1477-4b46-9cee-4ec72dfec629] with 1 results within PT0.001335S
127.0.0.1 - - [06/Jan/2023:20:45:23 +0000] "POST /api/datasets/AND$20Test%5B1%5D/queries HTTP/1.1" 201 1664 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:23,426] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test[1].16740cf9-1477-4b46-9cee-4ec72dfec629, workerId=AND$20Test[1].worker_AND$20Test[1]_96d03120-21e4-4d15-98fd-2c1907d6f575, startTime=2023-01-06T20:45:23.424613, finishTime=2023-01-06T20:45:23.425888) of size 1
INFO  [2023-01-06 20:45:23,426] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test[1].16740cf9-1477-4b46-9cee-4ec72dfec629, workerId=AND$20Test[1].worker_AND$20Test[1]_b1863e52-316a-4785-b99e-5cb05a3e00ac, startTime=2023-01-06T20:45:23.424557, finishTime=2023-01-06T20:45:23.425892) of size 1
INFO  [2023-01-06 20:45:23,426] com.bakdata.conquery.models.execution.ManagedExecution: DONE 16740cf9-1477-4b46-9cee-4ec72dfec629 ManagedQuery within PT0.017008S
127.0.0.1 - - [06/Jan/2023:20:45:23 +0000] "GET /api/datasets/AND$20Test%5B1%5D/queries/AND$20Test%5B1%5D.16740cf9-1477-4b46-9cee-4ec72dfec629 HTTP/1.1" 200 2112 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:23,447] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test[1]], queryId=16740cf9-1477-4b46-9cee-4ec72dfec629, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:23.409465, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@17435683[Count = 0], startTime=2023-01-06T20:45:23.409753, finishTime=2023-01-06T20:45:23.426761, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3f049b21), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1c63af32, com.bakdata.conquery.models.query.ColumnDescriptor@2734028f]) download on dataset Dataset[label=null, name=AND Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:23,447] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test[1]], queryId=16740cf9-1477-4b46-9cee-4ec72dfec629, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:23.409465, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@17435683[Count = 0], startTime=2023-01-06T20:45:23.409753, finishTime=2023-01-06T20:45:23.426761, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3f049b21), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1c63af32, com.bakdata.conquery.models.query.ColumnDescriptor@2734028f]) on dataset Dataset[label=null, name=AND Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:23 +0000] "GET /api/datasets/AND%20Test%5B1%5D/result/AND$20Test%5B1%5D.16740cf9-1477-4b46-9cee-4ec72dfec629.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:23,466] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND Test on 3 rows
INFO  [2023-01-06 20:45:23,466] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND Test[1]
INFO  [2023-01-06 20:45:23,467] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-06 20:45:23,467] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-06 20:45:23,467] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test[1]_96d03120-21e4-4d15-98fd-2c1907d6f575
INFO  [2023-01-06 20:45:23,467] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test[1]_b1863e52-316a-4785-b99e-5cb05a3e00ac
INFO  [2023-01-06 20:45:23,543] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND Test[1]
INFO  [2023-01-06 20:45:23,547] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test[1]_b1863e52-316a-4785-b99e-5cb05a3e00ac
INFO  [2023-01-06 20:45:23,547] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test[1]_96d03120-21e4-4d15-98fd-2c1907d6f575
INFO  [2023-01-06 20:45:23,570] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20Test[1]
INFO  [2023-01-06 20:45:23,570] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,697] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND Test
INFO  [2023-01-06 20:45:23,697] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test OR Test
INFO  [2023-01-06 20:45:23,697] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:23,697] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:23,698] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-06 20:45:23,698] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:23,698] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-06 20:45:23,698] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:23,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20Test.worker_OR$20Test_7c1dd91b-e36c-4f0b-b823-2712087102e4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:23,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20Test.worker_OR$20Test_7c1dd91b-e36c-4f0b-b823-2712087102e4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:23,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:23,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20Test.worker_OR$20Test_ba4628d8-0250-42a7-8362-b5af9aa09a15 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:23,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20Test.worker_OR$20Test_ba4628d8-0250-42a7-8362-b5af9aa09a15 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:23,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:23,705] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,803] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,810] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:23,810] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20Test.table
INFO  [2023-01-06 20:45:23,810] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20Test.table
INFO  [2023-01-06 20:45:23,925] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,035] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:24,035] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:24,035] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 167 B in total
INFO  [2023-01-06 20:45:24,035] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000334506sINFO  [2023-01-06 20:45:24,069] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=9, min=1, average=1.800000, max=3}
INFO  [2023-01-06 20:45:24,069] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=9, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:24,069] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@783064c5)
INFO  [2023-01-06 20:45:24,072] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:24,072] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:24,072] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:24,094] com.bakdata.conquery.models.jobs.ImportJob: Importing table into OR$20Test.table
INFO  [2023-01-06 20:45:24,095] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:24 +0000] "POST /admin/datasets/OR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_OR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:24,096] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:24,097] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:24,097] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:24,099] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:24,099] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20Test.table.table], containing 9 entries.
INFO  [2023-01-06 20:45:24,099] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20Test.table.table], containing 9 entries.
WARN  [2023-01-06 20:45:24,101] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:24,101] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20Test.table.table.0
INFO  [2023-01-06 20:45:24,101] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20Test.table.table.1
INFO  [2023-01-06 20:45:24,205] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,211] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,225] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,225] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:24,225] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:24,332] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: OR Test QUERY INIT
INFO  [2023-01-06 20:45:24,348] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[OR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:24,349] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[912879ff-1fac-466e-bbce-0e34bda18e9c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_OR Test))]]
INFO  [2023-01-06 20:45:24,352] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c
INFO  [2023-01-06 20:45:24,352] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c
INFO  [2023-01-06 20:45:24,354] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c] with 1 results within PT0.001106S
INFO  [2023-01-06 20:45:24,354] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c] with 3 results within PT0.001343S
127.0.0.1 - - [06/Jan/2023:20:45:24 +0000] "POST /api/datasets/OR$20Test/queries HTTP/1.1" 201 1480 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:24,354] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c, workerId=OR$20Test.worker_OR$20Test_7c1dd91b-e36c-4f0b-b823-2712087102e4, startTime=2023-01-06T20:45:24.353004, finishTime=2023-01-06T20:45:24.354110) of size 1
INFO  [2023-01-06 20:45:24,354] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c, workerId=OR$20Test.worker_OR$20Test_ba4628d8-0250-42a7-8362-b5af9aa09a15, startTime=2023-01-06T20:45:24.352871, finishTime=2023-01-06T20:45:24.354214) of size 3
INFO  [2023-01-06 20:45:24,354] com.bakdata.conquery.models.execution.ManagedExecution: DONE 912879ff-1fac-466e-bbce-0e34bda18e9c ManagedQuery within PT0.00562S
127.0.0.1 - - [06/Jan/2023:20:45:24 +0000] "GET /api/datasets/OR$20Test/queries/OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c HTTP/1.1" 200 1706 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:24,380] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR Test], queryId=912879ff-1fac-466e-bbce-0e34bda18e9c, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:24.349117, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b53e10f[Count = 0], startTime=2023-01-06T20:45:24.349350, finishTime=2023-01-06T20:45:24.354970, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6c43f786), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@533a258f, com.bakdata.conquery.models.query.ColumnDescriptor@503c95ea, com.bakdata.conquery.models.query.ColumnDescriptor@54e35d7b]) download on dataset Dataset[label=null, name=OR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:24,380] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR Test], queryId=912879ff-1fac-466e-bbce-0e34bda18e9c, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:24.349117, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b53e10f[Count = 0], startTime=2023-01-06T20:45:24.349350, finishTime=2023-01-06T20:45:24.354970, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6c43f786), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@533a258f, com.bakdata.conquery.models.query.ColumnDescriptor@503c95ea, com.bakdata.conquery.models.query.ColumnDescriptor@54e35d7b]) on dataset Dataset[label=null, name=OR Test]
127.0.0.1 - - [06/Jan/2023:20:45:24 +0000] "GET /api/datasets/OR%20Test/result/OR$20Test.912879ff-1fac-466e-bbce-0e34bda18e9c.csv?pretty=false HTTP/1.1" 200 142 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:24,397] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest OR Test on 5 rows
INFO  [2023-01-06 20:45:24,397] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast OR Test
INFO  [2023-01-06 20:45:24,398] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-06 20:45:24,398] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-06 20:45:24,398] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR Test_ba4628d8-0250-42a7-8362-b5af9aa09a15
INFO  [2023-01-06 20:45:24,398] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR Test_7c1dd91b-e36c-4f0b-b823-2712087102e4
INFO  [2023-01-06 20:45:24,398] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow OR Test
INFO  [2023-01-06 20:45:24,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR Test_7c1dd91b-e36c-4f0b-b823-2712087102e4
INFO  [2023-01-06 20:45:24,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR Test_ba4628d8-0250-42a7-8362-b5af9aa09a15
INFO  [2023-01-06 20:45:24,401] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of OR$20Test
INFO  [2023-01-06 20:45:24,401] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,530] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test OR Test
INFO  [2023-01-06 20:45:24,531] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test OR_AND Select test
INFO  [2023-01-06 20:45:24,531] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:24,531] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:24,532] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-06 20:45:24,532] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:24,532] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-06 20:45:24,532] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:24,533] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_eea029d1-1ad4-4704-bcd3-f43fa6856431 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:24,533] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_eea029d1-1ad4-4704-bcd3-f43fa6856431 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:24,533] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:24,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_a71b5757-bb3b-461e-99d5-95650c846aa5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:24,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_a71b5757-bb3b-461e-99d5-95650c846aa5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:24,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:24,538] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,638] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,645] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,645] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR_AND$20Select$20test.table
INFO  [2023-01-06 20:45:24,645] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR_AND$20Select$20test.table
INFO  [2023-01-06 20:45:24,757] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,865] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:24,865] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:24,865] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 154 B in total
INFO  [2023-01-06 20:45:24,865] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR_AND Select test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00025201sINFO  [2023-01-06 20:45:24,891] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=1, average=2.000000, max=3}
INFO  [2023-01-06 20:45:24,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:24,891] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6e1a86a1)
INFO  [2023-01-06 20:45:24,893] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR_AND Select test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:24,893] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:24,893] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR_AND Select test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:24,907] com.bakdata.conquery.models.jobs.ImportJob: Importing table into OR_AND$20Select$20test.table
127.0.0.1 - - [06/Jan/2023:20:45:24 +0000] "POST /admin/datasets/OR_AND%20Select%20test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_OR_AND+Select+test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:24,907] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:24,908] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:24,909] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:24,909] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:24,910] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:24,910] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR_AND$20Select$20test.table.table], containing 8 entries.
INFO  [2023-01-06 20:45:24,910] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR_AND$20Select$20test.table.table], containing 8 entries.
WARN  [2023-01-06 20:45:24,911] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:24,911] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR_AND$20Select$20test.table.table.0
INFO  [2023-01-06 20:45:24,911] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR_AND$20Select$20test.table.table.1
INFO  [2023-01-06 20:45:25,017] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,022] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,036] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,037] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:25,037] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:25,154] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: OR_AND Select test QUERY INIT
INFO  [2023-01-06 20:45:25,169] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[OR_AND$20Select$20test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:25,170] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[24b7debc-da92-4464-be18-8c4ed0ee6235] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_OR_AND Select test))]]
INFO  [2023-01-06 20:45:25,173] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235
INFO  [2023-01-06 20:45:25,173] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235
INFO  [2023-01-06 20:45:25,175] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235] with 1 results within PT0.001112S
INFO  [2023-01-06 20:45:25,175] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235] with 3 results within PT0.001491S
INFO  [2023-01-06 20:45:25,175] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235, workerId=OR_AND$20Select$20test.worker_OR_AND$20Select$20test_a71b5757-bb3b-461e-99d5-95650c846aa5, startTime=2023-01-06T20:45:25.174023, finishTime=2023-01-06T20:45:25.175135) of size 1
127.0.0.1 - - [06/Jan/2023:20:45:25 +0000] "POST /api/datasets/OR_AND$20Select$20test/queries HTTP/1.1" 201 2310 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:25,176] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235, workerId=OR_AND$20Select$20test.worker_OR_AND$20Select$20test_eea029d1-1ad4-4704-bcd3-f43fa6856431, startTime=2023-01-06T20:45:25.173745, finishTime=2023-01-06T20:45:25.175236) of size 3
INFO  [2023-01-06 20:45:25,176] com.bakdata.conquery.models.execution.ManagedExecution: DONE 24b7debc-da92-4464-be18-8c4ed0ee6235 ManagedQuery within PT0.005791S
127.0.0.1 - - [06/Jan/2023:20:45:25 +0000] "GET /api/datasets/OR_AND$20Select$20test/queries/OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235 HTTP/1.1" 200 2589 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:25,206] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR_AND Select test], queryId=24b7debc-da92-4464-be18-8c4ed0ee6235, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:25.170181, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5ed73412[Count = 0], startTime=2023-01-06T20:45:25.170371, finishTime=2023-01-06T20:45:25.176162, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3f82159f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR_AND Select test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@19cb1fd6, com.bakdata.conquery.models.query.ColumnDescriptor@2a82e22b, com.bakdata.conquery.models.query.ColumnDescriptor@4d1234cd, com.bakdata.conquery.models.query.ColumnDescriptor@6293ee1d]) download on dataset Dataset[label=null, name=OR_AND Select test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:25,206] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR_AND Select test], queryId=24b7debc-da92-4464-be18-8c4ed0ee6235, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:25.170181, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5ed73412[Count = 0], startTime=2023-01-06T20:45:25.170371, finishTime=2023-01-06T20:45:25.176162, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3f82159f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR_AND Select test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@19cb1fd6, com.bakdata.conquery.models.query.ColumnDescriptor@2a82e22b, com.bakdata.conquery.models.query.ColumnDescriptor@4d1234cd, com.bakdata.conquery.models.query.ColumnDescriptor@6293ee1d]) on dataset Dataset[label=null, name=OR_AND Select test]
INFO  [2023-01-06 20:45:25,225] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest OR_AND Select test on 5 rows
127.0.0.1 - - [06/Jan/2023:20:45:25 +0000] "GET /api/datasets/OR_AND%20Select%20test/result/OR_AND$20Select$20test.24b7debc-da92-4464-be18-8c4ed0ee6235.csv?pretty=false HTTP/1.1" 200 168 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:25,225] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast OR_AND Select test
INFO  [2023-01-06 20:45:25,225] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-06 20:45:25,225] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-06 20:45:25,225] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR_AND Select test_a71b5757-bb3b-461e-99d5-95650c846aa5
INFO  [2023-01-06 20:45:25,225] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR_AND Select test_eea029d1-1ad4-4704-bcd3-f43fa6856431
INFO  [2023-01-06 20:45:25,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow OR_AND Select test
INFO  [2023-01-06 20:45:25,233] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR_AND Select test_eea029d1-1ad4-4704-bcd3-f43fa6856431
INFO  [2023-01-06 20:45:25,233] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR_AND Select test_a71b5757-bb3b-461e-99d5-95650c846aa5
INFO  [2023-01-06 20:45:25,322] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of OR_AND$20Select$20test
INFO  [2023-01-06 20:45:25,322] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,342] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test OR_AND Select test
INFO  [2023-01-06 20:45:25,343] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test OR DATE LOGIC Test
INFO  [2023-01-06 20:45:25,343] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:25,343] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:25,345] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-06 20:45:25,345] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-06 20:45:25,345] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:25,345] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_780a9add-5811-4f0b-a437-18bc2f1a1b3d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_780a9add-5811-4f0b-a437-18bc2f1a1b3d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_4adecc91-9cd1-483a-9051-8eced9edcd23 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_4adecc91-9cd1-483a-9051-8eced9edcd23 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:25,349] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:25,452] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,460] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,460] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20DATE$20LOGIC$20Test.table
INFO  [2023-01-06 20:45:25,460] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20DATE$20LOGIC$20Test.table
INFO  [2023-01-06 20:45:25,578] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,703] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:25,703] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:25,703] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 240 B in total
INFO  [2023-01-06 20:45:25,703] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR DATE LOGIC Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000416336sINFO  [2023-01-06 20:45:25,746] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=13, min=2, average=2.600000, max=4}
INFO  [2023-01-06 20:45:25,746] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=13, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:25,746] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=0), subType=IntegerParser(super=Parser(lines=13, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@5b70fb59)
INFO  [2023-01-06 20:45:25,749] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:25,749] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:25,749] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_OR DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:25,774] com.bakdata.conquery.models.jobs.ImportJob: Importing table into OR$20DATE$20LOGIC$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:25 +0000] "POST /admin/datasets/OR%20DATE%20LOGIC%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_OR+DATE+LOGIC+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:25,775] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,776] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:25,777] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:25,777] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:25,780] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:25,780] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
INFO  [2023-01-06 20:45:25,781] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
INFO  [2023-01-06 20:45:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20DATE$20LOGIC$20Test.table.table.0
WARN  [2023-01-06 20:45:25,782] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20DATE$20LOGIC$20Test.table.table.1
INFO  [2023-01-06 20:45:25,887] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,894] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:25,905] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:25,910] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:25,912] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,088] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: OR DATE LOGIC Test QUERY INIT
INFO  [2023-01-06 20:45:26,100] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[OR$20DATE$20LOGIC$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:26,100] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[73aa05ef-ad0d-4567-a4d9-91ee4a35c331] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_OR DATE LOGIC Test))]]
INFO  [2023-01-06 20:45:26,103] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331
INFO  [2023-01-06 20:45:26,103] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "POST /api/datasets/OR$20DATE$20LOGIC$20Test/queries HTTP/1.1" 201 1558 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:26,130] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331] with 2 results within PT0.02691S
INFO  [2023-01-06 20:45:26,130] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331] with 3 results within PT0.026944S
INFO  [2023-01-06 20:45:26,131] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331, workerId=OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_4adecc91-9cd1-483a-9051-8eced9edcd23, startTime=2023-01-06T20:45:26.103545, finishTime=2023-01-06T20:45:26.130455) of size 2
INFO  [2023-01-06 20:45:26,131] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331, workerId=OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_780a9add-5811-4f0b-a437-18bc2f1a1b3d, startTime=2023-01-06T20:45:26.103512, finishTime=2023-01-06T20:45:26.130456) of size 3
INFO  [2023-01-06 20:45:26,131] com.bakdata.conquery.models.execution.ManagedExecution: DONE 73aa05ef-ad0d-4567-a4d9-91ee4a35c331 ManagedQuery within PT0.030371S
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "GET /api/datasets/OR$20DATE$20LOGIC$20Test/queries/OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331 HTTP/1.1" 200 1846 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:26,144] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR DATE LOGIC Test], queryId=73aa05ef-ad0d-4567-a4d9-91ee4a35c331, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:26.100614, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@eef0c3a[Count = 0], startTime=2023-01-06T20:45:26.100883, finishTime=2023-01-06T20:45:26.131254, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1c7263c4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2e449839, com.bakdata.conquery.models.query.ColumnDescriptor@780bc9a0, com.bakdata.conquery.models.query.ColumnDescriptor@ef39447]) download on dataset Dataset[label=null, name=OR DATE LOGIC Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:26,144] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR DATE LOGIC Test], queryId=73aa05ef-ad0d-4567-a4d9-91ee4a35c331, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:26.100614, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@eef0c3a[Count = 0], startTime=2023-01-06T20:45:26.100883, finishTime=2023-01-06T20:45:26.131254, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1c7263c4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2e449839, com.bakdata.conquery.models.query.ColumnDescriptor@780bc9a0, com.bakdata.conquery.models.query.ColumnDescriptor@ef39447]) on dataset Dataset[label=null, name=OR DATE LOGIC Test]
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "GET /api/datasets/OR%20DATE%20LOGIC%20Test/result/OR$20DATE$20LOGIC$20Test.73aa05ef-ad0d-4567-a4d9-91ee4a35c331.csv?pretty=false HTTP/1.1" 200 216 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:26,163] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest OR DATE LOGIC Test on 6 rows
INFO  [2023-01-06 20:45:26,163] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast OR DATE LOGIC Test
INFO  [2023-01-06 20:45:26,163] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-06 20:45:26,163] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-06 20:45:26,163] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR DATE LOGIC Test_4adecc91-9cd1-483a-9051-8eced9edcd23
INFO  [2023-01-06 20:45:26,163] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR DATE LOGIC Test_780a9add-5811-4f0b-a437-18bc2f1a1b3d
INFO  [2023-01-06 20:45:26,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow OR DATE LOGIC Test
INFO  [2023-01-06 20:45:26,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR DATE LOGIC Test_780a9add-5811-4f0b-a437-18bc2f1a1b3d
INFO  [2023-01-06 20:45:26,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR DATE LOGIC Test_4adecc91-9cd1-483a-9051-8eced9edcd23
INFO  [2023-01-06 20:45:26,180] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of OR$20DATE$20LOGIC$20Test
INFO  [2023-01-06 20:45:26,180] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,284] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test OR DATE LOGIC Test
INFO  [2023-01-06 20:45:26,285] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:26,285] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:26,285] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:26,286] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-06 20:45:26,286] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:26,286] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-06 20:45:26,286] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:26,287] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_8d44de39-a11d-4a25-9051-9d64f1ef3f52 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:26,288] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_8d44de39-a11d-4a25-9051-9d64f1ef3f52 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:26,288] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:26,288] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_e40c6a1c-2b6f-4d8d-a03a-5b5274202d27 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:26,288] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_e40c6a1c-2b6f-4d8d-a03a-5b5274202d27 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:26,288] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:26,292] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,392] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,398] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,399] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[2].table
INFO  [2023-01-06 20:45:26,399] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[2].table
INFO  [2023-01-06 20:45:26,514] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,627] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:26,627] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:26,634] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 307 B in total
INFO  [2023-01-06 20:45:26,634] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[2]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000477948sINFO  [2023-01-06 20:45:26,682] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-06 20:45:26,682] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:26,682] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@213d1de7)
INFO  [2023-01-06 20:45:26,685] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:26,685] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:26,685] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:26,704] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test[2].table
INFO  [2023-01-06 20:45:26,705] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DURATION+SUM+Test%5B2%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:26,705] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:26,706] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:26,706] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:26,708] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:26,708] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[2].table.table], containing 17 entries.
INFO  [2023-01-06 20:45:26,708] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[2].table.table], containing 17 entries.
WARN  [2023-01-06 20:45:26,709] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:26,709] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[2].table.table.0
INFO  [2023-01-06 20:45:26,709] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[2].table.table.1
INFO  [2023-01-06 20:45:26,814] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,819] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,830] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:26,830] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:26,830] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:26,937] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-06 20:45:26,946] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:26,946] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e454fa2c-3578-4027-b986-bb39c4029a50] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[2]))]]
INFO  [2023-01-06 20:45:26,949] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[2].e454fa2c-3578-4027-b986-bb39c4029a50
INFO  [2023-01-06 20:45:26,949] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[2].e454fa2c-3578-4027-b986-bb39c4029a50
INFO  [2023-01-06 20:45:26,950] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[2].e454fa2c-3578-4027-b986-bb39c4029a50] with 2 results within PT0.000854S
INFO  [2023-01-06 20:45:26,950] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[2].e454fa2c-3578-4027-b986-bb39c4029a50] with 2 results within PT0.000799S
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B2%5D/queries HTTP/1.1" 201 1848 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:26,950] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[2].e454fa2c-3578-4027-b986-bb39c4029a50, workerId=AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_8d44de39-a11d-4a25-9051-9d64f1ef3f52, startTime=2023-01-06T20:45:26.949502, finishTime=2023-01-06T20:45:26.950301) of size 2
INFO  [2023-01-06 20:45:26,950] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[2].e454fa2c-3578-4027-b986-bb39c4029a50, workerId=AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_e40c6a1c-2b6f-4d8d-a03a-5b5274202d27, startTime=2023-01-06T20:45:26.949449, finishTime=2023-01-06T20:45:26.950303) of size 2
INFO  [2023-01-06 20:45:26,950] com.bakdata.conquery.models.execution.ManagedExecution: DONE e454fa2c-3578-4027-b986-bb39c4029a50 ManagedQuery within PT0.004027S
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B2%5D/queries/AND$20DURATION$20SUM$20Test%5B2%5D.e454fa2c-3578-4027-b986-bb39c4029a50 HTTP/1.1" 200 2431 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:26,973] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[2]], queryId=e454fa2c-3578-4027-b986-bb39c4029a50, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:26.946706, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2bdc4ee8[Count = 0], startTime=2023-01-06T20:45:26.946872, finishTime=2023-01-06T20:45:26.950899, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@74a646), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d0ebb1a, com.bakdata.conquery.models.query.ColumnDescriptor@2cbdea46, com.bakdata.conquery.models.query.ColumnDescriptor@65df4a37, com.bakdata.conquery.models.query.ColumnDescriptor@40bc865c]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:26,973] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[2]], queryId=e454fa2c-3578-4027-b986-bb39c4029a50, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:26.946706, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2bdc4ee8[Count = 0], startTime=2023-01-06T20:45:26.946872, finishTime=2023-01-06T20:45:26.950899, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@74a646), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d0ebb1a, com.bakdata.conquery.models.query.ColumnDescriptor@2cbdea46, com.bakdata.conquery.models.query.ColumnDescriptor@65df4a37, com.bakdata.conquery.models.query.ColumnDescriptor@40bc865c]) on dataset Dataset[label=null, name=AND DURATION SUM Test[2]]
127.0.0.1 - - [06/Jan/2023:20:45:26 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B2%5D/result/AND$20DURATION$20SUM$20Test%5B2%5D.e454fa2c-3578-4027-b986-bb39c4029a50.csv?pretty=false HTTP/1.1" 200 173 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:45:26,995] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 5 rows
INFO  [2023-01-06 20:45:26,995] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[2]
INFO  [2023-01-06 20:45:26,995] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-06 20:45:26,995] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-06 20:45:26,995] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[2]_8d44de39-a11d-4a25-9051-9d64f1ef3f52
INFO  [2023-01-06 20:45:26,995] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[2]_e40c6a1c-2b6f-4d8d-a03a-5b5274202d27
INFO  [2023-01-06 20:45:27,022] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[2]_8d44de39-a11d-4a25-9051-9d64f1ef3f52
INFO  [2023-01-06 20:45:27,022] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[2]_e40c6a1c-2b6f-4d8d-a03a-5b5274202d27
INFO  [2023-01-06 20:45:27,022] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[2]
INFO  [2023-01-06 20:45:27,122] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[2]
INFO  [2023-01-06 20:45:27,122] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,136] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:27,137] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:27,137] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:27,137] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:27,138] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-06 20:45:27,138] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-06 20:45:27,138] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:27,138] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:27,140] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_fc81570a-3f57-45ba-998d-e117decb89a0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:27,140] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_fc81570a-3f57-45ba-998d-e117decb89a0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:27,140] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:27,140] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_cb5839b7-d15a-484f-9399-18c0de105560 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:27,140] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_cb5839b7-d15a-484f-9399-18c0de105560 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:27,140] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:27,144] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,245] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,261] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,261] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[3].table
INFO  [2023-01-06 20:45:27,261] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[3].table
INFO  [2023-01-06 20:45:27,384] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,497] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:27,497] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:27,497] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 308 B in total
INFO  [2023-01-06 20:45:27,497] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[3]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000404827sINFO  [2023-01-06 20:45:27,538] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-06 20:45:27,538] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:27,538] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@5ad05349)
INFO  [2023-01-06 20:45:27,541] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[3]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:27,541] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:27,541] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[3]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:27,555] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test[3].table
INFO  [2023-01-06 20:45:27,555] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:27 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B3%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DURATION+SUM+Test%5B3%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:27,557] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:27,558] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:27,558] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:27,562] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:27,562] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[3].table.table], containing 17 entries.
INFO  [2023-01-06 20:45:27,562] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[3].table.table], containing 17 entries.
WARN  [2023-01-06 20:45:27,563] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:27,564] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[3].table.table.1
INFO  [2023-01-06 20:45:27,564] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[3].table.table.0
INFO  [2023-01-06 20:45:27,671] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,676] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,691] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:27,691] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:27,691] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:27,797] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-06 20:45:27,813] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[3]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:27,813] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9ec2f773-9491-48c6-b11c-e7fb4ab2082a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[3]))]]
INFO  [2023-01-06 20:45:27,817] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[3].9ec2f773-9491-48c6-b11c-e7fb4ab2082a
INFO  [2023-01-06 20:45:27,817] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[3].9ec2f773-9491-48c6-b11c-e7fb4ab2082a
127.0.0.1 - - [06/Jan/2023:20:45:27 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B3%5D/queries HTTP/1.1" 201 1684 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:27,818] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[3].9ec2f773-9491-48c6-b11c-e7fb4ab2082a] with 2 results within PT0.001504S
INFO  [2023-01-06 20:45:27,818] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[3].9ec2f773-9491-48c6-b11c-e7fb4ab2082a] with 2 results within PT0.001668S
INFO  [2023-01-06 20:45:27,819] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[3].9ec2f773-9491-48c6-b11c-e7fb4ab2082a, workerId=AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_cb5839b7-d15a-484f-9399-18c0de105560, startTime=2023-01-06T20:45:27.817044, finishTime=2023-01-06T20:45:27.818712) of size 2
INFO  [2023-01-06 20:45:27,819] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[3].9ec2f773-9491-48c6-b11c-e7fb4ab2082a, workerId=AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_fc81570a-3f57-45ba-998d-e117decb89a0, startTime=2023-01-06T20:45:27.817191, finishTime=2023-01-06T20:45:27.818695) of size 2
INFO  [2023-01-06 20:45:27,819] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9ec2f773-9491-48c6-b11c-e7fb4ab2082a ManagedQuery within PT0.005762S
127.0.0.1 - - [06/Jan/2023:20:45:27 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B3%5D/queries/AND$20DURATION$20SUM$20Test%5B3%5D.9ec2f773-9491-48c6-b11c-e7fb4ab2082a HTTP/1.1" 200 2267 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:27,847] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[3]], queryId=9ec2f773-9491-48c6-b11c-e7fb4ab2082a, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:27.813485, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2cf7d3e4[Count = 0], startTime=2023-01-06T20:45:27.813710, finishTime=2023-01-06T20:45:27.819472, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b5cc4aa), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[3])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@374c0c57, com.bakdata.conquery.models.query.ColumnDescriptor@627e3d75, com.bakdata.conquery.models.query.ColumnDescriptor@7084ba2d]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[3]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:27,847] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[3]], queryId=9ec2f773-9491-48c6-b11c-e7fb4ab2082a, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:27.813485, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2cf7d3e4[Count = 0], startTime=2023-01-06T20:45:27.813710, finishTime=2023-01-06T20:45:27.819472, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b5cc4aa), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[3])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@374c0c57, com.bakdata.conquery.models.query.ColumnDescriptor@627e3d75, com.bakdata.conquery.models.query.ColumnDescriptor@7084ba2d]) on dataset Dataset[label=null, name=AND DURATION SUM Test[3]]
127.0.0.1 - - [06/Jan/2023:20:45:27 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B3%5D/result/AND$20DURATION$20SUM$20Test%5B3%5D.9ec2f773-9491-48c6-b11c-e7fb4ab2082a.csv?pretty=false HTTP/1.1" 200 226 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:27,866] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 5 rows
INFO  [2023-01-06 20:45:27,866] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[3]
INFO  [2023-01-06 20:45:27,866] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-06 20:45:27,866] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-06 20:45:27,866] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[3]_fc81570a-3f57-45ba-998d-e117decb89a0
INFO  [2023-01-06 20:45:27,866] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[3]_cb5839b7-d15a-484f-9399-18c0de105560
INFO  [2023-01-06 20:45:27,938] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[3]
INFO  [2023-01-06 20:45:27,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[3]_fc81570a-3f57-45ba-998d-e117decb89a0
INFO  [2023-01-06 20:45:27,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[3]_cb5839b7-d15a-484f-9399-18c0de105560
INFO  [2023-01-06 20:45:27,965] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[3]
INFO  [2023-01-06 20:45:27,965] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,101] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:28,101] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:28,101] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:28,101] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:28,102] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-06 20:45:28,102] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-06 20:45:28,102] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:28,102] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:28,104] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_61352ef9-5099-45f4-8cc5-2bf496ec31e6 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:28,104] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_61352ef9-5099-45f4-8cc5-2bf496ec31e6 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:28,104] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:28,104] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_01ee4b19-18dc-4c1f-baf4-69863acf57eb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:28,104] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_01ee4b19-18dc-4c1f-baf4-69863acf57eb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:28,104] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:28,109] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,208] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,215] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,215] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[4].table
INFO  [2023-01-06 20:45:28,215] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[4].table
INFO  [2023-01-06 20:45:28,339] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,450] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:28,450] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:28,450] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 308 B in total
INFO  [2023-01-06 20:45:28,450] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[4]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000215547sINFO  [2023-01-06 20:45:28,472] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-06 20:45:28,472] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:28,472] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@6045f93c)
INFO  [2023-01-06 20:45:28,475] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[4]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:28,475] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:28,475] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_AND DURATION SUM Test[4]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:28,488] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test[4].table
127.0.0.1 - - [06/Jan/2023:20:45:28 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B4%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_AND+DURATION+SUM+Test%5B4%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:28,488] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,489] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:28,490] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:28,490] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:28,492] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:28,493] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[4].table.table], containing 17 entries.
INFO  [2023-01-06 20:45:28,493] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[4].table.table], containing 17 entries.
WARN  [2023-01-06 20:45:28,493] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:28,494] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[4].table.table.0
INFO  [2023-01-06 20:45:28,494] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[4].table.table.1
INFO  [2023-01-06 20:45:28,620] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,625] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,636] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:28,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:28,742] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-06 20:45:28,752] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[4]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:28,752] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5735113f-3707-4dd0-af60-5046746c2c6f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[4]))]]
INFO  [2023-01-06 20:45:28,756] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[4].5735113f-3707-4dd0-af60-5046746c2c6f
INFO  [2023-01-06 20:45:28,756] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[4].5735113f-3707-4dd0-af60-5046746c2c6f
INFO  [2023-01-06 20:45:28,757] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[4].5735113f-3707-4dd0-af60-5046746c2c6f] with 2 results within PT0.001199S
INFO  [2023-01-06 20:45:28,758] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[4].5735113f-3707-4dd0-af60-5046746c2c6f] with 3 results within PT0.001384S
127.0.0.1 - - [06/Jan/2023:20:45:28 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B4%5D/queries HTTP/1.1" 201 1684 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:28,758] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[4].5735113f-3707-4dd0-af60-5046746c2c6f, workerId=AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_61352ef9-5099-45f4-8cc5-2bf496ec31e6, startTime=2023-01-06T20:45:28.756716, finishTime=2023-01-06T20:45:28.757915) of size 2
INFO  [2023-01-06 20:45:28,758] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[4].5735113f-3707-4dd0-af60-5046746c2c6f, workerId=AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_01ee4b19-18dc-4c1f-baf4-69863acf57eb, startTime=2023-01-06T20:45:28.756729, finishTime=2023-01-06T20:45:28.758113) of size 3
INFO  [2023-01-06 20:45:28,758] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5735113f-3707-4dd0-af60-5046746c2c6f ManagedQuery within PT0.005937S
127.0.0.1 - - [06/Jan/2023:20:45:28 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B4%5D/queries/AND$20DURATION$20SUM$20Test%5B4%5D.5735113f-3707-4dd0-af60-5046746c2c6f HTTP/1.1" 200 2267 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:28,787] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[4]], queryId=5735113f-3707-4dd0-af60-5046746c2c6f, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:28.752727, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@279d810e[Count = 0], startTime=2023-01-06T20:45:28.752965, finishTime=2023-01-06T20:45:28.758902, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1c180df8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[4])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2c7cdaa2, com.bakdata.conquery.models.query.ColumnDescriptor@f65bdeb, com.bakdata.conquery.models.query.ColumnDescriptor@44bbc8f8]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[4]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:28,787] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[4]], queryId=5735113f-3707-4dd0-af60-5046746c2c6f, label=tree-a tree-b	@§$, creationTime=2023-01-06T20:45:28.752727, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@279d810e[Count = 0], startTime=2023-01-06T20:45:28.752965, finishTime=2023-01-06T20:45:28.758902, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1c180df8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[4])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2c7cdaa2, com.bakdata.conquery.models.query.ColumnDescriptor@f65bdeb, com.bakdata.conquery.models.query.ColumnDescriptor@44bbc8f8]) on dataset Dataset[label=null, name=AND DURATION SUM Test[4]]
127.0.0.1 - - [06/Jan/2023:20:45:28 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B4%5D/result/AND$20DURATION$20SUM$20Test%5B4%5D.5735113f-3707-4dd0-af60-5046746c2c6f.csv?pretty=false HTTP/1.1" 200 255 "-" "Conquery (test client)" 27
INFO  [2023-01-06 20:45:28,812] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 6 rows
INFO  [2023-01-06 20:45:28,812] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[4]
INFO  [2023-01-06 20:45:28,812] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[4]
INFO  [2023-01-06 20:45:28,812] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-06 20:45:28,812] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-06 20:45:28,813] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[4]_61352ef9-5099-45f4-8cc5-2bf496ec31e6
INFO  [2023-01-06 20:45:28,813] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[4]_01ee4b19-18dc-4c1f-baf4-69863acf57eb
INFO  [2023-01-06 20:45:28,911] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[4]
INFO  [2023-01-06 20:45:28,911] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:28,912] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[4]_01ee4b19-18dc-4c1f-baf4-69863acf57eb
INFO  [2023-01-06 20:45:28,912] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[4]_61352ef9-5099-45f4-8cc5-2bf496ec31e6
INFO  [2023-01-06 20:45:29,042] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-06 20:45:29,042] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-06 20:45:29,042] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:29,042] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:29,043] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-06 20:45:29,043] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-06 20:45:29,043] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:29,043] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:29,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_d296bc92-768d-4475-8664-bf2ee351b2f6 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:29,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_d296bc92-768d-4475-8664-bf2ee351b2f6 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:29,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:29,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_56c95d73-3c3b-4ac6-a28e-2ce3bd2637f8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:29,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_56c95d73-3c3b-4ac6-a28e-2ce3bd2637f8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:29,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:29,048] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,149] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,157] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,157] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose
INFO  [2023-01-06 20:45:29,157] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose
INFO  [2023-01-06 20:45:29,157] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose
INFO  [2023-01-06 20:45:29,200] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose
INFO  [2023-01-06 20:45:29,317] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,430] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:29,430] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:29,430] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:29,430] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 106 B in total
INFO  [2023-01-06 20:45:29,430] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_CONNECTORS_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████                ▌  68%	est. time remaining: 0.017556888sINFO  [2023-01-06 20:45:29,467] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=1, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:29,467] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@50359e96)
INFO  [2023-01-06 20:45:29,467] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[aufnahmedatum] with DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@7edf3905)
INFO  [2023-01-06 20:45:29,467] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=1, nullLines=0), encoding=null, prefix=F20, suffix=F20)
INFO  [2023-01-06 20:45:29,469] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_CONNECTORS_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:29,469] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_CONNECTORS_QUERY Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:29,485] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=0, sum=0, min=2147483647, average=0.000000, max=-2147483648}
INFO  [2023-01-06 20:45:29,485] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au_ende] with DateParser(super=Parser(lines=0, nullLines=0), subType=IntegerParser(super=Parser(lines=0, nullLines=0), minValue=9223372036854775807, maxValue=-9223372036854775808), dateReader=com.bakdata.conquery.util.DateReader@3fcb3992)
INFO  [2023-01-06 20:45:29,486] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au_beginn] with DateParser(super=Parser(lines=0, nullLines=0), subType=IntegerParser(super=Parser(lines=0, nullLines=0), minValue=9223372036854775807, maxValue=-9223372036854775808), dateReader=com.bakdata.conquery.util.DateReader@4bad0248)
INFO  [2023-01-06 20:45:29,486] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au] with DateParser(super=Parser(lines=0, nullLines=0), subType=IntegerParser(super=Parser(lines=0, nullLines=0), minValue=9223372036854775807, maxValue=-9223372036854775808), dateReader=com.bakdata.conquery.util.DateReader@3c337c5e)
INFO  [2023-01-06 20:45:29,486] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=0, nullLines=0), encoding=null, prefix=null, suffix=null)
INFO  [2023-01-06 20:45:29,487] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_CONNECTORS_QUERY Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:29,487] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:29,487] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_CONNECTORS_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:29,487] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_CONNECTORS_QUERY Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:29,506] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose
127.0.0.1 - - [06/Jan/2023:20:45:29 +0000] "POST /admin/datasets/MULTIPLE_CONNECTORS_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTIPLE_CONNECTORS_QUERY+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:29,508] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:29,508] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:29,508] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:29,511] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:29,511] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose.kh_diagnose], containing 1 entries.
INFO  [2023-01-06 20:45:29,511] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose.kh_diagnose], containing 1 entries.
WARN  [2023-01-06 20:45:29,512] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:29,513] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-06 20:45:29,521] com.bakdata.conquery.models.jobs.ImportJob: Importing au_diagnose into MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose
INFO  [2023-01-06 20:45:29,521] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:29,521] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:29,521] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:29,521] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:29 +0000] "POST /admin/datasets/MULTIPLE_CONNECTORS_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTIPLE_CONNECTORS_QUERY+Test%2Fau_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:45:29,522] com.bakdata.conquery.models.jobs.ImportJob: Start sending 0 Buckets
WARN  [2023-01-06 20:45:29,522] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
WARN  [2023-01-06 20:45:29,522] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:29,522] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose.au_diagnose], containing 0 entries.
INFO  [2023-01-06 20:45:29,522] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose.au_diagnose], containing 0 entries.
INFO  [2023-01-06 20:45:29,627] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,632] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,656] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,657] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:29,762] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTIPLE_CONNECTORS_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:29,778] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTIPLE_CONNECTORS_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:29,778] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5636e631-8246-4a4e-aca5-be33f8f1e0a7] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_CONNECTORS_QUERY Test))]]
INFO  [2023-01-06 20:45:29,782] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7
INFO  [2023-01-06 20:45:29,782] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7
WARN  [2023-01-06 20:45:29,782] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:29,782] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7] with 0 results within PT0.000294S
INFO  [2023-01-06 20:45:29,783] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7, workerId=MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_56c95d73-3c3b-4ac6-a28e-2ce3bd2637f8, startTime=2023-01-06T20:45:29.782641, finishTime=2023-01-06T20:45:29.782935) of size 0
INFO  [2023-01-06 20:45:29,783] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7] with 1 results within PT0.001033S
127.0.0.1 - - [06/Jan/2023:20:45:29 +0000] "POST /api/datasets/MULTIPLE_CONNECTORS_QUERY$20Test/queries HTTP/1.1" 201 1611 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:29,784] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7, workerId=MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_d296bc92-768d-4475-8664-bf2ee351b2f6, startTime=2023-01-06T20:45:29.782674, finishTime=2023-01-06T20:45:29.783707) of size 1
INFO  [2023-01-06 20:45:29,784] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5636e631-8246-4a4e-aca5-be33f8f1e0a7 ManagedQuery within PT0.005549S
127.0.0.1 - - [06/Jan/2023:20:45:29 +0000] "GET /api/datasets/MULTIPLE_CONNECTORS_QUERY$20Test/queries/MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7 HTTP/1.1" 200 1929 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:29,812] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test], queryId=5636e631-8246-4a4e-aca5-be33f8f1e0a7, label=F00-F99 F20-F29	@§$, creationTime=2023-01-06T20:45:29.778661, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5f9afddc[Count = 0], startTime=2023-01-06T20:45:29.778891, finishTime=2023-01-06T20:45:29.784440, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3d4bbc6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_CONNECTORS_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7f572c4d, com.bakdata.conquery.models.query.ColumnDescriptor@d69c594]) download on dataset Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:29,812] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test], queryId=5636e631-8246-4a4e-aca5-be33f8f1e0a7, label=F00-F99 F20-F29	@§$, creationTime=2023-01-06T20:45:29.778661, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5f9afddc[Count = 0], startTime=2023-01-06T20:45:29.778891, finishTime=2023-01-06T20:45:29.784440, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3d4bbc6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_CONNECTORS_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7f572c4d, com.bakdata.conquery.models.query.ColumnDescriptor@d69c594]) on dataset Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:29 +0000] "GET /api/datasets/MULTIPLE_CONNECTORS_QUERY%20Test/result/MULTIPLE_CONNECTORS_QUERY$20Test.5636e631-8246-4a4e-aca5-be33f8f1e0a7.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 28
INFO  [2023-01-06 20:45:29,838] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTIPLE_CONNECTORS_QUERY Test on 2 rows
INFO  [2023-01-06 20:45:29,838] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-06 20:45:29,838] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-06 20:45:29,838] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-06 20:45:29,838] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_CONNECTORS_QUERY Test_56c95d73-3c3b-4ac6-a28e-2ce3bd2637f8
INFO  [2023-01-06 20:45:29,839] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_CONNECTORS_QUERY Test_d296bc92-768d-4475-8664-bf2ee351b2f6
INFO  [2023-01-06 20:45:29,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-06 20:45:29,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_CONNECTORS_QUERY Test_d296bc92-768d-4475-8664-bf2ee351b2f6
INFO  [2023-01-06 20:45:29,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_CONNECTORS_QUERY Test_56c95d73-3c3b-4ac6-a28e-2ce3bd2637f8
INFO  [2023-01-06 20:45:29,937] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTIPLE_CONNECTORS_QUERY$20Test
INFO  [2023-01-06 20:45:29,937] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:29,965] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-06 20:45:29,965] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-06 20:45:29,965] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:29,965] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:29,966] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-06 20:45:29,966] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-06 20:45:29,966] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:29,966] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:29,968] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_1a2a8ae3-a196-4b47-b58e-8836ecb73772 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:29,968] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_1a2a8ae3-a196-4b47-b58e-8836ecb73772 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:29,968] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:29,968] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_b6dafad8-60f7-470e-bd14-4a857de1f3b4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:29,968] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_b6dafad8-60f7-470e-bd14-4a857de1f3b4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:29,968] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:29,972] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,072] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,080] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,080] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose
INFO  [2023-01-06 20:45:30,080] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose
INFO  [2023-01-06 20:45:30,080] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose
INFO  [2023-01-06 20:45:30,124] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose
INFO  [2023-01-06 20:45:30,235] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,346] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:30,347] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:30,347] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:30,347] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-06 20:45:30,347] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/kh_diagnose.import.json, tag=Optional.empty)
████████████████████████                          ▌  48%	est. time remaining: 0.043384381sINFO  [2023-01-06 20:45:30,387] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=26, sum=37, min=1, average=1.423077, max=2}
INFO  [2023-01-06 20:45:30,387] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=37, nullLines=13), subType=IntegerParser(super=Parser(lines=37, nullLines=13), minValue=15430, maxValue=17317), dateReader=com.bakdata.conquery.util.DateReader@75dfd55b)
INFO  [2023-01-06 20:45:30,387] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=37, nullLines=0), encoding=null, prefix=F, suffix=)
INFO  [2023-01-06 20:45:30,391] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:30,391] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/au_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000613558sINFO  [2023-01-06 20:45:30,409] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=27, sum=40, min=1, average=1.481481, max=2}
INFO  [2023-01-06 20:45:30,409] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au_beginn] with DateParser(super=Parser(lines=40, nullLines=14), subType=IntegerParser(super=Parser(lines=40, nullLines=14), minValue=15492, maxValue=17410), dateReader=com.bakdata.conquery.util.DateReader@27513b22)
INFO  [2023-01-06 20:45:30,409] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=40, nullLines=0), encoding=null, prefix=F, suffix=)
INFO  [2023-01-06 20:45:30,412] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:30,412] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:30,412] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:30,412] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:30,427] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose
127.0.0.1 - - [06/Jan/2023:20:45:30 +0000] "POST /admin/datasets/MULTIPLE_TABLES_ICD_QUERY2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTIPLE_TABLES_ICD_QUERY2+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:30,429] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:30,431] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries [DictionaryMapping(sourceDictionary=SuccinctTrie[size=12], targetDictionary=MapDictionary[size=12], numberOfNewIds=12)]
INFO  [2023-01-06 20:45:30,443] com.bakdata.conquery.models.jobs.ImportJob: Start sending 9 Buckets
INFO  [2023-01-06 20:45:30,443] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose], containing 37 entries.
INFO  [2023-01-06 20:45:30,443] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose], containing 37 entries.
INFO  [2023-01-06 20:45:30,445] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-06 20:45:30,454] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.1
INFO  [2023-01-06 20:45:30,454] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.2
INFO  [2023-01-06 20:45:30,454] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.3
INFO  [2023-01-06 20:45:30,455] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.4
INFO  [2023-01-06 20:45:30,455] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.5
WARN  [2023-01-06 20:45:30,455] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:30,455] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.8
INFO  [2023-01-06 20:45:30,458] com.bakdata.conquery.models.jobs.ImportJob: Importing au_diagnose into MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose
INFO  [2023-01-06 20:45:30,459] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:30,459] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries [DictionaryMapping(sourceDictionary=SuccinctTrie[size=12], targetDictionary=MapDictionary[size=12], numberOfNewIds=0)]
127.0.0.1 - - [06/Jan/2023:20:45:30 +0000] "POST /admin/datasets/MULTIPLE_TABLES_ICD_QUERY2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTIPLE_TABLES_ICD_QUERY2+Test%2Fau_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:45:30,459] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,459] com.bakdata.conquery.models.jobs.ImportJob: Start sending 9 Buckets
INFO  [2023-01-06 20:45:30,460] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose], containing 40 entries.
INFO  [2023-01-06 20:45:30,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.3
INFO  [2023-01-06 20:45:30,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.4
INFO  [2023-01-06 20:45:30,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.5
WARN  [2023-01-06 20:45:30,460] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:30,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.8
INFO  [2023-01-06 20:45:30,488] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.6
INFO  [2023-01-06 20:45:30,488] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.7
INFO  [2023-01-06 20:45:30,488] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose], containing 40 entries.
INFO  [2023-01-06 20:45:30,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.0
INFO  [2023-01-06 20:45:30,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.1
INFO  [2023-01-06 20:45:30,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.2
INFO  [2023-01-06 20:45:30,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.6
INFO  [2023-01-06 20:45:30,489] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.7
INFO  [2023-01-06 20:45:30,595] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,601] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,632] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:30,632] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:30,632] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:30,738] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTIPLE_TABLES_ICD_QUERY2 Test QUERY INIT
INFO  [2023-01-06 20:45:30,755] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTIPLE_TABLES_ICD_QUERY2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:30,755] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[202e72dd-dee7-4f3c-892a-53958e9106d6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_TABLES_ICD_QUERY2 Test))]]
INFO  [2023-01-06 20:45:30,759] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6
INFO  [2023-01-06 20:45:30,759] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6
127.0.0.1 - - [06/Jan/2023:20:45:30 +0000] "POST /api/datasets/MULTIPLE_TABLES_ICD_QUERY2$20Test/queries HTTP/1.1" 201 1353 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:30,762] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6] with 1 results within PT0.002606S
INFO  [2023-01-06 20:45:30,762] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6] with 6 results within PT0.003195S
INFO  [2023-01-06 20:45:30,762] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6, workerId=MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_b6dafad8-60f7-470e-bd14-4a857de1f3b4, startTime=2023-01-06T20:45:30.759642, finishTime=2023-01-06T20:45:30.762248) of size 1
INFO  [2023-01-06 20:45:30,763] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6, workerId=MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_1a2a8ae3-a196-4b47-b58e-8836ecb73772, startTime=2023-01-06T20:45:30.759700, finishTime=2023-01-06T20:45:30.762895) of size 6
INFO  [2023-01-06 20:45:30,763] com.bakdata.conquery.models.execution.ManagedExecution: DONE 202e72dd-dee7-4f3c-892a-53958e9106d6 ManagedQuery within PT0.007905S
127.0.0.1 - - [06/Jan/2023:20:45:30 +0000] "GET /api/datasets/MULTIPLE_TABLES_ICD_QUERY2$20Test/queries/MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6 HTTP/1.1" 200 1676 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:30,788] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test], queryId=202e72dd-dee7-4f3c-892a-53958e9106d6, label=icd-f20	@§$, creationTime=2023-01-06T20:45:30.755494, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@551aa760[Count = 0], startTime=2023-01-06T20:45:30.755753, finishTime=2023-01-06T20:45:30.763658, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@68751b7d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_TABLES_ICD_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@64be0d9d, com.bakdata.conquery.models.query.ColumnDescriptor@61b94dcb]) download on dataset Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:30,788] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test], queryId=202e72dd-dee7-4f3c-892a-53958e9106d6, label=icd-f20	@§$, creationTime=2023-01-06T20:45:30.755494, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@551aa760[Count = 0], startTime=2023-01-06T20:45:30.755753, finishTime=2023-01-06T20:45:30.763658, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@68751b7d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_TABLES_ICD_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@64be0d9d, com.bakdata.conquery.models.query.ColumnDescriptor@61b94dcb]) on dataset Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
127.0.0.1 - - [06/Jan/2023:20:45:30 +0000] "GET /api/datasets/MULTIPLE_TABLES_ICD_QUERY2%20Test/result/MULTIPLE_TABLES_ICD_QUERY2$20Test.202e72dd-dee7-4f3c-892a-53958e9106d6.csv?pretty=false HTTP/1.1" 200 312 "-" "Conquery (test client)" 25
INFO  [2023-01-06 20:45:30,812] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTIPLE_TABLES_ICD_QUERY2 Test on 8 rows
INFO  [2023-01-06 20:45:30,812] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-06 20:45:30,813] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-06 20:45:30,813] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-06 20:45:30,813] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_TABLES_ICD_QUERY2 Test_b6dafad8-60f7-470e-bd14-4a857de1f3b4
INFO  [2023-01-06 20:45:30,813] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_TABLES_ICD_QUERY2 Test_1a2a8ae3-a196-4b47-b58e-8836ecb73772
INFO  [2023-01-06 20:45:30,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-06 20:45:30,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_TABLES_ICD_QUERY2 Test_1a2a8ae3-a196-4b47-b58e-8836ecb73772
INFO  [2023-01-06 20:45:30,868] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_TABLES_ICD_QUERY2 Test_b6dafad8-60f7-470e-bd14-4a857de1f3b4
INFO  [2023-01-06 20:45:30,961] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTIPLE_TABLES_ICD_QUERY2$20Test
INFO  [2023-01-06 20:45:30,961] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,038] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-06 20:45:31,038] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:31,039] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:31,039] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:31,040] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:31,040] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:31,040] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:31,040] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_aac92aba-9963-420b-b19d-54dd0669eb84 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_aac92aba-9963-420b-b19d-54dd0669eb84 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_13ee4439-c570-4f52-9e6f-3c05fb433aca are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_13ee4439-c570-4f52-9e6f-3c05fb433aca are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:31,042] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:31,146] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,153] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,153] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-06 20:45:31,153] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-06 20:45:31,153] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-06 20:45:31,196] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-06 20:45:31,313] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,424] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:31,424] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:31,424] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:31,425] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 214 B in total
INFO  [2023-01-06 20:45:31,425] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.028705681sINFO  [2023-01-06 20:45:31,453] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:45:31,453] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:31,453] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@75a88c1a)
INFO  [2023-01-06 20:45:31,456] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:31,456] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000458696sINFO  [2023-01-06 20:45:31,471] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:45:31,471] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:31,471] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2d3ad3fd)
INFO  [2023-01-06 20:45:31,473] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:31,473] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:31,473] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:31,473] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:31,492] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table
127.0.0.1 - - [06/Jan/2023:20:45:31 +0000] "POST /admin/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:31,493] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:31,493] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:31,493] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:31,495] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:31,495] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
INFO  [2023-01-06 20:45:31,495] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
WARN  [2023-01-06 20:45:31,496] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:31,496] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:31,496] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:31,513] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table2 into MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-06 20:45:31,513] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:31,513] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
127.0.0.1 - - [06/Jan/2023:20:45:31 +0000] "POST /admin/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES+Test%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
WARN  [2023-01-06 20:45:31,513] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:31,513] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:31,513] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
WARN  [2023-01-06 20:45:31,513] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:31,514] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
INFO  [2023-01-06 20:45:31,514] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
INFO  [2023-01-06 20:45:31,514] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.0
INFO  [2023-01-06 20:45:31,514] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.1
INFO  [2023-01-06 20:45:31,619] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,625] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,642] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:31,642] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:45:31,642] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:45:31,763] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_CONCEPT_QUERY_SEPARATE_DATES Test QUERY INIT
INFO  [2023-01-06 20:45:31,782] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:31,783] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1a68a054-dfb0-49ba-a0ee-4af7582bec93] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test))]]
INFO  [2023-01-06 20:45:31,789] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93
INFO  [2023-01-06 20:45:31,789] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93
INFO  [2023-01-06 20:45:31,790] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93] with 0 results within PT0.001148S
INFO  [2023-01-06 20:45:31,790] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93] with 2 results within PT0.001524S
INFO  [2023-01-06 20:45:31,791] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93, workerId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_aac92aba-9963-420b-b19d-54dd0669eb84, startTime=2023-01-06T20:45:31.789361, finishTime=2023-01-06T20:45:31.790509) of size 0
127.0.0.1 - - [06/Jan/2023:20:45:31 +0000] "POST /api/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test/queries HTTP/1.1" 201 2140 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:31,791] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93, workerId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_13ee4439-c570-4f52-9e6f-3c05fb433aca, startTime=2023-01-06T20:45:31.789399, finishTime=2023-01-06T20:45:31.790923) of size 2
INFO  [2023-01-06 20:45:31,791] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1a68a054-dfb0-49ba-a0ee-4af7582bec93 ManagedQuery within PT0.007807S
127.0.0.1 - - [06/Jan/2023:20:45:31 +0000] "GET /api/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test/queries/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93 HTTP/1.1" 200 2495 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:31,816] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test], queryId=1a68a054-dfb0-49ba-a0ee-4af7582bec93, label=test_tree-test_child1 test_tree2-test_child1	@§$, creationTime=2023-01-06T20:45:31.783558, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7e201a8a[Count = 0], startTime=2023-01-06T20:45:31.783845, finishTime=2023-01-06T20:45:31.791652, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7f08df6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@10ace1eb, com.bakdata.conquery.models.query.ColumnDescriptor@23a00932, com.bakdata.conquery.models.query.ColumnDescriptor@5daaa386, com.bakdata.conquery.models.query.ColumnDescriptor@5e360af8]) download on dataset Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:31,817] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test], queryId=1a68a054-dfb0-49ba-a0ee-4af7582bec93, label=test_tree-test_child1 test_tree2-test_child1	@§$, creationTime=2023-01-06T20:45:31.783558, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7e201a8a[Count = 0], startTime=2023-01-06T20:45:31.783845, finishTime=2023-01-06T20:45:31.791652, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7f08df6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@10ace1eb, com.bakdata.conquery.models.query.ColumnDescriptor@23a00932, com.bakdata.conquery.models.query.ColumnDescriptor@5daaa386, com.bakdata.conquery.models.query.ColumnDescriptor@5e360af8]) on dataset Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
127.0.0.1 - - [06/Jan/2023:20:45:31 +0000] "GET /api/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES%20Test/result/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.1a68a054-dfb0-49ba-a0ee-4af7582bec93.csv?pretty=false HTTP/1.1" 200 218 "-" "Conquery (test client)" 24
INFO  [2023-01-06 20:45:31,839] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_CONCEPT_QUERY_SEPARATE_DATES Test on 3 rows
INFO  [2023-01-06 20:45:31,839] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:31,839] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:31,839] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:31,839] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_aac92aba-9963-420b-b19d-54dd0669eb84
INFO  [2023-01-06 20:45:31,839] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_13ee4439-c570-4f52-9e6f-3c05fb433aca
INFO  [2023-01-06 20:45:31,858] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_13ee4439-c570-4f52-9e6f-3c05fb433aca
INFO  [2023-01-06 20:45:31,858] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:31,858] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_aac92aba-9963-420b-b19d-54dd0669eb84
INFO  [2023-01-06 20:45:31,914] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test
INFO  [2023-01-06 20:45:31,914] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,048] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:32,049] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:32,049] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:32,050] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:32,051] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:32,051] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:32,052] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:32,052] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:32,054] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_a5f63494-5c85-431b-9b3e-62282df5da84 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:32,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_a5f63494-5c85-431b-9b3e-62282df5da84 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:32,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:32,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_2a8bb2a5-8914-4dc4-beaf-d618ebe779c7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:32,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_2a8bb2a5-8914-4dc4-beaf-d618ebe779c7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:32,056] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:32,157] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,166] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,166] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-06 20:45:32,166] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-06 20:45:32,166] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-06 20:45:32,208] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-06 20:45:32,323] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,436] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:32,436] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:32,436] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:32,436] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 224 B in total
INFO  [2023-01-06 20:45:32,437] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.022121588sINFO  [2023-01-06 20:45:32,459] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:45:32,459] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:32,459] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@6279537f)
INFO  [2023-01-06 20:45:32,463] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:32,463] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000604162sINFO  [2023-01-06 20:45:32,498] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:45:32,498] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:32,498] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@450f02e2)
INFO  [2023-01-06 20:45:32,500] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:32,500] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:32,500] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:32,500] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:32,521] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table
127.0.0.1 - - [06/Jan/2023:20:45:32 +0000] "POST /admin/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:32,522] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:32,523] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:32,523] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:32,524] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:32,524] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
INFO  [2023-01-06 20:45:32,524] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
WARN  [2023-01-06 20:45:32,525] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:32,525] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:32,525] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:32,538] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table2 into MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-06 20:45:32,539] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:32,539] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:32 +0000] "POST /admin/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES+Test%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:32,539] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:32,539] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:32,540] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:32,540] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
INFO  [2023-01-06 20:45:32,540] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
WARN  [2023-01-06 20:45:32,540] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:32,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.0
INFO  [2023-01-06 20:45:32,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.1
INFO  [2023-01-06 20:45:32,648] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,654] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,667] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,668] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:32,668] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:32,773] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test QUERY INIT
INFO  [2023-01-06 20:45:32,789] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:32,790] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9569743b-bd76-43fb-a16b-87b10a5065ee] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test))]]
INFO  [2023-01-06 20:45:32,793] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee
INFO  [2023-01-06 20:45:32,793] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee
INFO  [2023-01-06 20:45:32,794] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee] with 0 results within PT0.001138S
INFO  [2023-01-06 20:45:32,795] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee] with 2 results within PT0.001346S
127.0.0.1 - - [06/Jan/2023:20:45:32 +0000] "POST /api/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test/queries HTTP/1.1" 201 1639 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:32,795] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee, workerId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_2a8bb2a5-8914-4dc4-beaf-d618ebe779c7, startTime=2023-01-06T20:45:32.793665, finishTime=2023-01-06T20:45:32.794803) of size 0
INFO  [2023-01-06 20:45:32,795] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee, workerId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_a5f63494-5c85-431b-9b3e-62282df5da84, startTime=2023-01-06T20:45:32.793665, finishTime=2023-01-06T20:45:32.795011) of size 2
INFO  [2023-01-06 20:45:32,795] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9569743b-bd76-43fb-a16b-87b10a5065ee ManagedQuery within PT0.005411S
127.0.0.1 - - [06/Jan/2023:20:45:32 +0000] "GET /api/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test/queries/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee HTTP/1.1" 200 2002 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:32,820] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test], queryId=9569743b-bd76-43fb-a16b-87b10a5065ee, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:32.790231, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3fbc190b[Count = 0], startTime=2023-01-06T20:45:32.790467, finishTime=2023-01-06T20:45:32.795878, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b7804a8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4bed21c5, com.bakdata.conquery.models.query.ColumnDescriptor@194ad46e, com.bakdata.conquery.models.query.ColumnDescriptor@4661e20d]) download on dataset Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:32,820] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test], queryId=9569743b-bd76-43fb-a16b-87b10a5065ee, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:32.790231, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3fbc190b[Count = 0], startTime=2023-01-06T20:45:32.790467, finishTime=2023-01-06T20:45:32.795878, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b7804a8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4bed21c5, com.bakdata.conquery.models.query.ColumnDescriptor@194ad46e, com.bakdata.conquery.models.query.ColumnDescriptor@4661e20d]) on dataset Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
127.0.0.1 - - [06/Jan/2023:20:45:32 +0000] "GET /api/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES%20Test/result/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.9569743b-bd76-43fb-a16b-87b10a5065ee.csv?pretty=false HTTP/1.1" 200 152 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:45:32,842] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test on 3 rows
INFO  [2023-01-06 20:45:32,842] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:32,842] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:32,842] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-06 20:45:32,843] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_2a8bb2a5-8914-4dc4-beaf-d618ebe779c7
INFO  [2023-01-06 20:45:32,843] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_a5f63494-5c85-431b-9b3e-62282df5da84
INFO  [2023-01-06 20:45:32,851] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:32,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_a5f63494-5c85-431b-9b3e-62282df5da84
INFO  [2023-01-06 20:45:32,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_2a8bb2a5-8914-4dc4-beaf-d618ebe779c7
INFO  [2023-01-06 20:45:32,943] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test
INFO  [2023-01-06 20:45:32,943] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:32,973] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-06 20:45:32,973] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:32,974] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:32,974] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:32,975] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:32,975] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:32,976] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:32,976] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:32,977] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_083c104c-c910-438b-ba4f-d9b042c6dca6 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:32,977] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_083c104c-c910-438b-ba4f-d9b042c6dca6 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:32,977] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:32,977] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_bdbf3879-f89a-4685-b071-b988091bf739 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:32,977] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_bdbf3879-f89a-4685-b071-b988091bf739 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:32,977] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:32,982] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,087] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,096] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,096] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:33,096] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:33,216] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,329] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:33,329] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:33,329] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:45:33,330] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000437809sINFO  [2023-01-06 20:45:33,374] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:33,374] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:33,374] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@74f68e05)
INFO  [2023-01-06 20:45:33,377] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:33,377] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:33,377] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:33,398] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:33,398] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:33 +0000] "POST /admin/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:33,399] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:33,400] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:33,400] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:33,401] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:33,401] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:45:33,401] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-06 20:45:33,402] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:33,402] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:33,402] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:33,403] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:45:33,507] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,512] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,527] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,528] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:33,528] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:33,634] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:33,649] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:33,650] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:33,654] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19
INFO  [2023-01-06 20:45:33,654] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19
127.0.0.1 - - [06/Jan/2023:20:45:33 +0000] "POST /api/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1558 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:33,656] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19] with 0 results within PT0.001226S
INFO  [2023-01-06 20:45:33,656] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19] with 1 results within PT0.001469S
INFO  [2023-01-06 20:45:33,656] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19, workerId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_083c104c-c910-438b-ba4f-d9b042c6dca6, startTime=2023-01-06T20:45:33.654779, finishTime=2023-01-06T20:45:33.656005) of size 0
INFO  [2023-01-06 20:45:33,656] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19, workerId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_bdbf3879-f89a-4685-b071-b988091bf739, startTime=2023-01-06T20:45:33.654780, finishTime=2023-01-06T20:45:33.656249) of size 1
INFO  [2023-01-06 20:45:33,656] com.bakdata.conquery.models.execution.ManagedExecution: DONE bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19 ManagedQuery within PT0.006415S
127.0.0.1 - - [06/Jan/2023:20:45:33 +0000] "GET /api/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19 HTTP/1.1" 200 1961 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:33,679] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:33.650306, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@703590af[Count = 0], startTime=2023-01-06T20:45:33.650530, finishTime=2023-01-06T20:45:33.656945, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@69975660), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@529f6fe7, com.bakdata.conquery.models.query.ColumnDescriptor@7d3f38f0]) download on dataset Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:33,679] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:33.650306, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@703590af[Count = 0], startTime=2023-01-06T20:45:33.650530, finishTime=2023-01-06T20:45:33.656945, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@69975660), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@529f6fe7, com.bakdata.conquery.models.query.ColumnDescriptor@7d3f38f0]) on dataset Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:33 +0000] "GET /api/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.bdd4353b-4cb3-4a84-9f33-7d5fd7abdb19.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:33,697] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 2 rows
INFO  [2023-01-06 20:45:33,698] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:33,698] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:33,698] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:33,698] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_083c104c-c910-438b-ba4f-d9b042c6dca6
INFO  [2023-01-06 20:45:33,698] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_bdbf3879-f89a-4685-b071-b988091bf739
INFO  [2023-01-06 20:45:33,776] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:33,777] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_083c104c-c910-438b-ba4f-d9b042c6dca6
INFO  [2023-01-06 20:45:33,777] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_bdbf3879-f89a-4685-b071-b988091bf739
INFO  [2023-01-06 20:45:33,806] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:33,807] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:33,933] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:33,933] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:33,933] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:33,934] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:33,935] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:33,935] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:33,935] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:33,935] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6f627ed1-c7ad-4d96-b9da-63561d09d45e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6f627ed1-c7ad-4d96-b9da-63561d09d45e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_825ced98-eade-4cfe-8c6d-15bc1e60ed65 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_825ced98-eade-4cfe-8c6d-15bc1e60ed65 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:33,937] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,041] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,050] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,050] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:34,050] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:34,172] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,284] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:34,284] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:34,284] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 157 B in total
INFO  [2023-01-06 20:45:34,284] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00034815sINFO  [2023-01-06 20:45:34,320] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:34,320] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:34,320] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@f280e5d)
INFO  [2023-01-06 20:45:34,323] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:34,323] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:34,323] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:34,346] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:34,347] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:34 +0000] "POST /admin/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:34,347] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:34,348] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:34,348] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:34,350] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:34,350] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:45:34,350] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-06 20:45:34,351] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:34,351] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:34,351] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:34,351] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:45:34,457] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,477] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,478] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:34,478] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:34,584] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:34,597] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:34,597] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[6de020b9-3315-469b-9e6d-bd92b4f47278] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:34,601] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278
INFO  [2023-01-06 20:45:34,601] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278
127.0.0.1 - - [06/Jan/2023:20:45:34 +0000] "POST /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 2180 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:34,602] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278] with 2 results within PT0.001063S
INFO  [2023-01-06 20:45:34,602] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278] with 4 results within PT0.001595S
INFO  [2023-01-06 20:45:34,603] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_825ced98-eade-4cfe-8c6d-15bc1e60ed65, startTime=2023-01-06T20:45:34.601453, finishTime=2023-01-06T20:45:34.602516) of size 2
INFO  [2023-01-06 20:45:34,603] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6f627ed1-c7ad-4d96-b9da-63561d09d45e, startTime=2023-01-06T20:45:34.601216, finishTime=2023-01-06T20:45:34.602811) of size 4
INFO  [2023-01-06 20:45:34,603] com.bakdata.conquery.models.execution.ManagedExecution: DONE 6de020b9-3315-469b-9e6d-bd92b4f47278 ManagedQuery within PT0.005491S
127.0.0.1 - - [06/Jan/2023:20:45:34 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278 HTTP/1.1" 200 2619 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:34,620] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=6de020b9-3315-469b-9e6d-bd92b4f47278, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:34.597749, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3c5f532b[Count = 0], startTime=2023-01-06T20:45:34.597924, finishTime=2023-01-06T20:45:34.603415, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1318fde2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@27c77233, com.bakdata.conquery.models.query.ColumnDescriptor@30c0c715]) download on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:34,621] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=6de020b9-3315-469b-9e6d-bd92b4f47278, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:34.597749, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3c5f532b[Count = 0], startTime=2023-01-06T20:45:34.597924, finishTime=2023-01-06T20:45:34.603415, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1318fde2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@27c77233, com.bakdata.conquery.models.query.ColumnDescriptor@30c0c715]) on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:34 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6de020b9-3315-469b-9e6d-bd92b4f47278.csv?pretty=false HTTP/1.1" 200 43 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:34,637] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 7 rows
INFO  [2023-01-06 20:45:34,638] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:34,638] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:34,638] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:34,638] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_825ced98-eade-4cfe-8c6d-15bc1e60ed65
INFO  [2023-01-06 20:45:34,638] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_6f627ed1-c7ad-4d96-b9da-63561d09d45e
INFO  [2023-01-06 20:45:34,736] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:34,736] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_825ced98-eade-4cfe-8c6d-15bc1e60ed65
INFO  [2023-01-06 20:45:34,736] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_6f627ed1-c7ad-4d96-b9da-63561d09d45e
INFO  [2023-01-06 20:45:34,751] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:34,751] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,885] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:34,886] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-06 20:45:34,886] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:34,886] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:34,887] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-06 20:45:34,887] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:34,887] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-06 20:45:34,887] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_f67db1a5-cd45-4842-8b03-bc332cca6efd are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_f67db1a5-cd45-4842-8b03-bc332cca6efd are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_1baa03d6-91e9-4e6b-bcb7-cee4e1c61097 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_1baa03d6-91e9-4e6b-bcb7-cee4e1c61097 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:34,889] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:34,993] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:34,999] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,000] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1
INFO  [2023-01-06 20:45:35,000] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1
INFO  [2023-01-06 20:45:35,126] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,239] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:35,239] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:35,239] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:45:35,239] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000313555sINFO  [2023-01-06 20:45:35,271] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:35,271] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:35,271] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2762566c)
INFO  [2023-01-06 20:45:35,275] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:35,275] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:35,275] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:35,288] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1
INFO  [2023-01-06 20:45:35,289] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:35 +0000] "POST /admin/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:35,289] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:35,290] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:35,290] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:35,291] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:35,291] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:45:35,291] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-06 20:45:35,292] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:35,292] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1.0
INFO  [2023-01-06 20:45:35,292] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1.1
INFO  [2023-01-06 20:45:35,292] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1.2
INFO  [2023-01-06 20:45:35,398] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,403] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,421] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,421] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:35,421] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:35,527] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test QUERY INIT
INFO  [2023-01-06 20:45:35,545] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:35,545] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[13471b40-f7ff-4a9f-a346-03205cf2c6a6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test))]]
INFO  [2023-01-06 20:45:35,550] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6
INFO  [2023-01-06 20:45:35,550] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6
127.0.0.1 - - [06/Jan/2023:20:45:35 +0000] "POST /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test/queries HTTP/1.1" 201 1640 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:35,552] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6] with 4 results within PT0.001654S
INFO  [2023-01-06 20:45:35,552] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6] with 3 results within PT0.001599S
INFO  [2023-01-06 20:45:35,553] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_f67db1a5-cd45-4842-8b03-bc332cca6efd, startTime=2023-01-06T20:45:35.550751, finishTime=2023-01-06T20:45:35.552405) of size 4
INFO  [2023-01-06 20:45:35,553] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_1baa03d6-91e9-4e6b-bcb7-cee4e1c61097, startTime=2023-01-06T20:45:35.550906, finishTime=2023-01-06T20:45:35.552505) of size 3
INFO  [2023-01-06 20:45:35,553] com.bakdata.conquery.models.execution.ManagedExecution: DONE 13471b40-f7ff-4a9f-a346-03205cf2c6a6 ManagedQuery within PT0.007415S
127.0.0.1 - - [06/Jan/2023:20:45:35 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test/queries/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6 HTTP/1.1" 200 2083 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:35,580] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test], queryId=13471b40-f7ff-4a9f-a346-03205cf2c6a6, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:35.545703, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6abd858b[Count = 0], startTime=2023-01-06T20:45:35.545964, finishTime=2023-01-06T20:45:35.553379, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7164703c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3b0cc6aa, com.bakdata.conquery.models.query.ColumnDescriptor@10078e44]) download on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:35,580] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test], queryId=13471b40-f7ff-4a9f-a346-03205cf2c6a6, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:35.545703, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6abd858b[Count = 0], startTime=2023-01-06T20:45:35.545964, finishTime=2023-01-06T20:45:35.553379, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7164703c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3b0cc6aa, com.bakdata.conquery.models.query.ColumnDescriptor@10078e44]) on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-06 20:45:35,598] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test on 8 rows
127.0.0.1 - - [06/Jan/2023:20:45:35 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2%20Test/result/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.13471b40-f7ff-4a9f-a346-03205cf2c6a6.csv?pretty=false HTTP/1.1" 200 48 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:35,598] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-06 20:45:35,599] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-06 20:45:35,599] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-06 20:45:35,599] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_f67db1a5-cd45-4842-8b03-bc332cca6efd
INFO  [2023-01-06 20:45:35,599] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_1baa03d6-91e9-4e6b-bcb7-cee4e1c61097
INFO  [2023-01-06 20:45:35,696] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_1baa03d6-91e9-4e6b-bcb7-cee4e1c61097
INFO  [2023-01-06 20:45:35,696] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_f67db1a5-cd45-4842-8b03-bc332cca6efd
INFO  [2023-01-06 20:45:35,696] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-06 20:45:35,796] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test
INFO  [2023-01-06 20:45:35,797] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,826] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-06 20:45:35,827] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_QUERY Test
INFO  [2023-01-06 20:45:35,827] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:35,827] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:35,828] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-06 20:45:35,828] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-06 20:45:35,828] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:35,828] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_4aa5ec8d-7cd9-4283-bbc6-0d20802b7737 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_4aa5ec8d-7cd9-4283-bbc6-0d20802b7737 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_37abe03f-6456-402f-a13e-14beae110c33 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_37abe03f-6456-402f-a13e-14beae110c33 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:35,830] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:35,934] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:35,941] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test.table1
INFO  [2023-01-06 20:45:35,941] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test.table1
INFO  [2023-01-06 20:45:36,058] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,168] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:36,168] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:36,168] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 372 B in total
INFO  [2023-01-06 20:45:36,169] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000261704sINFO  [2023-01-06 20:45:36,195] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=12, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:45:36,195] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16106, maxValue=16469), dateReader=com.bakdata.conquery.util.DateReader@53860178), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16121, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@ab8ba52), dateReader=com.bakdata.conquery.util.DateReader@62a5d0e8, onlyQuarters=false, maxValue=16616, minValue=16106, anyOpen=false)
INFO  [2023-01-06 20:45:36,195] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=12, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:36,198] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:36,198] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:36,198] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:36,216] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_QUERY$20Test.table1
INFO  [2023-01-06 20:45:36,216] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:36 +0000] "POST /admin/datasets/NUMBER_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:36,217] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:36,217] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:36,217] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:36,218] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:36,218] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test.table1.table1], containing 12 entries.
INFO  [2023-01-06 20:45:36,218] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test.table1.table1], containing 12 entries.
WARN  [2023-01-06 20:45:36,227] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:36,227] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:36,227] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:45:36,227] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:36,332] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,338] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,354] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,355] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:36,355] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:36,461] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:36,477] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:36,478] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b76ab020-cad9-4d86-94c5-c09362693ff0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test))]]
INFO  [2023-01-06 20:45:36,483] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0
INFO  [2023-01-06 20:45:36,483] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0
127.0.0.1 - - [06/Jan/2023:20:45:36 +0000] "POST /api/datasets/NUMBER_QUERY$20Test/queries HTTP/1.1" 201 1358 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:36,485] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0] with 2 results within PT0.001541S
INFO  [2023-01-06 20:45:36,485] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0] with 2 results within PT0.001494S
INFO  [2023-01-06 20:45:36,486] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0, workerId=NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_37abe03f-6456-402f-a13e-14beae110c33, startTime=2023-01-06T20:45:36.483928, finishTime=2023-01-06T20:45:36.485422) of size 2
INFO  [2023-01-06 20:45:36,486] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0, workerId=NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_4aa5ec8d-7cd9-4283-bbc6-0d20802b7737, startTime=2023-01-06T20:45:36.483832, finishTime=2023-01-06T20:45:36.485373) of size 2
INFO  [2023-01-06 20:45:36,486] com.bakdata.conquery.models.execution.ManagedExecution: DONE b76ab020-cad9-4d86-94c5-c09362693ff0 ManagedQuery within PT0.007896S
127.0.0.1 - - [06/Jan/2023:20:45:36 +0000] "GET /api/datasets/NUMBER_QUERY$20Test/queries/NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0 HTTP/1.1" 200 1625 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:36,505] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test], queryId=b76ab020-cad9-4d86-94c5-c09362693ff0, label=vs	@§$, creationTime=2023-01-06T20:45:36.478272, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1ecc61b8[Count = 0], startTime=2023-01-06T20:45:36.478492, finishTime=2023-01-06T20:45:36.486388, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@39cacc45), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2c9daf5c, com.bakdata.conquery.models.query.ColumnDescriptor@1cb3013a]) download on dataset Dataset[label=null, name=NUMBER_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:36,505] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test], queryId=b76ab020-cad9-4d86-94c5-c09362693ff0, label=vs	@§$, creationTime=2023-01-06T20:45:36.478272, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1ecc61b8[Count = 0], startTime=2023-01-06T20:45:36.478492, finishTime=2023-01-06T20:45:36.486388, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@39cacc45), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2c9daf5c, com.bakdata.conquery.models.query.ColumnDescriptor@1cb3013a]) on dataset Dataset[label=null, name=NUMBER_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:36 +0000] "GET /api/datasets/NUMBER_QUERY%20Test/result/NUMBER_QUERY$20Test.b76ab020-cad9-4d86-94c5-c09362693ff0.csv?pretty=false HTTP/1.1" 200 145 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:36,526] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_QUERY Test on 5 rows
INFO  [2023-01-06 20:45:36,526] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_QUERY Test
INFO  [2023-01-06 20:45:36,526] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-06 20:45:36,526] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-06 20:45:36,526] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test_37abe03f-6456-402f-a13e-14beae110c33
INFO  [2023-01-06 20:45:36,526] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test_4aa5ec8d-7cd9-4283-bbc6-0d20802b7737
INFO  [2023-01-06 20:45:36,529] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_QUERY Test
INFO  [2023-01-06 20:45:36,529] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test_4aa5ec8d-7cd9-4283-bbc6-0d20802b7737
INFO  [2023-01-06 20:45:36,530] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test_37abe03f-6456-402f-a13e-14beae110c33
INFO  [2023-01-06 20:45:36,627] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_QUERY$20Test
INFO  [2023-01-06 20:45:36,627] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,661] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_QUERY Test
INFO  [2023-01-06 20:45:36,661] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING_QUERY Test
INFO  [2023-01-06 20:45:36,661] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:36,661] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:36,662] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-06 20:45:36,662] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-06 20:45:36,662] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:36,662] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:36,664] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_03a85c19-d64c-4152-b56c-ca3197fc3de0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:36,664] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_03a85c19-d64c-4152-b56c-ca3197fc3de0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:36,664] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:36,664] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_1b2b3cdc-f7cc-4c78-8277-2b3503d3ba51 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:36,664] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_1b2b3cdc-f7cc-4c78-8277-2b3503d3ba51 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:36,664] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:36,668] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,768] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,775] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,775] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_QUERY$20Test.table1
INFO  [2023-01-06 20:45:36,775] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_QUERY$20Test.table1
INFO  [2023-01-06 20:45:36,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:36,999] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:36,999] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:37,000] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 129 B in total
INFO  [2023-01-06 20:45:37,000] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00039857sINFO  [2023-01-06 20:45:37,040] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:37,040] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16251, maxValue=16555), dateReader=com.bakdata.conquery.util.DateReader@18784322), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@262ea294), dateReader=com.bakdata.conquery.util.DateReader@5f7ed5b4, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-06 20:45:37,040] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=2), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-06 20:45:37,042] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:37,042] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:37,042] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:37,055] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_MISSING_QUERY$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:37 +0000] "POST /admin/datasets/NUMBER_MISSING_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_MISSING_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:37,055] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,056] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:37,056] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:37,056] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:37,059] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:37,059] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-06 20:45:37,059] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-06 20:45:37,060] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:37,061] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:37,061] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:37,166] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,171] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,182] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,182] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:37,182] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:37,288] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:37,316] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:37,316] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7ae4a940-4804-4097-bb5c-1d8b1c5479de] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_QUERY Test))]]
INFO  [2023-01-06 20:45:37,321] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de
INFO  [2023-01-06 20:45:37,321] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de
INFO  [2023-01-06 20:45:37,322] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de] with 0 results within PT0.000707S
127.0.0.1 - - [06/Jan/2023:20:45:37 +0000] "POST /api/datasets/NUMBER_MISSING_QUERY$20Test/queries HTTP/1.1" 201 1390 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:37,322] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de] with 1 results within PT0.001009S
INFO  [2023-01-06 20:45:37,322] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de, workerId=NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_03a85c19-d64c-4152-b56c-ca3197fc3de0, startTime=2023-01-06T20:45:37.321639, finishTime=2023-01-06T20:45:37.322346) of size 0
INFO  [2023-01-06 20:45:37,323] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de, workerId=NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_1b2b3cdc-f7cc-4c78-8277-2b3503d3ba51, startTime=2023-01-06T20:45:37.321524, finishTime=2023-01-06T20:45:37.322533) of size 1
INFO  [2023-01-06 20:45:37,323] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7ae4a940-4804-4097-bb5c-1d8b1c5479de ManagedQuery within PT0.006174S
127.0.0.1 - - [06/Jan/2023:20:45:37 +0000] "GET /api/datasets/NUMBER_MISSING_QUERY$20Test/queries/NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de HTTP/1.1" 200 1688 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:37,346] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_QUERY Test], queryId=7ae4a940-4804-4097-bb5c-1d8b1c5479de, label=vs	@§$, creationTime=2023-01-06T20:45:37.316747, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@264c7521[Count = 0], startTime=2023-01-06T20:45:37.316936, finishTime=2023-01-06T20:45:37.323110, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@e9e667d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@642dba2d, com.bakdata.conquery.models.query.ColumnDescriptor@539ba640]) download on dataset Dataset[label=null, name=NUMBER_MISSING_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:37,346] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_QUERY Test], queryId=7ae4a940-4804-4097-bb5c-1d8b1c5479de, label=vs	@§$, creationTime=2023-01-06T20:45:37.316747, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@264c7521[Count = 0], startTime=2023-01-06T20:45:37.316936, finishTime=2023-01-06T20:45:37.323110, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@e9e667d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@642dba2d, com.bakdata.conquery.models.query.ColumnDescriptor@539ba640]) on dataset Dataset[label=null, name=NUMBER_MISSING_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:37 +0000] "GET /api/datasets/NUMBER_MISSING_QUERY%20Test/result/NUMBER_MISSING_QUERY$20Test.7ae4a940-4804-4097-bb5c-1d8b1c5479de.csv?pretty=false HTTP/1.1" 200 40 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:37,365] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_MISSING_QUERY Test on 2 rows
INFO  [2023-01-06 20:45:37,365] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING_QUERY Test
INFO  [2023-01-06 20:45:37,365] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-06 20:45:37,365] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-06 20:45:37,365] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_QUERY Test_03a85c19-d64c-4152-b56c-ca3197fc3de0
INFO  [2023-01-06 20:45:37,365] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_QUERY Test_1b2b3cdc-f7cc-4c78-8277-2b3503d3ba51
INFO  [2023-01-06 20:45:37,464] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_QUERY Test_1b2b3cdc-f7cc-4c78-8277-2b3503d3ba51
INFO  [2023-01-06 20:45:37,464] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_QUERY Test_03a85c19-d64c-4152-b56c-ca3197fc3de0
INFO  [2023-01-06 20:45:37,465] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING_QUERY Test
INFO  [2023-01-06 20:45:37,565] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING_QUERY$20Test
INFO  [2023-01-06 20:45:37,565] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,588] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING_QUERY Test
INFO  [2023-01-06 20:45:37,589] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:37,589] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:37,589] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:37,590] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:37,590] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:37,590] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:37,590] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:37,591] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_4395f8fa-fdb5-43dc-877b-7ba6866a7e39 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:37,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_4395f8fa-fdb5-43dc-877b-7ba6866a7e39 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:37,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:37,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_fdf67de0-b4ac-4d97-aac2-6b2b59240f97 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:37,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_fdf67de0-b4ac-4d97-aac2-6b2b59240f97 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:37,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:37,695] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,701] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,702] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:37,702] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:37,821] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:37,932] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:37,933] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:37,933] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 129 B in total
INFO  [2023-01-06 20:45:37,933] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00034739sINFO  [2023-01-06 20:45:37,968] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:37,968] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16251, maxValue=16555), dateReader=com.bakdata.conquery.util.DateReader@3100147b), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@772ec1f5), dateReader=com.bakdata.conquery.util.DateReader@16115b75, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-06 20:45:37,968] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=2), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-06 20:45:37,971] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:37,971] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:37,971] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:37,993] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:37,994] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:37 +0000] "POST /admin/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:37,995] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:37,995] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:37,995] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:37,997] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:37,997] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-06 20:45:37,997] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-06 20:45:37,998] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:37,998] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:37,998] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:38,103] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,109] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,124] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,124] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:38,124] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:38,229] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:38,245] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:38,246] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[47208582-9461-4ac1-bf65-e5b60b064285] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test))]]
INFO  [2023-01-06 20:45:38,249] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285
INFO  [2023-01-06 20:45:38,249] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285
INFO  [2023-01-06 20:45:38,250] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285] with 0 results within PT0.00084S
127.0.0.1 - - [06/Jan/2023:20:45:38 +0000] "POST /api/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test/queries HTTP/1.1" 201 1340 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:38,251] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285] with 2 results within PT0.001217S
INFO  [2023-01-06 20:45:38,251] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285, workerId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_fdf67de0-b4ac-4d97-aac2-6b2b59240f97, startTime=2023-01-06T20:45:38.249925, finishTime=2023-01-06T20:45:38.250765) of size 0
INFO  [2023-01-06 20:45:38,251] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285, workerId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_4395f8fa-fdb5-43dc-877b-7ba6866a7e39, startTime=2023-01-06T20:45:38.249928, finishTime=2023-01-06T20:45:38.251145) of size 2
INFO  [2023-01-06 20:45:38,251] com.bakdata.conquery.models.execution.ManagedExecution: DONE 47208582-9461-4ac1-bf65-e5b60b064285 ManagedQuery within PT0.005773S
127.0.0.1 - - [06/Jan/2023:20:45:38 +0000] "GET /api/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test/queries/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285 HTTP/1.1" 200 1727 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:38,277] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test], queryId=47208582-9461-4ac1-bf65-e5b60b064285, label=vs	@§$, creationTime=2023-01-06T20:45:38.245957, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6c99e1cb[Count = 0], startTime=2023-01-06T20:45:38.246158, finishTime=2023-01-06T20:45:38.251931, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5599ae4f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1c8fcdb1, com.bakdata.conquery.models.query.ColumnDescriptor@2c7c9f19]) download on dataset Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:38,277] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test], queryId=47208582-9461-4ac1-bf65-e5b60b064285, label=vs	@§$, creationTime=2023-01-06T20:45:38.245957, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6c99e1cb[Count = 0], startTime=2023-01-06T20:45:38.246158, finishTime=2023-01-06T20:45:38.251931, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5599ae4f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1c8fcdb1, com.bakdata.conquery.models.query.ColumnDescriptor@2c7c9f19]) on dataset Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:38 +0000] "GET /api/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY%20Test/result/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.47208582-9461-4ac1-bf65-e5b60b064285.csv?pretty=false HTTP/1.1" 200 67 "-" "Conquery (test client)" 25
INFO  [2023-01-06 20:45:38,301] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:38,301] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:38,301] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:38,301] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:38,301] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_fdf67de0-b4ac-4d97-aac2-6b2b59240f97
INFO  [2023-01-06 20:45:38,301] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_4395f8fa-fdb5-43dc-877b-7ba6866a7e39
INFO  [2023-01-06 20:45:38,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_4395f8fa-fdb5-43dc-877b-7ba6866a7e39
INFO  [2023-01-06 20:45:38,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:38,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_fdf67de0-b4ac-4d97-aac2-6b2b59240f97
INFO  [2023-01-06 20:45:38,499] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test
INFO  [2023-01-06 20:45:38,499] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,530] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:38,531] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:38,531] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:38,531] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:38,532] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:38,532] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:38,532] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:38,532] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:38,533] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_24afb7fe-baa1-4f6f-8836-be36b06aa026 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:38,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_24afb7fe-baa1-4f6f-8836-be36b06aa026 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:38,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:38,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_272ef14b-9bfc-44a6-8f22-1d99546a3485 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:38,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_272ef14b-9bfc-44a6-8f22-1d99546a3485 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:38,534] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:38,637] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,644] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,645] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:38,645] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:38,762] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:38,872] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:38,873] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:38,873] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 129 B in total
INFO  [2023-01-06 20:45:38,873] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_NO_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00026289sINFO  [2023-01-06 20:45:38,899] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:38,900] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16251, maxValue=16555), dateReader=com.bakdata.conquery.util.DateReader@5de4d53), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@103853e5), dateReader=com.bakdata.conquery.util.DateReader@32452a29, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-06 20:45:38,900] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=2), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-06 20:45:38,903] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_NO_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:38,903] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:38,903] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_MISSING_NO_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:38,925] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:38,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:38 +0000] "POST /admin/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_MISSING_NO_RESTRICTION_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:38,926] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:38,926] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:38,926] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:38,928] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:38,928] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-06 20:45:38,928] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-06 20:45:38,929] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:38,929] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:38,929] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:39,034] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,039] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,052] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,053] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:39,053] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:39,159] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING_NO_RESTRICTION_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:39,174] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:39,175] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a3ba975b-628d-4250-a733-db8773c2fc37] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_RESTRICTION_QUERY Test))]]
INFO  [2023-01-06 20:45:39,179] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37
INFO  [2023-01-06 20:45:39,179] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37
INFO  [2023-01-06 20:45:39,180] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37] with 1 results within PT0.000817S
INFO  [2023-01-06 20:45:39,180] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37] with 3 results within PT0.001115S
127.0.0.1 - - [06/Jan/2023:20:45:39 +0000] "POST /api/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test/queries HTTP/1.1" 201 1278 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:39,180] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37, workerId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_24afb7fe-baa1-4f6f-8836-be36b06aa026, startTime=2023-01-06T20:45:39.179234, finishTime=2023-01-06T20:45:39.180051) of size 1
INFO  [2023-01-06 20:45:39,180] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37, workerId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_272ef14b-9bfc-44a6-8f22-1d99546a3485, startTime=2023-01-06T20:45:39.179142, finishTime=2023-01-06T20:45:39.180257) of size 3
INFO  [2023-01-06 20:45:39,181] com.bakdata.conquery.models.execution.ManagedExecution: DONE a3ba975b-628d-4250-a733-db8773c2fc37 ManagedQuery within PT0.005687S
127.0.0.1 - - [06/Jan/2023:20:45:39 +0000] "GET /api/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test/queries/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37 HTTP/1.1" 200 1637 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:39,209] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test], queryId=a3ba975b-628d-4250-a733-db8773c2fc37, label=vs	@§$, creationTime=2023-01-06T20:45:39.175174, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1918b1e0[Count = 0], startTime=2023-01-06T20:45:39.175372, finishTime=2023-01-06T20:45:39.181059, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1bc47775), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@341f5692, com.bakdata.conquery.models.query.ColumnDescriptor@666e3b2a]) download on dataset Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:39,209] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test], queryId=a3ba975b-628d-4250-a733-db8773c2fc37, label=vs	@§$, creationTime=2023-01-06T20:45:39.175174, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1918b1e0[Count = 0], startTime=2023-01-06T20:45:39.175372, finishTime=2023-01-06T20:45:39.181059, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1bc47775), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@341f5692, com.bakdata.conquery.models.query.ColumnDescriptor@666e3b2a]) on dataset Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:39 +0000] "GET /api/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY%20Test/result/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.a3ba975b-628d-4250-a733-db8773c2fc37.csv?pretty=false HTTP/1.1" 200 121 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:39,229] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_MISSING_NO_RESTRICTION_QUERY Test on 5 rows
INFO  [2023-01-06 20:45:39,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:39,229] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:39,229] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-06 20:45:39,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_24afb7fe-baa1-4f6f-8836-be36b06aa026
INFO  [2023-01-06 20:45:39,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_272ef14b-9bfc-44a6-8f22-1d99546a3485
INFO  [2023-01-06 20:45:39,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:39,233] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_272ef14b-9bfc-44a6-8f22-1d99546a3485
INFO  [2023-01-06 20:45:39,233] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_24afb7fe-baa1-4f6f-8836-be36b06aa026
INFO  [2023-01-06 20:45:39,329] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test
INFO  [2023-01-06 20:45:39,329] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,358] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-06 20:45:39,358] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_NEGATION_QUERY Test
INFO  [2023-01-06 20:45:39,358] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:39,358] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:39,359] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-06 20:45:39,359] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:39,359] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-06 20:45:39,359] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:39,360] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_a3bae8ba-281b-4242-8bf4-e38980ee3c4b are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:39,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_a3bae8ba-281b-4242-8bf4-e38980ee3c4b are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:39,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:39,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_4f03708b-5466-45f4-8030-683bbc8b4dce are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:39,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_4f03708b-5466-45f4-8030-683bbc8b4dce are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:39,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:39,465] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,472] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,472] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_NEGATION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:39,472] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_NEGATION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:39,589] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,698] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:39,699] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:39,699] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 372 B in total
INFO  [2023-01-06 20:45:39,699] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_NEGATION_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00042042sINFO  [2023-01-06 20:45:39,741] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=12, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:45:39,741] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16106, maxValue=16469), dateReader=com.bakdata.conquery.util.DateReader@4aad17ce), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16121, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@5c40401b), dateReader=com.bakdata.conquery.util.DateReader@21008c9a, onlyQuarters=false, maxValue=16616, minValue=16106, anyOpen=false)
INFO  [2023-01-06 20:45:39,741] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=12, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:39,744] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_NEGATION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:39,744] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:39,744] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_NEGATION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:39,763] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_NEGATION_QUERY$20Test.table1
INFO  [2023-01-06 20:45:39,763] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:39 +0000] "POST /admin/datasets/NUMBER_NEGATION_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_NEGATION_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:39,764] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:39,764] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:39,765] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:39,767] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:39,767] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_NEGATION_QUERY$20Test.table1.table1], containing 12 entries.
INFO  [2023-01-06 20:45:39,767] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_NEGATION_QUERY$20Test.table1.table1], containing 12 entries.
WARN  [2023-01-06 20:45:39,769] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:39,769] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_NEGATION_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:39,769] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_NEGATION_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:39,769] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_NEGATION_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:45:39,874] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,879] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,895] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:39,896] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:39,896] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:40,001] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_NEGATION_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:40,012] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_NEGATION_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:40,013] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a3558e8e-ac69-4444-aeeb-0d6dd974ec3b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_NEGATION_QUERY Test))]]
INFO  [2023-01-06 20:45:40,019] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b
INFO  [2023-01-06 20:45:40,019] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b
127.0.0.1 - - [06/Jan/2023:20:45:40 +0000] "POST /api/datasets/NUMBER_NEGATION_QUERY$20Test/queries HTTP/1.1" 201 1435 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:40,020] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b] with 1 results within PT0.001207S
INFO  [2023-01-06 20:45:40,020] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b] with 3 results within PT0.001629S
INFO  [2023-01-06 20:45:40,020] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b, workerId=NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_a3bae8ba-281b-4242-8bf4-e38980ee3c4b, startTime=2023-01-06T20:45:40.019107, finishTime=2023-01-06T20:45:40.020314) of size 1
INFO  [2023-01-06 20:45:40,021] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b, workerId=NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_4f03708b-5466-45f4-8030-683bbc8b4dce, startTime=2023-01-06T20:45:40.019102, finishTime=2023-01-06T20:45:40.020731) of size 3
INFO  [2023-01-06 20:45:40,021] com.bakdata.conquery.models.execution.ManagedExecution: DONE a3558e8e-ac69-4444-aeeb-0d6dd974ec3b ManagedQuery within PT0.007809S
127.0.0.1 - - [06/Jan/2023:20:45:40 +0000] "GET /api/datasets/NUMBER_NEGATION_QUERY$20Test/queries/NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b HTTP/1.1" 200 1737 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:40,047] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_NEGATION_QUERY Test], queryId=a3558e8e-ac69-4444-aeeb-0d6dd974ec3b, label=vs	@§$, creationTime=2023-01-06T20:45:40.013286, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5f91fbcd[Count = 0], startTime=2023-01-06T20:45:40.013521, finishTime=2023-01-06T20:45:40.021330, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@658034c0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_NEGATION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@373e3d15, com.bakdata.conquery.models.query.ColumnDescriptor@162a16a]) download on dataset Dataset[label=null, name=NUMBER_NEGATION_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:40,047] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_NEGATION_QUERY Test], queryId=a3558e8e-ac69-4444-aeeb-0d6dd974ec3b, label=vs	@§$, creationTime=2023-01-06T20:45:40.013286, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5f91fbcd[Count = 0], startTime=2023-01-06T20:45:40.013521, finishTime=2023-01-06T20:45:40.021330, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@658034c0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_NEGATION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@373e3d15, com.bakdata.conquery.models.query.ColumnDescriptor@162a16a]) on dataset Dataset[label=null, name=NUMBER_NEGATION_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:40 +0000] "GET /api/datasets/NUMBER_NEGATION_QUERY%20Test/result/NUMBER_NEGATION_QUERY$20Test.a3558e8e-ac69-4444-aeeb-0d6dd974ec3b.csv?pretty=false HTTP/1.1" 200 37 "-" "Conquery (test client)" 26
INFO  [2023-01-06 20:45:40,071] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_NEGATION_QUERY Test on 5 rows
INFO  [2023-01-06 20:45:40,071] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_NEGATION_QUERY Test
INFO  [2023-01-06 20:45:40,071] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-06 20:45:40,071] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-06 20:45:40,072] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_NEGATION_QUERY Test_a3bae8ba-281b-4242-8bf4-e38980ee3c4b
INFO  [2023-01-06 20:45:40,072] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_NEGATION_QUERY Test_4f03708b-5466-45f4-8030-683bbc8b4dce
INFO  [2023-01-06 20:45:40,170] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_NEGATION_QUERY Test_a3bae8ba-281b-4242-8bf4-e38980ee3c4b
INFO  [2023-01-06 20:45:40,170] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_NEGATION_QUERY Test_4f03708b-5466-45f4-8030-683bbc8b4dce
INFO  [2023-01-06 20:45:40,170] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_NEGATION_QUERY Test
INFO  [2023-01-06 20:45:40,270] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_NEGATION_QUERY$20Test
INFO  [2023-01-06 20:45:40,270] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,302] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_NEGATION_QUERY Test
INFO  [2023-01-06 20:45:40,302] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_QUERY Test
INFO  [2023-01-06 20:45:40,302] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:40,302] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:40,316] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-06 20:45:40,316] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:40,320] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-06 20:45:40,320] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:40,325] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_d0f67ff4-1702-4e01-9017-bbb055193eef are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:40,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_d0f67ff4-1702-4e01-9017-bbb055193eef are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:40,328] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:40,329] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_caabb7e9-b347-46c7-a08a-b1f01714740d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:40,329] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_caabb7e9-b347-46c7-a08a-b1f01714740d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:40,329] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:40,429] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,445] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,448] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test[1].table1
INFO  [2023-01-06 20:45:40,448] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test[1].table1
INFO  [2023-01-06 20:45:40,586] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,712] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:40,718] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:40,718] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 372 B in total
INFO  [2023-01-06 20:45:40,718] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_QUERY Test[1]/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00039786sINFO  [2023-01-06 20:45:40,759] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=12, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:45:40,759] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16106, maxValue=16469), dateReader=com.bakdata.conquery.util.DateReader@5b3f384b), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16121, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@51742ea5), dateReader=com.bakdata.conquery.util.DateReader@629ee30a, onlyQuarters=false, maxValue=16616, minValue=16106, anyOpen=false)
INFO  [2023-01-06 20:45:40,759] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=12, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:40,761] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_QUERY Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:40,761] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:40,761] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_NUMBER_QUERY Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:40,782] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_QUERY$20Test[1].table1
INFO  [2023-01-06 20:45:40,782] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:40 +0000] "POST /admin/datasets/NUMBER_QUERY%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_NUMBER_QUERY+Test%5B1%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:40,783] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:40,783] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:40,783] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:40,786] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-06 20:45:40,787] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:40,789] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test[1].table1.table1], containing 12 entries.
INFO  [2023-01-06 20:45:40,790] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test[1].table1.table1.0
INFO  [2023-01-06 20:45:40,796] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test[1].table1.table1], containing 12 entries.
INFO  [2023-01-06 20:45:40,901] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,906] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,932] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:40,933] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:41,041] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:41,072] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_QUERY$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:41,072] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7f916989-33fc-4a05-b441-76ea340a9497] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test[1]))]]
127.0.0.1 - - [06/Jan/2023:20:45:41 +0000] "POST /api/datasets/NUMBER_QUERY$20Test%5B1%5D/queries HTTP/1.1" 201 1370 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:41,088] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test[1].7f916989-33fc-4a05-b441-76ea340a9497
INFO  [2023-01-06 20:45:41,090] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test[1].7f916989-33fc-4a05-b441-76ea340a9497] with 4 results within PT0.001204S
INFO  [2023-01-06 20:45:41,093] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test[1].7f916989-33fc-4a05-b441-76ea340a9497
WARN  [2023-01-06 20:45:41,093] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:41,093] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test[1].7f916989-33fc-4a05-b441-76ea340a9497] with 0 results within PT0.000179S
INFO  [2023-01-06 20:45:41,094] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test[1].7f916989-33fc-4a05-b441-76ea340a9497, workerId=NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_caabb7e9-b347-46c7-a08a-b1f01714740d, startTime=2023-01-06T20:45:41.088894, finishTime=2023-01-06T20:45:41.090098) of size 4
INFO  [2023-01-06 20:45:41,094] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test[1].7f916989-33fc-4a05-b441-76ea340a9497, workerId=NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_d0f67ff4-1702-4e01-9017-bbb055193eef, startTime=2023-01-06T20:45:41.093303, finishTime=2023-01-06T20:45:41.093482) of size 0
INFO  [2023-01-06 20:45:41,094] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7f916989-33fc-4a05-b441-76ea340a9497 ManagedQuery within PT0.022305S
127.0.0.1 - - [06/Jan/2023:20:45:41 +0000] "GET /api/datasets/NUMBER_QUERY$20Test%5B1%5D/queries/NUMBER_QUERY$20Test%5B1%5D.7f916989-33fc-4a05-b441-76ea340a9497 HTTP/1.1" 200 1889 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:41,116] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test[1]], queryId=7f916989-33fc-4a05-b441-76ea340a9497, label=vs	@§$, creationTime=2023-01-06T20:45:41.072419, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@58780f76[Count = 0], startTime=2023-01-06T20:45:41.072645, finishTime=2023-01-06T20:45:41.094950, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7b98f1c8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60df6bb1], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@189d0f4], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@433084bd]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5be7e652, com.bakdata.conquery.models.query.ColumnDescriptor@437c41e7]) download on dataset Dataset[label=null, name=NUMBER_QUERY Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:41,116] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test[1]], queryId=7f916989-33fc-4a05-b441-76ea340a9497, label=vs	@§$, creationTime=2023-01-06T20:45:41.072419, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@58780f76[Count = 0], startTime=2023-01-06T20:45:41.072645, finishTime=2023-01-06T20:45:41.094950, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7b98f1c8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60df6bb1], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@189d0f4], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@433084bd]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5be7e652, com.bakdata.conquery.models.query.ColumnDescriptor@437c41e7]) on dataset Dataset[label=null, name=NUMBER_QUERY Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:41 +0000] "GET /api/datasets/NUMBER_QUERY%20Test%5B1%5D/result/NUMBER_QUERY$20Test%5B1%5D.7f916989-33fc-4a05-b441-76ea340a9497.csv?pretty=false HTTP/1.1" 200 143 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:41,131] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_QUERY Test on 5 rows
INFO  [2023-01-06 20:45:41,133] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_QUERY Test[1]
INFO  [2023-01-06 20:45:41,134] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-06 20:45:41,134] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-06 20:45:41,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test[1]_d0f67ff4-1702-4e01-9017-bbb055193eef
INFO  [2023-01-06 20:45:41,135] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test[1]_caabb7e9-b347-46c7-a08a-b1f01714740d
INFO  [2023-01-06 20:45:41,205] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_QUERY Test[1]
INFO  [2023-01-06 20:45:41,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test[1]_d0f67ff4-1702-4e01-9017-bbb055193eef
INFO  [2023-01-06 20:45:41,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test[1]_caabb7e9-b347-46c7-a08a-b1f01714740d
INFO  [2023-01-06 20:45:41,292] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_QUERY$20Test[1]
INFO  [2023-01-06 20:45:41,292] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,340] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_QUERY Test
INFO  [2023-01-06 20:45:41,341] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:41,341] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:41,341] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:41,342] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:41,342] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:41,342] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:41,342] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:41,343] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,343] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_b7dc9622-7a44-4226-9a3d-c596b13c0832 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:41,344] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_b7dc9622-7a44-4226-9a3d-c596b13c0832 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:41,344] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:41,344] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_32e40569-c55a-4b84-95d2-7d8fadc05a83 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:41,344] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_32e40569-c55a-4b84-95d2-7d8fadc05a83 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:41,344] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:41,448] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,455] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,455] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:41,455] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:41,573] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,683] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:41,683] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:41,683] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 178 B in total
INFO  [2023-01-06 20:45:41,684] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TEMPORAL_BEFORE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000385899sINFO  [2023-01-06 20:45:41,723] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=9, min=2, average=2.250000, max=3}
INFO  [2023-01-06 20:45:41,723] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:41,723] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=14975, maxValue=15006), dateReader=com.bakdata.conquery.util.DateReader@1ac75870)
INFO  [2023-01-06 20:45:41,725] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TEMPORAL_BEFORE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:41,725] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:41,725] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TEMPORAL_BEFORE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:41,739] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:41,739] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:41 +0000] "POST /admin/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_TEMPORAL_BEFORE_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:41,740] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:41,741] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:41,741] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:41,744] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:41,744] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1], containing 9 entries.
INFO  [2023-01-06 20:45:41,744] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1], containing 9 entries.
WARN  [2023-01-06 20:45:41,745] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:41,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:41,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:41,851] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,856] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,874] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:41,874] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:41,875] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:41,981] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: TEMPORAL_BEFORE_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:42,000] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:42,001] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e9acfaff-b746-45f8-819c-c6f9bb9e3b50] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TEMPORAL_BEFORE_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:42,005] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50
INFO  [2023-01-06 20:45:42,005] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "POST /api/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1932 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:42,007] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50] with 1 results within PT0.002247S
INFO  [2023-01-06 20:45:42,007] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50] with 1 results within PT0.002521S
INFO  [2023-01-06 20:45:42,008] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50, workerId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_32e40569-c55a-4b84-95d2-7d8fadc05a83, startTime=2023-01-06T20:45:42.005362, finishTime=2023-01-06T20:45:42.007883) of size 1
INFO  [2023-01-06 20:45:42,008] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50, workerId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_b7dc9622-7a44-4226-9a3d-c596b13c0832, startTime=2023-01-06T20:45:42.005631, finishTime=2023-01-06T20:45:42.007878) of size 1
INFO  [2023-01-06 20:45:42,008] com.bakdata.conquery.models.execution.ManagedExecution: DONE e9acfaff-b746-45f8-819c-c6f9bb9e3b50 ManagedQuery within PT0.007685S
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "GET /api/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test/queries/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50 HTTP/1.1" 200 2267 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:42,034] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test], queryId=e9acfaff-b746-45f8-819c-c6f9bb9e3b50, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:42.000909, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@86720d0[Count = 0], startTime=2023-01-06T20:45:42.001152, finishTime=2023-01-06T20:45:42.008837, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7e306485), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TEMPORAL_BEFORE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@442e8761, com.bakdata.conquery.models.query.ColumnDescriptor@37f7fd43]) download on dataset Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:42,034] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test], queryId=e9acfaff-b746-45f8-819c-c6f9bb9e3b50, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:42.000909, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@86720d0[Count = 0], startTime=2023-01-06T20:45:42.001152, finishTime=2023-01-06T20:45:42.008837, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7e306485), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TEMPORAL_BEFORE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@442e8761, com.bakdata.conquery.models.query.ColumnDescriptor@37f7fd43]) on dataset Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "GET /api/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY%20Test/result/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.e9acfaff-b746-45f8-819c-c6f9bb9e3b50.csv?pretty=false HTTP/1.1" 200 23 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:42,051] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest TEMPORAL_BEFORE_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:42,052] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:42,052] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:42,052] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:42,052] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_32e40569-c55a-4b84-95d2-7d8fadc05a83
INFO  [2023-01-06 20:45:42,052] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_b7dc9622-7a44-4226-9a3d-c596b13c0832
INFO  [2023-01-06 20:45:42,150] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_b7dc9622-7a44-4226-9a3d-c596b13c0832
INFO  [2023-01-06 20:45:42,150] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:42,150] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_32e40569-c55a-4b84-95d2-7d8fadc05a83
INFO  [2023-01-06 20:45:42,250] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of TEMPORAL_BEFORE_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:42,250] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,281] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:42,281] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-06 20:45:42,281] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:42,281] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:42,282] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-06 20:45:42,282] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-06 20:45:42,282] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:42,283] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:42,285] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_9f7034a5-d06b-44de-a9c2-f6f1e861a5a1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:42,285] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_9f7034a5-d06b-44de-a9c2-f6f1e861a5a1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:42,285] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:42,285] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_d53e7cc7-0ff6-44f6-828d-73b601a216a3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:42,285] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_d53e7cc7-0ff6-44f6-828d-73b601a216a3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:42,285] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:42,289] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,388] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,395] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,395] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITHOUT$20DATES$20Test.table
INFO  [2023-01-06 20:45:42,395] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITHOUT$20DATES$20Test.table
INFO  [2023-01-06 20:45:42,512] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,621] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:42,621] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:42,621] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 178 B in total
INFO  [2023-01-06 20:45:42,621] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL_EXPORT WITHOUT DATES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00033511sINFO  [2023-01-06 20:45:42,655] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=9, min=1, average=4.500000, max=8}
INFO  [2023-01-06 20:45:42,655] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:42,655] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=11109, maxValue=12935), dateReader=com.bakdata.conquery.util.DateReader@5a016b8b)
INFO  [2023-01-06 20:45:42,657] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL_EXPORT WITHOUT DATES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:42,657] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:42,657] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL_EXPORT WITHOUT DATES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:42,672] com.bakdata.conquery.models.jobs.ImportJob: Importing table into REL_EXPORT$20WITHOUT$20DATES$20Test.table
INFO  [2023-01-06 20:45:42,672] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "POST /admin/datasets/REL_EXPORT%20WITHOUT%20DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL_EXPORT+WITHOUT+DATES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:42,673] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:42,673] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:42,673] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:42,674] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:42,674] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITHOUT$20DATES$20Test.table.table], containing 9 entries.
INFO  [2023-01-06 20:45:42,674] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITHOUT$20DATES$20Test.table.table], containing 9 entries.
WARN  [2023-01-06 20:45:42,675] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:42,675] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL_EXPORT$20WITHOUT$20DATES$20Test.table.table.0
INFO  [2023-01-06 20:45:42,780] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,785] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,804] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:42,804] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-06 20:45:42,911] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: REL_EXPORT WITHOUT DATES Test QUERY INIT
INFO  [2023-01-06 20:45:42,927] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[REL_EXPORT$20WITHOUT$20DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:42,928] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITHOUT DATES Test))]]
INFO  [2023-01-06 20:45:42,933] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a
INFO  [2023-01-06 20:45:42,934] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a
WARN  [2023-01-06 20:45:42,934] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:42,934] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a] with 0 results within PT0.000223S
WARN  [2023-01-06 20:45:42,934] com.bakdata.conquery.models.forms.managed.RelativeFormQueryPlan: Sampled empty result for Entity[0]: `EARLIEST({-∞/+∞})`
INFO  [2023-01-06 20:45:42,935] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a, workerId=REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_d53e7cc7-0ff6-44f6-828d-73b601a216a3, startTime=2023-01-06T20:45:42.934323, finishTime=2023-01-06T20:45:42.934546) of size 0
INFO  [2023-01-06 20:45:42,935] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a] with 1 results within PT0.001495S
INFO  [2023-01-06 20:45:42,936] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a, workerId=REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_9f7034a5-d06b-44de-a9c2-f6f1e861a5a1, startTime=2023-01-06T20:45:42.933832, finishTime=2023-01-06T20:45:42.935327) of size 1
INFO  [2023-01-06 20:45:42,936] com.bakdata.conquery.models.execution.ManagedExecution: DONE d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a ManagedQuery within PT0.007526S
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "POST /api/datasets/REL_EXPORT$20WITHOUT$20DATES$20Test/queries HTTP/1.1" 201 3326 "-" "Conquery (test client)" 12
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "GET /api/datasets/REL_EXPORT$20WITHOUT$20DATES$20Test/queries/REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a HTTP/1.1" 200 3657 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:42,965] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test], queryId=d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a, label=concept_dateless-child1 concept-child1	@§$, creationTime=2023-01-06T20:45:42.928220, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4b49e1ef[Count = 0], startTime=2023-01-06T20:45:42.928595, finishTime=2023-01-06T20:45:42.936121, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3aa9775b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITHOUT DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7a3f5de1, com.bakdata.conquery.models.query.ColumnDescriptor@52038575, com.bakdata.conquery.models.query.ColumnDescriptor@56414043, com.bakdata.conquery.models.query.ColumnDescriptor@2723e67f, com.bakdata.conquery.models.query.ColumnDescriptor@27f3772f, com.bakdata.conquery.models.query.ColumnDescriptor@69156689, com.bakdata.conquery.models.query.ColumnDescriptor@358a70b0, com.bakdata.conquery.models.query.ColumnDescriptor@d226860]) download on dataset Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:42,965] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test], queryId=d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a, label=concept_dateless-child1 concept-child1	@§$, creationTime=2023-01-06T20:45:42.928220, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4b49e1ef[Count = 0], startTime=2023-01-06T20:45:42.928595, finishTime=2023-01-06T20:45:42.936121, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3aa9775b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITHOUT DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7a3f5de1, com.bakdata.conquery.models.query.ColumnDescriptor@52038575, com.bakdata.conquery.models.query.ColumnDescriptor@56414043, com.bakdata.conquery.models.query.ColumnDescriptor@2723e67f, com.bakdata.conquery.models.query.ColumnDescriptor@27f3772f, com.bakdata.conquery.models.query.ColumnDescriptor@69156689, com.bakdata.conquery.models.query.ColumnDescriptor@358a70b0, com.bakdata.conquery.models.query.ColumnDescriptor@d226860]) on dataset Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test]
127.0.0.1 - - [06/Jan/2023:20:45:42 +0000] "GET /api/datasets/REL_EXPORT%20WITHOUT%20DATES%20Test/result/REL_EXPORT$20WITHOUT$20DATES$20Test.d1eeceb7-ed44-47c1-a5ce-bf5ccdda740a.csv?pretty=false HTTP/1.1" 200 130 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:42,984] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest REL_EXPORT WITHOUT DATES Test on 2 rows
INFO  [2023-01-06 20:45:42,984] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-06 20:45:42,984] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-06 20:45:42,984] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-06 20:45:42,984] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITHOUT DATES Test_d53e7cc7-0ff6-44f6-828d-73b601a216a3
INFO  [2023-01-06 20:45:42,984] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITHOUT DATES Test_9f7034a5-d06b-44de-a9c2-f6f1e861a5a1
INFO  [2023-01-06 20:45:43,083] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-06 20:45:43,084] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITHOUT DATES Test_d53e7cc7-0ff6-44f6-828d-73b601a216a3
INFO  [2023-01-06 20:45:43,084] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITHOUT DATES Test_9f7034a5-d06b-44de-a9c2-f6f1e861a5a1
INFO  [2023-01-06 20:45:43,181] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL_EXPORT$20WITHOUT$20DATES$20Test
INFO  [2023-01-06 20:45:43,181] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,209] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-06 20:45:43,210] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL_EXPORT WITH DATES Test
INFO  [2023-01-06 20:45:43,210] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:43,210] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:43,211] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-06 20:45:43,211] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-06 20:45:43,211] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:43,211] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:43,212] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,213] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_79542819-8350-4919-af08-98903c835294 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:43,213] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_79542819-8350-4919-af08-98903c835294 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:43,213] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:43,213] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_2ce9f5e0-c0fe-41cb-b1e2-c0b806dcf83a are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:43,213] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_2ce9f5e0-c0fe-41cb-b1e2-c0b806dcf83a are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:43,213] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:43,316] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,323] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,323] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITH$20DATES$20Test.test_table
INFO  [2023-01-06 20:45:43,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITH$20DATES$20Test.test_table
INFO  [2023-01-06 20:45:43,439] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,549] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:43,549] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:43,549] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 183 B in total
INFO  [2023-01-06 20:45:43,549] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL_EXPORT WITH DATES Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000320418sINFO  [2023-01-06 20:45:43,582] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=9, min=1, average=4.500000, max=8}
INFO  [2023-01-06 20:45:43,582] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:43,582] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=11109, maxValue=12935), dateReader=com.bakdata.conquery.util.DateReader@1594cc70)
INFO  [2023-01-06 20:45:43,584] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL_EXPORT WITH DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:43,584] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:43,584] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REL_EXPORT WITH DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:43,605] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into REL_EXPORT$20WITH$20DATES$20Test.test_table
INFO  [2023-01-06 20:45:43,606] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:43 +0000] "POST /admin/datasets/REL_EXPORT%20WITH%20DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REL_EXPORT+WITH+DATES+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:43,606] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:43,607] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:43,607] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:43,608] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:43,608] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITH$20DATES$20Test.test_table.test_table], containing 9 entries.
INFO  [2023-01-06 20:45:43,609] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITH$20DATES$20Test.test_table.test_table], containing 9 entries.
WARN  [2023-01-06 20:45:43,609] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:43,609] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL_EXPORT$20WITH$20DATES$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:43,714] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,719] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,730] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:43,731] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:43,846] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: REL_EXPORT WITH DATES Test QUERY INIT
INFO  [2023-01-06 20:45:43,860] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[REL_EXPORT$20WITH$20DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:43,861] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7491ff67-f7bd-413a-bb72-86e387e58e96] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITH DATES Test))]]
INFO  [2023-01-06 20:45:43,865] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96
INFO  [2023-01-06 20:45:43,866] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96
WARN  [2023-01-06 20:45:43,866] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:43,866] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96] with 0 results within PT0.000199S
INFO  [2023-01-06 20:45:43,867] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96, workerId=REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_2ce9f5e0-c0fe-41cb-b1e2-c0b806dcf83a, startTime=2023-01-06T20:45:43.866212, finishTime=2023-01-06T20:45:43.866411) of size 0
INFO  [2023-01-06 20:45:43,867] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96] with 1 results within PT0.001901S
INFO  [2023-01-06 20:45:43,868] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96, workerId=REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_79542819-8350-4919-af08-98903c835294, startTime=2023-01-06T20:45:43.866025, finishTime=2023-01-06T20:45:43.867926) of size 1
INFO  [2023-01-06 20:45:43,868] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7491ff67-f7bd-413a-bb72-86e387e58e96 ManagedQuery within PT0.007009S
127.0.0.1 - - [06/Jan/2023:20:45:43 +0000] "POST /api/datasets/REL_EXPORT$20WITH$20DATES$20Test/queries HTTP/1.1" 201 3331 "-" "Conquery (test client)" 10
127.0.0.1 - - [06/Jan/2023:20:45:43 +0000] "GET /api/datasets/REL_EXPORT$20WITH$20DATES$20Test/queries/REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96 HTTP/1.1" 200 3650 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:43,893] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITH DATES Test], queryId=7491ff67-f7bd-413a-bb72-86e387e58e96, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:43.861187, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4bd0f9e9[Count = 0], startTime=2023-01-06T20:45:43.861516, finishTime=2023-01-06T20:45:43.868525, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@51d0a0ec), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITH DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1ec679e9, com.bakdata.conquery.models.query.ColumnDescriptor@25ccb8bd, com.bakdata.conquery.models.query.ColumnDescriptor@691f025c, com.bakdata.conquery.models.query.ColumnDescriptor@3d9c1853, com.bakdata.conquery.models.query.ColumnDescriptor@10b82738, com.bakdata.conquery.models.query.ColumnDescriptor@50717cc2, com.bakdata.conquery.models.query.ColumnDescriptor@41ed8972, com.bakdata.conquery.models.query.ColumnDescriptor@1c6e5f9c]) download on dataset Dataset[label=null, name=REL_EXPORT WITH DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:43,894] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITH DATES Test], queryId=7491ff67-f7bd-413a-bb72-86e387e58e96, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:43.861187, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4bd0f9e9[Count = 0], startTime=2023-01-06T20:45:43.861516, finishTime=2023-01-06T20:45:43.868525, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@51d0a0ec), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITH DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1ec679e9, com.bakdata.conquery.models.query.ColumnDescriptor@25ccb8bd, com.bakdata.conquery.models.query.ColumnDescriptor@691f025c, com.bakdata.conquery.models.query.ColumnDescriptor@3d9c1853, com.bakdata.conquery.models.query.ColumnDescriptor@10b82738, com.bakdata.conquery.models.query.ColumnDescriptor@50717cc2, com.bakdata.conquery.models.query.ColumnDescriptor@41ed8972, com.bakdata.conquery.models.query.ColumnDescriptor@1c6e5f9c]) on dataset Dataset[label=null, name=REL_EXPORT WITH DATES Test]
127.0.0.1 - - [06/Jan/2023:20:45:43 +0000] "GET /api/datasets/REL_EXPORT%20WITH%20DATES%20Test/result/REL_EXPORT$20WITH$20DATES$20Test.7491ff67-f7bd-413a-bb72-86e387e58e96.csv?pretty=false HTTP/1.1" 200 599 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:43,913] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest REL_EXPORT WITH DATES Test on 10 rows
INFO  [2023-01-06 20:45:43,913] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL_EXPORT WITH DATES Test
INFO  [2023-01-06 20:45:43,913] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-06 20:45:43,913] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-06 20:45:43,913] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITH DATES Test_2ce9f5e0-c0fe-41cb-b1e2-c0b806dcf83a
INFO  [2023-01-06 20:45:43,913] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITH DATES Test_79542819-8350-4919-af08-98903c835294
INFO  [2023-01-06 20:45:44,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL_EXPORT WITH DATES Test
INFO  [2023-01-06 20:45:44,012] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITH DATES Test_79542819-8350-4919-af08-98903c835294
INFO  [2023-01-06 20:45:44,012] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITH DATES Test_2ce9f5e0-c0fe-41cb-b1e2-c0b806dcf83a
INFO  [2023-01-06 20:45:44,110] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL_EXPORT$20WITH$20DATES$20Test
INFO  [2023-01-06 20:45:44,110] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,146] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL_EXPORT WITH DATES Test
INFO  [2023-01-06 20:45:44,147] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REUSED_QUERY Test
INFO  [2023-01-06 20:45:44,147] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:44,147] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:44,154] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-06 20:45:44,154] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-06 20:45:44,154] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:44,154] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_c79b7a2c-59a7-4e93-bd9b-89e251df99e2 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_c79b7a2c-59a7-4e93-bd9b-89e251df99e2 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_77e9cdbe-e34d-4940-a847-138487fd8efd are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_77e9cdbe-e34d-4940-a847-138487fd8efd are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:44,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:44,259] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,266] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REUSED_QUERY$20Test.test_table
INFO  [2023-01-06 20:45:44,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REUSED_QUERY$20Test.test_table
INFO  [2023-01-06 20:45:44,381] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,490] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:44,491] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:44,491] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 374 B in total
INFO  [2023-01-06 20:45:44,491] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000339621sINFO  [2023-01-06 20:45:44,525] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:44,525] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=21, nullLines=0), encoding=null, prefix=a, suffix=a)
INFO  [2023-01-06 20:45:44,525] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=11323, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@20702998)
INFO  [2023-01-06 20:45:44,527] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:44,527] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:44,527] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:44,545] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into REUSED_QUERY$20Test.test_table
INFO  [2023-01-06 20:45:44,545] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:44 +0000] "POST /admin/datasets/REUSED_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_REUSED_QUERY+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:44,546] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:44,546] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:44,546] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:44,549] com.bakdata.conquery.models.jobs.ImportJob: Start sending 7 Buckets
INFO  [2023-01-06 20:45:44,549] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REUSED_QUERY$20Test.test_table.test_table], containing 21 entries.
INFO  [2023-01-06 20:45:44,549] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REUSED_QUERY$20Test.test_table.test_table], containing 21 entries.
INFO  [2023-01-06 20:45:44,551] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:44,551] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:44,551] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.2
INFO  [2023-01-06 20:45:44,551] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.3
WARN  [2023-01-06 20:45:44,551] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:44,592] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.4
INFO  [2023-01-06 20:45:44,592] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.5
INFO  [2023-01-06 20:45:44,592] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.6
INFO  [2023-01-06 20:45:44,697] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,752] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,758] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,766] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,766] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:44,766] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:44,872] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: REUSED_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:44,891] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[REUSED_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:44,893] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[00000000-0000-0000-0000-000000000001] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_REUSED_QUERY Test))]]
INFO  [2023-01-06 20:45:44,896] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001
INFO  [2023-01-06 20:45:44,896] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001
127.0.0.1 - - [06/Jan/2023:20:45:44 +0000] "POST /api/datasets/REUSED_QUERY$20Test/queries HTTP/1.1" 201 1313 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:44,898] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001] with 5 results within PT0.002485S
INFO  [2023-01-06 20:45:44,899] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001] with 6 results within PT0.002924S
INFO  [2023-01-06 20:45:44,899] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001, workerId=REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_c79b7a2c-59a7-4e93-bd9b-89e251df99e2, startTime=2023-01-06T20:45:44.896358, finishTime=2023-01-06T20:45:44.898843) of size 5
INFO  [2023-01-06 20:45:44,899] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001, workerId=REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_77e9cdbe-e34d-4940-a847-138487fd8efd, startTime=2023-01-06T20:45:44.896372, finishTime=2023-01-06T20:45:44.899296) of size 6
INFO  [2023-01-06 20:45:44,899] com.bakdata.conquery.models.execution.ManagedExecution: DONE 00000000-0000-0000-0000-000000000001 ManagedQuery within PT0.006772S
127.0.0.1 - - [06/Jan/2023:20:45:44 +0000] "GET /api/datasets/REUSED_QUERY$20Test/queries/REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001 HTTP/1.1" 200 1581 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:44,926] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REUSED_QUERY Test], queryId=00000000-0000-0000-0000-000000000001, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:44.735701, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6c39357c[Count = 0], startTime=2023-01-06T20:45:44.893200, finishTime=2023-01-06T20:45:44.899972, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@236efcd8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=11, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@101c7423, com.bakdata.conquery.models.query.ColumnDescriptor@3272007]) download on dataset Dataset[label=null, name=REUSED_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:44,926] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REUSED_QUERY Test], queryId=00000000-0000-0000-0000-000000000001, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:44.735701, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6c39357c[Count = 0], startTime=2023-01-06T20:45:44.893200, finishTime=2023-01-06T20:45:44.899972, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@236efcd8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=11, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@101c7423, com.bakdata.conquery.models.query.ColumnDescriptor@3272007]) on dataset Dataset[label=null, name=REUSED_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:44 +0000] "GET /api/datasets/REUSED_QUERY%20Test/result/REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001.csv?pretty=false HTTP/1.1" 200 331 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:44,945] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest REUSED_QUERY Test on 12 rows
INFO  [2023-01-06 20:45:44,946] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REUSED_QUERY Test
INFO  [2023-01-06 20:45:44,946] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-06 20:45:44,946] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-06 20:45:44,946] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REUSED_QUERY Test_c79b7a2c-59a7-4e93-bd9b-89e251df99e2
INFO  [2023-01-06 20:45:44,947] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REUSED_QUERY Test_77e9cdbe-e34d-4940-a847-138487fd8efd
INFO  [2023-01-06 20:45:44,948] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REUSED_QUERY Test
INFO  [2023-01-06 20:45:44,952] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REUSED_QUERY$20Test
INFO  [2023-01-06 20:45:44,952] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:44,965] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REUSED_QUERY Test_77e9cdbe-e34d-4940-a847-138487fd8efd
INFO  [2023-01-06 20:45:44,965] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REUSED_QUERY Test_c79b7a2c-59a7-4e93-bd9b-89e251df99e2
INFO  [2023-01-06 20:45:45,072] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REUSED_QUERY Test
INFO  [2023-01-06 20:45:45,072] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID Test
INFO  [2023-01-06 20:45:45,072] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:45,072] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:45,073] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-06 20:45:45,073] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-06 20:45:45,073] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:45,073] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:45,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_1fbe7244-62b4-458b-9aa2-ca737e2d10d6 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:45,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_1fbe7244-62b4-458b-9aa2-ca737e2d10d6 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:45,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:45,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_3f96326c-b468-4547-86be-be133c839756 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:45,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_3f96326c-b468-4547-86be-be133c839756 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:45,075] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:45,079] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,184] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test.secondary]
INFO  [2023-01-06 20:45:45,185] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test.ignored]
INFO  [2023-01-06 20:45:45,185] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.secondary
INFO  [2023-01-06 20:45:45,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.secondary
INFO  [2023-01-06 20:45:45,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.ignored
INFO  [2023-01-06 20:45:45,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.ignored
INFO  [2023-01-06 20:45:45,292] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,292] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table1
INFO  [2023-01-06 20:45:45,292] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table1
INFO  [2023-01-06 20:45:45,292] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table12
INFO  [2023-01-06 20:45:45,293] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table12
INFO  [2023-01-06 20:45:45,412] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,523] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:45,524] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:45,524] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:45,524] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 433 B in total
INFO  [2023-01-06 20:45:45,524] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test/table1.import.json, tag=Optional.empty)
████████████████████████▌                         ▌  49%	est. time remaining: 0.028797658sINFO  [2023-01-06 20:45:45,552] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-06 20:45:45,552] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:45,552] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:45:45,552] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@2f014bfc), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@71140ace), dateReader=com.bakdata.conquery.util.DateReader@6f92b4fb, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-06 20:45:45,556] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:45,556] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test/table12.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000605379sINFO  [2023-01-06 20:45:45,585] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-06 20:45:45,585] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-06 20:45:45,585] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:45:45,585] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@3f717646), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@1a0a84ff), dateReader=com.bakdata.conquery.util.DateReader@4b19bf0a, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-06 20:45:45,588] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test/table12.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:45,588] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:45,588] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:45,588] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test/table12.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:45,606] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:45 +0000] "POST /admin/datasets/SECONDARY_ID%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:45,607] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:45,608] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:45,608] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:45,610] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:45,610] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-06 20:45:45,610] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table1.table1], containing 6 entries.
WARN  [2023-01-06 20:45:45,611] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:45,611] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test.table1.table1.0
INFO  [2023-01-06 20:45:45,630] com.bakdata.conquery.models.jobs.ImportJob: Importing table12 into SECONDARY_ID$20Test.table12
INFO  [2023-01-06 20:45:45,630] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:45,630] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:45,630] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:45,630] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,630] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
127.0.0.1 - - [06/Jan/2023:20:45:45 +0000] "POST /admin/datasets/SECONDARY_ID%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID+Test%2Ftable12.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
WARN  [2023-01-06 20:45:45,630] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:45,631] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table12.table12], containing 6 entries.
INFO  [2023-01-06 20:45:45,631] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table12.table12], containing 6 entries.
INFO  [2023-01-06 20:45:45,631] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test.table12.table12.0
INFO  [2023-01-06 20:45:45,737] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,742] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,757] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:45,758] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:45,863] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID Test QUERY INIT
INFO  [2023-01-06 20:45:45,881] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:45,881] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4a9a5617-3450-487a-a582-b011eb61f7fd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test))]]
INFO  [2023-01-06 20:45:45,889] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd
INFO  [2023-01-06 20:45:45,889] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd
WARN  [2023-01-06 20:45:45,889] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:45,890] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd] with 0 results within PT0.000532S
127.0.0.1 - - [06/Jan/2023:20:45:45 +0000] "POST /api/datasets/SECONDARY_ID$20Test/queries HTTP/1.1" 201 1683 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:45,890] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd, workerId=SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_1fbe7244-62b4-458b-9aa2-ca737e2d10d6, startTime=2023-01-06T20:45:45.889529, finishTime=2023-01-06T20:45:45.890061) of size 0
INFO  [2023-01-06 20:45:45,892] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd] with 2 results within PT0.003306S
INFO  [2023-01-06 20:45:45,893] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd, workerId=SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_3f96326c-b468-4547-86be-be133c839756, startTime=2023-01-06T20:45:45.889534, finishTime=2023-01-06T20:45:45.892840) of size 2
INFO  [2023-01-06 20:45:45,893] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4a9a5617-3450-487a-a582-b011eb61f7fd ManagedQuery within PT0.011808S
127.0.0.1 - - [06/Jan/2023:20:45:45 +0000] "GET /api/datasets/SECONDARY_ID$20Test/queries/SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd HTTP/1.1" 200 1951 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:45,917] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test], queryId=4a9a5617-3450-487a-a582-b011eb61f7fd, label=number	@§$, creationTime=2023-01-06T20:45:45.881588, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3df7f06e[Count = 0], startTime=2023-01-06T20:45:45.881986, finishTime=2023-01-06T20:45:45.893794, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@77d21cb0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3c66d5e6, com.bakdata.conquery.models.query.ColumnDescriptor@109d90f8, com.bakdata.conquery.models.query.ColumnDescriptor@126b7605]) download on dataset Dataset[label=null, name=SECONDARY_ID Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:45,918] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test], queryId=4a9a5617-3450-487a-a582-b011eb61f7fd, label=number	@§$, creationTime=2023-01-06T20:45:45.881588, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3df7f06e[Count = 0], startTime=2023-01-06T20:45:45.881986, finishTime=2023-01-06T20:45:45.893794, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@77d21cb0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3c66d5e6, com.bakdata.conquery.models.query.ColumnDescriptor@109d90f8, com.bakdata.conquery.models.query.ColumnDescriptor@126b7605]) on dataset Dataset[label=null, name=SECONDARY_ID Test]
127.0.0.1 - - [06/Jan/2023:20:45:45 +0000] "GET /api/datasets/SECONDARY_ID%20Test/result/SECONDARY_ID$20Test.4a9a5617-3450-487a-a582-b011eb61f7fd.csv?pretty=false HTTP/1.1" 200 226 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:45,935] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID Test on 6 rows
INFO  [2023-01-06 20:45:45,935] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID Test
INFO  [2023-01-06 20:45:45,935] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-06 20:45:45,935] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-06 20:45:45,935] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test_1fbe7244-62b4-458b-9aa2-ca737e2d10d6
INFO  [2023-01-06 20:45:45,935] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test_3f96326c-b468-4547-86be-be133c839756
INFO  [2023-01-06 20:45:45,975] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test_1fbe7244-62b4-458b-9aa2-ca737e2d10d6
INFO  [2023-01-06 20:45:45,975] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID Test
INFO  [2023-01-06 20:45:45,975] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test_3f96326c-b468-4547-86be-be133c839756
INFO  [2023-01-06 20:45:46,033] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID$20Test
INFO  [2023-01-06 20:45:46,033] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,163] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID Test
INFO  [2023-01-06 20:45:46,164] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID Test
INFO  [2023-01-06 20:45:46,164] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:46,164] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:46,165] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-06 20:45:46,165] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:46,165] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-06 20:45:46,166] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_8bbc0c79-8723-44ce-bbaa-01457699e801 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_8bbc0c79-8723-44ce-bbaa-01457699e801 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_9e7bce4c-a2b2-4dda-86cb-5b7b9d36c153 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_9e7bce4c-a2b2-4dda-86cb-5b7b9d36c153 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:46,167] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,271] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[1].secondary]
INFO  [2023-01-06 20:45:46,272] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[1].ignored]
INFO  [2023-01-06 20:45:46,272] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,273] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].secondary
INFO  [2023-01-06 20:45:46,273] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].secondary
INFO  [2023-01-06 20:45:46,273] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].ignored
INFO  [2023-01-06 20:45:46,316] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].ignored
INFO  [2023-01-06 20:45:46,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table1
INFO  [2023-01-06 20:45:46,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table1
INFO  [2023-01-06 20:45:46,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table12
INFO  [2023-01-06 20:45:46,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table12
INFO  [2023-01-06 20:45:46,541] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,651] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:46,651] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:46,651] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:46,651] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 295 B in total
INFO  [2023-01-06 20:45:46,651] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[1]/table1.import.json, tag=Optional.empty)
████████████████████████▌                         ▌  49%	est. time remaining: 0.044798348sINFO  [2023-01-06 20:45:46,695] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=4, min=4, average=4.000000, max=4}
INFO  [2023-01-06 20:45:46,695] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:46,695] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=4, nullLines=1), encoding=null, prefix=f_a, suffix=)
INFO  [2023-01-06 20:45:46,695] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@31d6effe), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14790, maxValue=20999), dateReader=com.bakdata.conquery.util.DateReader@5a17cc4), dateReader=com.bakdata.conquery.util.DateReader@4bc22df3, onlyQuarters=false, maxValue=20999, minValue=14790, anyOpen=false)
INFO  [2023-01-06 20:45:46,698] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:46,698] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[1]/table12.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000639748sINFO  [2023-01-06 20:45:46,716] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=4, min=4, average=4.000000, max=4}
INFO  [2023-01-06 20:45:46,716] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-06 20:45:46,716] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=4, nullLines=1), encoding=null, prefix=f_a, suffix=)
INFO  [2023-01-06 20:45:46,716] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@7c9e8cec), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@2671cfdb), dateReader=com.bakdata.conquery.util.DateReader@2161cd01, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-06 20:45:46,719] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[1]/table12.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:46,719] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:46,719] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:46,719] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[1]/table12.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:46,733] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID$20Test[1].table1
127.0.0.1 - - [06/Jan/2023:20:45:46 +0000] "POST /admin/datasets/SECONDARY_ID%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID+Test%5B1%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:46,735] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:46,736] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:46,736] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:46,740] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:46,740] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table1.table1], containing 4 entries.
INFO  [2023-01-06 20:45:46,740] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table1.table1], containing 4 entries.
WARN  [2023-01-06 20:45:46,742] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:46,742] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test[1].table1.table1.0
INFO  [2023-01-06 20:45:46,774] com.bakdata.conquery.models.jobs.ImportJob: Importing table12 into SECONDARY_ID$20Test[1].table12
INFO  [2023-01-06 20:45:46,774] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:46,775] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:46,775] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:46,775] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:46 +0000] "POST /admin/datasets/SECONDARY_ID%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID+Test%5B1%5D%2Ftable12.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:46,775] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:46,775] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table12.table12], containing 4 entries.
WARN  [2023-01-06 20:45:46,775] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:46,775] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table12.table12], containing 4 entries.
INFO  [2023-01-06 20:45:46,776] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test[1].table12.table12.0
INFO  [2023-01-06 20:45:46,881] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,886] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,904] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:46,904] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:47,010] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID Test QUERY INIT
INFO  [2023-01-06 20:45:47,026] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:47,027] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f6e4dbe9-5c0b-454a-b969-a4f127486bc3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[1]))]]
INFO  [2023-01-06 20:45:47,032] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[1].f6e4dbe9-5c0b-454a-b969-a4f127486bc3
INFO  [2023-01-06 20:45:47,032] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[1].f6e4dbe9-5c0b-454a-b969-a4f127486bc3
WARN  [2023-01-06 20:45:47,032] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:47,032] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[1].f6e4dbe9-5c0b-454a-b969-a4f127486bc3] with 0 results within PT0.000161S
127.0.0.1 - - [06/Jan/2023:20:45:47 +0000] "POST /api/datasets/SECONDARY_ID$20Test%5B1%5D/queries HTTP/1.1" 201 1870 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:47,033] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[1].f6e4dbe9-5c0b-454a-b969-a4f127486bc3, workerId=SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_9e7bce4c-a2b2-4dda-86cb-5b7b9d36c153, startTime=2023-01-06T20:45:47.032727, finishTime=2023-01-06T20:45:47.032888) of size 0
INFO  [2023-01-06 20:45:47,033] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[1].f6e4dbe9-5c0b-454a-b969-a4f127486bc3] with 1 results within PT0.00142S
INFO  [2023-01-06 20:45:47,034] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[1].f6e4dbe9-5c0b-454a-b969-a4f127486bc3, workerId=SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_8bbc0c79-8723-44ce-bbaa-01457699e801, startTime=2023-01-06T20:45:47.032506, finishTime=2023-01-06T20:45:47.033926) of size 1
INFO  [2023-01-06 20:45:47,034] com.bakdata.conquery.models.execution.ManagedExecution: DONE f6e4dbe9-5c0b-454a-b969-a4f127486bc3 ManagedQuery within PT0.007308S
127.0.0.1 - - [06/Jan/2023:20:45:47 +0000] "GET /api/datasets/SECONDARY_ID$20Test%5B1%5D/queries/SECONDARY_ID$20Test%5B1%5D.f6e4dbe9-5c0b-454a-b969-a4f127486bc3 HTTP/1.1" 200 2389 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:47,058] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[1]], queryId=f6e4dbe9-5c0b-454a-b969-a4f127486bc3, label=vs	@§$, creationTime=2023-01-06T20:45:47.026878, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5b271e91[Count = 0], startTime=2023-01-06T20:45:47.027154, finishTime=2023-01-06T20:45:47.034462, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@491d8cf2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5e5a33ef, com.bakdata.conquery.models.query.ColumnDescriptor@2c75ae2f, com.bakdata.conquery.models.query.ColumnDescriptor@4fc5eea6]) download on dataset Dataset[label=null, name=SECONDARY_ID Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:47,058] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[1]], queryId=f6e4dbe9-5c0b-454a-b969-a4f127486bc3, label=vs	@§$, creationTime=2023-01-06T20:45:47.026878, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5b271e91[Count = 0], startTime=2023-01-06T20:45:47.027154, finishTime=2023-01-06T20:45:47.034462, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@491d8cf2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5e5a33ef, com.bakdata.conquery.models.query.ColumnDescriptor@2c75ae2f, com.bakdata.conquery.models.query.ColumnDescriptor@4fc5eea6]) on dataset Dataset[label=null, name=SECONDARY_ID Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:47 +0000] "GET /api/datasets/SECONDARY_ID%20Test%5B1%5D/result/SECONDARY_ID$20Test%5B1%5D.f6e4dbe9-5c0b-454a-b969-a4f127486bc3.csv?pretty=false HTTP/1.1" 200 88 "-" "Conquery (test client)" 24
INFO  [2023-01-06 20:45:47,080] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID Test on 3 rows
INFO  [2023-01-06 20:45:47,080] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID Test[1]
INFO  [2023-01-06 20:45:47,081] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-06 20:45:47,081] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-06 20:45:47,081] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[1]_9e7bce4c-a2b2-4dda-86cb-5b7b9d36c153
INFO  [2023-01-06 20:45:47,081] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[1]_8bbc0c79-8723-44ce-bbaa-01457699e801
INFO  [2023-01-06 20:45:47,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[1]_8bbc0c79-8723-44ce-bbaa-01457699e801
INFO  [2023-01-06 20:45:47,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[1]_9e7bce4c-a2b2-4dda-86cb-5b7b9d36c153
INFO  [2023-01-06 20:45:47,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID Test[1]
INFO  [2023-01-06 20:45:47,179] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID$20Test[1]
INFO  [2023-01-06 20:45:47,179] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:47,310] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID Test
INFO  [2023-01-06 20:45:47,310] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-06 20:45:47,310] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:47,310] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:47,311] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-06 20:45:47,312] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-06 20:45:47,312] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:47,312] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_e27a8dcf-878b-4f1a-af02-20f2b8858c49 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_e27a8dcf-878b-4f1a-af02-20f2b8858c49 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_ed8f4ccc-bbc3-4be4-8997-d657c26e986d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_ed8f4ccc-bbc3-4be4-8997-d657c26e986d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:47,313] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:47,418] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID_EXCLUDED$20Test.secondary]
INFO  [2023-01-06 20:45:47,419] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:47,419] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_EXCLUDED$20Test.secondary
INFO  [2023-01-06 20:45:47,419] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_EXCLUDED$20Test.secondary
INFO  [2023-01-06 20:45:47,525] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:47,526] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_EXCLUDED$20Test.table1
INFO  [2023-01-06 20:45:47,526] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_EXCLUDED$20Test.table1
INFO  [2023-01-06 20:45:47,642] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:47,750] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:47,750] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:47,750] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 135 B in total
INFO  [2023-01-06 20:45:47,750] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_EXCLUDED Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000420888sINFO  [2023-01-06 20:45:47,793] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=6, min=2, average=2.000000, max=2}
INFO  [2023-01-06 20:45:47,793] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:47,793] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18262, maxValue=18262), dateReader=com.bakdata.conquery.util.DateReader@27d3427)
INFO  [2023-01-06 20:45:47,793] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=f, suffix=)
INFO  [2023-01-06 20:45:47,796] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_EXCLUDED Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:47,796] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:47,796] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_EXCLUDED Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:47,819] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID_EXCLUDED$20Test.table1
INFO  [2023-01-06 20:45:47,820] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:47,820] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:47,820] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:47,822] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
127.0.0.1 - - [06/Jan/2023:20:45:47 +0000] "POST /admin/datasets/SECONDARY_ID_EXCLUDED%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID_EXCLUDED+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:45:47,823] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
WARN  [2023-01-06 20:45:47,824] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:47,824] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_EXCLUDED$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-06 20:45:47,825] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID_EXCLUDED$20Test.table1.table1.0
INFO  [2023-01-06 20:45:47,871] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_EXCLUDED$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-06 20:45:47,976] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:47,981] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,006] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,007] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:48,113] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID_EXCLUDED Test QUERY INIT
INFO  [2023-01-06 20:45:48,129] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID_EXCLUDED$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:48,130] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[338e6530-43a5-4e24-bb65-8b535de8d739] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_EXCLUDED Test))]]
INFO  [2023-01-06 20:45:48,134] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739
INFO  [2023-01-06 20:45:48,135] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739
WARN  [2023-01-06 20:45:48,135] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:48,135] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739] with 0 results within PT0.000164S
127.0.0.1 - - [06/Jan/2023:20:45:48 +0000] "POST /api/datasets/SECONDARY_ID_EXCLUDED$20Test/queries HTTP/1.1" 201 1809 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:48,136] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739] with 3 results within PT0.00217S
INFO  [2023-01-06 20:45:48,137] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739, workerId=SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_ed8f4ccc-bbc3-4be4-8997-d657c26e986d, startTime=2023-01-06T20:45:48.135251, finishTime=2023-01-06T20:45:48.135415) of size 0
INFO  [2023-01-06 20:45:48,137] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739, workerId=SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_e27a8dcf-878b-4f1a-af02-20f2b8858c49, startTime=2023-01-06T20:45:48.134751, finishTime=2023-01-06T20:45:48.136921) of size 3
INFO  [2023-01-06 20:45:48,137] com.bakdata.conquery.models.execution.ManagedExecution: DONE 338e6530-43a5-4e24-bb65-8b535de8d739 ManagedQuery within PT0.00722S
127.0.0.1 - - [06/Jan/2023:20:45:48 +0000] "GET /api/datasets/SECONDARY_ID_EXCLUDED$20Test/queries/SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739 HTTP/1.1" 200 2112 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:48,164] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test], queryId=338e6530-43a5-4e24-bb65-8b535de8d739, label=vs concept	@§$, creationTime=2023-01-06T20:45:48.130021, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6752a61a[Count = 0], startTime=2023-01-06T20:45:48.130268, finishTime=2023-01-06T20:45:48.137488, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5d7d1255), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_EXCLUDED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1322f5c4, com.bakdata.conquery.models.query.ColumnDescriptor@1980cf44, com.bakdata.conquery.models.query.ColumnDescriptor@6b90d818]) download on dataset Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:48,164] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test], queryId=338e6530-43a5-4e24-bb65-8b535de8d739, label=vs concept	@§$, creationTime=2023-01-06T20:45:48.130021, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6752a61a[Count = 0], startTime=2023-01-06T20:45:48.130268, finishTime=2023-01-06T20:45:48.137488, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5d7d1255), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_EXCLUDED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1322f5c4, com.bakdata.conquery.models.query.ColumnDescriptor@1980cf44, com.bakdata.conquery.models.query.ColumnDescriptor@6b90d818]) on dataset Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test]
127.0.0.1 - - [06/Jan/2023:20:45:48 +0000] "GET /api/datasets/SECONDARY_ID_EXCLUDED%20Test/result/SECONDARY_ID_EXCLUDED$20Test.338e6530-43a5-4e24-bb65-8b535de8d739.csv?pretty=false HTTP/1.1" 200 173 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:48,183] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID_EXCLUDED Test on 6 rows
INFO  [2023-01-06 20:45:48,183] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-06 20:45:48,184] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-06 20:45:48,184] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-06 20:45:48,184] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_EXCLUDED Test_ed8f4ccc-bbc3-4be4-8997-d657c26e986d
INFO  [2023-01-06 20:45:48,184] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_EXCLUDED Test_e27a8dcf-878b-4f1a-af02-20f2b8858c49
INFO  [2023-01-06 20:45:48,215] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-06 20:45:48,215] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_EXCLUDED Test_e27a8dcf-878b-4f1a-af02-20f2b8858c49
INFO  [2023-01-06 20:45:48,216] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_EXCLUDED Test_ed8f4ccc-bbc3-4be4-8997-d657c26e986d
INFO  [2023-01-06 20:45:48,224] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID_EXCLUDED$20Test
INFO  [2023-01-06 20:45:48,224] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,413] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-06 20:45:48,413] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID Test
INFO  [2023-01-06 20:45:48,413] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:48,413] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:48,414] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-06 20:45:48,414] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-06 20:45:48,414] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:48,414] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_90be7426-302a-439f-ad20-ec055fddf679 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_90be7426-302a-439f-ad20-ec055fddf679 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_943527d9-b16b-4aab-b78b-9f5f3a462adc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_943527d9-b16b-4aab-b78b-9f5f3a462adc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:48,416] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:48,519] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[2].secondary]
INFO  [2023-01-06 20:45:48,520] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[2].ignored]
INFO  [2023-01-06 20:45:48,521] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,521] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].secondary
INFO  [2023-01-06 20:45:48,521] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].secondary
INFO  [2023-01-06 20:45:48,521] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].ignored
INFO  [2023-01-06 20:45:48,564] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].ignored
INFO  [2023-01-06 20:45:48,674] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,675] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[2].table1
INFO  [2023-01-06 20:45:48,675] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[2].table1
INFO  [2023-01-06 20:45:48,787] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:48,898] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:48,899] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:48,899] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 180 B in total
INFO  [2023-01-06 20:45:48,899] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[2]/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000282194sINFO  [2023-01-06 20:45:48,927] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=5, min=1, average=2.500000, max=4}
INFO  [2023-01-06 20:45:48,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=5, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:48,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:45:48,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=5, nullLines=0), minParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@a0c3168), maxParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@22ab28e5), dateReader=com.bakdata.conquery.util.DateReader@d8df074, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-06 20:45:48,935] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[2]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:48,935] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:48,935] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID Test[2]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:48,964] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID$20Test[2].table1
INFO  [2023-01-06 20:45:48,965] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:48 +0000] "POST /admin/datasets/SECONDARY_ID%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID+Test%5B2%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-06 20:45:48,966] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:48,967] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:48,967] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:48,971] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:48,971] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[2].table1.table1], containing 5 entries.
INFO  [2023-01-06 20:45:48,971] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[2].table1.table1], containing 5 entries.
WARN  [2023-01-06 20:45:48,972] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:48,973] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test[2].table1.table1.0
INFO  [2023-01-06 20:45:49,078] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,084] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,101] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:49,206] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID Test QUERY INIT
INFO  [2023-01-06 20:45:49,223] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:49,224] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2ed93033-218b-46f3-9a28-d6d9a4934273] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[2]))]]
INFO  [2023-01-06 20:45:49,230] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[2].2ed93033-218b-46f3-9a28-d6d9a4934273
INFO  [2023-01-06 20:45:49,230] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[2].2ed93033-218b-46f3-9a28-d6d9a4934273
WARN  [2023-01-06 20:45:49,230] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:49,230] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[2].2ed93033-218b-46f3-9a28-d6d9a4934273] with 0 results within PT0.000183S
127.0.0.1 - - [06/Jan/2023:20:45:49 +0000] "POST /api/datasets/SECONDARY_ID$20Test%5B2%5D/queries HTTP/1.1" 201 1552 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:49,231] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[2].2ed93033-218b-46f3-9a28-d6d9a4934273, workerId=SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_943527d9-b16b-4aab-b78b-9f5f3a462adc, startTime=2023-01-06T20:45:49.230558, finishTime=2023-01-06T20:45:49.230741) of size 0
INFO  [2023-01-06 20:45:49,231] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[2].2ed93033-218b-46f3-9a28-d6d9a4934273] with 1 results within PT0.001238S
INFO  [2023-01-06 20:45:49,232] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[2].2ed93033-218b-46f3-9a28-d6d9a4934273, workerId=SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_90be7426-302a-439f-ad20-ec055fddf679, startTime=2023-01-06T20:45:49.230129, finishTime=2023-01-06T20:45:49.231367) of size 1
INFO  [2023-01-06 20:45:49,232] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2ed93033-218b-46f3-9a28-d6d9a4934273 ManagedQuery within PT0.007496S
127.0.0.1 - - [06/Jan/2023:20:45:49 +0000] "GET /api/datasets/SECONDARY_ID$20Test%5B2%5D/queries/SECONDARY_ID$20Test%5B2%5D.2ed93033-218b-46f3-9a28-d6d9a4934273 HTTP/1.1" 200 2071 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:49,258] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[2]], queryId=2ed93033-218b-46f3-9a28-d6d9a4934273, label=Uploaded-List number	@§$, creationTime=2023-01-06T20:45:49.223439, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@43fe069c[Count = 0], startTime=2023-01-06T20:45:49.224597, finishTime=2023-01-06T20:45:49.232093, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7cb02d6c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[2])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@74609848, com.bakdata.conquery.models.query.ColumnDescriptor@6025885c, com.bakdata.conquery.models.query.ColumnDescriptor@53dca822]) download on dataset Dataset[label=null, name=SECONDARY_ID Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:49,259] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[2]], queryId=2ed93033-218b-46f3-9a28-d6d9a4934273, label=Uploaded-List number	@§$, creationTime=2023-01-06T20:45:49.223439, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@43fe069c[Count = 0], startTime=2023-01-06T20:45:49.224597, finishTime=2023-01-06T20:45:49.232093, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7cb02d6c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[2])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@74609848, com.bakdata.conquery.models.query.ColumnDescriptor@6025885c, com.bakdata.conquery.models.query.ColumnDescriptor@53dca822]) on dataset Dataset[label=null, name=SECONDARY_ID Test[2]]
127.0.0.1 - - [06/Jan/2023:20:45:49 +0000] "GET /api/datasets/SECONDARY_ID%20Test%5B2%5D/result/SECONDARY_ID$20Test%5B2%5D.2ed93033-218b-46f3-9a28-d6d9a4934273.csv?pretty=false HTTP/1.1" 200 64 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:45:49,276] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID Test on 3 rows
INFO  [2023-01-06 20:45:49,276] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID Test[2]
INFO  [2023-01-06 20:45:49,276] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-06 20:45:49,277] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[2]_943527d9-b16b-4aab-b78b-9f5f3a462adc
INFO  [2023-01-06 20:45:49,277] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-06 20:45:49,277] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[2]_90be7426-302a-439f-ad20-ec055fddf679
INFO  [2023-01-06 20:45:49,315] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID Test[2]
INFO  [2023-01-06 20:45:49,315] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[2]_90be7426-302a-439f-ad20-ec055fddf679
INFO  [2023-01-06 20:45:49,315] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[2]_943527d9-b16b-4aab-b78b-9f5f3a462adc
INFO  [2023-01-06 20:45:49,376] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID$20Test[2]
INFO  [2023-01-06 20:45:49,376] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,507] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID Test
INFO  [2023-01-06 20:45:49,508] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID_MIXED Test
INFO  [2023-01-06 20:45:49,508] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:49,508] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:49,509] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-06 20:45:49,509] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-06 20:45:49,509] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:49,509] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:49,510] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,510] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_567103f2-24a6-4e83-b7bc-0c9cdb5d2089 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:49,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_567103f2-24a6-4e83-b7bc-0c9cdb5d2089 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:49,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:49,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_80c4a376-8ff4-47da-a3f1-cafb11af6487 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:49,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_80c4a376-8ff4-47da-a3f1-cafb11af6487 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:49,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:49,615] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID_MIXED$20Test.secondary]
INFO  [2023-01-06 20:45:49,616] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID_MIXED$20Test.ignored]
INFO  [2023-01-06 20:45:49,616] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,617] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.secondary
INFO  [2023-01-06 20:45:49,617] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.secondary
INFO  [2023-01-06 20:45:49,660] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.ignored
INFO  [2023-01-06 20:45:49,660] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.ignored
INFO  [2023-01-06 20:45:49,766] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,767] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table
INFO  [2023-01-06 20:45:49,767] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table
INFO  [2023-01-06 20:45:49,767] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table2
INFO  [2023-01-06 20:45:49,767] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table2
INFO  [2023-01-06 20:45:49,880] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:49,988] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:49,988] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:49,988] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:49,989] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 465 B in total
INFO  [2023-01-06 20:45:49,989] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_MIXED Test/table.import.json, tag=Optional.empty)
██████████████████████████▌                       ▌  53%	est. time remaining: 0.033983784sINFO  [2023-01-06 20:45:50,027] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-06 20:45:50,027] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@5682c80c), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@1abaa407), dateReader=com.bakdata.conquery.util.DateReader@689672b2, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-06 20:45:50,027] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[ignored] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=a, suffix=a)
INFO  [2023-01-06 20:45:50,027] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:45:50,027] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:45:50,031] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_MIXED Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:50,031] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_MIXED Test/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000594591sINFO  [2023-01-06 20:45:50,049] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-06 20:45:50,049] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-06 20:45:50,049] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:45:50,049] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@4da52a6e), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@4b0a853b), dateReader=com.bakdata.conquery.util.DateReader@454387c2, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-06 20:45:50,052] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_MIXED Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:50,052] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:45:50,052] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_MIXED Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:50,052] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SECONDARY_ID_MIXED Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:50,065] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SECONDARY_ID_MIXED$20Test.table
127.0.0.1 - - [06/Jan/2023:20:45:50 +0000] "POST /admin/datasets/SECONDARY_ID_MIXED%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID_MIXED+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:50,067] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:50,068] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:50,068] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:50,073] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:50,073] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:45:50,073] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:45:50,074] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:50,075] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID_MIXED$20Test.table.table.0
INFO  [2023-01-06 20:45:50,084] com.bakdata.conquery.models.jobs.ImportJob: Importing table2 into SECONDARY_ID_MIXED$20Test.table2
INFO  [2023-01-06 20:45:50,084] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:50,084] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:50 +0000] "POST /admin/datasets/SECONDARY_ID_MIXED%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SECONDARY_ID_MIXED+Test%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:50,084] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:50,084] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:50,085] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:45:50,085] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table2.table2], containing 6 entries.
WARN  [2023-01-06 20:45:50,085] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:50,085] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table2.table2], containing 6 entries.
INFO  [2023-01-06 20:45:50,085] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID_MIXED$20Test.table2.table2.0
INFO  [2023-01-06 20:45:50,190] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,196] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,213] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,213] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:50,319] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID_MIXED Test QUERY INIT
INFO  [2023-01-06 20:45:50,336] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID_MIXED$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:50,336] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[06ba2641-2b3b-4c3d-ab96-9d0192a21587] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_MIXED Test))]]
INFO  [2023-01-06 20:45:50,341] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587
INFO  [2023-01-06 20:45:50,341] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587
WARN  [2023-01-06 20:45:50,341] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:45:50,341] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587] with 0 results within PT0.00016S
INFO  [2023-01-06 20:45:50,342] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587, workerId=SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_80c4a376-8ff4-47da-a3f1-cafb11af6487, startTime=2023-01-06T20:45:50.341760, finishTime=2023-01-06T20:45:50.341920) of size 0
127.0.0.1 - - [06/Jan/2023:20:45:50 +0000] "POST /api/datasets/SECONDARY_ID_MIXED$20Test/queries HTTP/1.1" 201 1694 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:50,342] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587] with 2 results within PT0.001338S
INFO  [2023-01-06 20:45:50,343] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587, workerId=SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_567103f2-24a6-4e83-b7bc-0c9cdb5d2089, startTime=2023-01-06T20:45:50.341383, finishTime=2023-01-06T20:45:50.342721) of size 2
INFO  [2023-01-06 20:45:50,343] com.bakdata.conquery.models.execution.ManagedExecution: DONE 06ba2641-2b3b-4c3d-ab96-9d0192a21587 ManagedQuery within PT0.006469S
127.0.0.1 - - [06/Jan/2023:20:45:50 +0000] "GET /api/datasets/SECONDARY_ID_MIXED$20Test/queries/SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587 HTTP/1.1" 200 1985 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:50,369] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_MIXED Test], queryId=06ba2641-2b3b-4c3d-ab96-9d0192a21587, label=concept	@§$, creationTime=2023-01-06T20:45:50.336597, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1accffa4[Count = 0], startTime=2023-01-06T20:45:50.336838, finishTime=2023-01-06T20:45:50.343307, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2cba9af1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_MIXED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@58bfc6b0, com.bakdata.conquery.models.query.ColumnDescriptor@2b6ae625, com.bakdata.conquery.models.query.ColumnDescriptor@56a61d33]) download on dataset Dataset[label=null, name=SECONDARY_ID_MIXED Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:50,369] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_MIXED Test], queryId=06ba2641-2b3b-4c3d-ab96-9d0192a21587, label=concept	@§$, creationTime=2023-01-06T20:45:50.336597, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1accffa4[Count = 0], startTime=2023-01-06T20:45:50.336838, finishTime=2023-01-06T20:45:50.343307, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2cba9af1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_MIXED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@58bfc6b0, com.bakdata.conquery.models.query.ColumnDescriptor@2b6ae625, com.bakdata.conquery.models.query.ColumnDescriptor@56a61d33]) on dataset Dataset[label=null, name=SECONDARY_ID_MIXED Test]
127.0.0.1 - - [06/Jan/2023:20:45:50 +0000] "GET /api/datasets/SECONDARY_ID_MIXED%20Test/result/SECONDARY_ID_MIXED$20Test.06ba2641-2b3b-4c3d-ab96-9d0192a21587.csv?pretty=false HTTP/1.1" 200 309 "-" "Conquery (test client)" 28
INFO  [2023-01-06 20:45:50,395] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID_MIXED Test on 5 rows
INFO  [2023-01-06 20:45:50,396] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID_MIXED Test
INFO  [2023-01-06 20:45:50,396] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-06 20:45:50,396] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-06 20:45:50,396] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_MIXED Test_80c4a376-8ff4-47da-a3f1-cafb11af6487
INFO  [2023-01-06 20:45:50,396] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_MIXED Test_567103f2-24a6-4e83-b7bc-0c9cdb5d2089
INFO  [2023-01-06 20:45:50,409] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID_MIXED Test
INFO  [2023-01-06 20:45:50,410] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_MIXED Test_567103f2-24a6-4e83-b7bc-0c9cdb5d2089
INFO  [2023-01-06 20:45:50,410] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_MIXED Test_80c4a376-8ff4-47da-a3f1-cafb11af6487
INFO  [2023-01-06 20:45:50,493] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID_MIXED$20Test
INFO  [2023-01-06 20:45:50,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,519] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID_MIXED Test
INFO  [2023-01-06 20:45:50,519] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-06 20:45:50,519] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:50,520] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:50,520] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-06 20:45:50,520] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-06 20:45:50,520] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:50,520] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:50,521] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_4def0b87-4e2b-4cad-aeb9-379521772023 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:50,521] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_4def0b87-4e2b-4cad-aeb9-379521772023 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:50,521] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:50,522] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_8e6e5f12-3245-42de-b9ec-36616fdb45a5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:50,522] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_8e6e5f12-3245-42de-b9ec-36616fdb45a5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:50,522] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:50,526] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,626] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,633] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,633] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test.test_table
INFO  [2023-01-06 20:45:50,633] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test.test_table
INFO  [2023-01-06 20:45:50,748] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,857] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:50,858] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:50,858] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 87 B in total
INFO  [2023-01-06 20:45:50,858] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_CQEXTERNAL_QUERY Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000271476sINFO  [2023-01-06 20:45:50,885] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=6, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:50,885] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5b767d8e)
INFO  [2023-01-06 20:45:50,887] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_CQEXTERNAL_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:50,887] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:50,888] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_CQEXTERNAL_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:50,902] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_CQEXTERNAL_QUERY$20Test.test_table
127.0.0.1 - - [06/Jan/2023:20:45:50 +0000] "POST /admin/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_CQEXTERNAL_QUERY+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:50,902] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:50,903] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:50,903] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:50,904] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:50,905] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:50,905] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table], containing 6 entries.
INFO  [2023-01-06 20:45:50,906] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table], containing 6 entries.
WARN  [2023-01-06 20:45:50,907] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:50,907] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:50,907] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:51,012] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,017] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,025] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,026] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:51,026] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:51,136] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_CQEXTERNAL_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:51,150] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_CQEXTERNAL_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:51,151] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ef8cd711-fc44-4460-a49e-ed3ab63a45b7] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test))]]
INFO  [2023-01-06 20:45:51,153] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7
INFO  [2023-01-06 20:45:51,153] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7
INFO  [2023-01-06 20:45:51,155] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7] with 0 results within PT0.001085S
127.0.0.1 - - [06/Jan/2023:20:45:51 +0000] "POST /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test/queries HTTP/1.1" 201 984 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:51,155] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7] with 2 results within PT0.001166S
INFO  [2023-01-06 20:45:51,155] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_4def0b87-4e2b-4cad-aeb9-379521772023, startTime=2023-01-06T20:45:51.153957, finishTime=2023-01-06T20:45:51.155042) of size 0
INFO  [2023-01-06 20:45:51,155] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_8e6e5f12-3245-42de-b9ec-36616fdb45a5, startTime=2023-01-06T20:45:51.153967, finishTime=2023-01-06T20:45:51.155133) of size 2
INFO  [2023-01-06 20:45:51,155] com.bakdata.conquery.models.execution.ManagedExecution: DONE ef8cd711-fc44-4460-a49e-ed3ab63a45b7 ManagedQuery within PT0.004634S
127.0.0.1 - - [06/Jan/2023:20:45:51 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test/queries/SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7 HTTP/1.1" 200 1295 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:45:51,189] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test], queryId=ef8cd711-fc44-4460-a49e-ed3ab63a45b7, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:51.150685, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@55054f3e[Count = 0], startTime=2023-01-06T20:45:51.151272, finishTime=2023-01-06T20:45:51.155906, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@69d1670a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6965be17, com.bakdata.conquery.models.query.ColumnDescriptor@19c704bc]) download on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:51,189] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test], queryId=ef8cd711-fc44-4460-a49e-ed3ab63a45b7, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:51.150685, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@55054f3e[Count = 0], startTime=2023-01-06T20:45:51.151272, finishTime=2023-01-06T20:45:51.155906, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@69d1670a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6965be17, com.bakdata.conquery.models.query.ColumnDescriptor@19c704bc]) on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:51 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test/result/SIMPLE_CQEXTERNAL_QUERY$20Test.ef8cd711-fc44-4460-a49e-ed3ab63a45b7.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:45:51,209] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_CQEXTERNAL_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:51,209] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-06 20:45:51,210] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-06 20:45:51,210] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-06 20:45:51,210] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test_4def0b87-4e2b-4cad-aeb9-379521772023
INFO  [2023-01-06 20:45:51,210] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test_8e6e5f12-3245-42de-b9ec-36616fdb45a5
INFO  [2023-01-06 20:45:51,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test_8e6e5f12-3245-42de-b9ec-36616fdb45a5
INFO  [2023-01-06 20:45:51,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-06 20:45:51,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test_4def0b87-4e2b-4cad-aeb9-379521772023
INFO  [2023-01-06 20:45:51,312] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_CQEXTERNAL_QUERY$20Test
INFO  [2023-01-06 20:45:51,312] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,437] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-06 20:45:51,437] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-06 20:45:51,438] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:51,438] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:51,439] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-06 20:45:51,439] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-06 20:45:51,440] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:51,440] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:51,442] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_3f5bb00b-4137-4443-9b77-e23dea0edf5c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:51,442] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_3f5bb00b-4137-4443-9b77-e23dea0edf5c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:51,442] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:51,442] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_30ca5114-61f5-4935-a031-c5e6512de263 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:51,442] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_30ca5114-61f5-4935-a031-c5e6512de263 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:51,442] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:51,446] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,546] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,555] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,556] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table
INFO  [2023-01-06 20:45:51,556] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table
INFO  [2023-01-06 20:45:51,671] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,780] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:51,781] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:51,781] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 87 B in total
INFO  [2023-01-06 20:45:51,781] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_CQEXTERNAL_QUERY Test[1]/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000419663sINFO  [2023-01-06 20:45:51,823] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=6, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:51,823] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@794a5d59)
INFO  [2023-01-06 20:45:51,826] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_CQEXTERNAL_QUERY Test[1]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:51,826] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:51,826] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_CQEXTERNAL_QUERY Test[1]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:51,852] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table
INFO  [2023-01-06 20:45:51,853] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:51 +0000] "POST /admin/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_CQEXTERNAL_QUERY+Test%5B1%5D%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:45:51,854] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:51,854] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:51,854] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:51,855] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:51,856] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table], containing 6 entries.
INFO  [2023-01-06 20:45:51,856] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table], containing 6 entries.
WARN  [2023-01-06 20:45:51,857] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:51,857] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table.0
INFO  [2023-01-06 20:45:51,857] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table.1
INFO  [2023-01-06 20:45:51,962] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,967] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,977] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:51,977] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:51,977] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:52,086] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_CQEXTERNAL_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:52,101] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_CQEXTERNAL_QUERY$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:52,102] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[63ef8d57-2c8a-4a86-82c5-e9a4fda9de07] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test[1]))]]
INFO  [2023-01-06 20:45:52,105] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test[1].63ef8d57-2c8a-4a86-82c5-e9a4fda9de07
INFO  [2023-01-06 20:45:52,105] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test[1].63ef8d57-2c8a-4a86-82c5-e9a4fda9de07
127.0.0.1 - - [06/Jan/2023:20:45:52 +0000] "POST /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D/queries HTTP/1.1" 201 1030 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:52,120] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test[1].63ef8d57-2c8a-4a86-82c5-e9a4fda9de07] with 2 results within PT0.015222S
INFO  [2023-01-06 20:45:52,120] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test[1].63ef8d57-2c8a-4a86-82c5-e9a4fda9de07] with 0 results within PT0.015231S
INFO  [2023-01-06 20:45:52,121] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].63ef8d57-2c8a-4a86-82c5-e9a4fda9de07, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_30ca5114-61f5-4935-a031-c5e6512de263, startTime=2023-01-06T20:45:52.105422, finishTime=2023-01-06T20:45:52.120653) of size 0
INFO  [2023-01-06 20:45:52,121] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].63ef8d57-2c8a-4a86-82c5-e9a4fda9de07, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_3f5bb00b-4137-4443-9b77-e23dea0edf5c, startTime=2023-01-06T20:45:52.105393, finishTime=2023-01-06T20:45:52.120615) of size 2
INFO  [2023-01-06 20:45:52,121] com.bakdata.conquery.models.execution.ManagedExecution: DONE 63ef8d57-2c8a-4a86-82c5-e9a4fda9de07 ManagedQuery within PT0.018895S
127.0.0.1 - - [06/Jan/2023:20:45:52 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D/queries/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D.63ef8d57-2c8a-4a86-82c5-e9a4fda9de07 HTTP/1.1" 200 1638 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:52,138] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]], queryId=63ef8d57-2c8a-4a86-82c5-e9a4fda9de07, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:52.101797, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3b9f27c5[Count = 0], startTime=2023-01-06T20:45:52.102593, finishTime=2023-01-06T20:45:52.121488, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1e94395b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@af3ee42, com.bakdata.conquery.models.query.ColumnDescriptor@1b27e22e]) download on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:52,139] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]], queryId=63ef8d57-2c8a-4a86-82c5-e9a4fda9de07, label=Uploaded-List	@§$, creationTime=2023-01-06T20:45:52.101797, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3b9f27c5[Count = 0], startTime=2023-01-06T20:45:52.102593, finishTime=2023-01-06T20:45:52.121488, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1e94395b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@af3ee42, com.bakdata.conquery.models.query.ColumnDescriptor@1b27e22e]) on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:52 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test%5B1%5D/result/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D.63ef8d57-2c8a-4a86-82c5-e9a4fda9de07.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:52,157] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_CQEXTERNAL_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:52,157] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_CQEXTERNAL_QUERY Test[1]
INFO  [2023-01-06 20:45:52,158] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-06 20:45:52,158] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-06 20:45:52,158] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_3f5bb00b-4137-4443-9b77-e23dea0edf5c
INFO  [2023-01-06 20:45:52,158] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_30ca5114-61f5-4935-a031-c5e6512de263
INFO  [2023-01-06 20:45:52,239] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_CQEXTERNAL_QUERY Test[1]
INFO  [2023-01-06 20:45:52,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_30ca5114-61f5-4935-a031-c5e6512de263
INFO  [2023-01-06 20:45:52,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_3f5bb00b-4137-4443-9b77-e23dea0edf5c
INFO  [2023-01-06 20:45:52,257] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_CQEXTERNAL_QUERY$20Test[1]
INFO  [2023-01-06 20:45:52,257] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,383] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-06 20:45:52,383] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:52,383] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:52,383] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:52,384] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-06 20:45:52,384] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-06 20:45:52,384] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:52,385] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:52,386] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_25fc8439-9b72-428b-8d7c-5322cf395695 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:52,386] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_25fc8439-9b72-428b-8d7c-5322cf395695 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:52,386] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:52,386] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_62db511f-fbfd-4f26-9e60-b3750d8c7c43 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:52,386] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_62db511f-fbfd-4f26-9e60-b3750d8c7c43 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:52,386] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:52,391] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,491] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,498] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,498] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[1].table
INFO  [2023-01-06 20:45:52,498] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[1].table
INFO  [2023-01-06 20:45:52,608] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,719] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:52,720] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:52,720] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 194 B in total
INFO  [2023-01-06 20:45:52,720] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000213237sINFO  [2023-01-06 20:45:52,741] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=7, sum=8, min=1, average=1.142857, max=2}
INFO  [2023-01-06 20:45:52,742] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:52,742] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@51588f9c)
INFO  [2023-01-06 20:45:52,742] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[include] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:52,752] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:52,752] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:52,752] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:52,773] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SIMPLE_TREECONCEPT_QUERY$20Test[1].table
INFO  [2023-01-06 20:45:52,773] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:52 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_TREECONCEPT_QUERY+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:45:52,774] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:52,776] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:52,776] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:52,780] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:52,780] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table], containing 8 entries.
INFO  [2023-01-06 20:45:52,780] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table], containing 8 entries.
INFO  [2023-01-06 20:45:52,782] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table.0
WARN  [2023-01-06 20:45:52,782] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:52,782] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table.1
INFO  [2023-01-06 20:45:52,783] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table.2
INFO  [2023-01-06 20:45:52,888] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,893] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,925] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:52,925] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:52,925] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:53,031] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:53,043] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:53,044] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0c3651ae-1378-412f-83c2-777b82336478] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[1]))]]
INFO  [2023-01-06 20:45:53,046] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[1].0c3651ae-1378-412f-83c2-777b82336478
INFO  [2023-01-06 20:45:53,046] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[1].0c3651ae-1378-412f-83c2-777b82336478
127.0.0.1 - - [06/Jan/2023:20:45:53 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D/queries HTTP/1.1" 201 1135 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:53,047] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[1].0c3651ae-1378-412f-83c2-777b82336478] with 0 results within PT0.001095S
INFO  [2023-01-06 20:45:53,048] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[1].0c3651ae-1378-412f-83c2-777b82336478] with 2 results within PT0.001283S
INFO  [2023-01-06 20:45:53,048] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[1].0c3651ae-1378-412f-83c2-777b82336478, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_62db511f-fbfd-4f26-9e60-b3750d8c7c43, startTime=2023-01-06T20:45:53.046866, finishTime=2023-01-06T20:45:53.047961) of size 0
INFO  [2023-01-06 20:45:53,048] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[1].0c3651ae-1378-412f-83c2-777b82336478, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_25fc8439-9b72-428b-8d7c-5322cf395695, startTime=2023-01-06T20:45:53.046775, finishTime=2023-01-06T20:45:53.048058) of size 2
INFO  [2023-01-06 20:45:53,048] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0c3651ae-1378-412f-83c2-777b82336478 ManagedQuery within PT0.004641S
127.0.0.1 - - [06/Jan/2023:20:45:53 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D/queries/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D.0c3651ae-1378-412f-83c2-777b82336478 HTTP/1.1" 200 1750 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:53,069] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]], queryId=0c3651ae-1378-412f-83c2-777b82336478, label=concept-a1	@§$, creationTime=2023-01-06T20:45:53.043990, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4a8c2daf[Count = 0], startTime=2023-01-06T20:45:53.044204, finishTime=2023-01-06T20:45:53.048845, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@61e78b65), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@59c9ece4, com.bakdata.conquery.models.query.ColumnDescriptor@2f652d4a]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:53,069] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]], queryId=0c3651ae-1378-412f-83c2-777b82336478, label=concept-a1	@§$, creationTime=2023-01-06T20:45:53.043990, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4a8c2daf[Count = 0], startTime=2023-01-06T20:45:53.044204, finishTime=2023-01-06T20:45:53.048845, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@61e78b65), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@59c9ece4, com.bakdata.conquery.models.query.ColumnDescriptor@2f652d4a]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]]
127.0.0.1 - - [06/Jan/2023:20:45:53 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B1%5D/result/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D.0c3651ae-1378-412f-83c2-777b82336478.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:53,090] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:53,090] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY Test[1]
INFO  [2023-01-06 20:45:53,090] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-06 20:45:53,090] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-06 20:45:53,090] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[1]_62db511f-fbfd-4f26-9e60-b3750d8c7c43
INFO  [2023-01-06 20:45:53,090] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[1]_25fc8439-9b72-428b-8d7c-5322cf395695
INFO  [2023-01-06 20:45:53,191] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY Test[1]
INFO  [2023-01-06 20:45:53,191] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[1]_62db511f-fbfd-4f26-9e60-b3750d8c7c43
INFO  [2023-01-06 20:45:53,191] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[1]_25fc8439-9b72-428b-8d7c-5322cf395695
INFO  [2023-01-06 20:45:53,291] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY$20Test[1]
INFO  [2023-01-06 20:45:53,291] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,331] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:53,331] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-06 20:45:53,331] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:53,331] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:53,332] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-06 20:45:53,332] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-06 20:45:53,333] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:53,333] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_b0e9d9d7-ff8b-4c0b-9bba-f4c5b511ed95 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_b0e9d9d7-ff8b-4c0b-9bba-f4c5b511ed95 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_164e5bed-b72e-473d-9912-45499ea4e90f are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_164e5bed-b72e-473d-9912-45499ea4e90f are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:53,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:53,439] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,445] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,445] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy
INFO  [2023-01-06 20:45:53,445] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy
INFO  [2023-01-06 20:45:53,577] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,727] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:53,727] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:53,727] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 112 B in total
INFO  [2023-01-06 20:45:53,727] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_GROOVY_QUERY Test/test_table_groovy.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000300976sINFO  [2023-01-06 20:45:53,758] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:53,758] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:53,758] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[groovy_column] with IntegerParser(super=Parser(lines=4, nullLines=0), minValue=1, maxValue=2)
INFO  [2023-01-06 20:45:53,758] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4e38fe36)
INFO  [2023-01-06 20:45:53,760] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_GROOVY_QUERY Test/test_table_groovy.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:53,760] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:53,760] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_GROOVY_QUERY Test/test_table_groovy.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:53,773] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table_groovy into SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy
INFO  [2023-01-06 20:45:53,774] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:53 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_TREECONCEPT_GROOVY_QUERY+Test%2Ftest_table_groovy.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:53,775] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:53,777] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:53,777] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:53,782] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:53,782] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy], containing 4 entries.
INFO  [2023-01-06 20:45:53,782] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy], containing 4 entries.
INFO  [2023-01-06 20:45:53,784] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy.0
WARN  [2023-01-06 20:45:53,784] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:53,784] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy.1
INFO  [2023-01-06 20:45:53,898] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,903] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,915] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:53,915] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:53,915] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:54,022] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_GROOVY_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:54,039] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:54,039] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5bb484c6-598e-4bfc-93bf-9b6b728390b5] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_GROOVY_QUERY Test))]]
INFO  [2023-01-06 20:45:54,043] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5
INFO  [2023-01-06 20:45:54,043] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5
INFO  [2023-01-06 20:45:54,044] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5] with 0 results within PT0.000666S
INFO  [2023-01-06 20:45:54,044] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5] with 1 results within PT0.001038S
INFO  [2023-01-06 20:45:54,045] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5, workerId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_b0e9d9d7-ff8b-4c0b-9bba-f4c5b511ed95, startTime=2023-01-06T20:45:54.043776, finishTime=2023-01-06T20:45:54.044442) of size 0
127.0.0.1 - - [06/Jan/2023:20:45:54 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test/queries HTTP/1.1" 201 1193 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:54,045] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5, workerId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_164e5bed-b72e-473d-9912-45499ea4e90f, startTime=2023-01-06T20:45:54.043693, finishTime=2023-01-06T20:45:54.044731) of size 1
INFO  [2023-01-06 20:45:54,045] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5bb484c6-598e-4bfc-93bf-9b6b728390b5 ManagedQuery within PT0.005545S
127.0.0.1 - - [06/Jan/2023:20:45:54 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test/queries/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5 HTTP/1.1" 200 1535 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:54,067] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test], queryId=5bb484c6-598e-4bfc-93bf-9b6b728390b5, label=test_child1_1	@§$, creationTime=2023-01-06T20:45:54.039670, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@485bee6[Count = 0], startTime=2023-01-06T20:45:54.039905, finishTime=2023-01-06T20:45:54.045450, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3aaca8c7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_GROOVY_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5f8b476f, com.bakdata.conquery.models.query.ColumnDescriptor@15f485f6]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:54,067] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test], queryId=5bb484c6-598e-4bfc-93bf-9b6b728390b5, label=test_child1_1	@§$, creationTime=2023-01-06T20:45:54.039670, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@485bee6[Count = 0], startTime=2023-01-06T20:45:54.039905, finishTime=2023-01-06T20:45:54.045450, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3aaca8c7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_GROOVY_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5f8b476f, com.bakdata.conquery.models.query.ColumnDescriptor@15f485f6]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:54 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY%20Test/result/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.5bb484c6-598e-4bfc-93bf-9b6b728390b5.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 29
INFO  [2023-01-06 20:45:54,095] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_GROOVY_QUERY Test on 2 rows
INFO  [2023-01-06 20:45:54,095] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-06 20:45:54,096] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-06 20:45:54,096] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-06 20:45:54,096] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_b0e9d9d7-ff8b-4c0b-9bba-f4c5b511ed95
INFO  [2023-01-06 20:45:54,096] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_164e5bed-b72e-473d-9912-45499ea4e90f
INFO  [2023-01-06 20:45:54,133] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-06 20:45:54,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_b0e9d9d7-ff8b-4c0b-9bba-f4c5b511ed95
INFO  [2023-01-06 20:45:54,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_164e5bed-b72e-473d-9912-45499ea4e90f
INFO  [2023-01-06 20:45:54,194] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test
INFO  [2023-01-06 20:45:54,195] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,321] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-06 20:45:54,321] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Concept Condition is Present
INFO  [2023-01-06 20:45:54,322] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:54,322] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:54,323] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-06 20:45:54,323] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-06 20:45:54,323] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:54,323] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_e47c9a79-c64e-42bc-a9ad-aabe9b0ea459 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_e47c9a79-c64e-42bc-a9ad-aabe9b0ea459 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_155b1844-c8b9-42d0-bbb9-929b86622694 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_155b1844-c8b9-42d0-bbb9-929b86622694 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:54,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:54,429] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,435] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,436] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Concept$20Condition$20is$20Present.table
INFO  [2023-01-06 20:45:54,436] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Concept$20Condition$20is$20Present.table
INFO  [2023-01-06 20:45:54,554] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,663] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:54,663] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:54,663] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 173 B in total
INFO  [2023-01-06 20:45:54,663] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Concept Condition is Present/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000353235sINFO  [2023-01-06 20:45:54,699] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=7, min=1, average=1.166667, max=2}
INFO  [2023-01-06 20:45:54,699] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:54,699] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@621c3e8b)
INFO  [2023-01-06 20:45:54,699] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[include] with StringParser(super=Parser(lines=7, nullLines=3), encoding=null, prefix=1, suffix=1)
INFO  [2023-01-06 20:45:54,702] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Concept Condition is Present/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:54,702] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:54,702] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Concept Condition is Present/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:54,717] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Concept$20Condition$20is$20Present.table
INFO  [2023-01-06 20:45:54,717] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:54 +0000] "POST /admin/datasets/Concept%20Condition%20is%20Present/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Concept+Condition+is+Present%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:54,719] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:54,719] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:54,719] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:54,723] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:54,723] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Concept$20Condition$20is$20Present.table.table], containing 7 entries.
INFO  [2023-01-06 20:45:54,723] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Concept$20Condition$20is$20Present.table.table], containing 7 entries.
WARN  [2023-01-06 20:45:54,724] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:54,724] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Concept$20Condition$20is$20Present.table.table.0
INFO  [2023-01-06 20:45:54,725] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Concept$20Condition$20is$20Present.table.table.1
INFO  [2023-01-06 20:45:54,829] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,847] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,859] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:54,860] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:54,860] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:54,966] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Concept Condition is Present QUERY INIT
INFO  [2023-01-06 20:45:54,981] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Concept$20Condition$20is$20Present] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:54,982] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[63f5a784-056b-4c82-9b39-52f66b81919a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Concept Condition is Present))]]
INFO  [2023-01-06 20:45:54,985] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a
INFO  [2023-01-06 20:45:54,985] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a
127.0.0.1 - - [06/Jan/2023:20:45:54 +0000] "POST /api/datasets/Concept$20Condition$20is$20Present/queries HTTP/1.1" 201 1139 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:54,986] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a] with 2 results within PT0.001297S
INFO  [2023-01-06 20:45:54,987] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a] with 0 results within PT0.001645S
INFO  [2023-01-06 20:45:54,987] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a, workerId=Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_155b1844-c8b9-42d0-bbb9-929b86622694, startTime=2023-01-06T20:45:54.985315, finishTime=2023-01-06T20:45:54.986612) of size 2
INFO  [2023-01-06 20:45:54,987] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a, workerId=Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_e47c9a79-c64e-42bc-a9ad-aabe9b0ea459, startTime=2023-01-06T20:45:54.985345, finishTime=2023-01-06T20:45:54.986990) of size 0
INFO  [2023-01-06 20:45:54,987] com.bakdata.conquery.models.execution.ManagedExecution: DONE 63f5a784-056b-4c82-9b39-52f66b81919a ManagedQuery within PT0.005642S
127.0.0.1 - - [06/Jan/2023:20:45:54 +0000] "GET /api/datasets/Concept$20Condition$20is$20Present/queries/Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a HTTP/1.1" 200 1465 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:55,010] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Concept Condition is Present], queryId=63f5a784-056b-4c82-9b39-52f66b81919a, label=concept-a1	@§$, creationTime=2023-01-06T20:45:54.981961, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@281a8c0a[Count = 0], startTime=2023-01-06T20:45:54.982138, finishTime=2023-01-06T20:45:54.987780, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@225e6f24), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Concept Condition is Present)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e428108, com.bakdata.conquery.models.query.ColumnDescriptor@15fbf252]) download on dataset Dataset[label=null, name=Concept Condition is Present] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:55,011] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Concept Condition is Present], queryId=63f5a784-056b-4c82-9b39-52f66b81919a, label=concept-a1	@§$, creationTime=2023-01-06T20:45:54.981961, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@281a8c0a[Count = 0], startTime=2023-01-06T20:45:54.982138, finishTime=2023-01-06T20:45:54.987780, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@225e6f24), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Concept Condition is Present)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e428108, com.bakdata.conquery.models.query.ColumnDescriptor@15fbf252]) on dataset Dataset[label=null, name=Concept Condition is Present]
127.0.0.1 - - [06/Jan/2023:20:45:55 +0000] "GET /api/datasets/Concept%20Condition%20is%20Present/result/Concept$20Condition$20is$20Present.63f5a784-056b-4c82-9b39-52f66b81919a.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:45:55,027] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Concept Condition is Present on 3 rows
INFO  [2023-01-06 20:45:55,027] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Concept Condition is Present
INFO  [2023-01-06 20:45:55,027] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-06 20:45:55,027] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-06 20:45:55,028] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Concept Condition is Present_155b1844-c8b9-42d0-bbb9-929b86622694
INFO  [2023-01-06 20:45:55,028] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Concept Condition is Present_e47c9a79-c64e-42bc-a9ad-aabe9b0ea459
INFO  [2023-01-06 20:45:55,125] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Concept Condition is Present_155b1844-c8b9-42d0-bbb9-929b86622694
INFO  [2023-01-06 20:45:55,125] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Concept Condition is Present_e47c9a79-c64e-42bc-a9ad-aabe9b0ea459
INFO  [2023-01-06 20:45:55,125] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Concept Condition is Present
INFO  [2023-01-06 20:45:55,225] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Concept$20Condition$20is$20Present
INFO  [2023-01-06 20:45:55,226] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,265] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Concept Condition is Present
INFO  [2023-01-06 20:45:55,266] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:55,266] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:55,266] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:55,267] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-06 20:45:55,267] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-06 20:45:55,267] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:55,267] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:55,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_7dbebef0-2cc1-4ca0-a51d-a79e217b02a3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:55,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_7dbebef0-2cc1-4ca0-a51d-a79e217b02a3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:55,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:55,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_b56e211f-340b-4cef-9710-92fc1a098844 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:55,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_b56e211f-340b-4cef-9710-92fc1a098844 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:55,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:55,273] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,379] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,380] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table
INFO  [2023-01-06 20:45:55,380] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table
INFO  [2023-01-06 20:45:55,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,605] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:55,605] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:55,605] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-06 20:45:55,605] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test[2]/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000329468sINFO  [2023-01-06 20:45:55,638] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:55,638] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:55,638] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1fb4872c)
INFO  [2023-01-06 20:45:55,641] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test[2]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:55,641] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:55,641] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY Test[2]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:55,655] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table
127.0.0.1 - - [06/Jan/2023:20:45:55 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_TREECONCEPT_QUERY+Test%5B2%5D%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:45:55,663] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,663] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:55,664] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:55,664] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:55,666] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:55,666] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:45:55,666] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table], containing 4 entries.
WARN  [2023-01-06 20:45:55,667] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:55,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table.1
INFO  [2023-01-06 20:45:55,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table.0
INFO  [2023-01-06 20:45:55,772] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,778] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,791] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:55,791] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:55,791] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:55,897] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:55,911] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:55,912] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[25ed36ad-74b4-4a52-a7c1-92058e798d97] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[2]))]]
INFO  [2023-01-06 20:45:55,914] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[2].25ed36ad-74b4-4a52-a7c1-92058e798d97
INFO  [2023-01-06 20:45:55,914] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[2].25ed36ad-74b4-4a52-a7c1-92058e798d97
INFO  [2023-01-06 20:45:55,915] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[2].25ed36ad-74b4-4a52-a7c1-92058e798d97] with 0 results within PT0.000764S
INFO  [2023-01-06 20:45:55,915] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[2].25ed36ad-74b4-4a52-a7c1-92058e798d97] with 2 results within PT0.001002S
127.0.0.1 - - [06/Jan/2023:20:45:55 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D/queries HTTP/1.1" 201 1165 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:55,916] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[2].25ed36ad-74b4-4a52-a7c1-92058e798d97, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_b56e211f-340b-4cef-9710-92fc1a098844, startTime=2023-01-06T20:45:55.914970, finishTime=2023-01-06T20:45:55.915734) of size 0
INFO  [2023-01-06 20:45:55,916] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[2].25ed36ad-74b4-4a52-a7c1-92058e798d97, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_7dbebef0-2cc1-4ca0-a51d-a79e217b02a3, startTime=2023-01-06T20:45:55.914968, finishTime=2023-01-06T20:45:55.915970) of size 2
INFO  [2023-01-06 20:45:55,916] com.bakdata.conquery.models.execution.ManagedExecution: DONE 25ed36ad-74b4-4a52-a7c1-92058e798d97 ManagedQuery within PT0.004529S
127.0.0.1 - - [06/Jan/2023:20:45:55 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D/queries/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D.25ed36ad-74b4-4a52-a7c1-92058e798d97 HTTP/1.1" 200 1780 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:55,945] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]], queryId=25ed36ad-74b4-4a52-a7c1-92058e798d97, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:55.911893, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2428680c[Count = 0], startTime=2023-01-06T20:45:55.912142, finishTime=2023-01-06T20:45:55.916671, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b0e4105), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@47e35830, com.bakdata.conquery.models.query.ColumnDescriptor@4f9642f5]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:55,945] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]], queryId=25ed36ad-74b4-4a52-a7c1-92058e798d97, label=test_tree-test_child1	@§$, creationTime=2023-01-06T20:45:55.911893, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2428680c[Count = 0], startTime=2023-01-06T20:45:55.912142, finishTime=2023-01-06T20:45:55.916671, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b0e4105), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@47e35830, com.bakdata.conquery.models.query.ColumnDescriptor@4f9642f5]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]]
127.0.0.1 - - [06/Jan/2023:20:45:55 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B2%5D/result/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D.25ed36ad-74b4-4a52-a7c1-92058e798d97.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:45:55,964] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:55,964] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY Test[2]
INFO  [2023-01-06 20:45:55,965] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-06 20:45:55,965] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-06 20:45:55,965] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[2]_b56e211f-340b-4cef-9710-92fc1a098844
INFO  [2023-01-06 20:45:55,965] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[2]_7dbebef0-2cc1-4ca0-a51d-a79e217b02a3
INFO  [2023-01-06 20:45:55,967] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY Test[2]
INFO  [2023-01-06 20:45:55,968] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[2]_7dbebef0-2cc1-4ca0-a51d-a79e217b02a3
INFO  [2023-01-06 20:45:55,968] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[2]_b56e211f-340b-4cef-9710-92fc1a098844
INFO  [2023-01-06 20:45:56,067] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY$20Test[2]
INFO  [2023-01-06 20:45:56,067] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,097] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-06 20:45:56,097] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-06 20:45:56,098] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:56,098] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:56,099] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-06 20:45:56,099] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-06 20:45:56,099] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:56,099] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_dab42f99-42f0-4859-82b0-54a32adf0589 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_dab42f99-42f0-4859-82b0-54a32adf0589 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_2dd0fa6c-3a08-4fad-9c8c-5451d48bf614 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_2dd0fa6c-3a08-4fad-9c8c-5451d48bf614 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:56,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:56,204] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,210] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,211] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table
INFO  [2023-01-06 20:45:56,211] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table
INFO  [2023-01-06 20:45:56,326] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,437] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:56,437] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:56,437] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 96 B in total
INFO  [2023-01-06 20:45:56,437] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000404457sINFO  [2023-01-06 20:45:56,478] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:56,478] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:56,478] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7ba638ee)
INFO  [2023-01-06 20:45:56,481] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:56,481] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:56,481] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:56,501] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table
INFO  [2023-01-06 20:45:56,501] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:56 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:56,503] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:56,504] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:56,504] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:56,507] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:56,507] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table], containing 4 entries.
INFO  [2023-01-06 20:45:56,507] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table], containing 4 entries.
WARN  [2023-01-06 20:45:56,508] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:56,508] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table.0
INFO  [2023-01-06 20:45:56,509] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table.1
INFO  [2023-01-06 20:45:56,614] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,619] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,629] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:56,630] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:56,630] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:56,735] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test QUERY INIT
INFO  [2023-01-06 20:45:56,750] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:56,750] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ea19fe32-9f30-493a-a5c7-3d357f233ada] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test))]]
INFO  [2023-01-06 20:45:56,753] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada
INFO  [2023-01-06 20:45:56,753] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada
INFO  [2023-01-06 20:45:56,753] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada] with 0 results within PT0.000538S
INFO  [2023-01-06 20:45:56,754] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada] with 2 results within PT0.000976S
INFO  [2023-01-06 20:45:56,754] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada, workerId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_2dd0fa6c-3a08-4fad-9c8c-5451d48bf614, startTime=2023-01-06T20:45:56.753436, finishTime=2023-01-06T20:45:56.753974) of size 0
127.0.0.1 - - [06/Jan/2023:20:45:56 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test/queries HTTP/1.1" 201 1198 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:45:56,755] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada, workerId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_dab42f99-42f0-4859-82b0-54a32adf0589, startTime=2023-01-06T20:45:56.753461, finishTime=2023-01-06T20:45:56.754437) of size 2
INFO  [2023-01-06 20:45:56,755] com.bakdata.conquery.models.execution.ManagedExecution: DONE ea19fe32-9f30-493a-a5c7-3d357f233ada ManagedQuery within PT0.004611S
127.0.0.1 - - [06/Jan/2023:20:45:56 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test/queries/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada HTTP/1.1" 200 1585 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:56,780] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test], queryId=ea19fe32-9f30-493a-a5c7-3d357f233ada, label=test_tree-Ä1	@§$, creationTime=2023-01-06T20:45:56.750479, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5cd0fcec[Count = 0], startTime=2023-01-06T20:45:56.750682, finishTime=2023-01-06T20:45:56.755293, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@398c83ac), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d339d62, com.bakdata.conquery.models.query.ColumnDescriptor@7fc3fa6f]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:56,780] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test], queryId=ea19fe32-9f30-493a-a5c7-3d357f233ada, label=test_tree-Ä1	@§$, creationTime=2023-01-06T20:45:56.750479, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5cd0fcec[Count = 0], startTime=2023-01-06T20:45:56.750682, finishTime=2023-01-06T20:45:56.755293, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@398c83ac), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d339d62, com.bakdata.conquery.models.query.ColumnDescriptor@7fc3fa6f]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
127.0.0.1 - - [06/Jan/2023:20:45:56 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA%20Test/result/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.ea19fe32-9f30-493a-a5c7-3d357f233ada.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 36
INFO  [2023-01-06 20:45:56,816] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test on 3 rows
INFO  [2023-01-06 20:45:56,816] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-06 20:45:56,816] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-06 20:45:56,816] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-06 20:45:56,816] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_2dd0fa6c-3a08-4fad-9c8c-5451d48bf614
INFO  [2023-01-06 20:45:56,816] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_dab42f99-42f0-4859-82b0-54a32adf0589
INFO  [2023-01-06 20:45:56,915] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_dab42f99-42f0-4859-82b0-54a32adf0589
INFO  [2023-01-06 20:45:56,915] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-06 20:45:56,915] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_2dd0fa6c-3a08-4fad-9c8c-5451d48bf614
INFO  [2023-01-06 20:45:57,015] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test
INFO  [2023-01-06 20:45:57,015] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,035] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-06 20:45:57,035] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:57,035] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:57,036] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:57,037] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:57,037] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:57,037] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:57,037] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:57,039] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_b0081810-e751-4188-85cc-f9bbbc9f89ca are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:57,039] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_b0081810-e751-4188-85cc-f9bbbc9f89ca are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:57,039] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:57,039] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_a17e396d-be8b-4f78-8f35-f12d011d6d62 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:57,039] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_a17e396d-be8b-4f78-8f35-f12d011d6d62 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:57,039] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:57,043] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,143] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,150] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,150] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:57,150] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:57,268] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,378] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:57,379] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:57,379] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 89 B in total
INFO  [2023-01-06 20:45:57,379] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_VIRTUAL_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000347065sINFO  [2023-01-06 20:45:57,414] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:57,414] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:57,414] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@641a49fc)
INFO  [2023-01-06 20:45:57,416] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_VIRTUAL_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:57,416] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:57,416] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SIMPLE_VIRTUAL_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:57,434] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:57 +0000] "POST /admin/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SIMPLE_VIRTUAL_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:45:57,435] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,436] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:57,437] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:57,437] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:57,439] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:45:57,439] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-06 20:45:57,439] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-06 20:45:57,440] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:57,441] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:57,441] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:57,545] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,551] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,565] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,566] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:57,566] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:57,672] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_VIRTUAL_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:57,687] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:57,687] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7846ea80-3c55-4cf1-b500-915782c355d8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_VIRTUAL_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:57,690] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8
INFO  [2023-01-06 20:45:57,691] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8
INFO  [2023-01-06 20:45:57,691] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8] with 0 results within PT0.000741S
INFO  [2023-01-06 20:45:57,692] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8] with 2 results within PT0.001111S
127.0.0.1 - - [06/Jan/2023:20:45:57 +0000] "POST /api/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1326 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:57,692] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8, workerId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_a17e396d-be8b-4f78-8f35-f12d011d6d62, startTime=2023-01-06T20:45:57.691076, finishTime=2023-01-06T20:45:57.691817) of size 0
INFO  [2023-01-06 20:45:57,692] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8, workerId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_b0081810-e751-4188-85cc-f9bbbc9f89ca, startTime=2023-01-06T20:45:57.690902, finishTime=2023-01-06T20:45:57.692013) of size 2
INFO  [2023-01-06 20:45:57,692] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7846ea80-3c55-4cf1-b500-915782c355d8 ManagedQuery within PT0.00465S
127.0.0.1 - - [06/Jan/2023:20:45:57 +0000] "GET /api/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test/queries/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8 HTTP/1.1" 200 1657 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:45:57,720] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test], queryId=7846ea80-3c55-4cf1-b500-915782c355d8, label=geschlecht_select	@§$, creationTime=2023-01-06T20:45:57.687693, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@61dd8b33[Count = 0], startTime=2023-01-06T20:45:57.687882, finishTime=2023-01-06T20:45:57.692532, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@21791fe1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_VIRTUAL_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5d7de5b8, com.bakdata.conquery.models.query.ColumnDescriptor@69fc8eaa]) download on dataset Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:57,720] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test], queryId=7846ea80-3c55-4cf1-b500-915782c355d8, label=geschlecht_select	@§$, creationTime=2023-01-06T20:45:57.687693, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@61dd8b33[Count = 0], startTime=2023-01-06T20:45:57.687882, finishTime=2023-01-06T20:45:57.692532, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@21791fe1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_VIRTUAL_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5d7de5b8, com.bakdata.conquery.models.query.ColumnDescriptor@69fc8eaa]) on dataset Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:57 +0000] "GET /api/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY%20Test/result/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.7846ea80-3c55-4cf1-b500-915782c355d8.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:57,739] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_VIRTUAL_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:45:57,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:57,739] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:57,739] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:57,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_a17e396d-be8b-4f78-8f35-f12d011d6d62
INFO  [2023-01-06 20:45:57,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_b0081810-e751-4188-85cc-f9bbbc9f89ca
INFO  [2023-01-06 20:45:57,837] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:57,838] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_b0081810-e751-4188-85cc-f9bbbc9f89ca
INFO  [2023-01-06 20:45:57,838] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_a17e396d-be8b-4f78-8f35-f12d011d6d62
INFO  [2023-01-06 20:45:57,843] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:57,843] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:57,973] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:57,973] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:57,973] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:57,973] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:57,974] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:57,974] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:57,974] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:57,974] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:57,976] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9f28d261-767f-4f71-ba6a-916fffbc1fc7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:57,976] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9f28d261-767f-4f71-ba6a-916fffbc1fc7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:57,976] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:57,976] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_fc9ff8af-2c85-45f4-a933-77dc952e7599 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:57,976] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_fc9ff8af-2c85-45f4-a933-77dc952e7599 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:57,976] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:57,981] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,080] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,087] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,088] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:58,088] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:58,209] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,318] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:58,318] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:58,318] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:45:58,318] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000307663sINFO  [2023-01-06 20:45:58,350] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:58,350] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:58,350] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3dbebf2a)
INFO  [2023-01-06 20:45:58,352] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:58,352] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:58,352] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:58,364] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:45:58 +0000] "POST /admin/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:45:58,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,365] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:58,366] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:58,366] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:58,368] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:58,368] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:45:58,368] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:45:58,369] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
WARN  [2023-01-06 20:45:58,369] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:58,369] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:45:58,369] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:58,474] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,480] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,497] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,497] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:58,497] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:58,602] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:58,619] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:58,620] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a029a2dc-5e70-43aa-b851-3770fa62bb66] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:58,625] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66
INFO  [2023-01-06 20:45:58,625] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66
127.0.0.1 - - [06/Jan/2023:20:45:58 +0000] "POST /api/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1555 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:58,626] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66] with 0 results within PT0.001318S
INFO  [2023-01-06 20:45:58,626] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66] with 1 results within PT0.001713S
INFO  [2023-01-06 20:45:58,627] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66, workerId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9f28d261-767f-4f71-ba6a-916fffbc1fc7, startTime=2023-01-06T20:45:58.625187, finishTime=2023-01-06T20:45:58.626505) of size 0
INFO  [2023-01-06 20:45:58,627] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66, workerId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_fc9ff8af-2c85-45f4-a933-77dc952e7599, startTime=2023-01-06T20:45:58.625142, finishTime=2023-01-06T20:45:58.626855) of size 1
INFO  [2023-01-06 20:45:58,627] com.bakdata.conquery.models.execution.ManagedExecution: DONE a029a2dc-5e70-43aa-b851-3770fa62bb66 ManagedQuery within PT0.007171S
127.0.0.1 - - [06/Jan/2023:20:45:58 +0000] "GET /api/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66 HTTP/1.1" 200 1962 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:45:58,653] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=a029a2dc-5e70-43aa-b851-3770fa62bb66, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:58.620191, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@36b443dd[Count = 0], startTime=2023-01-06T20:45:58.620382, finishTime=2023-01-06T20:45:58.627553, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7c57d6a1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3ed1a40a, com.bakdata.conquery.models.query.ColumnDescriptor@199fb9ef]) download on dataset Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:58,653] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=a029a2dc-5e70-43aa-b851-3770fa62bb66, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:58.620191, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@36b443dd[Count = 0], startTime=2023-01-06T20:45:58.620382, finishTime=2023-01-06T20:45:58.627553, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7c57d6a1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3ed1a40a, com.bakdata.conquery.models.query.ColumnDescriptor@199fb9ef]) on dataset Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:58 +0000] "GET /api/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.a029a2dc-5e70-43aa-b851-3770fa62bb66.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 22
INFO  [2023-01-06 20:45:58,673] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 2 rows
INFO  [2023-01-06 20:45:58,673] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:58,673] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:58,673] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:58,673] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_9f28d261-767f-4f71-ba6a-916fffbc1fc7
INFO  [2023-01-06 20:45:58,673] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_fc9ff8af-2c85-45f4-a933-77dc952e7599
INFO  [2023-01-06 20:45:58,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_9f28d261-767f-4f71-ba6a-916fffbc1fc7
INFO  [2023-01-06 20:45:58,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_fc9ff8af-2c85-45f4-a933-77dc952e7599
INFO  [2023-01-06 20:45:58,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:58,772] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:58,772] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,802] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:58,802] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:58,802] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:58,802] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:58,803] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:58,803] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:58,803] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:58,803] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_ecd482d3-1ba9-48a5-b877-b2da0d9a664a are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_ecd482d3-1ba9-48a5-b877-b2da0d9a664a are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6cd78c8d-b299-4221-8e34-c8d829123abc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6cd78c8d-b299-4221-8e34-c8d829123abc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:58,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:58,908] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,915] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:58,915] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:58,915] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:59,028] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,137] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:59,137] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:59,138] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:45:59,138] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00038782sINFO  [2023-01-06 20:45:59,177] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:45:59,177] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:45:59,177] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@608d256)
INFO  [2023-01-06 20:45:59,179] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:59,179] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:45:59,179] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:45:59,194] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:59,194] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:45:59 +0000] "POST /admin/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:45:59,196] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:45:59,197] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:45:59,197] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:45:59,200] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-06 20:45:59,200] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:45:59,200] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-06 20:45:59,201] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:45:59,202] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:45:59,202] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:45:59,202] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-06 20:45:59,307] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,312] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,324] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:59,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:45:59,430] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:45:59,446] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:45:59,446] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[6f8fa8ac-f252-4bd4-9153-b096e268eddc] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:45:59,451] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc
INFO  [2023-01-06 20:45:59,451] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc
127.0.0.1 - - [06/Jan/2023:20:45:59 +0000] "POST /api/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1632 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:45:59,452] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc] with 3 results within PT0.001278S
INFO  [2023-01-06 20:45:59,452] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc] with 4 results within PT0.001762S
INFO  [2023-01-06 20:45:59,453] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc, workerId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_6cd78c8d-b299-4221-8e34-c8d829123abc, startTime=2023-01-06T20:45:59.451443, finishTime=2023-01-06T20:45:59.452721) of size 3
INFO  [2023-01-06 20:45:59,453] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc, workerId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_ecd482d3-1ba9-48a5-b877-b2da0d9a664a, startTime=2023-01-06T20:45:59.451217, finishTime=2023-01-06T20:45:59.452979) of size 4
INFO  [2023-01-06 20:45:59,453] com.bakdata.conquery.models.execution.ManagedExecution: DONE 6f8fa8ac-f252-4bd4-9153-b096e268eddc ManagedQuery within PT0.007018S
127.0.0.1 - - [06/Jan/2023:20:45:59 +0000] "GET /api/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc HTTP/1.1" 200 2075 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:45:59,482] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=6f8fa8ac-f252-4bd4-9153-b096e268eddc, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:59.446446, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@49ff91c7[Count = 0], startTime=2023-01-06T20:45:59.446654, finishTime=2023-01-06T20:45:59.453672, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@31caa9c8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1cb10285, com.bakdata.conquery.models.query.ColumnDescriptor@646f3d17]) download on dataset Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:45:59,483] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=6f8fa8ac-f252-4bd4-9153-b096e268eddc, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:45:59.446446, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@49ff91c7[Count = 0], startTime=2023-01-06T20:45:59.446654, finishTime=2023-01-06T20:45:59.453672, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@31caa9c8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1cb10285, com.bakdata.conquery.models.query.ColumnDescriptor@646f3d17]) on dataset Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:45:59 +0000] "GET /api/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.6f8fa8ac-f252-4bd4-9153-b096e268eddc.csv?pretty=false HTTP/1.1" 200 48 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:45:59,501] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 8 rows
INFO  [2023-01-06 20:45:59,501] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:59,502] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:59,502] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:59,502] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_6cd78c8d-b299-4221-8e34-c8d829123abc
INFO  [2023-01-06 20:45:59,502] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_ecd482d3-1ba9-48a5-b877-b2da0d9a664a
INFO  [2023-01-06 20:45:59,503] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:59,504] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_6cd78c8d-b299-4221-8e34-c8d829123abc
INFO  [2023-01-06 20:45:59,504] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_ecd482d3-1ba9-48a5-b877-b2da0d9a664a
INFO  [2023-01-06 20:45:59,602] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:45:59,602] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,630] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:59,630] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:45:59,630] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:45:59,630] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:45:59,631] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:59,631] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:45:59,631] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:59,631] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:45:59,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_1cf1ed8d-a881-4617-8c94-297b9ec704f5 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:59,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_1cf1ed8d-a881-4617-8c94-297b9ec704f5 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:59,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:59,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_87ba0c2a-c21a-4edb-8a05-8388f3706bd8 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:45:59,633] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_87ba0c2a-c21a-4edb-8a05-8388f3706bd8 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:45:59,633] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:45:59,633] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,737] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,744] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,745] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:59,745] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:45:59,863] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:45:59,972] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:45:59,972] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:45:59,972] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 197 B in total
INFO  [2023-01-06 20:45:59,973] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000406882sINFO  [2023-01-06 20:46:00,014] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=13, min=2, average=2.166667, max=3}
INFO  [2023-01-06 20:46:00,014] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=4), subType=IntegerParser(super=Parser(lines=13, nullLines=4), minValue=14246, maxValue=14642), dateReader=com.bakdata.conquery.util.DateReader@3d1f9926)
INFO  [2023-01-06 20:46:00,014] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=13, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:46:00,016] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:00,016] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:00,016] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:00,028] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-06 20:46:00,029] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:00 +0000] "POST /admin/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SUM_EMPTY_DATE_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:46:00,030] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:00,030] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:00,030] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:00,032] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:00,032] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 13 entries.
INFO  [2023-01-06 20:46:00,032] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 13 entries.
WARN  [2023-01-06 20:46:00,033] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:00,034] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-06 20:46:00,034] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-06 20:46:00,139] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,144] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,156] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,156] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:00,156] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:00,269] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_EMPTY_DATE_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-06 20:46:00,283] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:00,284] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[dbaac3c8-a04e-4469-92e1-731c270a6b9f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_EMPTY_DATE_CONCEPT_QUERY Test))]]
INFO  [2023-01-06 20:46:00,288] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f
INFO  [2023-01-06 20:46:00,288] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f
127.0.0.1 - - [06/Jan/2023:20:46:00 +0000] "POST /api/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1407 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:00,290] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f] with 1 results within PT0.001378S
INFO  [2023-01-06 20:46:00,290] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f] with 1 results within PT0.001535S
INFO  [2023-01-06 20:46:00,290] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f, workerId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_87ba0c2a-c21a-4edb-8a05-8388f3706bd8, startTime=2023-01-06T20:46:00.288968, finishTime=2023-01-06T20:46:00.290346) of size 1
INFO  [2023-01-06 20:46:00,291] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f, workerId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_1cf1ed8d-a881-4617-8c94-297b9ec704f5, startTime=2023-01-06T20:46:00.288962, finishTime=2023-01-06T20:46:00.290497) of size 1
INFO  [2023-01-06 20:46:00,291] com.bakdata.conquery.models.execution.ManagedExecution: DONE dbaac3c8-a04e-4469-92e1-731c270a6b9f ManagedQuery within PT0.006884S
127.0.0.1 - - [06/Jan/2023:20:46:00 +0000] "GET /api/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f HTTP/1.1" 200 1738 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:00,313] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=dbaac3c8-a04e-4469-92e1-731c270a6b9f, label=vs	@§$, creationTime=2023-01-06T20:46:00.284226, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@574567c7[Count = 0], startTime=2023-01-06T20:46:00.284385, finishTime=2023-01-06T20:46:00.291269, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1074e064), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@236ad99e, com.bakdata.conquery.models.query.ColumnDescriptor@39f1c13f]) download on dataset Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:00,313] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=dbaac3c8-a04e-4469-92e1-731c270a6b9f, label=vs	@§$, creationTime=2023-01-06T20:46:00.284226, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@574567c7[Count = 0], startTime=2023-01-06T20:46:00.284385, finishTime=2023-01-06T20:46:00.291269, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1074e064), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@236ad99e, com.bakdata.conquery.models.query.ColumnDescriptor@39f1c13f]) on dataset Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:46:00 +0000] "GET /api/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/result/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.dbaac3c8-a04e-4469-92e1-731c270a6b9f.csv?pretty=false HTTP/1.1" 200 114 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:00,333] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SUM_EMPTY_DATE_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-06 20:46:00,333] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:46:00,333] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:46:00,333] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-06 20:46:00,333] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_1cf1ed8d-a881-4617-8c94-297b9ec704f5
INFO  [2023-01-06 20:46:00,333] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_87ba0c2a-c21a-4edb-8a05-8388f3706bd8
INFO  [2023-01-06 20:46:00,334] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_87ba0c2a-c21a-4edb-8a05-8388f3706bd8
INFO  [2023-01-06 20:46:00,432] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:46:00,432] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_1cf1ed8d-a881-4617-8c94-297b9ec704f5
INFO  [2023-01-06 20:46:00,434] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_EMPTY_DATE_CONCEPT_QUERY$20Test
INFO  [2023-01-06 20:46:00,434] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,569] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-06 20:46:00,569] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test TABLE_EXPORT Test
INFO  [2023-01-06 20:46:00,569] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:00,569] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:00,570] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-06 20:46:00,570] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:00,570] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-06 20:46:00,570] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:00,572] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_df9e9256-4224-417f-abed-cecb0a4bc545 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:00,572] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_df9e9256-4224-417f-abed-cecb0a4bc545 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:00,572] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:00,572] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_1a83afc2-585f-408e-a0b3-9cb0b3426cb7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:00,572] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_1a83afc2-585f-408e-a0b3-9cb0b3426cb7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:00,572] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:00,576] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,676] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[TABLE_EXPORT$20Test.sid]
INFO  [2023-01-06 20:46:00,676] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,677] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId TABLE_EXPORT$20Test.sid
INFO  [2023-01-06 20:46:00,677] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId TABLE_EXPORT$20Test.sid
INFO  [2023-01-06 20:46:00,783] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:00,783] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table1
INFO  [2023-01-06 20:46:00,783] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table1
INFO  [2023-01-06 20:46:00,784] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table2
INFO  [2023-01-06 20:46:00,824] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table2
INFO  [2023-01-06 20:46:00,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,052] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:01,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:01,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:01,052] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 433 B in total
INFO  [2023-01-06 20:46:01,053] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TABLE_EXPORT Test/table1.import.json, tag=Optional.empty)
████████████████████████▌                         ▌  49%	est. time remaining: 0.034348927sINFO  [2023-01-06 20:46:01,086] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-06 20:46:01,086] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:46:01,086] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-06 20:46:01,086] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@2e7c146a), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@57ba9b76), dateReader=com.bakdata.conquery.util.DateReader@20bed6a1, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-06 20:46:01,089] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TABLE_EXPORT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:01,089] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TABLE_EXPORT Test/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000700813sINFO  [2023-01-06 20:46:01,123] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-06 20:46:01,124] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-06 20:46:01,124] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-06 20:46:01,124] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@4c23fb31), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@7a400ee0), dateReader=com.bakdata.conquery.util.DateReader@32cfa96d, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-06 20:46:01,127] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TABLE_EXPORT Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:01,127] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-06 20:46:01,127] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TABLE_EXPORT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:01,127] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_TABLE_EXPORT Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:01,143] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into TABLE_EXPORT$20Test.table1
127.0.0.1 - - [06/Jan/2023:20:46:01 +0000] "POST /admin/datasets/TABLE_EXPORT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_TABLE_EXPORT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:46:01,144] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:01,145] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:01,145] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:01,147] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:46:01,147] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-06 20:46:01,147] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table1.table1], containing 6 entries.
WARN  [2023-01-06 20:46:01,148] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:01,148] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TABLE_EXPORT$20Test.table1.table1.0
INFO  [2023-01-06 20:46:01,173] com.bakdata.conquery.models.jobs.ImportJob: Importing table2 into TABLE_EXPORT$20Test.table2
INFO  [2023-01-06 20:46:01,173] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:01,173] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:01,173] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [06/Jan/2023:20:46:01 +0000] "POST /admin/datasets/TABLE_EXPORT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_TABLE_EXPORT+Test%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:01,174] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,174] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-06 20:46:01,174] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:01,174] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table2.table2], containing 6 entries.
INFO  [2023-01-06 20:46:01,174] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table2.table2], containing 6 entries.
INFO  [2023-01-06 20:46:01,174] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TABLE_EXPORT$20Test.table2.table2.0
INFO  [2023-01-06 20:46:01,279] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,284] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,306] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,306] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:01,412] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: TABLE_EXPORT Test QUERY INIT
INFO  [2023-01-06 20:46:01,431] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[TABLE_EXPORT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:01,431] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[8a9d0a2c-38cb-4d77-87f1-9e92a500a396] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TABLE_EXPORT Test))]]
INFO  [2023-01-06 20:46:01,437] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started TableExportQuery TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396
INFO  [2023-01-06 20:46:01,437] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started TableExportQuery TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396
WARN  [2023-01-06 20:46:01,437] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:46:01,437] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396] with 0 results within PT0.000319S
127.0.0.1 - - [06/Jan/2023:20:46:01 +0000] "POST /api/datasets/TABLE_EXPORT$20Test/queries HTTP/1.1" 201 2067 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:46:01,438] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396, workerId=TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_df9e9256-4224-417f-abed-cecb0a4bc545, startTime=2023-01-06T20:46:01.437319, finishTime=2023-01-06T20:46:01.437638) of size 0
INFO  [2023-01-06 20:46:01,438] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396] with 2 results within PT0.001257S
INFO  [2023-01-06 20:46:01,440] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396, workerId=TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_1a83afc2-585f-408e-a0b3-9cb0b3426cb7, startTime=2023-01-06T20:46:01.437318, finishTime=2023-01-06T20:46:01.438575) of size 2
INFO  [2023-01-06 20:46:01,440] com.bakdata.conquery.models.execution.ManagedExecution: DONE 8a9d0a2c-38cb-4d77-87f1-9e92a500a396 ManagedQuery within PT0.009009S
127.0.0.1 - - [06/Jan/2023:20:46:01 +0000] "GET /api/datasets/TABLE_EXPORT$20Test/queries/TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396 HTTP/1.1" 200 2334 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:01,466] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TABLE_EXPORT Test], queryId=8a9d0a2c-38cb-4d77-87f1-9e92a500a396, label=concept	@§$, creationTime=2023-01-06T20:46:01.431547, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@898bcd0[Count = 0], startTime=2023-01-06T20:46:01.431947, finishTime=2023-01-06T20:46:01.440956, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@b57ff76), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TABLE_EXPORT Test)), query=com.bakdata.conquery.apiv1.query.TableExportQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6aa6b5b8, com.bakdata.conquery.models.query.ColumnDescriptor@75ff69dc, com.bakdata.conquery.models.query.ColumnDescriptor@384c258f, com.bakdata.conquery.models.query.ColumnDescriptor@c54d1bf, com.bakdata.conquery.models.query.ColumnDescriptor@5300aef8]) download on dataset Dataset[label=null, name=TABLE_EXPORT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:01,466] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TABLE_EXPORT Test], queryId=8a9d0a2c-38cb-4d77-87f1-9e92a500a396, label=concept	@§$, creationTime=2023-01-06T20:46:01.431547, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@898bcd0[Count = 0], startTime=2023-01-06T20:46:01.431947, finishTime=2023-01-06T20:46:01.440956, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@b57ff76), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TABLE_EXPORT Test)), query=com.bakdata.conquery.apiv1.query.TableExportQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6aa6b5b8, com.bakdata.conquery.models.query.ColumnDescriptor@75ff69dc, com.bakdata.conquery.models.query.ColumnDescriptor@384c258f, com.bakdata.conquery.models.query.ColumnDescriptor@c54d1bf, com.bakdata.conquery.models.query.ColumnDescriptor@5300aef8]) on dataset Dataset[label=null, name=TABLE_EXPORT Test]
127.0.0.1 - - [06/Jan/2023:20:46:01 +0000] "GET /api/datasets/TABLE_EXPORT%20Test/result/TABLE_EXPORT$20Test.8a9d0a2c-38cb-4d77-87f1-9e92a500a396.csv?pretty=false HTTP/1.1" 200 297 "-" "Conquery (test client)" 28
INFO  [2023-01-06 20:46:01,493] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest TABLE_EXPORT Test on 8 rows
INFO  [2023-01-06 20:46:01,493] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast TABLE_EXPORT Test
INFO  [2023-01-06 20:46:01,494] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-06 20:46:01,494] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-06 20:46:01,494] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TABLE_EXPORT Test_df9e9256-4224-417f-abed-cecb0a4bc545
INFO  [2023-01-06 20:46:01,494] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TABLE_EXPORT Test_1a83afc2-585f-408e-a0b3-9cb0b3426cb7
INFO  [2023-01-06 20:46:01,591] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TABLE_EXPORT Test_1a83afc2-585f-408e-a0b3-9cb0b3426cb7
INFO  [2023-01-06 20:46:01,591] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow TABLE_EXPORT Test
INFO  [2023-01-06 20:46:01,591] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TABLE_EXPORT Test_df9e9256-4224-417f-abed-cecb0a4bc545
INFO  [2023-01-06 20:46:01,591] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of TABLE_EXPORT$20Test
INFO  [2023-01-06 20:46:01,591] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,712] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test TABLE_EXPORT Test
INFO  [2023-01-06 20:46:01,712] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before (with Aggregation)
INFO  [2023-01-06 20:46:01,712] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:01,712] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:01,713] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-06 20:46:01,713] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-06 20:46:01,713] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:01,713] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:01,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_ca8a0e0f-1ce2-4cc4-a006-768181635405 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:01,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_ca8a0e0f-1ce2-4cc4-a006-768181635405 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:01,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:01,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_b607c33b-e442-434b-95d4-b566451e78c9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:01,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_b607c33b-e442-434b-95d4-b566451e78c9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:01,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:01,720] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,819] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,826] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:01,826] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20(with$20Aggregation).table
INFO  [2023-01-06 20:46:01,826] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20(with$20Aggregation).table
INFO  [2023-01-06 20:46:01,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,052] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:02,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:02,052] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 153 B in total
INFO  [2023-01-06 20:46:02,052] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before (with Aggregation)/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000215396sINFO  [2023-01-06 20:46:02,074] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=2, average=2.000000, max=2}
INFO  [2023-01-06 20:46:02,074] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14975, maxValue=14980), dateReader=com.bakdata.conquery.util.DateReader@297e55ce)
INFO  [2023-01-06 20:46:02,074] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:02,083] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before (with Aggregation)/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:02,083] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:02,083] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before (with Aggregation)/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:02,101] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Temporal$20Before$20(with$20Aggregation).table
INFO  [2023-01-06 20:46:02,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:02 +0000] "POST /admin/datasets/Temporal%20Before%20(with%20Aggregation)/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Temporal+Before+%28with+Aggregation%29%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:02,102] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:02,102] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:02,102] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:02,114] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:02,115] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20(with$20Aggregation).table.table], containing 8 entries.
INFO  [2023-01-06 20:46:02,115] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20(with$20Aggregation).table.table], containing 8 entries.
WARN  [2023-01-06 20:46:02,115] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:02,116] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20(with$20Aggregation).table.table.1
INFO  [2023-01-06 20:46:02,116] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20(with$20Aggregation).table.table.0
INFO  [2023-01-06 20:46:02,220] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,226] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,239] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,239] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:02,239] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:02,345] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before (with Aggregation) QUERY INIT
INFO  [2023-01-06 20:46:02,362] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before$20(with$20Aggregation)] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:02,362] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[afb780e6-8ef5-44fe-a276-2055a4a35062] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before (with Aggregation)))]]
INFO  [2023-01-06 20:46:02,364] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062
127.0.0.1 - - [06/Jan/2023:20:46:02 +0000] "POST /api/datasets/Temporal$20Before$20(with$20Aggregation)/queries HTTP/1.1" 201 2251 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:46:02,365] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062
INFO  [2023-01-06 20:46:02,365] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062] with 0 results within PT0.001131S
INFO  [2023-01-06 20:46:02,366] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062, workerId=Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_b607c33b-e442-434b-95d4-b566451e78c9, startTime=2023-01-06T20:46:02.364846, finishTime=2023-01-06T20:46:02.365977) of size 0
INFO  [2023-01-06 20:46:02,366] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062] with 1 results within PT0.001093S
INFO  [2023-01-06 20:46:02,367] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062, workerId=Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_ca8a0e0f-1ce2-4cc4-a006-768181635405, startTime=2023-01-06T20:46:02.365675, finishTime=2023-01-06T20:46:02.366768) of size 1
INFO  [2023-01-06 20:46:02,367] com.bakdata.conquery.models.execution.ManagedExecution: DONE afb780e6-8ef5-44fe-a276-2055a4a35062 ManagedQuery within PT0.004752S
127.0.0.1 - - [06/Jan/2023:20:46:02 +0000] "GET /api/datasets/Temporal$20Before$20(with$20Aggregation)/queries/Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062 HTTP/1.1" 200 2602 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:02,388] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before (with Aggregation)], queryId=afb780e6-8ef5-44fe-a276-2055a4a35062, label=concept	@§$, creationTime=2023-01-06T20:46:02.362414, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@21b5db24[Count = 0], startTime=2023-01-06T20:46:02.362537, finishTime=2023-01-06T20:46:02.367289, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4095bd32), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before (with Aggregation))), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@731b29fd, com.bakdata.conquery.models.query.ColumnDescriptor@629e8807, com.bakdata.conquery.models.query.ColumnDescriptor@6d7d038b, com.bakdata.conquery.models.query.ColumnDescriptor@ec5f3b]) download on dataset Dataset[label=null, name=Temporal Before (with Aggregation)] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:02,388] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before (with Aggregation)], queryId=afb780e6-8ef5-44fe-a276-2055a4a35062, label=concept	@§$, creationTime=2023-01-06T20:46:02.362414, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@21b5db24[Count = 0], startTime=2023-01-06T20:46:02.362537, finishTime=2023-01-06T20:46:02.367289, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4095bd32), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before (with Aggregation))), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@731b29fd, com.bakdata.conquery.models.query.ColumnDescriptor@629e8807, com.bakdata.conquery.models.query.ColumnDescriptor@6d7d038b, com.bakdata.conquery.models.query.ColumnDescriptor@ec5f3b]) on dataset Dataset[label=null, name=Temporal Before (with Aggregation)]
127.0.0.1 - - [06/Jan/2023:20:46:02 +0000] "GET /api/datasets/Temporal%20Before%20(with%20Aggregation)/result/Temporal$20Before$20(with$20Aggregation).afb780e6-8ef5-44fe-a276-2055a4a35062.csv?pretty=false HTTP/1.1" 200 73 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:46:02,404] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before (with Aggregation) on 2 rows
INFO  [2023-01-06 20:46:02,404] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before (with Aggregation)
INFO  [2023-01-06 20:46:02,405] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-06 20:46:02,405] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-06 20:46:02,405] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before (with Aggregation)_b607c33b-e442-434b-95d4-b566451e78c9
INFO  [2023-01-06 20:46:02,405] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before (with Aggregation)_ca8a0e0f-1ce2-4cc4-a006-768181635405
INFO  [2023-01-06 20:46:02,418] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before (with Aggregation)
INFO  [2023-01-06 20:46:02,418] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before (with Aggregation)_ca8a0e0f-1ce2-4cc4-a006-768181635405
INFO  [2023-01-06 20:46:02,418] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before (with Aggregation)_b607c33b-e442-434b-95d4-b566451e78c9
INFO  [2023-01-06 20:46:02,519] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before$20(with$20Aggregation)
INFO  [2023-01-06 20:46:02,519] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,545] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before (with Aggregation)
INFO  [2023-01-06 20:46:02,546] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before
INFO  [2023-01-06 20:46:02,546] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:02,546] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:02,549] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-06 20:46:02,549] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-06 20:46:02,549] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:02,549] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:02,550] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before.worker_Temporal$20Before_ffa2c0d7-4daf-482a-abe9-8b7219187210 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:02,550] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before.worker_Temporal$20Before_ffa2c0d7-4daf-482a-abe9-8b7219187210 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:02,550] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:02,551] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before.worker_Temporal$20Before_34023a70-ba30-46f2-8333-dd1ce4528666 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:02,551] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before.worker_Temporal$20Before_34023a70-ba30-46f2-8333-dd1ce4528666 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:02,551] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:02,555] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,654] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,661] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,662] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before.table
INFO  [2023-01-06 20:46:02,662] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before.table
INFO  [2023-01-06 20:46:02,779] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,889] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:02,889] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:02,889] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 153 B in total
INFO  [2023-01-06 20:46:02,889] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000269554sINFO  [2023-01-06 20:46:02,916] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=2, average=2.000000, max=2}
INFO  [2023-01-06 20:46:02,916] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:02,916] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14975, maxValue=14980), dateReader=com.bakdata.conquery.util.DateReader@676fcbcf)
INFO  [2023-01-06 20:46:02,920] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:02,920] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:02,920] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:02,937] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Temporal$20Before.table
127.0.0.1 - - [06/Jan/2023:20:46:02 +0000] "POST /admin/datasets/Temporal%20Before/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Temporal+Before%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:02,937] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:02,938] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:02,939] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:02,939] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:02,942] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:02,942] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before.table.table], containing 8 entries.
INFO  [2023-01-06 20:46:02,942] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before.table.table], containing 8 entries.
WARN  [2023-01-06 20:46:02,944] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:02,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before.table.table.0
INFO  [2023-01-06 20:46:02,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before.table.table.1
INFO  [2023-01-06 20:46:03,049] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,054] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,065] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,066] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:03,066] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:03,174] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before QUERY INIT
INFO  [2023-01-06 20:46:03,187] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:03,187] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[bd5d19b2-94d5-42f0-9a4b-0111043529f1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before))]]
INFO  [2023-01-06 20:46:03,190] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1
INFO  [2023-01-06 20:46:03,190] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1
INFO  [2023-01-06 20:46:03,191] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1] with 0 results within PT0.000741S
127.0.0.1 - - [06/Jan/2023:20:46:03 +0000] "POST /api/datasets/Temporal$20Before/queries HTTP/1.1" 201 1603 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:03,191] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1] with 1 results within PT0.001066S
INFO  [2023-01-06 20:46:03,191] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1, workerId=Temporal$20Before.worker_Temporal$20Before_ffa2c0d7-4daf-482a-abe9-8b7219187210, startTime=2023-01-06T20:46:03.190631, finishTime=2023-01-06T20:46:03.191372) of size 0
INFO  [2023-01-06 20:46:03,192] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1, workerId=Temporal$20Before.worker_Temporal$20Before_34023a70-ba30-46f2-8333-dd1ce4528666, startTime=2023-01-06T20:46:03.190711, finishTime=2023-01-06T20:46:03.191777) of size 1
INFO  [2023-01-06 20:46:03,192] com.bakdata.conquery.models.execution.ManagedExecution: DONE bd5d19b2-94d5-42f0-9a4b-0111043529f1 ManagedQuery within PT0.004516S
127.0.0.1 - - [06/Jan/2023:20:46:03 +0000] "GET /api/datasets/Temporal$20Before/queries/Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1 HTTP/1.1" 200 1862 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:03,220] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before], queryId=bd5d19b2-94d5-42f0-9a4b-0111043529f1, label=concept	@§$, creationTime=2023-01-06T20:46:03.187580, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@386aeb00[Count = 0], startTime=2023-01-06T20:46:03.187772, finishTime=2023-01-06T20:46:03.192288, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@666c429c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6aebcac, com.bakdata.conquery.models.query.ColumnDescriptor@d28a315]) download on dataset Dataset[label=null, name=Temporal Before] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:03,220] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before], queryId=bd5d19b2-94d5-42f0-9a4b-0111043529f1, label=concept	@§$, creationTime=2023-01-06T20:46:03.187580, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@386aeb00[Count = 0], startTime=2023-01-06T20:46:03.187772, finishTime=2023-01-06T20:46:03.192288, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@666c429c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6aebcac, com.bakdata.conquery.models.query.ColumnDescriptor@d28a315]) on dataset Dataset[label=null, name=Temporal Before]
127.0.0.1 - - [06/Jan/2023:20:46:03 +0000] "GET /api/datasets/Temporal%20Before/result/Temporal$20Before.bd5d19b2-94d5-42f0-9a4b-0111043529f1.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 15
INFO  [2023-01-06 20:46:03,234] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before on 2 rows
INFO  [2023-01-06 20:46:03,234] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before
INFO  [2023-01-06 20:46:03,234] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-06 20:46:03,234] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-06 20:46:03,234] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before_ffa2c0d7-4daf-482a-abe9-8b7219187210
INFO  [2023-01-06 20:46:03,235] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before_34023a70-ba30-46f2-8333-dd1ce4528666
INFO  [2023-01-06 20:46:03,249] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before
INFO  [2023-01-06 20:46:03,250] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before_34023a70-ba30-46f2-8333-dd1ce4528666
INFO  [2023-01-06 20:46:03,250] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before_ffa2c0d7-4daf-482a-abe9-8b7219187210
INFO  [2023-01-06 20:46:03,344] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before
INFO  [2023-01-06 20:46:03,344] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,375] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before
INFO  [2023-01-06 20:46:03,375] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before or Same
INFO  [2023-01-06 20:46:03,376] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:03,376] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:03,376] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-06 20:46:03,376] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-06 20:46:03,376] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:03,376] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:03,378] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_ae44b12e-6e25-4799-9b6c-18bf52d0a6c6 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:03,378] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_ae44b12e-6e25-4799-9b6c-18bf52d0a6c6 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:03,378] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:03,378] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_98d046ea-e1f5-4dd8-bad1-7b545c07da81 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:03,378] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_98d046ea-e1f5-4dd8-bad1-7b545c07da81 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:03,378] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:03,382] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,489] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,489] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Same.table1
INFO  [2023-01-06 20:46:03,489] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Same.table1
INFO  [2023-01-06 20:46:03,602] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,710] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:03,710] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:03,710] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 231 B in total
INFO  [2023-01-06 20:46:03,710] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before or Same/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000287211sINFO  [2023-01-06 20:46:03,739] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=12, min=2, average=2.400000, max=4}
INFO  [2023-01-06 20:46:03,740] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=12, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:03,740] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=14442, maxValue=15655), dateReader=com.bakdata.conquery.util.DateReader@648d9d7f)
INFO  [2023-01-06 20:46:03,743] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before or Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:03,743] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:03,743] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before or Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:03,764] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Before$20or$20Same.table1
INFO  [2023-01-06 20:46:03,765] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:03 +0000] "POST /admin/datasets/Temporal%20Before%20or%20Same/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Temporal+Before+or+Same%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:46:03,766] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:03,767] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:03,767] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:03,770] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:03,770] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Same.table1.table1], containing 12 entries.
INFO  [2023-01-06 20:46:03,771] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Same.table1.table1], containing 12 entries.
WARN  [2023-01-06 20:46:03,772] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:03,772] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Same.table1.table1.0
INFO  [2023-01-06 20:46:03,772] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Same.table1.table1.1
INFO  [2023-01-06 20:46:03,877] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,883] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,897] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:03,897] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:03,897] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:04,003] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before or Same QUERY INIT
INFO  [2023-01-06 20:46:04,016] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before$20or$20Same] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:04,016] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1b49b704-952c-4d87-b659-140bff6ac5e1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Same))]]
INFO  [2023-01-06 20:46:04,019] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1
INFO  [2023-01-06 20:46:04,019] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "POST /api/datasets/Temporal$20Before$20or$20Same/queries HTTP/1.1" 201 1848 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:46:04,020] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1] with 0 results within PT0.001227S
INFO  [2023-01-06 20:46:04,020] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1] with 3 results within PT0.001296S
INFO  [2023-01-06 20:46:04,020] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1, workerId=Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_ae44b12e-6e25-4799-9b6c-18bf52d0a6c6, startTime=2023-01-06T20:46:04.019059, finishTime=2023-01-06T20:46:04.020355) of size 3
INFO  [2023-01-06 20:46:04,020] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1, workerId=Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_98d046ea-e1f5-4dd8-bad1-7b545c07da81, startTime=2023-01-06T20:46:04.019052, finishTime=2023-01-06T20:46:04.020279) of size 0
INFO  [2023-01-06 20:46:04,021] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1b49b704-952c-4d87-b659-140bff6ac5e1 ManagedQuery within PT0.004085S
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "GET /api/datasets/Temporal$20Before$20or$20Same/queries/Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1 HTTP/1.1" 200 2155 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:04,038] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Same], queryId=1b49b704-952c-4d87-b659-140bff6ac5e1, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:46:04.016845, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@32fcbd78[Count = 0], startTime=2023-01-06T20:46:04.016963, finishTime=2023-01-06T20:46:04.021048, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7ee0c72e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@27ed7dde, com.bakdata.conquery.models.query.ColumnDescriptor@5f829ba5]) download on dataset Dataset[label=null, name=Temporal Before or Same] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:04,038] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Same], queryId=1b49b704-952c-4d87-b659-140bff6ac5e1, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:46:04.016845, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@32fcbd78[Count = 0], startTime=2023-01-06T20:46:04.016963, finishTime=2023-01-06T20:46:04.021048, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7ee0c72e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@27ed7dde, com.bakdata.conquery.models.query.ColumnDescriptor@5f829ba5]) on dataset Dataset[label=null, name=Temporal Before or Same]
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "GET /api/datasets/Temporal%20Before%20or%20Same/result/Temporal$20Before$20or$20Same.1b49b704-952c-4d87-b659-140bff6ac5e1.csv?pretty=false HTTP/1.1" 200 115 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:04,058] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before or Same on 4 rows
INFO  [2023-01-06 20:46:04,058] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before or Same
INFO  [2023-01-06 20:46:04,058] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-06 20:46:04,058] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-06 20:46:04,058] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Same_98d046ea-e1f5-4dd8-bad1-7b545c07da81
INFO  [2023-01-06 20:46:04,058] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Same_ae44b12e-6e25-4799-9b6c-18bf52d0a6c6
INFO  [2023-01-06 20:46:04,077] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before or Same
INFO  [2023-01-06 20:46:04,077] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Same_ae44b12e-6e25-4799-9b6c-18bf52d0a6c6
INFO  [2023-01-06 20:46:04,078] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Same_98d046ea-e1f5-4dd8-bad1-7b545c07da81
INFO  [2023-01-06 20:46:04,173] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before$20or$20Same
INFO  [2023-01-06 20:46:04,173] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,204] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before or Same
INFO  [2023-01-06 20:46:04,204] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Days Before
INFO  [2023-01-06 20:46:04,204] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:04,204] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:04,205] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-06 20:46:04,205] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:04,205] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-06 20:46:04,205] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:04,206] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_e1ec3932-79a8-4294-82ec-7f17827c44bb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:04,206] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_e1ec3932-79a8-4294-82ec-7f17827c44bb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:04,206] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:04,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_b893a464-8877-4a05-a9ca-3f8ca2d2c53c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:04,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_b893a464-8877-4a05-a9ca-3f8ca2d2c53c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:04,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:04,211] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,310] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,317] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,317] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Days$20Before.table1
INFO  [2023-01-06 20:46:04,317] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Days$20Before.table1
INFO  [2023-01-06 20:46:04,435] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,545] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:04,545] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:04,545] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-06 20:46:04,545] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Days Before/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000421851sINFO  [2023-01-06 20:46:04,588] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=2, average=2.000000, max=2}
INFO  [2023-01-06 20:46:04,588] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:04,588] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14975, maxValue=14980), dateReader=com.bakdata.conquery.util.DateReader@32504491)
INFO  [2023-01-06 20:46:04,590] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Days Before/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:04,590] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:04,590] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Days Before/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:04,610] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Days$20Before.table1
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "POST /admin/datasets/Temporal%20Days%20Before/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Temporal+Days+Before%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:46:04,610] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,611] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:04,612] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:04,612] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:04,615] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:04,615] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Days$20Before.table1.table1], containing 8 entries.
INFO  [2023-01-06 20:46:04,616] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Days$20Before.table1.table1], containing 8 entries.
WARN  [2023-01-06 20:46:04,617] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:04,617] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Days$20Before.table1.table1.0
INFO  [2023-01-06 20:46:04,617] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Days$20Before.table1.table1.1
INFO  [2023-01-06 20:46:04,722] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,728] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:04,740] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:04,740] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:04,845] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Days Before QUERY INIT
INFO  [2023-01-06 20:46:04,863] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Days$20Before] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:04,863] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c8d495cc-744f-4493-ad06-4862def8de54] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Days Before))]]
INFO  [2023-01-06 20:46:04,868] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54
INFO  [2023-01-06 20:46:04,868] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "POST /api/datasets/Temporal$20Days$20Before/queries HTTP/1.1" 201 1835 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:04,869] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54] with 1 results within PT0.001499S
INFO  [2023-01-06 20:46:04,870] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54] with 0 results within PT0.001706S
INFO  [2023-01-06 20:46:04,870] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54, workerId=Temporal$20Days$20Before.worker_Temporal$20Days$20Before_b893a464-8877-4a05-a9ca-3f8ca2d2c53c, startTime=2023-01-06T20:46:04.868449, finishTime=2023-01-06T20:46:04.869948) of size 1
INFO  [2023-01-06 20:46:04,870] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54, workerId=Temporal$20Days$20Before.worker_Temporal$20Days$20Before_e1ec3932-79a8-4294-82ec-7f17827c44bb, startTime=2023-01-06T20:46:04.868601, finishTime=2023-01-06T20:46:04.870307) of size 0
INFO  [2023-01-06 20:46:04,870] com.bakdata.conquery.models.execution.ManagedExecution: DONE c8d495cc-744f-4493-ad06-4862def8de54 ManagedQuery within PT0.006862S
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "GET /api/datasets/Temporal$20Days$20Before/queries/Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54 HTTP/1.1" 200 2122 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:04,896] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Days Before], queryId=c8d495cc-744f-4493-ad06-4862def8de54, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:46:04.863853, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@18f714d9[Count = 0], startTime=2023-01-06T20:46:04.864036, finishTime=2023-01-06T20:46:04.870898, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@180b9096), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Days Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5b7abfe8, com.bakdata.conquery.models.query.ColumnDescriptor@3b8ac61a]) download on dataset Dataset[label=null, name=Temporal Days Before] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:04,897] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Days Before], queryId=c8d495cc-744f-4493-ad06-4862def8de54, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:46:04.863853, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@18f714d9[Count = 0], startTime=2023-01-06T20:46:04.864036, finishTime=2023-01-06T20:46:04.870898, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@180b9096), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Days Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5b7abfe8, com.bakdata.conquery.models.query.ColumnDescriptor@3b8ac61a]) on dataset Dataset[label=null, name=Temporal Days Before]
127.0.0.1 - - [06/Jan/2023:20:46:04 +0000] "GET /api/datasets/Temporal%20Days%20Before/result/Temporal$20Days$20Before.c8d495cc-744f-4493-ad06-4862def8de54.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:04,916] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Days Before on 2 rows
INFO  [2023-01-06 20:46:04,916] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Days Before
INFO  [2023-01-06 20:46:04,916] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-06 20:46:04,916] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-06 20:46:04,916] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Days Before_b893a464-8877-4a05-a9ca-3f8ca2d2c53c
INFO  [2023-01-06 20:46:04,916] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Days Before_e1ec3932-79a8-4294-82ec-7f17827c44bb
INFO  [2023-01-06 20:46:05,015] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Days Before
INFO  [2023-01-06 20:46:05,015] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Days Before_b893a464-8877-4a05-a9ca-3f8ca2d2c53c
INFO  [2023-01-06 20:46:05,015] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Days Before_e1ec3932-79a8-4294-82ec-7f17827c44bb
INFO  [2023-01-06 20:46:05,020] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Days$20Before
INFO  [2023-01-06 20:46:05,020] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,146] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Days Before
INFO  [2023-01-06 20:46:05,146] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before or Never
INFO  [2023-01-06 20:46:05,146] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:05,146] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:05,147] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-06 20:46:05,147] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-06 20:46:05,147] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:05,148] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:05,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_d8199c4c-66ae-499e-9711-421e60ade08e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:05,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_d8199c4c-66ae-499e-9711-421e60ade08e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:05,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:05,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_b014cc96-63db-4ecf-9879-adfc1d81671e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:05,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_b014cc96-63db-4ecf-9879-adfc1d81671e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:05,155] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:05,159] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,259] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,266] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,267] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Never.table1
INFO  [2023-01-06 20:46:05,267] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Never.table1
INFO  [2023-01-06 20:46:05,385] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,492] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:05,493] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:05,493] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 144 B in total
INFO  [2023-01-06 20:46:05,493] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before or Never/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000365401sINFO  [2023-01-06 20:46:05,530] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=7, min=1, average=1.750000, max=2}
INFO  [2023-01-06 20:46:05,530] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=7, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:05,530] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14976, maxValue=15656), dateReader=com.bakdata.conquery.util.DateReader@695ba730)
INFO  [2023-01-06 20:46:05,532] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before or Never/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:05,532] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:05,532] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Before or Never/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:05,550] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Before$20or$20Never.table1
INFO  [2023-01-06 20:46:05,551] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:05 +0000] "POST /admin/datasets/Temporal%20Before%20or%20Never/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Temporal+Before+or+Never%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-06 20:46:05,551] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:05,552] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:05,552] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:05,553] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:05,553] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Never.table1.table1], containing 7 entries.
INFO  [2023-01-06 20:46:05,553] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Never.table1.table1], containing 7 entries.
WARN  [2023-01-06 20:46:05,554] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:05,554] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Never.table1.table1.0
INFO  [2023-01-06 20:46:05,554] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Never.table1.table1.1
INFO  [2023-01-06 20:46:05,659] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,664] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,682] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,682] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:05,682] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:05,788] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before or Never QUERY INIT
INFO  [2023-01-06 20:46:05,799] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before$20or$20Never] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:05,800] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Never))]]
INFO  [2023-01-06 20:46:05,803] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e
INFO  [2023-01-06 20:46:05,804] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e
127.0.0.1 - - [06/Jan/2023:20:46:05 +0000] "POST /api/datasets/Temporal$20Before$20or$20Never/queries HTTP/1.1" 201 1873 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:05,805] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e] with 0 results within PT0.001209S
INFO  [2023-01-06 20:46:05,805] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e] with 2 results within PT0.001345S
INFO  [2023-01-06 20:46:05,805] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e, workerId=Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_b014cc96-63db-4ecf-9879-adfc1d81671e, startTime=2023-01-06T20:46:05.803898, finishTime=2023-01-06T20:46:05.805107) of size 0
INFO  [2023-01-06 20:46:05,806] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e, workerId=Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_d8199c4c-66ae-499e-9711-421e60ade08e, startTime=2023-01-06T20:46:05.804135, finishTime=2023-01-06T20:46:05.805480) of size 2
INFO  [2023-01-06 20:46:05,806] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e ManagedQuery within PT0.005933S
127.0.0.1 - - [06/Jan/2023:20:46:05 +0000] "GET /api/datasets/Temporal$20Before$20or$20Never/queries/Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e HTTP/1.1" 200 2184 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:05,825] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Never], queryId=1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e, label=geschlecht_select	@§$, creationTime=2023-01-06T20:46:05.800012, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3c60114b[Count = 0], startTime=2023-01-06T20:46:05.800196, finishTime=2023-01-06T20:46:05.806129, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@320aab6a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Never)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5b5f8fd5, com.bakdata.conquery.models.query.ColumnDescriptor@2a6b66ff]) download on dataset Dataset[label=null, name=Temporal Before or Never] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:05,825] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Never], queryId=1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e, label=geschlecht_select	@§$, creationTime=2023-01-06T20:46:05.800012, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3c60114b[Count = 0], startTime=2023-01-06T20:46:05.800196, finishTime=2023-01-06T20:46:05.806129, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@320aab6a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Never)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5b5f8fd5, com.bakdata.conquery.models.query.ColumnDescriptor@2a6b66ff]) on dataset Dataset[label=null, name=Temporal Before or Never]
127.0.0.1 - - [06/Jan/2023:20:46:05 +0000] "GET /api/datasets/Temporal%20Before%20or%20Never/result/Temporal$20Before$20or$20Never.1aa0ec7c-3d71-4cef-bfe7-367c7a89ef2e.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:46:05,843] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before or Never on 3 rows
INFO  [2023-01-06 20:46:05,843] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before or Never
INFO  [2023-01-06 20:46:05,843] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-06 20:46:05,844] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-06 20:46:05,844] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Never_b014cc96-63db-4ecf-9879-adfc1d81671e
INFO  [2023-01-06 20:46:05,844] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Never_d8199c4c-66ae-499e-9711-421e60ade08e
INFO  [2023-01-06 20:46:05,848] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before or Never
INFO  [2023-01-06 20:46:05,854] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Never_d8199c4c-66ae-499e-9711-421e60ade08e
INFO  [2023-01-06 20:46:05,854] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Never_b014cc96-63db-4ecf-9879-adfc1d81671e
INFO  [2023-01-06 20:46:05,854] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before$20or$20Never
INFO  [2023-01-06 20:46:05,854] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:05,987] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before or Never
INFO  [2023-01-06 20:46:05,988] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Same
INFO  [2023-01-06 20:46:05,988] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:05,988] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:05,989] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-06 20:46:05,989] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-06 20:46:05,989] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:05,989] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:05,990] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Same.worker_Temporal$20Same_b70ac74c-e9f9-4341-a140-e53b1f694148 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:05,990] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Same.worker_Temporal$20Same_b70ac74c-e9f9-4341-a140-e53b1f694148 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:05,990] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:05,990] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Same.worker_Temporal$20Same_ffd2f49e-dac9-408c-9aa8-d26c96439eda are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:05,990] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Same.worker_Temporal$20Same_ffd2f49e-dac9-408c-9aa8-d26c96439eda are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:05,990] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:05,995] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,095] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,107] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Same.table1
INFO  [2023-01-06 20:46:06,107] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Same.table1
INFO  [2023-01-06 20:46:06,220] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,328] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:06,328] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:06,328] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 178 B in total
INFO  [2023-01-06 20:46:06,328] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Same/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000418973sINFO  [2023-01-06 20:46:06,371] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=9, min=2, average=2.250000, max=3}
INFO  [2023-01-06 20:46:06,371] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:06,371] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=14975, maxValue=15006), dateReader=com.bakdata.conquery.util.DateReader@70f32867)
INFO  [2023-01-06 20:46:06,374] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:06,374] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:06,374] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_Temporal Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:06,389] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Same.table1
INFO  [2023-01-06 20:46:06,389] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:06 +0000] "POST /admin/datasets/Temporal%20Same/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_Temporal+Same%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:06,390] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:06,390] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:06,390] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:06,392] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:06,392] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Same.table1.table1], containing 9 entries.
INFO  [2023-01-06 20:46:06,392] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Same.table1.table1], containing 9 entries.
WARN  [2023-01-06 20:46:06,393] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:06,393] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Same.table1.table1.0
INFO  [2023-01-06 20:46:06,393] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Same.table1.table1.1
INFO  [2023-01-06 20:46:06,499] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,504] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,519] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,519] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:06,519] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:06,625] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Same QUERY INIT
INFO  [2023-01-06 20:46:06,640] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Same] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:06,641] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ecac9b62-e006-4371-8c74-8ae094db01cb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Same))]]
INFO  [2023-01-06 20:46:06,645] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb
INFO  [2023-01-06 20:46:06,645] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb
INFO  [2023-01-06 20:46:06,645] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb] with 0 results within PT0.000638S
INFO  [2023-01-06 20:46:06,646] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb, workerId=Temporal$20Same.worker_Temporal$20Same_b70ac74c-e9f9-4341-a140-e53b1f694148, startTime=2023-01-06T20:46:06.645212, finishTime=2023-01-06T20:46:06.645850) of size 0
INFO  [2023-01-06 20:46:06,646] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb] with 2 results within PT0.001266S
127.0.0.1 - - [06/Jan/2023:20:46:06 +0000] "POST /api/datasets/Temporal$20Same/queries HTTP/1.1" 201 1744 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:06,647] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb, workerId=Temporal$20Same.worker_Temporal$20Same_ffd2f49e-dac9-408c-9aa8-d26c96439eda, startTime=2023-01-06T20:46:06.645290, finishTime=2023-01-06T20:46:06.646556) of size 2
INFO  [2023-01-06 20:46:06,647] com.bakdata.conquery.models.execution.ManagedExecution: DONE ecac9b62-e006-4371-8c74-8ae094db01cb ManagedQuery within PT0.00587S
127.0.0.1 - - [06/Jan/2023:20:46:06 +0000] "GET /api/datasets/Temporal$20Same/queries/Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb HTTP/1.1" 200 1995 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:06,672] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Same], queryId=ecac9b62-e006-4371-8c74-8ae094db01cb, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:46:06.641268, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5949397[Count = 0], startTime=2023-01-06T20:46:06.641453, finishTime=2023-01-06T20:46:06.647323, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@af0d8e4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2091d333, com.bakdata.conquery.models.query.ColumnDescriptor@3fb1851d]) download on dataset Dataset[label=null, name=Temporal Same] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:06,672] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Same], queryId=ecac9b62-e006-4371-8c74-8ae094db01cb, label=Geschlecht-SELECT	@§$, creationTime=2023-01-06T20:46:06.641268, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5949397[Count = 0], startTime=2023-01-06T20:46:06.641453, finishTime=2023-01-06T20:46:06.647323, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@af0d8e4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2091d333, com.bakdata.conquery.models.query.ColumnDescriptor@3fb1851d]) on dataset Dataset[label=null, name=Temporal Same]
127.0.0.1 - - [06/Jan/2023:20:46:06 +0000] "GET /api/datasets/Temporal%20Same/result/Temporal$20Same.ecac9b62-e006-4371-8c74-8ae094db01cb.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:06,690] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Same on 3 rows
INFO  [2023-01-06 20:46:06,690] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Same
INFO  [2023-01-06 20:46:06,690] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-06 20:46:06,691] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-06 20:46:06,691] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Same_b70ac74c-e9f9-4341-a140-e53b1f694148
INFO  [2023-01-06 20:46:06,691] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Same_ffd2f49e-dac9-408c-9aa8-d26c96439eda
INFO  [2023-01-06 20:46:06,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Same_b70ac74c-e9f9-4341-a140-e53b1f694148
INFO  [2023-01-06 20:46:06,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Same
INFO  [2023-01-06 20:46:06,792] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Same_ffd2f49e-dac9-408c-9aa8-d26c96439eda
INFO  [2023-01-06 20:46:06,794] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Same
INFO  [2023-01-06 20:46:06,794] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:06,925] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Same
INFO  [2023-01-06 20:46:06,926] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test VALIDITY_DATE_QUERY Test
INFO  [2023-01-06 20:46:06,926] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:06,926] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:06,926] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-06 20:46:06,926] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-06 20:46:06,927] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:06,927] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:06,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_b3dfff3c-da6e-4251-b2bb-ef1ad20c44ec are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:06,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_b3dfff3c-da6e-4251-b2bb-ef1ad20c44ec are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:06,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:06,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_dff9f944-3d28-445b-882d-3a41c2edd1d7 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:06,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_dff9f944-3d28-445b-882d-3a41c2edd1d7 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:06,928] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:06,933] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,032] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,039] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,039] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALIDITY_DATE_QUERY$20Test.table
INFO  [2023-01-06 20:46:07,039] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALIDITY_DATE_QUERY$20Test.table
INFO  [2023-01-06 20:46:07,150] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,260] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:07,260] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:07,260] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 149 B in total
INFO  [2023-01-06 20:46:07,260] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VALIDITY_DATE_QUERY Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000328094sINFO  [2023-01-06 20:46:07,293] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:46:07,293] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[other_date] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=15170, maxValue=16384), dateReader=com.bakdata.conquery.util.DateReader@23183d59)
INFO  [2023-01-06 20:46:07,293] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@789d3563)
INFO  [2023-01-06 20:46:07,293] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:07,296] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VALIDITY_DATE_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:07,296] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:07,296] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VALIDITY_DATE_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:07,310] com.bakdata.conquery.models.jobs.ImportJob: Importing table into VALIDITY_DATE_QUERY$20Test.table
INFO  [2023-01-06 20:46:07,311] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:07 +0000] "POST /admin/datasets/VALIDITY_DATE_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_VALIDITY_DATE_QUERY+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:07,319] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:07,320] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:07,320] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:07,323] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:07,323] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALIDITY_DATE_QUERY$20Test.table.table], containing 4 entries.
INFO  [2023-01-06 20:46:07,323] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALIDITY_DATE_QUERY$20Test.table.table], containing 4 entries.
WARN  [2023-01-06 20:46:07,324] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:07,324] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALIDITY_DATE_QUERY$20Test.table.table.0
INFO  [2023-01-06 20:46:07,324] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALIDITY_DATE_QUERY$20Test.table.table.1
INFO  [2023-01-06 20:46:07,429] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,434] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,449] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,449] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:07,449] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:07,555] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: VALIDITY_DATE_QUERY Test QUERY INIT
INFO  [2023-01-06 20:46:07,567] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[VALIDITY_DATE_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:07,567] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5f0d9ef4-8253-4edf-9804-0bdd85af194c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_VALIDITY_DATE_QUERY Test))]]
INFO  [2023-01-06 20:46:07,570] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c
INFO  [2023-01-06 20:46:07,570] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c
INFO  [2023-01-06 20:46:07,571] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c] with 0 results within PT0.000969S
INFO  [2023-01-06 20:46:07,572] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c] with 2 results within PT0.001232S
INFO  [2023-01-06 20:46:07,572] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c, workerId=VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_b3dfff3c-da6e-4251-b2bb-ef1ad20c44ec, startTime=2023-01-06T20:46:07.570990, finishTime=2023-01-06T20:46:07.571959) of size 0
INFO  [2023-01-06 20:46:07,572] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c, workerId=VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_dff9f944-3d28-445b-882d-3a41c2edd1d7, startTime=2023-01-06T20:46:07.571004, finishTime=2023-01-06T20:46:07.572236) of size 2
127.0.0.1 - - [06/Jan/2023:20:46:07 +0000] "POST /api/datasets/VALIDITY_DATE_QUERY$20Test/queries HTTP/1.1" 201 1196 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:07,573] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5f0d9ef4-8253-4edf-9804-0bdd85af194c ManagedQuery within PT0.005152S
127.0.0.1 - - [06/Jan/2023:20:46:07 +0000] "GET /api/datasets/VALIDITY_DATE_QUERY$20Test/queries/VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c HTTP/1.1" 200 1491 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:07,598] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALIDITY_DATE_QUERY Test], queryId=5f0d9ef4-8253-4edf-9804-0bdd85af194c, label=concept-test_child1	@§$, creationTime=2023-01-06T20:46:07.567648, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@288e9cda[Count = 0], startTime=2023-01-06T20:46:07.567826, finishTime=2023-01-06T20:46:07.572978, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7e895736), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALIDITY_DATE_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@785f98af, com.bakdata.conquery.models.query.ColumnDescriptor@6191bc6f]) download on dataset Dataset[label=null, name=VALIDITY_DATE_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:07,598] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALIDITY_DATE_QUERY Test], queryId=5f0d9ef4-8253-4edf-9804-0bdd85af194c, label=concept-test_child1	@§$, creationTime=2023-01-06T20:46:07.567648, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@288e9cda[Count = 0], startTime=2023-01-06T20:46:07.567826, finishTime=2023-01-06T20:46:07.572978, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7e895736), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALIDITY_DATE_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@785f98af, com.bakdata.conquery.models.query.ColumnDescriptor@6191bc6f]) on dataset Dataset[label=null, name=VALIDITY_DATE_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:46:07 +0000] "GET /api/datasets/VALIDITY_DATE_QUERY%20Test/result/VALIDITY_DATE_QUERY$20Test.5f0d9ef4-8253-4edf-9804-0bdd85af194c.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:07,617] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest VALIDITY_DATE_QUERY Test on 3 rows
INFO  [2023-01-06 20:46:07,618] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast VALIDITY_DATE_QUERY Test
INFO  [2023-01-06 20:46:07,618] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-06 20:46:07,618] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-06 20:46:07,618] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALIDITY_DATE_QUERY Test_b3dfff3c-da6e-4251-b2bb-ef1ad20c44ec
INFO  [2023-01-06 20:46:07,618] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALIDITY_DATE_QUERY Test_dff9f944-3d28-445b-882d-3a41c2edd1d7
INFO  [2023-01-06 20:46:07,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow VALIDITY_DATE_QUERY Test
INFO  [2023-01-06 20:46:07,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALIDITY_DATE_QUERY Test_dff9f944-3d28-445b-882d-3a41c2edd1d7
INFO  [2023-01-06 20:46:07,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALIDITY_DATE_QUERY Test_b3dfff3c-da6e-4251-b2bb-ef1ad20c44ec
INFO  [2023-01-06 20:46:07,725] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of VALIDITY_DATE_QUERY$20Test
INFO  [2023-01-06 20:46:07,725] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,755] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test VALIDITY_DATE_QUERY Test
INFO  [2023-01-06 20:46:07,755] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-06 20:46:07,755] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:07,755] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:07,756] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-06 20:46:07,756] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-06 20:46:07,756] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:07,756] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:07,757] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_7da6a70b-4a59-457f-9da2-6bdb83aac883 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:07,757] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_7da6a70b-4a59-457f-9da2-6bdb83aac883 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:07,757] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:07,757] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_27d1d13b-176a-4c86-a15b-6d36773a046e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:07,757] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_27d1d13b-176a-4c86-a15b-6d36773a046e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:07,757] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:07,762] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,861] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,868] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:07,869] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table
INFO  [2023-01-06 20:46:07,869] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table
INFO  [2023-01-06 20:46:07,984] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,092] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:08,093] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:08,093] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 390 B in total
INFO  [2023-01-06 20:46:08,093] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VIRTUAL_CONCEPT_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000394645sINFO  [2023-01-06 20:46:08,133] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:46:08,133] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:08,133] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=13149, maxValue=16071), dateReader=com.bakdata.conquery.util.DateReader@72154700)
INFO  [2023-01-06 20:46:08,136] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VIRTUAL_CONCEPT_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:08,136] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:08,136] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VIRTUAL_CONCEPT_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:08,155] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table
INFO  [2023-01-06 20:46:08,155] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:08 +0000] "POST /admin/datasets/VIRTUAL_CONCEPT_REUSED_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_VIRTUAL_CONCEPT_REUSED_QUERY+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:46:08,156] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:08,157] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:08,157] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:08,158] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-06 20:46:08,158] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table], containing 22 entries.
INFO  [2023-01-06 20:46:08,159] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table], containing 22 entries.
INFO  [2023-01-06 20:46:08,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.0
INFO  [2023-01-06 20:46:08,159] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.1
WARN  [2023-01-06 20:46:08,159] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:08,160] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.2
INFO  [2023-01-06 20:46:08,160] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.3
INFO  [2023-01-06 20:46:08,204] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.5
INFO  [2023-01-06 20:46:08,204] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.7
INFO  [2023-01-06 20:46:08,207] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.4
INFO  [2023-01-06 20:46:08,207] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.6
INFO  [2023-01-06 20:46:08,312] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,360] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,376] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,377] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:08,377] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:08,482] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: VIRTUAL_CONCEPT_REUSED_QUERY Test QUERY INIT
INFO  [2023-01-06 20:46:08,496] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[VIRTUAL_CONCEPT_REUSED_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:08,497] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d54268d6-6474-40fb-99f9-e941a1945046] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_VIRTUAL_CONCEPT_REUSED_QUERY Test))]]
INFO  [2023-01-06 20:46:08,503] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046
INFO  [2023-01-06 20:46:08,504] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046
127.0.0.1 - - [06/Jan/2023:20:46:08 +0000] "POST /api/datasets/VIRTUAL_CONCEPT_REUSED_QUERY$20Test/queries HTTP/1.1" 201 1507 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:46:08,506] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046] with 2 results within PT0.002268S
INFO  [2023-01-06 20:46:08,506] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046] with 2 results within PT0.002361S
INFO  [2023-01-06 20:46:08,506] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046, workerId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_27d1d13b-176a-4c86-a15b-6d36773a046e, startTime=2023-01-06T20:46:08.503889, finishTime=2023-01-06T20:46:08.506157) of size 2
INFO  [2023-01-06 20:46:08,506] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046, workerId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_7da6a70b-4a59-457f-9da2-6bdb83aac883, startTime=2023-01-06T20:46:08.504200, finishTime=2023-01-06T20:46:08.506561) of size 2
INFO  [2023-01-06 20:46:08,506] com.bakdata.conquery.models.execution.ManagedExecution: DONE d54268d6-6474-40fb-99f9-e941a1945046 ManagedQuery within PT0.009337S
127.0.0.1 - - [06/Jan/2023:20:46:08 +0000] "GET /api/datasets/VIRTUAL_CONCEPT_REUSED_QUERY$20Test/queries/VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046 HTTP/1.1" 200 1838 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:08,525] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test], queryId=d54268d6-6474-40fb-99f9-e941a1945046, label=Query test_concept	@§$, creationTime=2023-01-06T20:46:08.496591, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5b49192f[Count = 0], startTime=2023-01-06T20:46:08.497626, finishTime=2023-01-06T20:46:08.506963, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@67374a56), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VIRTUAL_CONCEPT_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@e0b5a16, com.bakdata.conquery.models.query.ColumnDescriptor@431d3cdf]) download on dataset Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:08,525] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test], queryId=d54268d6-6474-40fb-99f9-e941a1945046, label=Query test_concept	@§$, creationTime=2023-01-06T20:46:08.496591, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5b49192f[Count = 0], startTime=2023-01-06T20:46:08.497626, finishTime=2023-01-06T20:46:08.506963, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@67374a56), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VIRTUAL_CONCEPT_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@e0b5a16, com.bakdata.conquery.models.query.ColumnDescriptor@431d3cdf]) on dataset Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
127.0.0.1 - - [06/Jan/2023:20:46:08 +0000] "GET /api/datasets/VIRTUAL_CONCEPT_REUSED_QUERY%20Test/result/VIRTUAL_CONCEPT_REUSED_QUERY$20Test.d54268d6-6474-40fb-99f9-e941a1945046.csv?pretty=false HTTP/1.1" 200 120 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:46:08,541] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest VIRTUAL_CONCEPT_REUSED_QUERY Test on 5 rows
INFO  [2023-01-06 20:46:08,542] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-06 20:46:08,542] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-06 20:46:08,542] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-06 20:46:08,542] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_7da6a70b-4a59-457f-9da2-6bdb83aac883
INFO  [2023-01-06 20:46:08,543] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_27d1d13b-176a-4c86-a15b-6d36773a046e
INFO  [2023-01-06 20:46:08,556] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-06 20:46:08,557] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_7da6a70b-4a59-457f-9da2-6bdb83aac883
INFO  [2023-01-06 20:46:08,557] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_27d1d13b-176a-4c86-a15b-6d36773a046e
INFO  [2023-01-06 20:46:08,560] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of VIRTUAL_CONCEPT_REUSED_QUERY$20Test
INFO  [2023-01-06 20:46:08,560] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,682] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-06 20:46:08,683] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_AGGREGATOR Test
INFO  [2023-01-06 20:46:08,683] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:08,683] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:08,683] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:08,683] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:08,684] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:08,684] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:08,684] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_fec67a64-1c5e-42ab-8769-c97a1177092d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:08,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_fec67a64-1c5e-42ab-8769-c97a1177092d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:08,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:08,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_1586cb76-dd9d-4853-b5e4-befe5c528ef1 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:08,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_1586cb76-dd9d-4853-b5e4-befe5c528ef1 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:08,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:08,791] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,798] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:08,799] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:08,799] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:08,916] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,026] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:09,026] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:09,026] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 98 B in total
INFO  [2023-01-06 20:46:09,026] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000342862sINFO  [2023-01-06 20:46:09,061] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:46:09,061] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:09,061] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@21bcd000)
INFO  [2023-01-06 20:46:09,063] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:09,063] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:09,063] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:09,079] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:09,080] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:09 +0000] "POST /admin/datasets/COUNT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:46:09,081] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:09,082] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:09,082] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:09,085] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:09,085] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:46:09,085] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:46:09,087] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:09,087] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:09,087] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:09,192] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,197] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,207] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,208] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:09,208] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:09,314] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:09,329] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:09,329] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[901b0c8c-4acb-40eb-8b55-2da26236a9f0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:09,332] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0
INFO  [2023-01-06 20:46:09,332] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0
INFO  [2023-01-06 20:46:09,332] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0] with 1 results within PT0.000424S
INFO  [2023-01-06 20:46:09,333] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0, workerId=COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_1586cb76-dd9d-4853-b5e4-befe5c528ef1, startTime=2023-01-06T20:46:09.332465, finishTime=2023-01-06T20:46:09.332889) of size 1
127.0.0.1 - - [06/Jan/2023:20:46:09 +0000] "POST /api/datasets/COUNT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1312 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:09,333] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0] with 3 results within PT0.000992S
INFO  [2023-01-06 20:46:09,334] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0, workerId=COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_fec67a64-1c5e-42ab-8769-c97a1177092d, startTime=2023-01-06T20:46:09.332583, finishTime=2023-01-06T20:46:09.333575) of size 3
INFO  [2023-01-06 20:46:09,334] com.bakdata.conquery.models.execution.ManagedExecution: DONE 901b0c8c-4acb-40eb-8b55-2da26236a9f0 ManagedQuery within PT0.0045S
127.0.0.1 - - [06/Jan/2023:20:46:09 +0000] "GET /api/datasets/COUNT_AGGREGATOR$20Test/queries/COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0 HTTP/1.1" 200 1595 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:09,358] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_AGGREGATOR Test], queryId=901b0c8c-4acb-40eb-8b55-2da26236a9f0, label=concept	@§$, creationTime=2023-01-06T20:46:09.329559, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5dc90f22[Count = 0], startTime=2023-01-06T20:46:09.329753, finishTime=2023-01-06T20:46:09.334253, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@14af1863), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3fe40f28, com.bakdata.conquery.models.query.ColumnDescriptor@237deb28, com.bakdata.conquery.models.query.ColumnDescriptor@3658507a]) download on dataset Dataset[label=null, name=COUNT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:09,358] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_AGGREGATOR Test], queryId=901b0c8c-4acb-40eb-8b55-2da26236a9f0, label=concept	@§$, creationTime=2023-01-06T20:46:09.329559, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5dc90f22[Count = 0], startTime=2023-01-06T20:46:09.329753, finishTime=2023-01-06T20:46:09.334253, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@14af1863), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3fe40f28, com.bakdata.conquery.models.query.ColumnDescriptor@237deb28, com.bakdata.conquery.models.query.ColumnDescriptor@3658507a]) on dataset Dataset[label=null, name=COUNT_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:09 +0000] "GET /api/datasets/COUNT_AGGREGATOR%20Test/result/COUNT_AGGREGATOR$20Test.901b0c8c-4acb-40eb-8b55-2da26236a9f0.csv?pretty=false HTTP/1.1" 200 139 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:09,376] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COUNT_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:09,376] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_AGGREGATOR Test
INFO  [2023-01-06 20:46:09,377] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:09,377] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:09,377] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_AGGREGATOR Test_1586cb76-dd9d-4853-b5e4-befe5c528ef1
INFO  [2023-01-06 20:46:09,377] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_AGGREGATOR Test_fec67a64-1c5e-42ab-8769-c97a1177092d
INFO  [2023-01-06 20:46:09,387] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_AGGREGATOR Test_fec67a64-1c5e-42ab-8769-c97a1177092d
INFO  [2023-01-06 20:46:09,387] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_AGGREGATOR Test_1586cb76-dd9d-4853-b5e4-befe5c528ef1
INFO  [2023-01-06 20:46:09,388] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_AGGREGATOR Test
INFO  [2023-01-06 20:46:09,487] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:09,487] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,514] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_AGGREGATOR Test
INFO  [2023-01-06 20:46:09,514] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-06 20:46:09,514] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:09,514] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:09,515] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:09,515] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:09,515] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:09,515] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:09,516] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,517] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_5005a60e-18d4-47de-96fa-45f41f892e56 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:09,517] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_5005a60e-18d4-47de-96fa-45f41f892e56 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:09,517] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:09,517] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_737f6f6f-20c3-45df-af04-dc4277357761 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:09,517] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_737f6f6f-20c3-45df-af04-dc4277357761 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:09,517] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:09,621] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,628] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,628] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_DISTINCT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:09,628] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_DISTINCT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:09,745] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,854] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:09,854] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:09,854] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 115 B in total
INFO  [2023-01-06 20:46:09,854] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_DISTINCT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000418022sINFO  [2023-01-06 20:46:09,897] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:46:09,897] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:09,897] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5bf1fdd9)
INFO  [2023-01-06 20:46:09,899] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_DISTINCT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:09,899] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:09,899] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_DISTINCT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:09,915] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_DISTINCT_AGGREGATOR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:09 +0000] "POST /admin/datasets/COUNT_DISTINCT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT_DISTINCT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:46:09,915] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:09,916] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:09,917] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:09,917] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:09,920] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:09,921] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_DISTINCT_AGGREGATOR$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:46:09,921] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_DISTINCT_AGGREGATOR$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:46:09,922] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:09,922] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_DISTINCT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:09,922] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_DISTINCT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:10,029] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,034] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,046] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,047] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:10,047] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:10,152] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_DISTINCT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:10,167] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_DISTINCT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:10,168] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_DISTINCT_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:10,171] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01
INFO  [2023-01-06 20:46:10,171] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01
INFO  [2023-01-06 20:46:10,172] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01] with 1 results within PT0.000841S
INFO  [2023-01-06 20:46:10,172] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01] with 3 results within PT0.000932S
127.0.0.1 - - [06/Jan/2023:20:46:10 +0000] "POST /api/datasets/COUNT_DISTINCT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1357 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:10,172] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01, workerId=COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_5005a60e-18d4-47de-96fa-45f41f892e56, startTime=2023-01-06T20:46:10.171231, finishTime=2023-01-06T20:46:10.172163) of size 3
INFO  [2023-01-06 20:46:10,172] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01, workerId=COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_737f6f6f-20c3-45df-af04-dc4277357761, startTime=2023-01-06T20:46:10.171237, finishTime=2023-01-06T20:46:10.172078) of size 1
INFO  [2023-01-06 20:46:10,172] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01 ManagedQuery within PT0.004529S
127.0.0.1 - - [06/Jan/2023:20:46:10 +0000] "GET /api/datasets/COUNT_DISTINCT_AGGREGATOR$20Test/queries/COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01 HTTP/1.1" 200 1676 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:10,197] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test], queryId=1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01, label=concept	@§$, creationTime=2023-01-06T20:46:10.168137, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77b8fe1d[Count = 0], startTime=2023-01-06T20:46:10.168337, finishTime=2023-01-06T20:46:10.172866, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@68b70ad0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_DISTINCT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@496c70a, com.bakdata.conquery.models.query.ColumnDescriptor@584ab263, com.bakdata.conquery.models.query.ColumnDescriptor@68b2f761]) download on dataset Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:10,198] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test], queryId=1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01, label=concept	@§$, creationTime=2023-01-06T20:46:10.168137, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77b8fe1d[Count = 0], startTime=2023-01-06T20:46:10.168337, finishTime=2023-01-06T20:46:10.172866, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@68b70ad0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_DISTINCT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@496c70a, com.bakdata.conquery.models.query.ColumnDescriptor@584ab263, com.bakdata.conquery.models.query.ColumnDescriptor@68b2f761]) on dataset Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:10 +0000] "GET /api/datasets/COUNT_DISTINCT_AGGREGATOR%20Test/result/COUNT_DISTINCT_AGGREGATOR$20Test.1ddd0c59-f77a-4bf2-b9d3-87d6b8d49a01.csv?pretty=false HTTP/1.1" 200 139 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:46:10,215] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COUNT_DISTINCT_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:10,216] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-06 20:46:10,216] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:10,216] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:10,216] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_DISTINCT_AGGREGATOR Test_5005a60e-18d4-47de-96fa-45f41f892e56
INFO  [2023-01-06 20:46:10,216] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_DISTINCT_AGGREGATOR Test_737f6f6f-20c3-45df-af04-dc4277357761
INFO  [2023-01-06 20:46:10,225] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_DISTINCT_AGGREGATOR Test_5005a60e-18d4-47de-96fa-45f41f892e56
INFO  [2023-01-06 20:46:10,225] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_DISTINCT_AGGREGATOR Test_737f6f6f-20c3-45df-af04-dc4277357761
INFO  [2023-01-06 20:46:10,225] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-06 20:46:10,325] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_DISTINCT_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:10,325] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,354] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-06 20:46:10,355] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-06 20:46:10,355] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:10,355] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:10,359] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-06 20:46:10,360] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-06 20:46:10,360] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:10,360] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:10,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_db8265d0-0a7a-4507-b04c-0ff777807525 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:10,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_db8265d0-0a7a-4507-b04c-0ff777807525 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:10,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:10,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_2c52ef7d-b36a-4db6-973d-b701da188a89 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:10,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_2c52ef7d-b36a-4db6-973d-b701da188a89 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:10,361] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:10,366] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,465] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,472] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,473] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:10,473] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:10,594] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,705] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:10,706] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:10,706] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 460 B in total
INFO  [2023-01-06 20:46:10,706] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000331857sINFO  [2023-01-06 20:46:10,739] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=12, min=1, average=2.400000, max=4}
INFO  [2023-01-06 20:46:10,739] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16436, maxValue=16436), dateReader=com.bakdata.conquery.util.DateReader@3d2a1ca0), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=17166, maxValue=17166), dateReader=com.bakdata.conquery.util.DateReader@1066a086), dateReader=com.bakdata.conquery.util.DateReader@7b031291, onlyQuarters=false, maxValue=17166, minValue=16436, anyOpen=false)
INFO  [2023-01-06 20:46:10,740] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateParser(super=Parser(lines=12, nullLines=1), subType=IntegerParser(super=Parser(lines=12, nullLines=1), minValue=16467, maxValue=16740), dateReader=com.bakdata.conquery.util.DateReader@14baa30b)
INFO  [2023-01-06 20:46:10,742] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:10,742] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:10,742] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_COUNT_QUARTERS_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:10,755] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_QUARTERS_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:10,755] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:10 +0000] "POST /admin/datasets/COUNT_QUARTERS_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_COUNT_QUARTERS_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:10,757] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:10,757] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:10,757] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:10,759] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:10,759] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS_AGGREGATOR$20Test.table.table], containing 12 entries.
INFO  [2023-01-06 20:46:10,760] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS_AGGREGATOR$20Test.table.table], containing 12 entries.
WARN  [2023-01-06 20:46:10,761] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:10,761] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:10,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:10,866] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,872] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,883] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:10,884] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:10,884] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:10,990] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_QUARTERS_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:11,005] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_QUARTERS_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:11,005] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a8827691-90bd-4393-bedc-85a4b7c9d34b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:11,008] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b
INFO  [2023-01-06 20:46:11,008] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "POST /api/datasets/COUNT_QUARTERS_AGGREGATOR$20Test/queries HTTP/1.1" 201 1357 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:11,009] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b] with 2 results within PT0.001318S
INFO  [2023-01-06 20:46:11,010] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b] with 3 results within PT0.001567S
INFO  [2023-01-06 20:46:11,010] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b, workerId=COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_db8265d0-0a7a-4507-b04c-0ff777807525, startTime=2023-01-06T20:46:11.008547, finishTime=2023-01-06T20:46:11.009865) of size 2
INFO  [2023-01-06 20:46:11,010] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b, workerId=COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_2c52ef7d-b36a-4db6-973d-b701da188a89, startTime=2023-01-06T20:46:11.008559, finishTime=2023-01-06T20:46:11.010126) of size 3
INFO  [2023-01-06 20:46:11,010] com.bakdata.conquery.models.execution.ManagedExecution: DONE a8827691-90bd-4393-bedc-85a4b7c9d34b ManagedQuery within PT0.005107S
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "GET /api/datasets/COUNT_QUARTERS_AGGREGATOR$20Test/queries/COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b HTTP/1.1" 200 1676 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:11,035] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test], queryId=a8827691-90bd-4393-bedc-85a4b7c9d34b, label=concept	@§$, creationTime=2023-01-06T20:46:11.005504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@76007d8d[Count = 0], startTime=2023-01-06T20:46:11.005687, finishTime=2023-01-06T20:46:11.010794, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3a7c1c95), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@71c5fbfd, com.bakdata.conquery.models.query.ColumnDescriptor@58429025, com.bakdata.conquery.models.query.ColumnDescriptor@2ddb72b5]) download on dataset Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:11,035] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test], queryId=a8827691-90bd-4393-bedc-85a4b7c9d34b, label=concept	@§$, creationTime=2023-01-06T20:46:11.005504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@76007d8d[Count = 0], startTime=2023-01-06T20:46:11.005687, finishTime=2023-01-06T20:46:11.010794, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3a7c1c95), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@71c5fbfd, com.bakdata.conquery.models.query.ColumnDescriptor@58429025, com.bakdata.conquery.models.query.ColumnDescriptor@2ddb72b5]) on dataset Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "GET /api/datasets/COUNT_QUARTERS_AGGREGATOR%20Test/result/COUNT_QUARTERS_AGGREGATOR$20Test.a8827691-90bd-4393-bedc-85a4b7c9d34b.csv?pretty=false HTTP/1.1" 200 167 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:11,055] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COUNT_QUARTERS_AGGREGATOR Test on 6 rows
INFO  [2023-01-06 20:46:11,055] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-06 20:46:11,055] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-06 20:46:11,055] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-06 20:46:11,055] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS_AGGREGATOR Test_db8265d0-0a7a-4507-b04c-0ff777807525
INFO  [2023-01-06 20:46:11,055] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS_AGGREGATOR Test_2c52ef7d-b36a-4db6-973d-b701da188a89
INFO  [2023-01-06 20:46:11,060] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-06 20:46:11,061] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS_AGGREGATOR Test_2c52ef7d-b36a-4db6-973d-b701da188a89
INFO  [2023-01-06 20:46:11,062] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_QUARTERS_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:11,062] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS_AGGREGATOR Test_db8265d0-0a7a-4507-b04c-0ff777807525
INFO  [2023-01-06 20:46:11,190] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-06 20:46:11,190] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-06 20:46:11,190] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:11,190] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:11,192] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-06 20:46:11,192] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-06 20:46:11,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:11,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:11,193] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_774db1e6-bb8e-4c66-91fc-043f1d522cb3 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:11,193] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_774db1e6-bb8e-4c66-91fc-043f1d522cb3 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:11,193] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:11,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_fb3df928-e737-4179-8a17-77a1471bef49 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:11,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_fb3df928-e737-4179-8a17-77a1471bef49 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:11,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:11,297] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,303] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,304] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:11,304] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:11,422] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,531] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:11,531] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:11,531] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 381 B in total
INFO  [2023-01-06 20:46:11,531] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000320659sINFO  [2023-01-06 20:46:11,564] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:46:11,564] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@661c466a)
INFO  [2023-01-06 20:46:11,564] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@4ccfcbdb), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@bb42e92), dateReader=com.bakdata.conquery.util.DateReader@5fff0508, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:46:11,566] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:11,566] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:11,566] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:11,578] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DATE_DISTANCE_AGGREGATOR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "POST /admin/datasets/DATE_DISTANCE_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:46:11,578] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,579] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:11,579] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:11,579] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:11,582] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:46:11,583] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:11,583] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:11,584] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:11,584] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.2
WARN  [2023-01-06 20:46:11,584] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:11,584] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:11,585] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.3
INFO  [2023-01-06 20:46:11,690] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,695] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,708] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:11,709] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:11,709] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:11,815] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:11,834] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:11,834] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[953a15ed-dd26-411f-ab8a-bc8cb14b6cd4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:11,837] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4
INFO  [2023-01-06 20:46:11,837] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "POST /api/datasets/DATE_DISTANCE_AGGREGATOR$20Test/queries HTTP/1.1" 201 1452 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:11,839] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4] with 4 results within PT0.001435S
INFO  [2023-01-06 20:46:11,839] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4] with 6 results within PT0.001818S
INFO  [2023-01-06 20:46:11,840] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4, workerId=DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_774db1e6-bb8e-4c66-91fc-043f1d522cb3, startTime=2023-01-06T20:46:11.837946, finishTime=2023-01-06T20:46:11.839381) of size 4
INFO  [2023-01-06 20:46:11,840] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4, workerId=DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_fb3df928-e737-4179-8a17-77a1471bef49, startTime=2023-01-06T20:46:11.837935, finishTime=2023-01-06T20:46:11.839753) of size 6
INFO  [2023-01-06 20:46:11,840] com.bakdata.conquery.models.execution.ManagedExecution: DONE 953a15ed-dd26-411f-ab8a-bc8cb14b6cd4 ManagedQuery within PT0.00568S
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR$20Test/queries/DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4 HTTP/1.1" 200 1767 "-" "Conquery (test client)" 1
INFO  [2023-01-06 20:46:11,864] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test], queryId=953a15ed-dd26-411f-ab8a-bc8cb14b6cd4, label=concept	@§$, creationTime=2023-01-06T20:46:11.834384, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@27a0a885[Count = 0], startTime=2023-01-06T20:46:11.834590, finishTime=2023-01-06T20:46:11.840270, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5fe0d491), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@75865b43, com.bakdata.conquery.models.query.ColumnDescriptor@48b7019e, com.bakdata.conquery.models.query.ColumnDescriptor@6af17c8d]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:11,864] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test], queryId=953a15ed-dd26-411f-ab8a-bc8cb14b6cd4, label=concept	@§$, creationTime=2023-01-06T20:46:11.834384, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@27a0a885[Count = 0], startTime=2023-01-06T20:46:11.834590, finishTime=2023-01-06T20:46:11.840270, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5fe0d491), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@75865b43, com.bakdata.conquery.models.query.ColumnDescriptor@48b7019e, com.bakdata.conquery.models.query.ColumnDescriptor@6af17c8d]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:11 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR%20Test/result/DATE_DISTANCE_AGGREGATOR$20Test.953a15ed-dd26-411f-ab8a-bc8cb14b6cd4.csv?pretty=false HTTP/1.1" 200 314 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:46:11,881] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGGREGATOR Test on 11 rows
INFO  [2023-01-06 20:46:11,881] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-06 20:46:11,881] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-06 20:46:11,881] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-06 20:46:11,881] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR Test_774db1e6-bb8e-4c66-91fc-043f1d522cb3
INFO  [2023-01-06 20:46:11,882] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR Test_fb3df928-e737-4179-8a17-77a1471bef49
INFO  [2023-01-06 20:46:11,892] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-06 20:46:11,900] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR Test_fb3df928-e737-4179-8a17-77a1471bef49
INFO  [2023-01-06 20:46:11,900] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR Test_774db1e6-bb8e-4c66-91fc-043f1d522cb3
INFO  [2023-01-06 20:46:11,985] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:11,985] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,014] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-06 20:46:12,015] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-06 20:46:12,015] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:12,015] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:12,016] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-06 20:46:12,016] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-06 20:46:12,017] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:12,017] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_734cff28-6f5f-40bd-914f-3a0eb1a49249 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_734cff28-6f5f-40bd-914f-3a0eb1a49249 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_3b11208f-f314-4a6a-8338-b3e4fc7260ee are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_3b11208f-f314-4a6a-8338-b3e4fc7260ee are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:12,018] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,122] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,132] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,133] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR2$20Test.table
INFO  [2023-01-06 20:46:12,133] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR2$20Test.table
INFO  [2023-01-06 20:46:12,249] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,355] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:12,355] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:12,355] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 381 B in total
INFO  [2023-01-06 20:46:12,355] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGGREGATOR2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000429158sINFO  [2023-01-06 20:46:12,411] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:46:12,411] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6a8f0772)
INFO  [2023-01-06 20:46:12,412] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@1e9c5707), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@2c3fc61c), dateReader=com.bakdata.conquery.util.DateReader@655adb7, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:46:12,414] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGGREGATOR2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:12,414] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:12,415] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DATE_DISTANCE_AGGREGATOR2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:12,447] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DATE_DISTANCE_AGGREGATOR2$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:12 +0000] "POST /admin/datasets/DATE_DISTANCE_AGGREGATOR2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DATE_DISTANCE_AGGREGATOR2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:12,448] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,457] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:12,457] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:12,457] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:12,459] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:46:12,459] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR2$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:12,459] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR2$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:12,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.0
WARN  [2023-01-06 20:46:12,460] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:12,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.2
INFO  [2023-01-06 20:46:12,460] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.1
INFO  [2023-01-06 20:46:12,461] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.3
INFO  [2023-01-06 20:46:12,566] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,571] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,585] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,586] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:12,586] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:12,691] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGGREGATOR2 Test QUERY INIT
INFO  [2023-01-06 20:46:12,704] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGGREGATOR2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:12,704] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[fa965a63-7839-48f7-a9f4-87d3e006e5f8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR2 Test))]]
INFO  [2023-01-06 20:46:12,707] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8
INFO  [2023-01-06 20:46:12,707] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8
127.0.0.1 - - [06/Jan/2023:20:46:12 +0000] "POST /api/datasets/DATE_DISTANCE_AGGREGATOR2$20Test/queries HTTP/1.1" 201 1457 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:46:12,708] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8] with 4 results within PT0.001542S
INFO  [2023-01-06 20:46:12,708] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8] with 6 results within PT0.001796S
INFO  [2023-01-06 20:46:12,709] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8, workerId=DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_3b11208f-f314-4a6a-8338-b3e4fc7260ee, startTime=2023-01-06T20:46:12.707177, finishTime=2023-01-06T20:46:12.708719) of size 4
INFO  [2023-01-06 20:46:12,709] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8, workerId=DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_734cff28-6f5f-40bd-914f-3a0eb1a49249, startTime=2023-01-06T20:46:12.707119, finishTime=2023-01-06T20:46:12.708915) of size 6
INFO  [2023-01-06 20:46:12,709] com.bakdata.conquery.models.execution.ManagedExecution: DONE fa965a63-7839-48f7-a9f4-87d3e006e5f8 ManagedQuery within PT0.004612S
127.0.0.1 - - [06/Jan/2023:20:46:12 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR2$20Test/queries/DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8 HTTP/1.1" 200 1777 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:12,738] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test], queryId=fa965a63-7839-48f7-a9f4-87d3e006e5f8, label=concept	@§$, creationTime=2023-01-06T20:46:12.704869, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f2fdf54[Count = 0], startTime=2023-01-06T20:46:12.704990, finishTime=2023-01-06T20:46:12.709602, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@546655ef), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5da0add6, com.bakdata.conquery.models.query.ColumnDescriptor@48555a4a, com.bakdata.conquery.models.query.ColumnDescriptor@3971e96b]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:12,738] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test], queryId=fa965a63-7839-48f7-a9f4-87d3e006e5f8, label=concept	@§$, creationTime=2023-01-06T20:46:12.704869, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f2fdf54[Count = 0], startTime=2023-01-06T20:46:12.704990, finishTime=2023-01-06T20:46:12.709602, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@546655ef), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5da0add6, com.bakdata.conquery.models.query.ColumnDescriptor@48555a4a, com.bakdata.conquery.models.query.ColumnDescriptor@3971e96b]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test]
127.0.0.1 - - [06/Jan/2023:20:46:12 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR2%20Test/result/DATE_DISTANCE_AGGREGATOR2$20Test.fa965a63-7839-48f7-a9f4-87d3e006e5f8.csv?pretty=false HTTP/1.1" 200 314 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:46:12,753] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGGREGATOR2 Test on 11 rows
INFO  [2023-01-06 20:46:12,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-06 20:46:12,754] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-06 20:46:12,754] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-06 20:46:12,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR2 Test_3b11208f-f314-4a6a-8338-b3e4fc7260ee
INFO  [2023-01-06 20:46:12,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR2 Test_734cff28-6f5f-40bd-914f-3a0eb1a49249
INFO  [2023-01-06 20:46:12,819] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR2 Test_734cff28-6f5f-40bd-914f-3a0eb1a49249
INFO  [2023-01-06 20:46:12,819] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-06 20:46:12,819] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR2 Test_3b11208f-f314-4a6a-8338-b3e4fc7260ee
INFO  [2023-01-06 20:46:12,868] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGGREGATOR2$20Test
INFO  [2023-01-06 20:46:12,868] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:12,992] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-06 20:46:12,993] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:12,993] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:12,993] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:12,994] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:12,994] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:12,994] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:12,994] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:12,996] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_def3f372-6b30-423e-b859-4c9f12f6ffba are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:12,996] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_def3f372-6b30-423e-b859-4c9f12f6ffba are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:12,996] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:12,996] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_c3b44f75-fd8d-4bb7-93e0-ca4d65923363 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:12,996] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_c3b44f75-fd8d-4bb7-93e0-ca4d65923363 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:12,996] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:13,000] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,100] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,107] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,107] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:13,107] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:13,225] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,333] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:13,333] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:13,333] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 166 B in total
INFO  [2023-01-06 20:46:13,333] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000354637sINFO  [2023-01-06 20:46:13,369] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:46:13,370] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14610, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@157b8b13), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14640, maxValue=15928), dateReader=com.bakdata.conquery.util.DateReader@7ed981f8), dateReader=com.bakdata.conquery.util.DateReader@1a0e90c4, onlyQuarters=false, maxValue=15928, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:46:13,372] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:13,372] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:13,372] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_DURATION_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:13,385] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DURATION_SUM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:13,385] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:13 +0000] "POST /admin/datasets/DURATION_SUM_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_DURATION_SUM_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:13,386] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:13,386] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:13,386] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:13,387] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:13,388] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_AGGREGATOR$20Test.table.table], containing 6 entries.
INFO  [2023-01-06 20:46:13,388] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_AGGREGATOR$20Test.table.table], containing 6 entries.
WARN  [2023-01-06 20:46:13,388] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:13,388] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:13,389] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:13,493] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,499] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,513] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,513] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:13,513] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:13,619] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:13,633] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:13,633] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[fe72d02d-1817-4406-8585-1fcd82d9f5f2] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:13,636] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2
INFO  [2023-01-06 20:46:13,636] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2
INFO  [2023-01-06 20:46:13,637] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2] with 1 results within PT0.000807S
INFO  [2023-01-06 20:46:13,637] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2] with 3 results within PT0.000895S
127.0.0.1 - - [06/Jan/2023:20:46:13 +0000] "POST /api/datasets/DURATION_SUM_AGGREGATOR$20Test/queries HTTP/1.1" 201 1347 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:13,637] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2, workerId=DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_def3f372-6b30-423e-b859-4c9f12f6ffba, startTime=2023-01-06T20:46:13.636379, finishTime=2023-01-06T20:46:13.637186) of size 1
INFO  [2023-01-06 20:46:13,637] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2, workerId=DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_c3b44f75-fd8d-4bb7-93e0-ca4d65923363, startTime=2023-01-06T20:46:13.636361, finishTime=2023-01-06T20:46:13.637256) of size 3
INFO  [2023-01-06 20:46:13,638] com.bakdata.conquery.models.execution.ManagedExecution: DONE fe72d02d-1817-4406-8585-1fcd82d9f5f2 ManagedQuery within PT0.004363S
127.0.0.1 - - [06/Jan/2023:20:46:13 +0000] "GET /api/datasets/DURATION_SUM_AGGREGATOR$20Test/queries/DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2 HTTP/1.1" 200 1658 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:13,658] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test], queryId=fe72d02d-1817-4406-8585-1fcd82d9f5f2, label=concept	@§$, creationTime=2023-01-06T20:46:13.633504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@73684439[Count = 0], startTime=2023-01-06T20:46:13.633679, finishTime=2023-01-06T20:46:13.638042, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@556fbee3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6ae4a8b9, com.bakdata.conquery.models.query.ColumnDescriptor@78cf44b8, com.bakdata.conquery.models.query.ColumnDescriptor@49379e14]) download on dataset Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:13,658] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test], queryId=fe72d02d-1817-4406-8585-1fcd82d9f5f2, label=concept	@§$, creationTime=2023-01-06T20:46:13.633504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@73684439[Count = 0], startTime=2023-01-06T20:46:13.633679, finishTime=2023-01-06T20:46:13.638042, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@556fbee3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6ae4a8b9, com.bakdata.conquery.models.query.ColumnDescriptor@78cf44b8, com.bakdata.conquery.models.query.ColumnDescriptor@49379e14]) on dataset Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:13 +0000] "GET /api/datasets/DURATION_SUM_AGGREGATOR%20Test/result/DURATION_SUM_AGGREGATOR$20Test.fe72d02d-1817-4406-8585-1fcd82d9f5f2.csv?pretty=false HTTP/1.1" 200 142 "-" "Conquery (test client)" 25
INFO  [2023-01-06 20:46:13,682] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DURATION_SUM_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:13,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:13,682] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:13,682] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:13,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_AGGREGATOR Test_def3f372-6b30-423e-b859-4c9f12f6ffba
INFO  [2023-01-06 20:46:13,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_AGGREGATOR Test_c3b44f75-fd8d-4bb7-93e0-ca4d65923363
INFO  [2023-01-06 20:46:13,694] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:13,695] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_AGGREGATOR Test_def3f372-6b30-423e-b859-4c9f12f6ffba
INFO  [2023-01-06 20:46:13,695] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_AGGREGATOR Test_c3b44f75-fd8d-4bb7-93e0-ca4d65923363
INFO  [2023-01-06 20:46:13,789] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:13,789] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,819] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:13,819] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-06 20:46:13,819] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:13,819] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:13,820] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-06 20:46:13,820] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-06 20:46:13,820] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:13,820] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:13,821] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_c5d79cb5-aa3f-437f-b183-00bd4b10b4e9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:13,821] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_c5d79cb5-aa3f-437f-b183-00bd4b10b4e9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:13,821] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:13,821] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_4299a1e2-e3c4-4026-8e3d-069313fb0b86 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:13,821] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_4299a1e2-e3c4-4026-8e3d-069313fb0b86 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:13,821] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:13,826] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,925] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,932] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:13,932] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table
INFO  [2023-01-06 20:46:13,932] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table
INFO  [2023-01-06 20:46:14,049] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,160] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:14,160] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:14,160] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 388 B in total
INFO  [2023-01-06 20:46:14,161] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000383103sINFO  [2023-01-06 20:46:14,199] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:46:14,199] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@2e553f26)
INFO  [2023-01-06 20:46:14,199] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@21253f30), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@5d7d607c), dateReader=com.bakdata.conquery.util.DateReader@f2052e9, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:46:14,201] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:14,201] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:14,201] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:14,214] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table
INFO  [2023-01-06 20:46:14,214] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:14 +0000] "POST /admin/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:14,215] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:14,215] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:14,215] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:14,216] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:46:14,216] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:14,216] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table], containing 10 entries.
WARN  [2023-01-06 20:46:14,217] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:14,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.0
INFO  [2023-01-06 20:46:14,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.1
INFO  [2023-01-06 20:46:14,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.2
INFO  [2023-01-06 20:46:14,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.3
INFO  [2023-01-06 20:46:14,330] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,335] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,349] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,349] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:14,349] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:14,455] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test QUERY INIT
INFO  [2023-01-06 20:46:14,466] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:14,467] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[65347904-0e15-4fbf-92e8-1977903160ef] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test))]]
INFO  [2023-01-06 20:46:14,469] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef
INFO  [2023-01-06 20:46:14,469] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef
127.0.0.1 - - [06/Jan/2023:20:46:14 +0000] "POST /api/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test/queries HTTP/1.1" 201 1675 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:14,472] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef] with 4 results within PT0.002809S
INFO  [2023-01-06 20:46:14,473] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef, workerId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_4299a1e2-e3c4-4026-8e3d-069313fb0b86, startTime=2023-01-06T20:46:14.469856, finishTime=2023-01-06T20:46:14.472665) of size 4
INFO  [2023-01-06 20:46:14,474] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef] with 6 results within PT0.004585S
INFO  [2023-01-06 20:46:14,474] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef, workerId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_c5d79cb5-aa3f-437f-b183-00bd4b10b4e9, startTime=2023-01-06T20:46:14.469827, finishTime=2023-01-06T20:46:14.474412) of size 6
INFO  [2023-01-06 20:46:14,474] com.bakdata.conquery.models.execution.ManagedExecution: DONE 65347904-0e15-4fbf-92e8-1977903160ef ManagedQuery within PT0.007314S
127.0.0.1 - - [06/Jan/2023:20:46:14 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test/queries/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef HTTP/1.1" 200 2039 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:14,488] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test], queryId=65347904-0e15-4fbf-92e8-1977903160ef, label=concept	@§$, creationTime=2023-01-06T20:46:14.467380, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@74abed25[Count = 0], startTime=2023-01-06T20:46:14.467554, finishTime=2023-01-06T20:46:14.474868, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@26ac39a6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3000cca0, com.bakdata.conquery.models.query.ColumnDescriptor@1632b38b, com.bakdata.conquery.models.query.ColumnDescriptor@3ddba6ad, com.bakdata.conquery.models.query.ColumnDescriptor@7a7dd9b9]) download on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:14,489] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test], queryId=65347904-0e15-4fbf-92e8-1977903160ef, label=concept	@§$, creationTime=2023-01-06T20:46:14.467380, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@74abed25[Count = 0], startTime=2023-01-06T20:46:14.467554, finishTime=2023-01-06T20:46:14.474868, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@26ac39a6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3000cca0, com.bakdata.conquery.models.query.ColumnDescriptor@1632b38b, com.bakdata.conquery.models.query.ColumnDescriptor@3ddba6ad, com.bakdata.conquery.models.query.ColumnDescriptor@7a7dd9b9]) on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
127.0.0.1 - - [06/Jan/2023:20:46:14 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION%20Test/result/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.65347904-0e15-4fbf-92e8-1977903160ef.csv?pretty=false HTTP/1.1" 200 784 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:46:14,506] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test on 11 rows
INFO  [2023-01-06 20:46:14,506] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-06 20:46:14,506] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-06 20:46:14,506] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-06 20:46:14,506] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_4299a1e2-e3c4-4026-8e3d-069313fb0b86
INFO  [2023-01-06 20:46:14,506] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_c5d79cb5-aa3f-437f-b183-00bd4b10b4e9
INFO  [2023-01-06 20:46:14,526] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_4299a1e2-e3c4-4026-8e3d-069313fb0b86
INFO  [2023-01-06 20:46:14,526] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_c5d79cb5-aa3f-437f-b183-00bd4b10b4e9
INFO  [2023-01-06 20:46:14,526] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-06 20:46:14,626] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test
INFO  [2023-01-06 20:46:14,626] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,654] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-06 20:46:14,655] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-06 20:46:14,655] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:14,655] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:14,656] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-06 20:46:14,656] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:14,656] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-06 20:46:14,656] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_b636577a-46d9-48e7-9b00-ea625bedfefb are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_b636577a-46d9-48e7-9b00-ea625bedfefb are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_aeafc31e-a6a2-4314-b91f-422f810f1b81 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_aeafc31e-a6a2-4314-b91f-422f810f1b81 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:14,657] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:14,761] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,767] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,768] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table
INFO  [2023-01-06 20:46:14,768] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table
INFO  [2023-01-06 20:46:14,881] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:14,990] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:14,990] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:14,990] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 388 B in total
INFO  [2023-01-06 20:46:14,990] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EVENT_DATE_AGGREGATOR_RESTRICTION Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000342815sINFO  [2023-01-06 20:46:15,025] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-06 20:46:15,025] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6691c201)
INFO  [2023-01-06 20:46:15,025] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@24b36eaf), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@2ce709b6), dateReader=com.bakdata.conquery.util.DateReader@1aafe2d6, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-06 20:46:15,027] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EVENT_DATE_AGGREGATOR_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:15,027] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:15,027] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EVENT_DATE_AGGREGATOR_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:15,046] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table
INFO  [2023-01-06 20:46:15,046] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:15 +0000] "POST /admin/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_EVENT_DATE_AGGREGATOR_RESTRICTION+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:46:15,047] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:15,047] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:15,047] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:15,048] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-06 20:46:15,048] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:15,048] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:15,049] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.0
WARN  [2023-01-06 20:46:15,049] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:15,049] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.2
INFO  [2023-01-06 20:46:15,049] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.1
INFO  [2023-01-06 20:46:15,050] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.3
INFO  [2023-01-06 20:46:15,154] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,159] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,174] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,174] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:15,174] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:15,280] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EVENT_DATE_AGGREGATOR_RESTRICTION Test QUERY INIT
INFO  [2023-01-06 20:46:15,295] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:15,296] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[377772e5-46ac-4c67-906b-f100ee8971d5] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_RESTRICTION Test))]]
INFO  [2023-01-06 20:46:15,300] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5
INFO  [2023-01-06 20:46:15,300] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5
127.0.0.1 - - [06/Jan/2023:20:46:15 +0000] "POST /api/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test/queries HTTP/1.1" 201 1756 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:46:15,302] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5] with 4 results within PT0.001504S
INFO  [2023-01-06 20:46:15,302] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5] with 6 results within PT0.001816S
INFO  [2023-01-06 20:46:15,303] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5, workerId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_b636577a-46d9-48e7-9b00-ea625bedfefb, startTime=2023-01-06T20:46:15.300906, finishTime=2023-01-06T20:46:15.302410) of size 4
INFO  [2023-01-06 20:46:15,303] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5, workerId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_aeafc31e-a6a2-4314-b91f-422f810f1b81, startTime=2023-01-06T20:46:15.300778, finishTime=2023-01-06T20:46:15.302594) of size 6
INFO  [2023-01-06 20:46:15,303] com.bakdata.conquery.models.execution.ManagedExecution: DONE 377772e5-46ac-4c67-906b-f100ee8971d5 ManagedQuery within PT0.007062S
127.0.0.1 - - [06/Jan/2023:20:46:15 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test/queries/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5 HTTP/1.1" 200 2108 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:15,333] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test], queryId=377772e5-46ac-4c67-906b-f100ee8971d5, label=concept	@§$, creationTime=2023-01-06T20:46:15.296014, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7d1d3e0a[Count = 0], startTime=2023-01-06T20:46:15.296231, finishTime=2023-01-06T20:46:15.303293, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@660815d1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@745f5ffd, com.bakdata.conquery.models.query.ColumnDescriptor@6ed11164, com.bakdata.conquery.models.query.ColumnDescriptor@7aa88fda, com.bakdata.conquery.models.query.ColumnDescriptor@5ca8bbc3]) download on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:15,334] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test], queryId=377772e5-46ac-4c67-906b-f100ee8971d5, label=concept	@§$, creationTime=2023-01-06T20:46:15.296014, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7d1d3e0a[Count = 0], startTime=2023-01-06T20:46:15.296231, finishTime=2023-01-06T20:46:15.303293, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@660815d1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@745f5ffd, com.bakdata.conquery.models.query.ColumnDescriptor@6ed11164, com.bakdata.conquery.models.query.ColumnDescriptor@7aa88fda, com.bakdata.conquery.models.query.ColumnDescriptor@5ca8bbc3]) on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
127.0.0.1 - - [06/Jan/2023:20:46:15 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION%20Test/result/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.377772e5-46ac-4c67-906b-f100ee8971d5.csv?pretty=false HTTP/1.1" 200 784 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:46:15,354] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EVENT_DATE_AGGREGATOR_RESTRICTION Test on 11 rows
INFO  [2023-01-06 20:46:15,354] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-06 20:46:15,354] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-06 20:46:15,354] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-06 20:46:15,355] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_b636577a-46d9-48e7-9b00-ea625bedfefb
INFO  [2023-01-06 20:46:15,355] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_aeafc31e-a6a2-4314-b91f-422f810f1b81
INFO  [2023-01-06 20:46:15,356] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-06 20:46:15,357] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_b636577a-46d9-48e7-9b00-ea625bedfefb
INFO  [2023-01-06 20:46:15,357] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_aeafc31e-a6a2-4314-b91f-422f810f1b81
INFO  [2023-01-06 20:46:15,452] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EVENT_DATE_AGGREGATOR_RESTRICTION$20Test
INFO  [2023-01-06 20:46:15,452] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,480] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-06 20:46:15,480] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:15,480] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:15,480] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:15,481] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-06 20:46:15,481] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:15,481] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-06 20:46:15,481] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_aa98b8ee-2ce8-4c7a-9388-9305ab73a853 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_aa98b8ee-2ce8-4c7a-9388-9305ab73a853 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_b51c9e3a-8818-4223-91f6-b868d01c10f9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_b51c9e3a-8818-4223-91f6-b868d01c10f9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:15,483] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:15,587] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,593] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,593] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table
INFO  [2023-01-06 20:46:15,594] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table
INFO  [2023-01-06 20:46:15,711] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,819] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:15,819] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:15,819] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 210 B in total
INFO  [2023-01-06 20:46:15,819] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000395334sINFO  [2023-01-06 20:46:15,859] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-06 20:46:15,859] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=7, nullLines=0), minParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16251, maxValue=16251), dateReader=com.bakdata.conquery.util.DateReader@215f76f4), maxParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@103fb47a), dateReader=com.bakdata.conquery.util.DateReader@ce79fb6, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-06 20:46:15,859] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=7, nullLines=2), requiredPrecision=4.9E-324, floatULP=2.384185791015625E-7)
INFO  [2023-01-06 20:46:15,861] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:15,862] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:15,862] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:15,880] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table
INFO  [2023-01-06 20:46:15,880] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:15 +0000] "POST /admin/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_EXISTS_AGGREGATOR+%26+NUMBER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:46:15,881] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:15,881] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:15,881] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:15,884] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:15,884] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table], containing 7 entries.
INFO  [2023-01-06 20:46:15,885] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table], containing 7 entries.
WARN  [2023-01-06 20:46:15,886] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:15,886] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table.0
INFO  [2023-01-06 20:46:15,886] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table.1
INFO  [2023-01-06 20:46:15,992] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:15,998] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,010] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,011] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:16,011] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:16,116] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EXISTS_AGGREGATOR & NUMBER Test QUERY INIT
INFO  [2023-01-06 20:46:16,131] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EXISTS_AGGREGATOR$20&$20NUMBER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:16,131] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3ae0e359-82ee-47ab-9510-05b78d1cb793] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test))]]
INFO  [2023-01-06 20:46:16,135] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793
INFO  [2023-01-06 20:46:16,135] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793
127.0.0.1 - - [06/Jan/2023:20:46:16 +0000] "POST /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test/queries HTTP/1.1" 201 1483 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:16,137] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793] with 1 results within PT0.00124S
INFO  [2023-01-06 20:46:16,137] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793] with 2 results within PT0.001295S
INFO  [2023-01-06 20:46:16,137] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_aa98b8ee-2ce8-4c7a-9388-9305ab73a853, startTime=2023-01-06T20:46:16.135864, finishTime=2023-01-06T20:46:16.137159) of size 2
INFO  [2023-01-06 20:46:16,138] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_b51c9e3a-8818-4223-91f6-b868d01c10f9, startTime=2023-01-06T20:46:16.135923, finishTime=2023-01-06T20:46:16.137163) of size 1
INFO  [2023-01-06 20:46:16,138] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3ae0e359-82ee-47ab-9510-05b78d1cb793 ManagedQuery within PT0.006207S
127.0.0.1 - - [06/Jan/2023:20:46:16 +0000] "GET /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test/queries/EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793 HTTP/1.1" 200 1822 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:16,163] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test], queryId=3ae0e359-82ee-47ab-9510-05b78d1cb793, label=concept	@§$, creationTime=2023-01-06T20:46:16.131733, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@78bd3b4d[Count = 0], startTime=2023-01-06T20:46:16.131925, finishTime=2023-01-06T20:46:16.138132, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5c13578e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2b060193, com.bakdata.conquery.models.query.ColumnDescriptor@727d2bd4, com.bakdata.conquery.models.query.ColumnDescriptor@630b0185]) download on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:16,164] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test], queryId=3ae0e359-82ee-47ab-9510-05b78d1cb793, label=concept	@§$, creationTime=2023-01-06T20:46:16.131733, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@78bd3b4d[Count = 0], startTime=2023-01-06T20:46:16.131925, finishTime=2023-01-06T20:46:16.138132, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5c13578e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2b060193, com.bakdata.conquery.models.query.ColumnDescriptor@727d2bd4, com.bakdata.conquery.models.query.ColumnDescriptor@630b0185]) on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test]
127.0.0.1 - - [06/Jan/2023:20:46:16 +0000] "GET /api/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test/result/EXISTS_AGGREGATOR$20&$20NUMBER$20Test.3ae0e359-82ee-47ab-9510-05b78d1cb793.csv?pretty=false HTTP/1.1" 200 112 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:46:16,182] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EXISTS_AGGREGATOR & NUMBER Test on 4 rows
INFO  [2023-01-06 20:46:16,182] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:16,182] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-06 20:46:16,182] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-06 20:46:16,182] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test_b51c9e3a-8818-4223-91f6-b868d01c10f9
INFO  [2023-01-06 20:46:16,182] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test_aa98b8ee-2ce8-4c7a-9388-9305ab73a853
INFO  [2023-01-06 20:46:16,186] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test_b51c9e3a-8818-4223-91f6-b868d01c10f9
INFO  [2023-01-06 20:46:16,186] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test_aa98b8ee-2ce8-4c7a-9388-9305ab73a853
INFO  [2023-01-06 20:46:16,186] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:16,187] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EXISTS_AGGREGATOR$20&$20NUMBER$20Test
INFO  [2023-01-06 20:46:16,187] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,316] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:16,316] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:16,316] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:16,317] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:16,318] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-06 20:46:16,318] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-06 20:46:16,318] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:16,318] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:16,320] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_f50aeda3-7fc6-4961-ac1b-0b625139cd25 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:16,320] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_f50aeda3-7fc6-4961-ac1b-0b625139cd25 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:16,320] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:16,320] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_cdc4dd53-579c-4d81-ac9b-429a1c8c5a99 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:16,320] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_cdc4dd53-579c-4d81-ac9b-429a1c8c5a99 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:16,320] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:16,324] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,425] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,431] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table
INFO  [2023-01-06 20:46:16,432] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table
INFO  [2023-01-06 20:46:16,548] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,658] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:16,658] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:16,658] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 78 B in total
INFO  [2023-01-06 20:46:16,658] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000323504sINFO  [2023-01-06 20:46:16,691] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=4, min=1, average=1.333333, max=2}
INFO  [2023-01-06 20:46:16,691] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=18262, maxValue=18262), dateReader=com.bakdata.conquery.util.DateReader@7f4729a5)
INFO  [2023-01-06 20:46:16,691] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:16,694] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:16,694] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:16,694] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:16,707] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table
127.0.0.1 - - [06/Jan/2023:20:46:16 +0000] "POST /admin/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_EXISTS_AGGREGATOR+%26+NUMBER+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:16,707] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,708] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:16,708] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:16,708] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:16,710] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:46:16,710] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table.table], containing 4 entries.
INFO  [2023-01-06 20:46:16,710] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table.table], containing 4 entries.
WARN  [2023-01-06 20:46:16,710] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:16,711] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table.table.0
INFO  [2023-01-06 20:46:16,816] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,821] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,833] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:16,834] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:16,939] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EXISTS_AGGREGATOR & NUMBER Test QUERY INIT
INFO  [2023-01-06 20:46:16,955] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:16,955] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e82afdee-90f3-44c4-b888-a6749294b385] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[1]))]]
INFO  [2023-01-06 20:46:16,960] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].e82afdee-90f3-44c4-b888-a6749294b385
INFO  [2023-01-06 20:46:16,960] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].e82afdee-90f3-44c4-b888-a6749294b385
WARN  [2023-01-06 20:46:16,961] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:46:16,961] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].e82afdee-90f3-44c4-b888-a6749294b385] with 0 results within PT0.00026S
INFO  [2023-01-06 20:46:16,961] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].e82afdee-90f3-44c4-b888-a6749294b385] with 3 results within PT0.001124S
INFO  [2023-01-06 20:46:16,961] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].e82afdee-90f3-44c4-b888-a6749294b385, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_cdc4dd53-579c-4d81-ac9b-429a1c8c5a99, startTime=2023-01-06T20:46:16.960855, finishTime=2023-01-06T20:46:16.961115) of size 0
INFO  [2023-01-06 20:46:16,962] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].e82afdee-90f3-44c4-b888-a6749294b385, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_f50aeda3-7fc6-4961-ac1b-0b625139cd25, startTime=2023-01-06T20:46:16.960532, finishTime=2023-01-06T20:46:16.961656) of size 3
INFO  [2023-01-06 20:46:16,962] com.bakdata.conquery.models.execution.ManagedExecution: DONE e82afdee-90f3-44c4-b888-a6749294b385 ManagedQuery within PT0.006465S
127.0.0.1 - - [06/Jan/2023:20:46:16 +0000] "POST /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D/queries HTTP/1.1" 201 3469 "-" "Conquery (test client)" 10
127.0.0.1 - - [06/Jan/2023:20:46:16 +0000] "GET /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D/queries/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D.e82afdee-90f3-44c4-b888-a6749294b385 HTTP/1.1" 200 4132 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:16,990] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]], queryId=e82afdee-90f3-44c4-b888-a6749294b385, label=concept-1 concept-2	@§$, creationTime=2023-01-06T20:46:16.955613, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@65c8259c[Count = 0], startTime=2023-01-06T20:46:16.955836, finishTime=2023-01-06T20:46:16.962301, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@353c84a2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[1])), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7a078e8e, com.bakdata.conquery.models.query.ColumnDescriptor@1ffabd78, com.bakdata.conquery.models.query.ColumnDescriptor@3bba9fde, com.bakdata.conquery.models.query.ColumnDescriptor@6a72059f, com.bakdata.conquery.models.query.ColumnDescriptor@2cc785a1, com.bakdata.conquery.models.query.ColumnDescriptor@661cfc3c, com.bakdata.conquery.models.query.ColumnDescriptor@937db4e, com.bakdata.conquery.models.query.ColumnDescriptor@7393790a]) download on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:16,990] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]], queryId=e82afdee-90f3-44c4-b888-a6749294b385, label=concept-1 concept-2	@§$, creationTime=2023-01-06T20:46:16.955613, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@65c8259c[Count = 0], startTime=2023-01-06T20:46:16.955836, finishTime=2023-01-06T20:46:16.962301, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@353c84a2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[1])), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7a078e8e, com.bakdata.conquery.models.query.ColumnDescriptor@1ffabd78, com.bakdata.conquery.models.query.ColumnDescriptor@3bba9fde, com.bakdata.conquery.models.query.ColumnDescriptor@6a72059f, com.bakdata.conquery.models.query.ColumnDescriptor@2cc785a1, com.bakdata.conquery.models.query.ColumnDescriptor@661cfc3c, com.bakdata.conquery.models.query.ColumnDescriptor@937db4e, com.bakdata.conquery.models.query.ColumnDescriptor@7393790a]) on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]]
127.0.0.1 - - [06/Jan/2023:20:46:17 +0000] "GET /api/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B1%5D/result/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D.e82afdee-90f3-44c4-b888-a6749294b385.csv?pretty=false HTTP/1.1" 200 206 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:46:17,010] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EXISTS_AGGREGATOR & NUMBER Test on 4 rows
INFO  [2023-01-06 20:46:17,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EXISTS_AGGREGATOR & NUMBER Test[1]
INFO  [2023-01-06 20:46:17,011] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-06 20:46:17,011] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-06 20:46:17,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[1]_cdc4dd53-579c-4d81-ac9b-429a1c8c5a99
INFO  [2023-01-06 20:46:17,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[1]_f50aeda3-7fc6-4961-ac1b-0b625139cd25
INFO  [2023-01-06 20:46:17,023] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[1]_cdc4dd53-579c-4d81-ac9b-429a1c8c5a99
INFO  [2023-01-06 20:46:17,023] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[1]_f50aeda3-7fc6-4961-ac1b-0b625139cd25
INFO  [2023-01-06 20:46:17,024] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EXISTS_AGGREGATOR & NUMBER Test[1]
INFO  [2023-01-06 20:46:17,111] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]
INFO  [2023-01-06 20:46:17,111] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,140] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:17,140] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:17,140] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:17,140] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:17,141] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-06 20:46:17,141] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:17,141] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-06 20:46:17,141] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_5ce92ef2-1511-4a54-a345-e5844243f2d2 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_5ce92ef2-1511-4a54-a345-e5844243f2d2 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_49f5394a-0f6b-4f59-a332-fdb27fdffd96 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_49f5394a-0f6b-4f59-a332-fdb27fdffd96 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:17,142] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:17,247] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,253] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,254] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table
INFO  [2023-01-06 20:46:17,254] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table
INFO  [2023-01-06 20:46:17,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,481] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:17,481] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:17,481] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 210 B in total
INFO  [2023-01-06 20:46:17,481] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test[2]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000490173sINFO  [2023-01-06 20:46:17,531] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-06 20:46:17,531] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=7, nullLines=0), minParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16251, maxValue=16251), dateReader=com.bakdata.conquery.util.DateReader@48b18c32), maxParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@17c15e9d), dateReader=com.bakdata.conquery.util.DateReader@62683036, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-06 20:46:17,531] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=7, nullLines=2), requiredPrecision=4.9E-324, floatULP=2.384185791015625E-7)
INFO  [2023-01-06 20:46:17,535] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:17,535] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:17,535] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_EXISTS_AGGREGATOR & NUMBER Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:17,552] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table
INFO  [2023-01-06 20:46:17,552] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:17 +0000] "POST /admin/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_EXISTS_AGGREGATOR+%26+NUMBER+Test%5B2%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:17,553] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:17,553] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:17,553] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:17,554] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:17,555] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table], containing 7 entries.
INFO  [2023-01-06 20:46:17,555] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table], containing 7 entries.
WARN  [2023-01-06 20:46:17,555] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:17,556] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table.0
INFO  [2023-01-06 20:46:17,556] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table.1
INFO  [2023-01-06 20:46:17,661] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,666] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,676] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:17,676] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:17,676] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:17,797] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EXISTS_AGGREGATOR & NUMBER Test QUERY INIT
INFO  [2023-01-06 20:46:17,814] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:17,815] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[22b8ad95-bd95-41f0-bd58-0c4cb4a3017c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[2]))]]
INFO  [2023-01-06 20:46:17,820] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].22b8ad95-bd95-41f0-bd58-0c4cb4a3017c
127.0.0.1 - - [06/Jan/2023:20:46:17 +0000] "POST /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D/queries HTTP/1.1" 201 2203 "-" "Conquery (test client)" 11
INFO  [2023-01-06 20:46:17,823] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].22b8ad95-bd95-41f0-bd58-0c4cb4a3017c
INFO  [2023-01-06 20:46:17,833] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].22b8ad95-bd95-41f0-bd58-0c4cb4a3017c] with 2 results within PT0.012672S
INFO  [2023-01-06 20:46:17,834] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].22b8ad95-bd95-41f0-bd58-0c4cb4a3017c, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_49f5394a-0f6b-4f59-a332-fdb27fdffd96, startTime=2023-01-06T20:46:17.820737, finishTime=2023-01-06T20:46:17.833409) of size 2
INFO  [2023-01-06 20:46:17,834] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].22b8ad95-bd95-41f0-bd58-0c4cb4a3017c] with 1 results within PT0.010909S
INFO  [2023-01-06 20:46:17,835] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].22b8ad95-bd95-41f0-bd58-0c4cb4a3017c, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_5ce92ef2-1511-4a54-a345-e5844243f2d2, startTime=2023-01-06T20:46:17.823885, finishTime=2023-01-06T20:46:17.834794) of size 1
INFO  [2023-01-06 20:46:17,835] com.bakdata.conquery.models.execution.ManagedExecution: DONE 22b8ad95-bd95-41f0-bd58-0c4cb4a3017c ManagedQuery within PT0.02008S
127.0.0.1 - - [06/Jan/2023:20:46:17 +0000] "GET /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D/queries/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D.22b8ad95-bd95-41f0-bd58-0c4cb4a3017c HTTP/1.1" 200 2866 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:17,844] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]], queryId=22b8ad95-bd95-41f0-bd58-0c4cb4a3017c, label=concept	@§$, creationTime=2023-01-06T20:46:17.814918, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5321d871[Count = 0], startTime=2023-01-06T20:46:17.815150, finishTime=2023-01-06T20:46:17.835230, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7c047aa0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d98c2fd, com.bakdata.conquery.models.query.ColumnDescriptor@32c23fff, com.bakdata.conquery.models.query.ColumnDescriptor@6ffd8b86, com.bakdata.conquery.models.query.ColumnDescriptor@30733da3]) download on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:17,844] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]], queryId=22b8ad95-bd95-41f0-bd58-0c4cb4a3017c, label=concept	@§$, creationTime=2023-01-06T20:46:17.814918, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5321d871[Count = 0], startTime=2023-01-06T20:46:17.815150, finishTime=2023-01-06T20:46:17.835230, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7c047aa0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d98c2fd, com.bakdata.conquery.models.query.ColumnDescriptor@32c23fff, com.bakdata.conquery.models.query.ColumnDescriptor@6ffd8b86, com.bakdata.conquery.models.query.ColumnDescriptor@30733da3]) on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]]
127.0.0.1 - - [06/Jan/2023:20:46:17 +0000] "GET /api/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B2%5D/result/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D.22b8ad95-bd95-41f0-bd58-0c4cb4a3017c.csv?pretty=false HTTP/1.1" 200 135 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:17,845] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EXISTS_AGGREGATOR & NUMBER Test on 4 rows
INFO  [2023-01-06 20:46:17,845] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EXISTS_AGGREGATOR & NUMBER Test[2]
INFO  [2023-01-06 20:46:17,846] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-06 20:46:17,846] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-06 20:46:17,846] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[2]_5ce92ef2-1511-4a54-a345-e5844243f2d2
INFO  [2023-01-06 20:46:17,846] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[2]_49f5394a-0f6b-4f59-a332-fdb27fdffd96
INFO  [2023-01-06 20:46:17,941] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EXISTS_AGGREGATOR & NUMBER Test[2]
INFO  [2023-01-06 20:46:17,942] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[2]_49f5394a-0f6b-4f59-a332-fdb27fdffd96
INFO  [2023-01-06 20:46:17,942] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[2]_5ce92ef2-1511-4a54-a345-e5844243f2d2
INFO  [2023-01-06 20:46:17,956] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]
INFO  [2023-01-06 20:46:17,956] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,097] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-06 20:46:18,097] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FIRST_AGGREGATOR Test
INFO  [2023-01-06 20:46:18,097] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:18,097] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:18,098] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:18,098] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:18,098] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:18,098] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_7d08a350-0421-4ffe-a17f-2f2860823511 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_7d08a350-0421-4ffe-a17f-2f2860823511 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_f702b409-10dc-4c64-9717-c825908f71aa are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_f702b409-10dc-4c64-9717-c825908f71aa are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:18,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:18,204] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,212] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,212] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FIRST_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:18,212] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FIRST_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:18,324] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,431] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:18,431] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:18,431] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 161 B in total
INFO  [2023-01-06 20:46:18,431] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FIRST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000384807sINFO  [2023-01-06 20:46:18,470] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=2}
INFO  [2023-01-06 20:46:18,470] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=10, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:18,470] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=2), subType=IntegerParser(super=Parser(lines=10, nullLines=2), minValue=14805, maxValue=15341), dateReader=com.bakdata.conquery.util.DateReader@17955c70)
INFO  [2023-01-06 20:46:18,473] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FIRST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:18,473] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:18,473] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_FIRST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:18,491] com.bakdata.conquery.models.jobs.ImportJob: Importing table into FIRST_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:18,492] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:18 +0000] "POST /admin/datasets/FIRST_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_FIRST_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-06 20:46:18,492] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:18,493] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:18,493] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:18,495] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:18,495] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FIRST_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:18,495] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FIRST_AGGREGATOR$20Test.table.table], containing 10 entries.
WARN  [2023-01-06 20:46:18,496] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:18,496] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FIRST_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:18,496] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FIRST_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:18,602] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,608] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,617] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,617] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:18,617] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:18,723] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: FIRST_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:18,738] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[FIRST_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:18,738] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f7db2db8-b17e-4fa4-a064-646d6d9106fb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_FIRST_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:18,741] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb
INFO  [2023-01-06 20:46:18,741] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb
INFO  [2023-01-06 20:46:18,742] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb] with 3 results within PT0.001005S
127.0.0.1 - - [06/Jan/2023:20:46:18 +0000] "POST /api/datasets/FIRST_AGGREGATOR$20Test/queries HTTP/1.1" 201 1311 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:18,742] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb] with 2 results within PT0.00113S
INFO  [2023-01-06 20:46:18,743] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb, workerId=FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_7d08a350-0421-4ffe-a17f-2f2860823511, startTime=2023-01-06T20:46:18.741487, finishTime=2023-01-06T20:46:18.742492) of size 3
INFO  [2023-01-06 20:46:18,743] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb, workerId=FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_f702b409-10dc-4c64-9717-c825908f71aa, startTime=2023-01-06T20:46:18.741538, finishTime=2023-01-06T20:46:18.742668) of size 2
INFO  [2023-01-06 20:46:18,743] com.bakdata.conquery.models.execution.ManagedExecution: DONE f7db2db8-b17e-4fa4-a064-646d6d9106fb ManagedQuery within PT0.004807S
127.0.0.1 - - [06/Jan/2023:20:46:18 +0000] "GET /api/datasets/FIRST_AGGREGATOR$20Test/queries/FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb HTTP/1.1" 200 1594 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:18,768] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=FIRST_AGGREGATOR Test], queryId=f7db2db8-b17e-4fa4-a064-646d6d9106fb, label=concept	@§$, creationTime=2023-01-06T20:46:18.738332, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@41ba2b64[Count = 0], startTime=2023-01-06T20:46:18.738509, finishTime=2023-01-06T20:46:18.743316, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4e09c812), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_FIRST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3a45bfdf, com.bakdata.conquery.models.query.ColumnDescriptor@6be7530, com.bakdata.conquery.models.query.ColumnDescriptor@16cf9f88]) download on dataset Dataset[label=null, name=FIRST_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:18,768] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=FIRST_AGGREGATOR Test], queryId=f7db2db8-b17e-4fa4-a064-646d6d9106fb, label=concept	@§$, creationTime=2023-01-06T20:46:18.738332, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@41ba2b64[Count = 0], startTime=2023-01-06T20:46:18.738509, finishTime=2023-01-06T20:46:18.743316, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4e09c812), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_FIRST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3a45bfdf, com.bakdata.conquery.models.query.ColumnDescriptor@6be7530, com.bakdata.conquery.models.query.ColumnDescriptor@16cf9f88]) on dataset Dataset[label=null, name=FIRST_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:18 +0000] "GET /api/datasets/FIRST_AGGREGATOR%20Test/result/FIRST_AGGREGATOR$20Test.f7db2db8-b17e-4fa4-a064-646d6d9106fb.csv?pretty=false HTTP/1.1" 200 167 "-" "Conquery (test client)" 17
INFO  [2023-01-06 20:46:18,785] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest FIRST_AGGREGATOR Test on 6 rows
INFO  [2023-01-06 20:46:18,785] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FIRST_AGGREGATOR Test
INFO  [2023-01-06 20:46:18,785] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:18,785] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:18,785] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FIRST_AGGREGATOR Test_7d08a350-0421-4ffe-a17f-2f2860823511
INFO  [2023-01-06 20:46:18,785] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FIRST_AGGREGATOR Test_f702b409-10dc-4c64-9717-c825908f71aa
INFO  [2023-01-06 20:46:18,799] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FIRST_AGGREGATOR Test
INFO  [2023-01-06 20:46:18,799] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FIRST_AGGREGATOR Test_f702b409-10dc-4c64-9717-c825908f71aa
INFO  [2023-01-06 20:46:18,800] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FIRST_AGGREGATOR Test_7d08a350-0421-4ffe-a17f-2f2860823511
INFO  [2023-01-06 20:46:18,897] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FIRST_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:18,897] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:18,922] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FIRST_AGGREGATOR Test
INFO  [2023-01-06 20:46:18,923] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test LAST_AGGREGATOR Test
INFO  [2023-01-06 20:46:18,923] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:18,923] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:18,924] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:18,924] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:18,924] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:18,924] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:18,926] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_30a31e4c-61f4-4c65-a151-7e315f7e3d50 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:18,926] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_30a31e4c-61f4-4c65-a151-7e315f7e3d50 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:18,926] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:18,926] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_3607ff6c-1f66-402f-bc44-4b70c4bc2a2f are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:18,926] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_3607ff6c-1f66-402f-bc44-4b70c4bc2a2f are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:18,926] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:18,930] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,030] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,036] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,037] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table LAST_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:19,037] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table LAST_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:19,157] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,271] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:19,272] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:19,272] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 161 B in total
INFO  [2023-01-06 20:46:19,272] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_LAST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000291623sINFO  [2023-01-06 20:46:19,301] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=2}
INFO  [2023-01-06 20:46:19,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=10, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:19,302] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=2), subType=IntegerParser(super=Parser(lines=10, nullLines=2), minValue=14805, maxValue=15341), dateReader=com.bakdata.conquery.util.DateReader@28ec8e3b)
INFO  [2023-01-06 20:46:19,304] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_LAST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:19,304] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:19,304] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_LAST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:19,325] com.bakdata.conquery.models.jobs.ImportJob: Importing table into LAST_AGGREGATOR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:19 +0000] "POST /admin/datasets/LAST_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_LAST_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:46:19,325] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,327] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:19,328] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:19,328] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:19,331] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:19,331] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[LAST_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-06 20:46:19,331] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[LAST_AGGREGATOR$20Test.table.table], containing 10 entries.
WARN  [2023-01-06 20:46:19,332] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:19,332] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received LAST_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:19,332] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received LAST_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:19,437] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,442] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,455] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,455] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:19,455] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:19,561] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: LAST_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:19,577] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[LAST_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:19,578] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2a9db79f-697a-4865-a2ea-2f53d8edad92] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_LAST_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:19,582] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92
INFO  [2023-01-06 20:46:19,582] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92
INFO  [2023-01-06 20:46:19,583] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92] with 2 results within PT0.001159S
INFO  [2023-01-06 20:46:19,583] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92] with 3 results within PT0.00125S
127.0.0.1 - - [06/Jan/2023:20:46:19 +0000] "POST /api/datasets/LAST_AGGREGATOR$20Test/queries HTTP/1.1" 201 1306 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:19,584] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92, workerId=LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_30a31e4c-61f4-4c65-a151-7e315f7e3d50, startTime=2023-01-06T20:46:19.582223, finishTime=2023-01-06T20:46:19.583382) of size 2
INFO  [2023-01-06 20:46:19,584] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92, workerId=LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_3607ff6c-1f66-402f-bc44-4b70c4bc2a2f, startTime=2023-01-06T20:46:19.582274, finishTime=2023-01-06T20:46:19.583524) of size 3
INFO  [2023-01-06 20:46:19,584] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2a9db79f-697a-4865-a2ea-2f53d8edad92 ManagedQuery within PT0.006001S
127.0.0.1 - - [06/Jan/2023:20:46:19 +0000] "GET /api/datasets/LAST_AGGREGATOR$20Test/queries/LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92 HTTP/1.1" 200 1585 "-" "Conquery (test client)" 4
INFO  [2023-01-06 20:46:19,607] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=LAST_AGGREGATOR Test], queryId=2a9db79f-697a-4865-a2ea-2f53d8edad92, label=concept	@§$, creationTime=2023-01-06T20:46:19.578016, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3ba9aeb5[Count = 0], startTime=2023-01-06T20:46:19.578253, finishTime=2023-01-06T20:46:19.584254, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@44454f5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_LAST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@528a0749, com.bakdata.conquery.models.query.ColumnDescriptor@15172d8e, com.bakdata.conquery.models.query.ColumnDescriptor@63bde59f]) download on dataset Dataset[label=null, name=LAST_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:19,607] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=LAST_AGGREGATOR Test], queryId=2a9db79f-697a-4865-a2ea-2f53d8edad92, label=concept	@§$, creationTime=2023-01-06T20:46:19.578016, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3ba9aeb5[Count = 0], startTime=2023-01-06T20:46:19.578253, finishTime=2023-01-06T20:46:19.584254, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@44454f5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_LAST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@528a0749, com.bakdata.conquery.models.query.ColumnDescriptor@15172d8e, com.bakdata.conquery.models.query.ColumnDescriptor@63bde59f]) on dataset Dataset[label=null, name=LAST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:19,626] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest LAST_AGGREGATOR Test on 6 rows
127.0.0.1 - - [06/Jan/2023:20:46:19 +0000] "GET /api/datasets/LAST_AGGREGATOR%20Test/result/LAST_AGGREGATOR$20Test.2a9db79f-697a-4865-a2ea-2f53d8edad92.csv?pretty=false HTTP/1.1" 200 167 "-" "Conquery (test client)" 20
INFO  [2023-01-06 20:46:19,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast LAST_AGGREGATOR Test
INFO  [2023-01-06 20:46:19,627] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:19,627] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-06 20:46:19,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_LAST_AGGREGATOR Test_3607ff6c-1f66-402f-bc44-4b70c4bc2a2f
INFO  [2023-01-06 20:46:19,627] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_LAST_AGGREGATOR Test_30a31e4c-61f4-4c65-a151-7e315f7e3d50
INFO  [2023-01-06 20:46:19,725] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_LAST_AGGREGATOR Test_30a31e4c-61f4-4c65-a151-7e315f7e3d50
INFO  [2023-01-06 20:46:19,725] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow LAST_AGGREGATOR Test
INFO  [2023-01-06 20:46:19,727] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_LAST_AGGREGATOR Test_3607ff6c-1f66-402f-bc44-4b70c4bc2a2f
INFO  [2023-01-06 20:46:19,733] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of LAST_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:19,733] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,861] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test LAST_AGGREGATOR Test
INFO  [2023-01-06 20:46:19,861] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:19,861] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:19,861] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:19,862] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:19,862] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:19,862] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:19,862] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_5444980e-8ab1-4655-9603-ab9fca8f3e25 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_5444980e-8ab1-4655-9603-ab9fca8f3e25 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_0e28690b-c43f-4ca0-8bf0-9e1fa7170a62 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_0e28690b-c43f-4ca0-8bf0-9e1fa7170a62 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:19,864] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:19,968] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,975] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:19,975] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:19,975] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:20,100] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,220] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:20,221] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:20,221] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 166 B in total
INFO  [2023-01-06 20:46:20,221] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000451616sINFO  [2023-01-06 20:46:20,267] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=9, min=1, average=1.500000, max=2}
INFO  [2023-01-06 20:46:20,267] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=9, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:20,267] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3cd28829)
INFO  [2023-01-06 20:46:20,269] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:20,269] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:20,269] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_MULTI_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:20,282] com.bakdata.conquery.models.jobs.ImportJob: Importing table into MULTI_SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:20,283] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:20 +0000] "POST /admin/datasets/MULTI_SELECT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_MULTI_SELECT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:20,284] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:20,284] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:20,284] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:20,286] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:20,286] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_AGGREGATOR$20Test.table.table], containing 9 entries.
INFO  [2023-01-06 20:46:20,286] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_AGGREGATOR$20Test.table.table], containing 9 entries.
WARN  [2023-01-06 20:46:20,287] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:20,287] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:20,287] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:20,392] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,398] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,407] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,408] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:20,408] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:20,513] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:20,528] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:20,529] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[eb910cb2-f45e-4d90-935d-11dae00380ce] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:20,531] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce
INFO  [2023-01-06 20:46:20,532] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce
127.0.0.1 - - [06/Jan/2023:20:46:20 +0000] "POST /api/datasets/MULTI_SELECT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1346 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:20,533] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce] with 3 results within PT0.001448S
INFO  [2023-01-06 20:46:20,533] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce] with 3 results within PT0.001379S
INFO  [2023-01-06 20:46:20,534] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce, workerId=MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_0e28690b-c43f-4ca0-8bf0-9e1fa7170a62, startTime=2023-01-06T20:46:20.531998, finishTime=2023-01-06T20:46:20.533446) of size 3
INFO  [2023-01-06 20:46:20,534] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce, workerId=MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_5444980e-8ab1-4655-9603-ab9fca8f3e25, startTime=2023-01-06T20:46:20.532191, finishTime=2023-01-06T20:46:20.533570) of size 3
INFO  [2023-01-06 20:46:20,534] com.bakdata.conquery.models.execution.ManagedExecution: DONE eb910cb2-f45e-4d90-935d-11dae00380ce ManagedQuery within PT0.005108S
127.0.0.1 - - [06/Jan/2023:20:46:20 +0000] "GET /api/datasets/MULTI_SELECT_AGGREGATOR$20Test/queries/MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce HTTP/1.1" 200 1657 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:20,556] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test], queryId=eb910cb2-f45e-4d90-935d-11dae00380ce, label=concept	@§$, creationTime=2023-01-06T20:46:20.529233, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@133d9d31[Count = 0], startTime=2023-01-06T20:46:20.529408, finishTime=2023-01-06T20:46:20.534516, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7464761d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@160cec16, com.bakdata.conquery.models.query.ColumnDescriptor@e0ee5dd, com.bakdata.conquery.models.query.ColumnDescriptor@71253538]) download on dataset Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:20,557] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test], queryId=eb910cb2-f45e-4d90-935d-11dae00380ce, label=concept	@§$, creationTime=2023-01-06T20:46:20.529233, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@133d9d31[Count = 0], startTime=2023-01-06T20:46:20.529408, finishTime=2023-01-06T20:46:20.534516, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7464761d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@160cec16, com.bakdata.conquery.models.query.ColumnDescriptor@e0ee5dd, com.bakdata.conquery.models.query.ColumnDescriptor@71253538]) on dataset Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:20 +0000] "GET /api/datasets/MULTI_SELECT_AGGREGATOR%20Test/result/MULTI_SELECT_AGGREGATOR$20Test.eb910cb2-f45e-4d90-935d-11dae00380ce.csv?pretty=false HTTP/1.1" 200 222 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:46:20,573] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_AGGREGATOR Test on 7 rows
INFO  [2023-01-06 20:46:20,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:20,573] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:20,573] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:20,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_AGGREGATOR Test_5444980e-8ab1-4655-9603-ab9fca8f3e25
INFO  [2023-01-06 20:46:20,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_AGGREGATOR Test_0e28690b-c43f-4ca0-8bf0-9e1fa7170a62
INFO  [2023-01-06 20:46:20,672] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:20,672] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_AGGREGATOR Test_0e28690b-c43f-4ca0-8bf0-9e1fa7170a62
INFO  [2023-01-06 20:46:20,672] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_AGGREGATOR Test_5444980e-8ab1-4655-9603-ab9fca8f3e25
INFO  [2023-01-06 20:46:20,690] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:20,690] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,813] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:20,814] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test PREFIX_AGGREGATOR Test
INFO  [2023-01-06 20:46:20,814] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:20,814] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:20,815] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-06 20:46:20,815] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-06 20:46:20,815] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:20,815] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:20,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_f8f5d5c0-a8a3-4b7f-b9c7-a9a8e749f0f4 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:20,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_f8f5d5c0-a8a3-4b7f-b9c7-a9a8e749f0f4 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:20,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:20,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_770557cd-2d1b-4180-9fcd-78dca2e29cbc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:20,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_770557cd-2d1b-4180-9fcd-78dca2e29cbc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:20,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:20,821] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,921] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,927] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:20,928] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table PREFIX_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:20,928] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table PREFIX_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:21,049] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,163] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:21,163] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:21,163] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 100 B in total
INFO  [2023-01-06 20:46:21,163] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_PREFIX_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000394829sINFO  [2023-01-06 20:46:21,203] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:46:21,203] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:21,203] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@598e6437)
INFO  [2023-01-06 20:46:21,206] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_PREFIX_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:21,206] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:21,206] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_PREFIX_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:21,220] com.bakdata.conquery.models.jobs.ImportJob: Importing table into PREFIX_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:21,221] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:21 +0000] "POST /admin/datasets/PREFIX_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_PREFIX_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:21,222] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:21,223] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:21,223] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:21,226] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:21,226] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[PREFIX_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:46:21,226] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[PREFIX_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:46:21,228] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:21,228] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received PREFIX_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:21,228] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received PREFIX_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:21,333] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,338] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,348] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,349] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:21,349] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:21,455] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: PREFIX_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:21,470] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[PREFIX_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:21,470] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f468ad7a-3d68-4bde-8649-0e8072e19833] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_PREFIX_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:21,473] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833
INFO  [2023-01-06 20:46:21,473] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833
INFO  [2023-01-06 20:46:21,474] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833] with 1 results within PT0.000968S
127.0.0.1 - - [06/Jan/2023:20:46:21 +0000] "POST /api/datasets/PREFIX_AGGREGATOR$20Test/queries HTTP/1.1" 201 1316 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:21,474] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833] with 3 results within PT0.001476S
INFO  [2023-01-06 20:46:21,474] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833, workerId=PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_f8f5d5c0-a8a3-4b7f-b9c7-a9a8e749f0f4, startTime=2023-01-06T20:46:21.473163, finishTime=2023-01-06T20:46:21.474131) of size 1
INFO  [2023-01-06 20:46:21,476] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833, workerId=PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_770557cd-2d1b-4180-9fcd-78dca2e29cbc, startTime=2023-01-06T20:46:21.473167, finishTime=2023-01-06T20:46:21.474643) of size 3
INFO  [2023-01-06 20:46:21,476] com.bakdata.conquery.models.execution.ManagedExecution: DONE f468ad7a-3d68-4bde-8649-0e8072e19833 ManagedQuery within PT0.005841S
127.0.0.1 - - [06/Jan/2023:20:46:21 +0000] "GET /api/datasets/PREFIX_AGGREGATOR$20Test/queries/PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833 HTTP/1.1" 200 1603 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:21,498] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=PREFIX_AGGREGATOR Test], queryId=f468ad7a-3d68-4bde-8649-0e8072e19833, label=concept	@§$, creationTime=2023-01-06T20:46:21.470326, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5a33585[Count = 0], startTime=2023-01-06T20:46:21.470501, finishTime=2023-01-06T20:46:21.476342, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@608bec3b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_PREFIX_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4455ed32, com.bakdata.conquery.models.query.ColumnDescriptor@2be13b6, com.bakdata.conquery.models.query.ColumnDescriptor@1ead0179]) download on dataset Dataset[label=null, name=PREFIX_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:21,498] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=PREFIX_AGGREGATOR Test], queryId=f468ad7a-3d68-4bde-8649-0e8072e19833, label=concept	@§$, creationTime=2023-01-06T20:46:21.470326, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5a33585[Count = 0], startTime=2023-01-06T20:46:21.470501, finishTime=2023-01-06T20:46:21.476342, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@608bec3b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_PREFIX_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4455ed32, com.bakdata.conquery.models.query.ColumnDescriptor@2be13b6, com.bakdata.conquery.models.query.ColumnDescriptor@1ead0179]) on dataset Dataset[label=null, name=PREFIX_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:21 +0000] "GET /api/datasets/PREFIX_AGGREGATOR%20Test/result/PREFIX_AGGREGATOR$20Test.f468ad7a-3d68-4bde-8649-0e8072e19833.csv?pretty=false HTTP/1.1" 200 149 "-" "Conquery (test client)" 18
INFO  [2023-01-06 20:46:21,515] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest PREFIX_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:21,515] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast PREFIX_AGGREGATOR Test
INFO  [2023-01-06 20:46:21,515] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-06 20:46:21,515] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-06 20:46:21,515] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PREFIX_AGGREGATOR Test_f8f5d5c0-a8a3-4b7f-b9c7-a9a8e749f0f4
INFO  [2023-01-06 20:46:21,515] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PREFIX_AGGREGATOR Test_770557cd-2d1b-4180-9fcd-78dca2e29cbc
INFO  [2023-01-06 20:46:21,516] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PREFIX_AGGREGATOR Test_f8f5d5c0-a8a3-4b7f-b9c7-a9a8e749f0f4
INFO  [2023-01-06 20:46:21,516] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PREFIX_AGGREGATOR Test_770557cd-2d1b-4180-9fcd-78dca2e29cbc
INFO  [2023-01-06 20:46:21,516] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow PREFIX_AGGREGATOR Test
INFO  [2023-01-06 20:46:21,528] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of PREFIX_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:21,528] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,655] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test PREFIX_AGGREGATOR Test
INFO  [2023-01-06 20:46:21,655] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test QUARTER_AGGREGATOR
INFO  [2023-01-06 20:46:21,655] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:21,655] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:21,656] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-06 20:46:21,656] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-06 20:46:21,656] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:21,656] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:21,657] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,658] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_ae4fba65-659a-42be-9083-cab1c322aebc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:21,658] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_ae4fba65-659a-42be-9083-cab1c322aebc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:21,658] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:21,658] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_d7dbb447-6e88-481c-827b-3a287ef99da0 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:21,658] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_d7dbb447-6e88-481c-827b-3a287ef99da0 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:21,658] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:21,767] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,774] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:21,774] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTER_AGGREGATOR.table
INFO  [2023-01-06 20:46:21,774] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTER_AGGREGATOR.table
INFO  [2023-01-06 20:46:21,893] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,003] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:22,004] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:22,004] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 51 B in total
INFO  [2023-01-06 20:46:22,004] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_QUARTER_AGGREGATOR/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000345694sINFO  [2023-01-06 20:46:22,039] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=4, min=1, average=1.333333, max=2}
INFO  [2023-01-06 20:46:22,039] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=1), subType=IntegerParser(super=Parser(lines=4, nullLines=1), minValue=16452, maxValue=16679), dateReader=com.bakdata.conquery.util.DateReader@56696b18)
INFO  [2023-01-06 20:46:22,041] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_QUARTER_AGGREGATOR/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:22,041] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:22,041] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_QUARTER_AGGREGATOR/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:22,053] com.bakdata.conquery.models.jobs.ImportJob: Importing table into QUARTER_AGGREGATOR.table
127.0.0.1 - - [06/Jan/2023:20:46:22 +0000] "POST /admin/datasets/QUARTER_AGGREGATOR/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_QUARTER_AGGREGATOR%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-06 20:46:22,053] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,055] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:22,055] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:22,055] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:22,056] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-06 20:46:22,057] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTER_AGGREGATOR.table.table], containing 4 entries.
INFO  [2023-01-06 20:46:22,057] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTER_AGGREGATOR.table.table], containing 4 entries.
WARN  [2023-01-06 20:46:22,058] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:22,058] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTER_AGGREGATOR.table.table.0
INFO  [2023-01-06 20:46:22,163] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,168] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,181] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,181] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:22,287] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: QUARTER_AGGREGATOR QUERY INIT
INFO  [2023-01-06 20:46:22,297] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[QUARTER_AGGREGATOR] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:22,297] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_QUARTER_AGGREGATOR))]]
INFO  [2023-01-06 20:46:22,300] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6
INFO  [2023-01-06 20:46:22,300] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6
WARN  [2023-01-06 20:46:22,300] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-06 20:46:22,300] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6] with 0 results within PT0.000321S
INFO  [2023-01-06 20:46:22,300] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6, workerId=QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_ae4fba65-659a-42be-9083-cab1c322aebc, startTime=2023-01-06T20:46:22.300110, finishTime=2023-01-06T20:46:22.300431) of size 0
127.0.0.1 - - [06/Jan/2023:20:46:22 +0000] "POST /api/datasets/QUARTER_AGGREGATOR/queries HTTP/1.1" 201 1286 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:22,304] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6] with 2 results within PT0.004175S
INFO  [2023-01-06 20:46:22,304] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6, workerId=QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_d7dbb447-6e88-481c-827b-3a287ef99da0, startTime=2023-01-06T20:46:22.300096, finishTime=2023-01-06T20:46:22.304271) of size 2
INFO  [2023-01-06 20:46:22,304] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6 ManagedQuery within PT0.007373S
127.0.0.1 - - [06/Jan/2023:20:46:22 +0000] "GET /api/datasets/QUARTER_AGGREGATOR/queries/QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6 HTTP/1.1" 200 1549 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:22,324] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTER_AGGREGATOR], queryId=5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6, label=concept	@§$, creationTime=2023-01-06T20:46:22.297378, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1ef7976f[Count = 0], startTime=2023-01-06T20:46:22.297552, finishTime=2023-01-06T20:46:22.304925, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2f0e3101), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTER_AGGREGATOR)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@94c1ad1, com.bakdata.conquery.models.query.ColumnDescriptor@7194467f, com.bakdata.conquery.models.query.ColumnDescriptor@53fafbcd]) download on dataset Dataset[label=null, name=QUARTER_AGGREGATOR] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:22,324] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTER_AGGREGATOR], queryId=5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6, label=concept	@§$, creationTime=2023-01-06T20:46:22.297378, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1ef7976f[Count = 0], startTime=2023-01-06T20:46:22.297552, finishTime=2023-01-06T20:46:22.304925, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2f0e3101), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTER_AGGREGATOR)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@94c1ad1, com.bakdata.conquery.models.query.ColumnDescriptor@7194467f, com.bakdata.conquery.models.query.ColumnDescriptor@53fafbcd]) on dataset Dataset[label=null, name=QUARTER_AGGREGATOR]
127.0.0.1 - - [06/Jan/2023:20:46:22 +0000] "GET /api/datasets/QUARTER_AGGREGATOR/result/QUARTER_AGGREGATOR.5f3045f7-3bf8-4bb9-a4f8-155ef6d84fb6.csv?pretty=false HTTP/1.1" 200 120 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:46:22,345] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest QUARTER_AGGREGATOR on 3 rows
INFO  [2023-01-06 20:46:22,345] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast QUARTER_AGGREGATOR
INFO  [2023-01-06 20:46:22,345] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-06 20:46:22,345] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-06 20:46:22,345] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTER_AGGREGATOR_ae4fba65-659a-42be-9083-cab1c322aebc
INFO  [2023-01-06 20:46:22,346] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTER_AGGREGATOR_d7dbb447-6e88-481c-827b-3a287ef99da0
INFO  [2023-01-06 20:46:22,356] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow QUARTER_AGGREGATOR
INFO  [2023-01-06 20:46:22,358] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of QUARTER_AGGREGATOR
INFO  [2023-01-06 20:46:22,358] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,362] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTER_AGGREGATOR_d7dbb447-6e88-481c-827b-3a287ef99da0
INFO  [2023-01-06 20:46:22,362] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTER_AGGREGATOR_ae4fba65-659a-42be-9083-cab1c322aebc
INFO  [2023-01-06 20:46:22,487] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test QUARTER_AGGREGATOR
INFO  [2023-01-06 20:46:22,487] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test RANDOM_AGGREGATOR Test
INFO  [2023-01-06 20:46:22,487] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:22,487] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:22,488] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:22,488] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:22,488] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:22,488] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_3e2dad59-0a7a-4f76-a227-bd5cdf7ef762 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_3e2dad59-0a7a-4f76-a227-bd5cdf7ef762 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_c3cb21c8-dc45-4428-a271-5a0b72e82a8e are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_c3cb21c8-dc45-4428-a271-5a0b72e82a8e are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:22,489] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:22,597] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,603] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,604] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RANDOM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:22,604] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RANDOM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:22,720] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:22,830] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:22,830] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:22,830] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 111 B in total
INFO  [2023-01-06 20:46:22,830] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_RANDOM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000336091sINFO  [2023-01-06 20:46:22,865] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-06 20:46:22,865] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:22,865] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=2), subType=IntegerParser(super=Parser(lines=7, nullLines=2), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6445d5aa)
INFO  [2023-01-06 20:46:22,867] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_RANDOM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:22,867] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:22,867] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_RANDOM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:22,887] com.bakdata.conquery.models.jobs.ImportJob: Importing table into RANDOM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:22,888] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:22 +0000] "POST /admin/datasets/RANDOM_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_RANDOM_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-06 20:46:22,889] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:22,890] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:22,890] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:22,892] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:22,892] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RANDOM_AGGREGATOR$20Test.table.table], containing 7 entries.
INFO  [2023-01-06 20:46:22,893] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RANDOM_AGGREGATOR$20Test.table.table], containing 7 entries.
WARN  [2023-01-06 20:46:22,894] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:22,894] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RANDOM_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:22,894] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RANDOM_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:22,999] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,004] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,017] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,018] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:23,018] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:23,123] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: RANDOM_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:23,138] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[RANDOM_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:23,138] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[655a27bb-486c-4fff-a9da-0371bc010b35] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_RANDOM_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:23,141] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35
INFO  [2023-01-06 20:46:23,141] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35
INFO  [2023-01-06 20:46:23,142] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35] with 2 results within PT0.000861S
INFO  [2023-01-06 20:46:23,142] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35] with 2 results within PT0.000984S
127.0.0.1 - - [06/Jan/2023:20:46:23 +0000] "POST /api/datasets/RANDOM_AGGREGATOR$20Test/queries HTTP/1.1" 201 1316 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:23,142] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35, workerId=RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_3e2dad59-0a7a-4f76-a227-bd5cdf7ef762, startTime=2023-01-06T20:46:23.141277, finishTime=2023-01-06T20:46:23.142138) of size 2
INFO  [2023-01-06 20:46:23,142] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35, workerId=RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_c3cb21c8-dc45-4428-a271-5a0b72e82a8e, startTime=2023-01-06T20:46:23.141265, finishTime=2023-01-06T20:46:23.142249) of size 2
INFO  [2023-01-06 20:46:23,142] com.bakdata.conquery.models.execution.ManagedExecution: DONE 655a27bb-486c-4fff-a9da-0371bc010b35 ManagedQuery within PT0.004475S
127.0.0.1 - - [06/Jan/2023:20:46:23 +0000] "GET /api/datasets/RANDOM_AGGREGATOR$20Test/queries/RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35 HTTP/1.1" 200 1603 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:23,167] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RANDOM_AGGREGATOR Test], queryId=655a27bb-486c-4fff-a9da-0371bc010b35, label=concept	@§$, creationTime=2023-01-06T20:46:23.138289, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2c919b23[Count = 0], startTime=2023-01-06T20:46:23.138492, finishTime=2023-01-06T20:46:23.142967, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6dc343cb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RANDOM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5b885eb9, com.bakdata.conquery.models.query.ColumnDescriptor@312b964d, com.bakdata.conquery.models.query.ColumnDescriptor@11c35bc8]) download on dataset Dataset[label=null, name=RANDOM_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:23,169] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RANDOM_AGGREGATOR Test], queryId=655a27bb-486c-4fff-a9da-0371bc010b35, label=concept	@§$, creationTime=2023-01-06T20:46:23.138289, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2c919b23[Count = 0], startTime=2023-01-06T20:46:23.138492, finishTime=2023-01-06T20:46:23.142967, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6dc343cb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RANDOM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5b885eb9, com.bakdata.conquery.models.query.ColumnDescriptor@312b964d, com.bakdata.conquery.models.query.ColumnDescriptor@11c35bc8]) on dataset Dataset[label=null, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:23,195] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest RANDOM_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:23,195] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast RANDOM_AGGREGATOR Test
127.0.0.1 - - [06/Jan/2023:20:46:23 +0000] "GET /api/datasets/RANDOM_AGGREGATOR%20Test/result/RANDOM_AGGREGATOR$20Test.655a27bb-486c-4fff-a9da-0371bc010b35.csv?pretty=false HTTP/1.1" 200 139 "-" "Conquery (test client)" 29
INFO  [2023-01-06 20:46:23,196] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:23,196] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:23,196] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RANDOM_AGGREGATOR Test_3e2dad59-0a7a-4f76-a227-bd5cdf7ef762
INFO  [2023-01-06 20:46:23,196] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RANDOM_AGGREGATOR Test_c3cb21c8-dc45-4428-a271-5a0b72e82a8e
INFO  [2023-01-06 20:46:23,294] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow RANDOM_AGGREGATOR Test
INFO  [2023-01-06 20:46:23,294] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RANDOM_AGGREGATOR Test_3e2dad59-0a7a-4f76-a227-bd5cdf7ef762
INFO  [2023-01-06 20:46:23,294] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RANDOM_AGGREGATOR Test_c3cb21c8-dc45-4428-a271-5a0b72e82a8e
INFO  [2023-01-06 20:46:23,296] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of RANDOM_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:23,296] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,423] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test RANDOM_AGGREGATOR Test
INFO  [2023-01-06 20:46:23,423] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:23,424] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:23,424] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:23,425] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:23,425] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:23,425] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:23,425] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:23,426] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,426] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_f5e65076-1d42-49be-b8bc-258d3255130c are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:23,426] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_f5e65076-1d42-49be-b8bc-258d3255130c are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:23,426] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:23,427] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_ccfa3510-5879-4b5a-9f92-3d0f44531f34 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:23,427] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_ccfa3510-5879-4b5a-9f92-3d0f44531f34 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:23,427] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:23,530] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,537] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,538] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:23,538] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:23,654] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,764] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:23,764] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:23,765] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 98 B in total
INFO  [2023-01-06 20:46:23,765] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000249043sINFO  [2023-01-06 20:46:23,790] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:46:23,790] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:23,790] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2814124f)
INFO  [2023-01-06 20:46:23,793] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:23,793] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:23,793] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:23,809] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SELECT_AGGREGATOR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:23 +0000] "POST /admin/datasets/SELECT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SELECT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:23,810] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,810] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:23,811] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:23,811] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:23,812] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:23,812] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:46:23,812] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:46:23,813] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:23,813] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:23,813] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:23,918] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,924] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,940] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:23,940] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:23,940] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:24,047] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SELECT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:24,060] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SELECT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:24,061] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ff9e9ddb-cf3c-4403-9555-152444273954] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SELECT_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:24,064] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954
INFO  [2023-01-06 20:46:24,064] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954
INFO  [2023-01-06 20:46:24,064] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954] with 1 results within PT0.000713S
INFO  [2023-01-06 20:46:24,065] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954] with 3 results within PT0.000973S
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "POST /api/datasets/SELECT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1317 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:24,065] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954, workerId=SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_f5e65076-1d42-49be-b8bc-258d3255130c, startTime=2023-01-06T20:46:24.064204, finishTime=2023-01-06T20:46:24.064917) of size 1
INFO  [2023-01-06 20:46:24,065] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954, workerId=SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_ccfa3510-5879-4b5a-9f92-3d0f44531f34, startTime=2023-01-06T20:46:24.064082, finishTime=2023-01-06T20:46:24.065055) of size 3
INFO  [2023-01-06 20:46:24,065] com.bakdata.conquery.models.execution.ManagedExecution: DONE ff9e9ddb-cf3c-4403-9555-152444273954 ManagedQuery within PT0.004546S
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "GET /api/datasets/SELECT_AGGREGATOR$20Test/queries/SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954 HTTP/1.1" 200 1604 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:24,094] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT_AGGREGATOR Test], queryId=ff9e9ddb-cf3c-4403-9555-152444273954, label=concept	@§$, creationTime=2023-01-06T20:46:24.061101, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@420cdced[Count = 0], startTime=2023-01-06T20:46:24.061302, finishTime=2023-01-06T20:46:24.065848, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@350ffe88), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@165c6566, com.bakdata.conquery.models.query.ColumnDescriptor@2607163b, com.bakdata.conquery.models.query.ColumnDescriptor@2c84da2]) download on dataset Dataset[label=null, name=SELECT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:24,095] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT_AGGREGATOR Test], queryId=ff9e9ddb-cf3c-4403-9555-152444273954, label=concept	@§$, creationTime=2023-01-06T20:46:24.061101, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@420cdced[Count = 0], startTime=2023-01-06T20:46:24.061302, finishTime=2023-01-06T20:46:24.065848, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@350ffe88), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@165c6566, com.bakdata.conquery.models.query.ColumnDescriptor@2607163b, com.bakdata.conquery.models.query.ColumnDescriptor@2c84da2]) on dataset Dataset[label=null, name=SELECT_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "GET /api/datasets/SELECT_AGGREGATOR%20Test/result/SELECT_AGGREGATOR$20Test.ff9e9ddb-cf3c-4403-9555-152444273954.csv?pretty=false HTTP/1.1" 200 138 "-" "Conquery (test client)" 19
INFO  [2023-01-06 20:46:24,112] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SELECT_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:24,112] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:24,113] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:24,113] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-06 20:46:24,113] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT_AGGREGATOR Test_f5e65076-1d42-49be-b8bc-258d3255130c
INFO  [2023-01-06 20:46:24,117] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT_AGGREGATOR Test_ccfa3510-5879-4b5a-9f92-3d0f44531f34
INFO  [2023-01-06 20:46:24,125] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:24,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT_AGGREGATOR Test_ccfa3510-5879-4b5a-9f92-3d0f44531f34
INFO  [2023-01-06 20:46:24,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT_AGGREGATOR Test_f5e65076-1d42-49be-b8bc-258d3255130c
INFO  [2023-01-06 20:46:24,219] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SELECT_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:24,219] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,246] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SELECT_AGGREGATOR Test
INFO  [2023-01-06 20:46:24,246] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:24,247] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:24,247] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:24,248] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:24,248] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:24,248] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:24,248] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:24,250] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_046d6c88-29de-4e3c-8473-effefce093b9 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:24,250] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_046d6c88-29de-4e3c-8473-effefce093b9 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:24,250] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:24,250] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_0f50499e-303c-423f-8efa-a981632866cc are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:24,250] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_0f50499e-303c-423f-8efa-a981632866cc are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:24,250] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:24,254] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,354] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,362] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,362] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:24,362] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:24,481] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,597] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:24,598] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:24,598] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 91 B in total
INFO  [2023-01-06 20:46:24,598] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000318538sINFO  [2023-01-06 20:46:24,630] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:46:24,630] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with IntegerParser(super=Parser(lines=5, nullLines=1), minValue=-1, maxValue=1)
INFO  [2023-01-06 20:46:24,630] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2f54a4)
INFO  [2023-01-06 20:46:24,632] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:24,633] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:24,633] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:24,651] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:24,651] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "POST /admin/datasets/SUM_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SUM_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-06 20:46:24,653] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:24,653] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:24,653] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:24,655] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:24,655] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:46:24,655] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:46:24,656] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:24,657] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:24,657] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:24,762] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,768] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,782] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:24,783] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:24,783] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:24,888] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:24,905] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:24,906] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[115cd982-83a8-434d-b8e5-0cf4df623884] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:24,909] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884
INFO  [2023-01-06 20:46:24,909] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884
INFO  [2023-01-06 20:46:24,911] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884] with 1 results within PT0.002401S
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "POST /api/datasets/SUM_AGGREGATOR$20Test/queries HTTP/1.1" 201 1302 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:24,911] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884] with 3 results within PT0.0024S
INFO  [2023-01-06 20:46:24,912] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884, workerId=SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_0f50499e-303c-423f-8efa-a981632866cc, startTime=2023-01-06T20:46:24.909302, finishTime=2023-01-06T20:46:24.911702) of size 3
INFO  [2023-01-06 20:46:24,912] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884, workerId=SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_046d6c88-29de-4e3c-8473-effefce093b9, startTime=2023-01-06T20:46:24.909297, finishTime=2023-01-06T20:46:24.911698) of size 1
INFO  [2023-01-06 20:46:24,912] com.bakdata.conquery.models.execution.ManagedExecution: DONE 115cd982-83a8-434d-b8e5-0cf4df623884 ManagedQuery within PT0.006288S
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "GET /api/datasets/SUM_AGGREGATOR$20Test/queries/SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884 HTTP/1.1" 200 1577 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:24,938] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_AGGREGATOR Test], queryId=115cd982-83a8-434d-b8e5-0cf4df623884, label=concept	@§$, creationTime=2023-01-06T20:46:24.905948, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6569bcae[Count = 0], startTime=2023-01-06T20:46:24.906193, finishTime=2023-01-06T20:46:24.912481, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@581b8202), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3ad68e27, com.bakdata.conquery.models.query.ColumnDescriptor@70d21011, com.bakdata.conquery.models.query.ColumnDescriptor@58f64e69]) download on dataset Dataset[label=null, name=SUM_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:24,939] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_AGGREGATOR Test], queryId=115cd982-83a8-434d-b8e5-0cf4df623884, label=concept	@§$, creationTime=2023-01-06T20:46:24.905948, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6569bcae[Count = 0], startTime=2023-01-06T20:46:24.906193, finishTime=2023-01-06T20:46:24.912481, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@581b8202), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3ad68e27, com.bakdata.conquery.models.query.ColumnDescriptor@70d21011, com.bakdata.conquery.models.query.ColumnDescriptor@58f64e69]) on dataset Dataset[label=null, name=SUM_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:24 +0000] "GET /api/datasets/SUM_AGGREGATOR%20Test/result/SUM_AGGREGATOR$20Test.115cd982-83a8-434d-b8e5-0cf4df623884.csv?pretty=false HTTP/1.1" 200 140 "-" "Conquery (test client)" 23
INFO  [2023-01-06 20:46:24,960] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SUM_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:24,960] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:24,960] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:24,960] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-06 20:46:24,960] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_AGGREGATOR Test_046d6c88-29de-4e3c-8473-effefce093b9
INFO  [2023-01-06 20:46:24,960] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_AGGREGATOR Test_0f50499e-303c-423f-8efa-a981632866cc
INFO  [2023-01-06 20:46:25,060] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_AGGREGATOR Test_0f50499e-303c-423f-8efa-a981632866cc
INFO  [2023-01-06 20:46:25,060] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:25,060] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_AGGREGATOR Test_046d6c88-29de-4e3c-8473-effefce093b9
INFO  [2023-01-06 20:46:25,160] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:25,161] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,188] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_AGGREGATOR Test
INFO  [2023-01-06 20:46:25,189] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-06 20:46:25,189] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:25,189] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:25,190] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-06 20:46:25,190] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-06 20:46:25,190] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:25,190] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:25,191] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_f683eb62-44a3-4d8c-8935-5728bbb73f91 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:25,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_f683eb62-44a3-4d8c-8935-5728bbb73f91 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:25,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:25,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_770ab396-cd94-4344-b6ac-39e101f7a02d are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:25,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_770ab396-cd94-4344-b6ac-39e101f7a02d are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:25,192] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:25,296] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,303] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,304] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DIFF_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:25,304] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DIFF_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:25,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,532] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:25,532] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:25,532] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 103 B in total
INFO  [2023-01-06 20:46:25,532] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_DIFF_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000324248sINFO  [2023-01-06 20:46:25,565] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-06 20:46:25,565] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sum] with IntegerParser(super=Parser(lines=5, nullLines=1), minValue=-1, maxValue=1)
INFO  [2023-01-06 20:46:25,565] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sub] with IntegerParser(super=Parser(lines=5, nullLines=2), minValue=-1, maxValue=1)
INFO  [2023-01-06 20:46:25,565] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@48d9922d)
INFO  [2023-01-06 20:46:25,567] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_DIFF_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:25,567] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:25,567] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_SUM_DIFF_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:25,582] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_DIFF_AGGREGATOR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:25 +0000] "POST /admin/datasets/SUM_DIFF_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_SUM_DIFF_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-06 20:46:25,582] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,584] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:25,584] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:25,584] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:25,586] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:25,586] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DIFF_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-06 20:46:25,586] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DIFF_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-06 20:46:25,587] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:25,588] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DIFF_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:25,588] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DIFF_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:25,692] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,698] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,710] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:25,711] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:25,711] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:25,816] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_DIFF_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:25,825] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_DIFF_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:25,825] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[6627a279-e6e6-4a3f-91ae-fb94cc1092b7] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DIFF_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:25,828] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7
INFO  [2023-01-06 20:46:25,828] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7
INFO  [2023-01-06 20:46:25,828] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7] with 1 results within PT0.00054S
INFO  [2023-01-06 20:46:25,829] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7] with 3 results within PT0.000837S
INFO  [2023-01-06 20:46:25,829] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7, workerId=SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_f683eb62-44a3-4d8c-8935-5728bbb73f91, startTime=2023-01-06T20:46:25.828278, finishTime=2023-01-06T20:46:25.828818) of size 1
127.0.0.1 - - [06/Jan/2023:20:46:25 +0000] "POST /api/datasets/SUM_DIFF_AGGREGATOR$20Test/queries HTTP/1.1" 201 1326 "-" "Conquery (test client)" 6
INFO  [2023-01-06 20:46:25,829] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7, workerId=SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_770ab396-cd94-4344-b6ac-39e101f7a02d, startTime=2023-01-06T20:46:25.828275, finishTime=2023-01-06T20:46:25.829112) of size 3
INFO  [2023-01-06 20:46:25,829] com.bakdata.conquery.models.execution.ManagedExecution: DONE 6627a279-e6e6-4a3f-91ae-fb94cc1092b7 ManagedQuery within PT0.003948S
127.0.0.1 - - [06/Jan/2023:20:46:25 +0000] "GET /api/datasets/SUM_DIFF_AGGREGATOR$20Test/queries/SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7 HTTP/1.1" 200 1621 "-" "Conquery (test client)" 2
INFO  [2023-01-06 20:46:25,846] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test], queryId=6627a279-e6e6-4a3f-91ae-fb94cc1092b7, label=concept	@§$, creationTime=2023-01-06T20:46:25.825490, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6780be83[Count = 0], startTime=2023-01-06T20:46:25.825691, finishTime=2023-01-06T20:46:25.829639, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@65cd937c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DIFF_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@721a9e4c, com.bakdata.conquery.models.query.ColumnDescriptor@707e4942, com.bakdata.conquery.models.query.ColumnDescriptor@12689568]) download on dataset Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:25,846] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test], queryId=6627a279-e6e6-4a3f-91ae-fb94cc1092b7, label=concept	@§$, creationTime=2023-01-06T20:46:25.825490, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6780be83[Count = 0], startTime=2023-01-06T20:46:25.825691, finishTime=2023-01-06T20:46:25.829639, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@65cd937c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DIFF_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@721a9e4c, com.bakdata.conquery.models.query.ColumnDescriptor@707e4942, com.bakdata.conquery.models.query.ColumnDescriptor@12689568]) on dataset Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:25 +0000] "GET /api/datasets/SUM_DIFF_AGGREGATOR%20Test/result/SUM_DIFF_AGGREGATOR$20Test.6627a279-e6e6-4a3f-91ae-fb94cc1092b7.csv?pretty=false HTTP/1.1" 200 140 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:46:25,866] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SUM_DIFF_AGGREGATOR Test on 5 rows
INFO  [2023-01-06 20:46:25,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-06 20:46:25,867] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-06 20:46:25,867] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-06 20:46:25,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DIFF_AGGREGATOR Test_f683eb62-44a3-4d8c-8935-5728bbb73f91
INFO  [2023-01-06 20:46:25,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DIFF_AGGREGATOR Test_770ab396-cd94-4344-b6ac-39e101f7a02d
INFO  [2023-01-06 20:46:25,890] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-06 20:46:25,891] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DIFF_AGGREGATOR Test_f683eb62-44a3-4d8c-8935-5728bbb73f91
INFO  [2023-01-06 20:46:25,891] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DIFF_AGGREGATOR Test_770ab396-cd94-4344-b6ac-39e101f7a02d
INFO  [2023-01-06 20:46:25,988] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_DIFF_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:25,988] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,016] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-06 20:46:26,016] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test VALUES_AGGREGATOR Test
INFO  [2023-01-06 20:46:26,016] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-06 20:46:26,016] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-06 20:46:26,018] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-06 20:46:26,018] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-06 20:46:26,018] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:26,018] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_f35214d4-00d9-4828-b2fe-3e166890927b are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_f35214d4-00d9-4828-b2fe-3e166890927b are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_7cc28b64-fbf1-4e3a-b1ce-8b562bf80964 are consistent with the manager: 0 Imports
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_7cc28b64-fbf1-4e3a-b1ce-8b562bf80964 are consistent with the manager: 0 Buckets
INFO  [2023-01-06 20:46:26,020] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-06 20:46:26,124] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,131] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,132] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALUES_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:26,132] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALUES_AGGREGATOR$20Test.table
INFO  [2023-01-06 20:46:26,254] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,370] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-06 20:46:26,370] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-06 20:46:26,370] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 235 B in total
INFO  [2023-01-06 20:46:26,370] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VALUES_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000185269sINFO  [2023-01-06 20:46:26,389] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=13, min=1, average=2.600000, max=4}
INFO  [2023-01-06 20:46:26,389] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=13, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-06 20:46:26,389] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=0), subType=IntegerParser(super=Parser(lines=13, nullLines=0), minValue=14805, maxValue=15343), dateReader=com.bakdata.conquery.util.DateReader@7c0fce21)
INFO  [2023-01-06 20:46:26,392] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VALUES_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:26,392] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-06 20:46:26,392] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest6499145362419833625/tmp_VALUES_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-06 20:46:26,409] com.bakdata.conquery.models.jobs.ImportJob: Importing table into VALUES_AGGREGATOR$20Test.table
127.0.0.1 - - [06/Jan/2023:20:46:26 +0000] "POST /admin/datasets/VALUES_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest6499145362419833625%2Ftmp_VALUES_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-06 20:46:26,410] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,411] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-06 20:46:26,412] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-06 20:46:26,412] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-06 20:46:26,415] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-06 20:46:26,416] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALUES_AGGREGATOR$20Test.table.table], containing 13 entries.
INFO  [2023-01-06 20:46:26,416] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALUES_AGGREGATOR$20Test.table.table], containing 13 entries.
WARN  [2023-01-06 20:46:26,417] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-06 20:46:26,417] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALUES_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-06 20:46:26,417] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALUES_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-06 20:46:26,523] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,528] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,538] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,538] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:26,538] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-06 20:46:26,644] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: VALUES_AGGREGATOR Test QUERY INIT
INFO  [2023-01-06 20:46:26,660] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[VALUES_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-06 20:46:26,660] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3ca85d03-19ae-4bd2-8494-196bbc9102c0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_VALUES_AGGREGATOR Test))]]
INFO  [2023-01-06 20:46:26,663] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0
INFO  [2023-01-06 20:46:26,663] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0
127.0.0.1 - - [06/Jan/2023:20:46:26 +0000] "POST /api/datasets/VALUES_AGGREGATOR$20Test/queries HTTP/1.1" 201 1322 "-" "Conquery (test client)" 7
INFO  [2023-01-06 20:46:26,665] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0] with 2 results within PT0.001306S
INFO  [2023-01-06 20:46:26,666] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0] with 3 results within PT0.002517S
INFO  [2023-01-06 20:46:26,667] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0, workerId=VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_f35214d4-00d9-4828-b2fe-3e166890927b, startTime=2023-01-06T20:46:26.663779, finishTime=2023-01-06T20:46:26.666296) of size 3
INFO  [2023-01-06 20:46:26,667] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0, workerId=VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_7cc28b64-fbf1-4e3a-b1ce-8b562bf80964, startTime=2023-01-06T20:46:26.663808, finishTime=2023-01-06T20:46:26.665114) of size 2
INFO  [2023-01-06 20:46:26,667] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3ca85d03-19ae-4bd2-8494-196bbc9102c0 ManagedQuery within PT0.006681S
127.0.0.1 - - [06/Jan/2023:20:46:26 +0000] "GET /api/datasets/VALUES_AGGREGATOR$20Test/queries/VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0 HTTP/1.1" 200 1609 "-" "Conquery (test client)" 3
INFO  [2023-01-06 20:46:26,692] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALUES_AGGREGATOR Test], queryId=3ca85d03-19ae-4bd2-8494-196bbc9102c0, label=concept	@§$, creationTime=2023-01-06T20:46:26.660407, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@76b61e37[Count = 0], startTime=2023-01-06T20:46:26.660675, finishTime=2023-01-06T20:46:26.667356, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@638923f7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALUES_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e17e14a, com.bakdata.conquery.models.query.ColumnDescriptor@5359cd7c, com.bakdata.conquery.models.query.ColumnDescriptor@15582686]) download on dataset Dataset[label=null, name=VALUES_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-06 20:46:26,692] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALUES_AGGREGATOR Test], queryId=3ca85d03-19ae-4bd2-8494-196bbc9102c0, label=concept	@§$, creationTime=2023-01-06T20:46:26.660407, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@76b61e37[Count = 0], startTime=2023-01-06T20:46:26.660675, finishTime=2023-01-06T20:46:26.667356, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@638923f7), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALUES_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e9bf3b], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@79e1df9c], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@d004e6]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e17e14a, com.bakdata.conquery.models.query.ColumnDescriptor@5359cd7c, com.bakdata.conquery.models.query.ColumnDescriptor@15582686]) on dataset Dataset[label=null, name=VALUES_AGGREGATOR Test]
127.0.0.1 - - [06/Jan/2023:20:46:26 +0000] "GET /api/datasets/VALUES_AGGREGATOR%20Test/result/VALUES_AGGREGATOR$20Test.3ca85d03-19ae-4bd2-8494-196bbc9102c0.csv?pretty=false HTTP/1.1" 200 193 "-" "Conquery (test client)" 21
INFO  [2023-01-06 20:46:26,711] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest VALUES_AGGREGATOR Test on 6 rows
INFO  [2023-01-06 20:46:26,711] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast VALUES_AGGREGATOR Test
INFO  [2023-01-06 20:46:26,712] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-06 20:46:26,712] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-06 20:46:26,712] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALUES_AGGREGATOR Test_7cc28b64-fbf1-4e3a-b1ce-8b562bf80964
INFO  [2023-01-06 20:46:26,712] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALUES_AGGREGATOR Test_f35214d4-00d9-4828-b2fe-3e166890927b
INFO  [2023-01-06 20:46:26,721] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALUES_AGGREGATOR Test_7cc28b64-fbf1-4e3a-b1ce-8b562bf80964
INFO  [2023-01-06 20:46:26,721] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALUES_AGGREGATOR Test_f35214d4-00d9-4828-b2fe-3e166890927b
INFO  [2023-01-06 20:46:26,721] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow VALUES_AGGREGATOR Test
INFO  [2023-01-06 20:46:26,821] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of VALUES_AGGREGATOR$20Test
INFO  [2023-01-06 20:46:26,821] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-06 20:46:26,844] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test VALUES_AGGREGATOR Test
[INFO] Tests run: 172, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 174.508 s - in com.bakdata.conquery.integration.ConqueryIntegrationTests
[INFO] Running com.bakdata.conquery.tasks.PermissionCleanupTaskTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.tasks.PermissionCleanupTaskTest
[INFO] Running com.bakdata.conquery.tasks.IsUUIDTestTest
INFO  [2023-01-06 20:46:26,853] com.bakdata.conquery.tasks.IsUUIDTestTest: e4848188-b0c9-4f1e-b7eb-baf33d16dece
INFO  [2023-01-06 20:46:26,854] com.bakdata.conquery.tasks.IsUUIDTestTest: ad43671d-7942-4dc8-96ac-f77cf0a61311
INFO  [2023-01-06 20:46:26,854] com.bakdata.conquery.tasks.IsUUIDTestTest: f2952340-648e-43e0-a161-02c541d74475
INFO  [2023-01-06 20:46:26,855] com.bakdata.conquery.tasks.IsUUIDTestTest: ea01d107-103e-44e7-a416-8a73d3185bb0
INFO  [2023-01-06 20:46:26,856] com.bakdata.conquery.tasks.IsUUIDTestTest: 3d63f466-58f0-454e-b509-a1e13f3c13e7
INFO  [2023-01-06 20:46:26,856] com.bakdata.conquery.tasks.IsUUIDTestTest: 79e806b3-6d61-45ab-9ec6-467ec6e645e4
INFO  [2023-01-06 20:46:26,856] com.bakdata.conquery.tasks.IsUUIDTestTest: f14e9fdb-105c-4edb-9f9c-fbb8fd324d36
INFO  [2023-01-06 20:46:26,857] com.bakdata.conquery.tasks.IsUUIDTestTest: 6dc18d2a-6935-4d00-96c9-d5d66205e91f
INFO  [2023-01-06 20:46:26,857] com.bakdata.conquery.tasks.IsUUIDTestTest: 773ce968-755c-4633-a1be-d3029022ed58
INFO  [2023-01-06 20:46:26,858] com.bakdata.conquery.tasks.IsUUIDTestTest: 3691b271-9080-4ec9-a62c-63469c15e316
INFO  [2023-01-06 20:46:26,858] com.bakdata.conquery.tasks.IsUUIDTestTest: 55d53997-98f4-458b-abc0-3911918b84fa
INFO  [2023-01-06 20:46:26,859] com.bakdata.conquery.tasks.IsUUIDTestTest: ab1f44cd-90a9-4051-8b00-41c6d187d862
INFO  [2023-01-06 20:46:26,859] com.bakdata.conquery.tasks.IsUUIDTestTest: bde50b0d-b6b3-438e-95d3-354694d7d98d
INFO  [2023-01-06 20:46:26,860] com.bakdata.conquery.tasks.IsUUIDTestTest: 176351e3-ebf4-4436-a414-307cb9a88162
INFO  [2023-01-06 20:46:26,860] com.bakdata.conquery.tasks.IsUUIDTestTest: 693d22ab-01ef-4f70-9530-020db1454ead
INFO  [2023-01-06 20:46:26,861] com.bakdata.conquery.tasks.IsUUIDTestTest: 9c6f19ee-12fe-42a5-b24e-4f3cb3ec6bd9
INFO  [2023-01-06 20:46:26,861] com.bakdata.conquery.tasks.IsUUIDTestTest: 1a43fae9-5520-4762-8945-69b0f208f343
INFO  [2023-01-06 20:46:26,861] com.bakdata.conquery.tasks.IsUUIDTestTest: 7bacd5bc-668c-4253-aed3-c085c872313f
INFO  [2023-01-06 20:46:26,862] com.bakdata.conquery.tasks.IsUUIDTestTest: 6ecc290a-06fe-4da6-8e79-1ea163bfe2f7
INFO  [2023-01-06 20:46:26,862] com.bakdata.conquery.tasks.IsUUIDTestTest: 1f380526-9b2d-4674-872c-6719cdfc67d3
INFO  [2023-01-06 20:46:26,862] com.bakdata.conquery.tasks.IsUUIDTestTest: 7ffb821d-093a-4d13-b8bc-44741bd61b69
INFO  [2023-01-06 20:46:26,863] com.bakdata.conquery.tasks.IsUUIDTestTest: cef7318e-8e04-4998-8365-66112021a9e7
INFO  [2023-01-06 20:46:26,863] com.bakdata.conquery.tasks.IsUUIDTestTest: d26acf14-61ab-4a0b-a20b-42cc22cf1af4
INFO  [2023-01-06 20:46:26,863] com.bakdata.conquery.tasks.IsUUIDTestTest: 3af6ba92-4fd7-4861-8f83-08b0220733bd
INFO  [2023-01-06 20:46:26,864] com.bakdata.conquery.tasks.IsUUIDTestTest: acdff624-c4b1-49ab-ae88-360c8a21af2b
INFO  [2023-01-06 20:46:26,864] com.bakdata.conquery.tasks.IsUUIDTestTest: 1fef47a3-f0ba-481f-b0dd-0d654db41ea2
INFO  [2023-01-06 20:46:26,864] com.bakdata.conquery.tasks.IsUUIDTestTest: df36cbd0-6896-4e8b-83c3-82d3404329f4
INFO  [2023-01-06 20:46:26,865] com.bakdata.conquery.tasks.IsUUIDTestTest: d4951641-d924-4fb2-a695-7b028b7dbe00
INFO  [2023-01-06 20:46:26,865] com.bakdata.conquery.tasks.IsUUIDTestTest: 328921e9-1079-4361-8679-67c12ea43973
INFO  [2023-01-06 20:46:26,865] com.bakdata.conquery.tasks.IsUUIDTestTest: bab85a89-02da-4362-b5b0-ce09cb7584ff
INFO  [2023-01-06 20:46:26,865] com.bakdata.conquery.tasks.IsUUIDTestTest: 9ca8d0ce-e2ea-4a22-8f61-cc3b56577c6d
INFO  [2023-01-06 20:46:26,866] com.bakdata.conquery.tasks.IsUUIDTestTest: c6428736-86a4-4ec5-9f6d-4a63f4b14435
INFO  [2023-01-06 20:46:26,866] com.bakdata.conquery.tasks.IsUUIDTestTest: 8f788a4e-a5dc-4e61-9b5c-56d5dd9b04db
INFO  [2023-01-06 20:46:26,866] com.bakdata.conquery.tasks.IsUUIDTestTest: 35d88e52-206b-4dcf-918d-850abbe3536b
INFO  [2023-01-06 20:46:26,867] com.bakdata.conquery.tasks.IsUUIDTestTest: 5bb996ab-d3c1-4768-ad43-3c79ab73c87d
INFO  [2023-01-06 20:46:26,867] com.bakdata.conquery.tasks.IsUUIDTestTest: bac26978-d8f1-4fb0-8923-faf40f9c0078
INFO  [2023-01-06 20:46:26,867] com.bakdata.conquery.tasks.IsUUIDTestTest: ade8c044-ee07-431c-8e12-ce8ea72b1b92
INFO  [2023-01-06 20:46:26,867] com.bakdata.conquery.tasks.IsUUIDTestTest: f9402f08-4fa7-4a06-9896-e227e1771cba
INFO  [2023-01-06 20:46:26,868] com.bakdata.conquery.tasks.IsUUIDTestTest: 24767dc2-694a-4b16-9822-57ace0576be0
INFO  [2023-01-06 20:46:26,868] com.bakdata.conquery.tasks.IsUUIDTestTest: e31c3fa6-df0b-4508-aad3-21b5717199e4
INFO  [2023-01-06 20:46:26,868] com.bakdata.conquery.tasks.IsUUIDTestTest: 583ad0a7-be13-438f-b1fb-467f1a8b7999
INFO  [2023-01-06 20:46:26,869] com.bakdata.conquery.tasks.IsUUIDTestTest: 39aeb9fd-9a5c-41d8-b2f4-d84a3113cc63
INFO  [2023-01-06 20:46:26,869] com.bakdata.conquery.tasks.IsUUIDTestTest: e4549d93-1bd0-4ec7-8208-0729679bcb70
INFO  [2023-01-06 20:46:26,869] com.bakdata.conquery.tasks.IsUUIDTestTest: 7c6d97af-eac0-4574-bc6d-814aacaeaf73
INFO  [2023-01-06 20:46:26,870] com.bakdata.conquery.tasks.IsUUIDTestTest: 627ccd4f-e610-4607-af7a-591b076a78e8
INFO  [2023-01-06 20:46:26,870] com.bakdata.conquery.tasks.IsUUIDTestTest: ff40b222-5eea-4793-aabd-0268414650c4
INFO  [2023-01-06 20:46:26,872] com.bakdata.conquery.tasks.IsUUIDTestTest: ac7e32a9-2fc0-4c9e-b980-a62e31c759b9
INFO  [2023-01-06 20:46:26,872] com.bakdata.conquery.tasks.IsUUIDTestTest: 160cd8c6-26b9-4705-9631-b8d2284b4e85
INFO  [2023-01-06 20:46:26,872] com.bakdata.conquery.tasks.IsUUIDTestTest: 692fc527-5d31-43da-a59f-201e5668f581
INFO  [2023-01-06 20:46:26,873] com.bakdata.conquery.tasks.IsUUIDTestTest: ca9cf3db-687f-4d25-a058-52002f4d01ab
[INFO] Tests run: 51, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 s - in com.bakdata.conquery.tasks.IsUUIDTestTest
[INFO] Running com.bakdata.conquery.tasks.QueryCleanupTaskTest
INFO  [2023-01-06 20:46:26,874] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 1
INFO  [2023-01-06 20:46:26,875] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,875] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-06 20:46:26,875] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,875] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-06 20:46:26,876] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 1
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT719H of 1
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-06 20:46:26,877] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.tasks.QueryCleanupTaskTest
[INFO] Running com.bakdata.conquery.io.storage.xodus.stores.SerializingStoreDumpTest
INFO  [2023-01-06 20:46:26,905] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/1673037986884-0
INFO  [2023-01-06 20:46:26,905] com.bakdata.conquery.io.storage.xodus.stores.SerializingStoreDumpTest: This test will throw some warnings from the SerializingStore.
WARN  [2023-01-06 20:46:26,928] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse value for key [user.testU2]
WARN  [2023-01-06 20:46:26,929] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse key [:)
Vnot a valid conquery Id]
WARN  [2023-01-06 20:46:26,929] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Removing 2 unreadable elements from the store AUTH_USER.
WARN  [2023-01-06 20:46:26,931] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Removing 0 unreadable elements from the store AUTH_USER.
INFO  [2023-01-06 20:46:26,957] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/1673037986938-0
INFO  [2023-01-06 20:46:26,987] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Dumping value of key :)
Vnot a valid conquery Id to /tmp/1673037986938-0/20230106-AUTH_USER-____Vnot a valid conquery Id.json (because it cannot be deserialized anymore).
WARN  [2023-01-06 20:46:26,987] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse key [:)
Vnot a valid conquery Id]
INFO  [2023-01-06 20:46:27,014] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/1673037986996-0
INFO  [2023-01-06 20:46:27,031] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Dumping value of key user.testU2 to /tmp/1673037986996-0/20230106-AUTH_USER-user.testU2.json (because it cannot be deserialized anymore).
WARN  [2023-01-06 20:46:27,031] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse value for key [user.testU2]
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.149 s - in com.bakdata.conquery.io.storage.xodus.stores.SerializingStoreDumpTest
[INFO] Running com.bakdata.conquery.io.storage.xodus.stores.BigStoreTest
INFO  [2023-01-06 20:46:27,057] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/BigStoreTest526417917215027645
INFO  [2023-01-06 20:46:27,107] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/BigStoreTest9082602028147406621
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.946 s - in com.bakdata.conquery.io.storage.xodus.stores.BigStoreTest
[INFO] Running com.bakdata.conquery.io.jackson.SerializationBlockTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.io.jackson.SerializationBlockTest
[INFO] Running com.bakdata.conquery.io.jackson.JacksonTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 s - in com.bakdata.conquery.io.jackson.JacksonTest
[INFO] Running com.bakdata.conquery.io.jackson.serializer.RangeSerializerTest
[INFO] Tests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.288 s - in com.bakdata.conquery.io.jackson.serializer.RangeSerializerTest
[INFO] Running com.bakdata.conquery.io.jackson.serializer.ClassToInstanceMapDeserializerTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 s - in com.bakdata.conquery.io.jackson.serializer.ClassToInstanceMapDeserializerTest
[INFO] Running com.bakdata.conquery.io.jackson.serializer.IdRefrenceTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.048 s - in com.bakdata.conquery.io.jackson.serializer.IdRefrenceTest
[INFO] Running com.bakdata.conquery.io.cps.CPSBaseTest
[INFO] Tests run: 188, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.008 s - in com.bakdata.conquery.io.cps.CPSBaseTest
[INFO] Running com.bakdata.conquery.io.result.csv.CsvResultGenerationTest
INFO  [2023-01-06 20:46:30,396] com.bakdata.conquery.io.result.csv.CsvResultGenerationTest: Wrote and than read this csv data: id1,id2,BOOLEAN,INTEGER,NUMERIC,CATEGORICAL,RESOLUTION,DATE,DATE_RANGE,STRING,MONEY,LIST[BOOLEAN],LIST[DATE_RANGE],LIST[STRING]
1,1,Ja,2.345.634,"123.423,34",CAT1,Tag,17.06.1985,12.12.1970 - 19.06.1971,test_string,"45,21","Ja, Nein","12.12.1970 - 19.06.1971, 02.01.1970 - 03.01.1970","fizz, buzz"
2,2,Nein,,,,,,,,,,19.05.1973 - +∞,
2,2,Ja,,,,,,,,,"Nein, Nein",,
3,3,Nein,,,,,,,,,Nein,,
3,3,Ja,,,,,,,,,,,
3,3,Ja,,,,,,,,"0,04","Ja, Nein, Ja, Nein",,

[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in com.bakdata.conquery.io.result.csv.CsvResultGenerationTest
[INFO] Running com.bakdata.conquery.io.result.arrow.ArrowResultGenerationTest
INFO  [2023-01-06 20:46:30,409] org.apache.arrow.memory.BaseAllocator: Debug mode enabled.
INFO  [2023-01-06 20:46:30,412] org.apache.arrow.memory.DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
INFO  [2023-01-06 20:46:30,413] org.apache.arrow.memory.CheckAllocator: Using DefaultAllocationManager at memory-netty/6.0.1/arrow-memory-netty-6.0.1.jar!/org/apache/arrow/memory/DefaultAllocationManagerFactory.class
INFO  [2023-01-06 20:46:30,582] com.bakdata.conquery.io.result.arrow.ArrowResultGenerationTest: Reading the produced arrow data.
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.228 s - in com.bakdata.conquery.io.result.arrow.ArrowResultGenerationTest
[INFO] Running com.bakdata.conquery.io.result.excel.ExcelResultRenderTest
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.46 s <<< FAILURE! - in com.bakdata.conquery.io.result.excel.ExcelResultRenderTest
[ERROR] writeAndRead  Time elapsed: 0.46 s  <<< ERROR!
java.lang.AbstractMethodError: Receiver class org.apache.poi.xssf.streaming.SXSSFRow does not define or inherit an implementation of the resolved method 'java.util.Iterator iterator()' of interface org.apache.poi.ss.usermodel.Row.
	at org.apache.poi.xssf.streaming.AutoSizeColumnTracker.updateColumnWidths(AutoSizeColumnTracker.java:323)
	at org.apache.poi.xssf.streaming.SXSSFSheet.flushOneRow(SXSSFSheet.java:1806)
	at org.apache.poi.xssf.streaming.SXSSFSheet.flushRows(SXSSFSheet.java:1775)
	at org.apache.poi.xssf.streaming.SXSSFSheet.flushRows(SXSSFSheet.java:1788)
	at org.apache.poi.xssf.streaming.SXSSFSheet.dispose(SXSSFSheet.java:1831)
	at org.apache.poi.xssf.streaming.SXSSFWorkbook.dispose(SXSSFWorkbook.java:1031)
	at com.bakdata.conquery.io.result.excel.ExcelRenderer.renderToStream(ExcelRenderer.java:94)
	at com.bakdata.conquery.io.result.excel.ExcelResultRenderTest.writeAndRead(ExcelResultRenderTest.java:86)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] Running com.bakdata.conquery.io.result.ResultNameTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.io.result.ResultNameTest
INFO  [2023-01-06 20:46:31,119] org.eclipse.jetty.server.AbstractConnector: Stopped application@74367715{HTTP/1.1, (http/1.1)}{0.0.0.0:42569}
INFO  [2023-01-06 20:46:31,119] org.eclipse.jetty.server.AbstractConnector: Stopped admin@710ee5b1{HTTP/1.1, (http/1.1)}{0.0.0.0:39367}
INFO  [2023-01-06 20:46:31,127] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@dd29815{/,null,STOPPED}
INFO  [2023-01-06 20:46:31,132] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@6d13d52e{/,null,STOPPED}
INFO  [2023-01-06 20:46:31,132] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-06 20:46:31,172] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-06 20:46:31,234] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:31,234] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:31,234] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:31,236] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-06 20:46:31,272] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-06 20:46:31,373] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:31,373] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:31,373] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:31,373] com.bakdata.conquery.models.auth.apitoken.TokenStorage: Closing the environment.
INFO  [2023-01-06 20:46:31,385] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-06 20:46:31,440] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-06 20:46:31,473] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/executions:EXECUTIONS}
INFO  [2023-01-06 20:46:31,473] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/executions
INFO  [2023-01-06 20:46:31,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/formConfigs:FORM_CONFIG}
INFO  [2023-01-06 20:46:31,488] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/formConfigs
INFO  [2023-01-06 20:46:31,503] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/users:AUTH_USER}
INFO  [2023-01-06 20:46:31,503] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/users
INFO  [2023-01-06 20:46:31,515] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/roles:AUTH_ROLE}
INFO  [2023-01-06 20:46:31,515] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/roles
INFO  [2023-01-06 20:46:31,524] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/groups:AUTH_GROUP}
INFO  [2023-01-06 20:46:31,524] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/ApiTokenRealmTest/manager/meta/groups
INFO  [2023-01-06 20:46:31,541] org.eclipse.jetty.server.AbstractConnector: Stopped application@688daecb{HTTP/1.1, (http/1.1)}{0.0.0.0:46651}
INFO  [2023-01-06 20:46:31,542] org.eclipse.jetty.server.AbstractConnector: Stopped admin@422e8da{HTTP/1.1, (http/1.1)}{0.0.0.0:42541}
INFO  [2023-01-06 20:46:31,547] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@5ee4a094{/,null,STOPPED}
INFO  [2023-01-06 20:46:31,551] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@486d9c97{/,null,STOPPED}
INFO  [2023-01-06 20:46:31,551] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-06 20:46:31,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-06 20:46:31,673] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:31,673] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:31,673] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:31,673] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-06 20:46:31,738] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-06 20:46:31,773] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:31,773] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:31,773] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:31,774] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-06 20:46:31,800] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-06 20:46:31,838] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions:EXECUTIONS}
INFO  [2023-01-06 20:46:31,838] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/executions
INFO  [2023-01-06 20:46:31,849] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs:FORM_CONFIG}
INFO  [2023-01-06 20:46:31,849] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/formConfigs
INFO  [2023-01-06 20:46:31,859] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users:AUTH_USER}
INFO  [2023-01-06 20:46:31,859] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/users
INFO  [2023-01-06 20:46:31,867] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles:AUTH_ROLE}
INFO  [2023-01-06 20:46:31,867] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/roles
INFO  [2023-01-06 20:46:31,874] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups:AUTH_GROUP}
INFO  [2023-01-06 20:46:31,874] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest6175202265804509010/manager/meta/groups
INFO  [2023-01-06 20:46:31,884] org.eclipse.jetty.server.AbstractConnector: Stopped application@72f04ef6{HTTP/1.1, (http/1.1)}{0.0.0.0:43121}
INFO  [2023-01-06 20:46:31,884] org.eclipse.jetty.server.AbstractConnector: Stopped admin@6f06231a{HTTP/1.1, (http/1.1)}{0.0.0.0:37917}
INFO  [2023-01-06 20:46:31,890] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@30b4f96c{/,null,STOPPED}
INFO  [2023-01-06 20:46:31,893] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@1c824fb3{/,null,STOPPED}
INFO  [2023-01-06 20:46:31,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-06 20:46:31,938] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-06 20:46:31,973] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:31,974] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:31,974] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:31,974] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-06 20:46:32,038] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-06 20:46:32,074] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:32,074] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:32,074] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:32,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-06 20:46:32,178] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-06 20:46:32,242] org.eclipse.jetty.server.AbstractConnector: Stopped application@5acbe4e9{HTTP/1.1, (http/1.1)}{0.0.0.0:34843}
INFO  [2023-01-06 20:46:32,243] org.eclipse.jetty.server.AbstractConnector: Stopped admin@6cfcbc1e{HTTP/1.1, (http/1.1)}{0.0.0.0:45265}
INFO  [2023-01-06 20:46:32,251] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@a984ac7{/,null,STOPPED}
INFO  [2023-01-06 20:46:32,257] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@6ae5981f{/,null,STOPPED}
INFO  [2023-01-06 20:46:32,257] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-06 20:46:32,273] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-06 20:46:32,274] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:32,274] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:32,274] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:32,274] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-06 20:46:32,373] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-06 20:46:32,474] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-06 20:46:32,474] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-06 20:46:32,474] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-06 20:46:32,474] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-06 20:46:32,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   ExcelResultRenderTest.writeAndRead:86 » AbstractMethod Receiver class org.apac...
[INFO] 
[ERROR] Tests run: 1666, Failures: 0, Errors: 1, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Conquery Parent 0.0.0-SNAPSHOT:
[INFO] 
[INFO] Conquery Parent .................................... SUCCESS [  0.150 s]
[INFO] backend ............................................ FAILURE [03:30 min]
[INFO] executable ......................................... SKIPPED
[INFO] autodoc ............................................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  03:30 min
[INFO] Finished at: 2023-01-06T20:46:33Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (default-test) on project backend: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/gabsko/breaking-updates/backend/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :backend
