[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Conquery Parent                                                    [pom]
[INFO] backend                                                            [jar]
[INFO] executable                                                         [jar]
[INFO] autodoc                                                            [jar]
[INFO] 
[INFO] --------------------< com.bakdata.conquery:parent >---------------------
[INFO] Building Conquery Parent 0.0.0-SNAPSHOT                            [1/4]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ parent ---
[INFO] 
[INFO] --------------------< com.bakdata.conquery:backend >--------------------
[INFO] Building backend 0.0.0-SNAPSHOT                                    [2/4]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ backend ---
[INFO] Deleting /home/gabsko/breaking-updates/backend/target
[INFO] 
[INFO] --- maven-resources-plugin:3.2.0:resources (default-resources) @ backend ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Using 'UTF-8' encoding to copy filtered properties files.
[INFO] Copying 2 resources
[INFO] Copying 32 resources
[INFO] The encoding used to copy filtered properties files have not been set. This means that the same encoding will be used to copy filtered properties files as when copying other filtered resources. This might not be what you want! Run your build with --debug to see which files might be affected. Read more at https://maven.apache.org/plugins/maven-resources-plugin/examples/filtering-properties-files.html
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ backend ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 822 source files to /home/gabsko/breaking-updates/backend/target/classes
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Some input files use or override a deprecated API.
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Some input files use unchecked or unsafe operations.
[INFO] /home/gabsko/breaking-updates/backend/src/main/java/com/bakdata/conquery/models/common/Range.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:3.2.0:testResources (default-testResources) @ backend ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Using 'UTF-8' encoding to copy filtered properties files.
[INFO] Copying 467 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ backend ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 128 source files to /home/gabsko/breaking-updates/backend/target/test-classes
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/tasks/PermissionCleanupTaskTest.java: Some input files use or override a deprecated API.
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/tasks/PermissionCleanupTaskTest.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/integration/tests/ReusedQueryTest.java: Some input files use unchecked or unsafe operations.
[INFO] /home/gabsko/breaking-updates/backend/src/test/java/com/bakdata/conquery/integration/tests/ReusedQueryTest.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.22.2:test (default-test) @ backend ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.bakdata.conquery.models.forms.DateContextTest
[INFO] Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.082 s - in com.bakdata.conquery.models.forms.DateContextTest
[INFO] Running com.bakdata.conquery.models.execution.DefaultLabelTest
[INFO] [TEST] [2023-01-17 00:49:44]	c.b.c.i.c.CPSTypeIdResolver		Scanning Classpath
[INFO] [TEST] [2023-01-17 00:49:44]	c.b.c.i.c.CPSTypeIdResolver		Scanned: 1096 classes in classpath
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class CredentialType
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				PASSWORD	->	PasswordCredential
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class Mode
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ABSOLUTE	->	AbsoluteMode
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				RELATIVE	->	RelativeMode
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ENTITY_DATE	->	EntityDateMode
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class CQElement
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SAVED_QUERY	->	CQReusedQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				AND	->	CQAnd
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				NEGATION	->	CQNegation
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DAYS_BEFORE	->	CQDaysBeforeTemporalQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BEFORE_OR_SAME	->	CQBeforeOrSameTemporalQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				RESULT_INFO_DECORATOR	->	ResultInfoDecorator
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EXTERNAL	->	CQExternal
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				OR	->	CQOr
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DAYS_OR_NO_EVENT_BEFORE	->	CQDaysBeforeOrNeverTemporalQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BEFORE	->	CQBeforeTemporalQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CONCEPT	->	CQConcept
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SAME	->	CQSameTemporalQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_RESTRICTION	->	CQDateRestriction
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class QueryDescription
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SECONDARY_ID_QUERY	->	SecondaryIdQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ENTITY_DATE_QUERY	->	EntityDateQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				TABLE_EXPORT	->	TableExportQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CONCEPT_QUERY	->	ConceptQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FULL_EXPORT_FORM	->	FullExportForm
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EXPORT_FORM	->	ExportForm
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ABSOLUTE_FORM_QUERY	->	AbsoluteFormQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				RELATIVE_FORM_QUERY	->	RelativeFormQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ARRAY_CONCEPT_QUERY	->	ArrayConceptQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class FilterValue
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REAL_RANGE	->	CQRealRangeFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING	->	CQStringFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				INTEGER_RANGE	->	CQIntegerRangeFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MONEY_RANGE	->	CQMoneyRangeFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BIG_MULTI_SELECT	->	CQBigMultiSelectFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SELECT	->	CQSelectFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MULTI_SELECT	->	CQMultiSelectFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ConqueryTestSpec
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FILTER_TEST	->	FilterTest
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				QUERY_TEST	->	QueryTest
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORM_TEST	->	FormTest
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ResultRendererProvider
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				XLSX	->	XlsxResultProvider
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ARROW_STREAM	->	ArrowStreamResultProvider
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ARROW_FILE	->	ArrowFileResultProvider
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CSV	->	CsvResultRendererProvider
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ConqueryPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				WILDCARD_PERMISSION	->	WildcardPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class StringPermissionBuilder
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORM_CONFIG	->	FormConfigPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ADMIN	->	AdminPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORM_TYPE	->	FormPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATASET	->	DatasetPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SUPER	->	SuperPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CONCEPT	->	ConceptPermission
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EXECUTION	->	ExecutionPermission
[WARN] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class interface com.bakdata.conquery.models.config.PluginConfig:	No registered types
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class StoreFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				NON_PERSISTENT	->	NonPersistentStoreFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				XODUS	->	XodusStoreFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class AuthenticationRealmFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DEVELOPMENT	->	DevAuthConfig
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				API_TOKEN	->	ApiTokenRealmFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				JWT_PKCE_REALM	->	JwtPkceVerifyingRealmFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				LOCAL_AUTHENTICATION	->	LocalAuthenticationConfig
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				OIDC_AUTHORIZATION_CODE_FLOW	->	OIDCAuthorizationCodeFlowRealmFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				OIDC_RESOURCE_OWNER_PASSWORD_CREDENTIAL_AUTHENTICATION	->	OIDCResourceOwnerPasswordCredentialRealmFactory
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class AuthorizationConfig
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DEVELOPMENT	->	DevelopmentAuthorizationConfig
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DEFAULT	->	DefaultAuthorizationConfig
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class Concept
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				TREE	->	TreeConcept
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class CTCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EQUAL	->	EqualCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				OR	->	OrCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				GROOVY	->	GroovyCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				PREFIX_LIST	->	PrefixCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				AND	->	AndCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				PREFIX_RANGE	->	PrefixRangeCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				PRESENT	->	IsPresentCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COLUMN_EQUAL	->	ColumnEqualCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				NOT	->	NotCondition
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class Filter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COUNT	->	CountFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COUNT_QUARTERS	->	CountQuartersFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				PREFIX_TEXT	->	PrefixTextFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				NUMBER	->	NumberFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BIG_MULTI_SELECT	->	BigMultiSelectFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_DISTANCE	->	DateDistanceFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SELECT	->	MultiSelectFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DURATION_SUM	->	DurationSumFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				QUARTERS_IN_YEAR	->	QuartersInYearFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SINGLE_SELECT	->	SelectFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SUM	->	SumFilter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class Select
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				QUARTER	->	QuarterSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COUNT_QUARTERS	->	CountQuartersSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EXISTS	->	ExistsSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				QUARTERS_IN_YEAR	->	QuartersInYearSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FIRST	->	FirstValueSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_UNION	->	DateUnionSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DURATION_SUM	->	DurationSumSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COUNT	->	CountSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EVENT_DURATION_SUM	->	EventDurationSumSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				PREFIX	->	PrefixSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DISTINCT	->	DistinctSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				RANDOM	->	RandomValueSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COUNT_OCCURENCES	->	CountOccurencesSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				LAST	->	LastValueSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EVENT_DATE_UNION	->	EventDateUnionSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_DISTANCE	->	DateDistanceSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SUM	->	SumSelect
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class Dictionary
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MAP_DICTIONARY	->	MapDictionary
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SUCCINCT_TRIE	->	SuccinctTrie
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ConqueryError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_PLAN	->	ExecutionCreationPlanError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_JOB	->	ExecutionJobErrorWrapper
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_NO_SECONDARY_ID	->	NoSecondaryIdSelectedError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_PROCESSING	->	ExecutionProcessingError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE_EXTERNAL	->	ExternalResolveError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE_EXTERNAL_FORMAT	->	ExternalResolveFormatError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION	->	ExecutionCreationErrorUnspecified
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_PROCESSING_TIMEOUT	->	ExecutionProcessingTimeoutError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_UNKNOWN_ERROR	->	UnknownError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE	->	ExecutionCreationResolveError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_CREATION_PLAN_DATECONTEXT_MISMATCH	->	ExecutionCreationPlanDateContextError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CQ_EXECUTION_CREATION_RESOLVE_EXTERNAL_EMPTY	->	ExternalResolveEmptyError
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ColumnStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BOOLEANS	->	BitSetStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING_NUMBER	->	StringTypeNumber
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BYTES	->	ByteArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				LONGS	->	LongArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SHORTS	->	ShortArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING_PREFIX	->	StringTypePrefixSuffix
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EMPTY	->	EmptyStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				INTEGERS	->	IntArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATES	->	IntegerDateStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REBASE	->	RebasingStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MONEY_VARINT	->	MoneyIntStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE_COMPOUND	->	DateRangeTypeCompound
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING_DICTIONARY	->	StringTypeDictionary
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DECIMALS	->	DecimalArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE_DATE_RANGE	->	DateRangeTypeDateRange
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DOUBLES	->	DoubleArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FLOATS	->	FloatArrayStore
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING_SINGLETON	->	StringTypeSingleton
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DECIMAL_SCALED	->	DecimalTypeScaled
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE_QUARTER	->	DateRangeTypeQuarter
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING_ENCODED	->	StringTypeEncoded
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ManagedExecution
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MANAGED_QUERY	->	ManagedQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				INTERNAL_FORM	->	ManagedInternalForm
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MANAGED_FORM	->	ManagedForm
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ResultType
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				INTEGER	->	IntegerT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MONEY	->	MoneyT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				RESOLUTION	->	ResolutionT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE	->	DateT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				STRING	->	StringT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ID	->	IdT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE	->	DateRangeT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				BOOLEAN	->	BooleanT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				LIST	->	ListT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				NUMERIC	->	NumericT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CATEGORICAL	->	CategoricalT
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class NamespacedMessage
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_SECONDARYID	->	RemoveSecondaryId
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				IMPORT_BIT	->	ImportBucket
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_IMPORT	->	RemoveImportJob
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_TABLE	->	RemoveTable
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_CONCEPT	->	RemoveConcept
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SHUTDOWN_WORKER	->	ShutdownWorkerStorage
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_SHARD_WORKER_IDENTITY	->	UpdateWorkerBucket
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EXECUTE_FORM	->	ExecuteForm
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CANCEL_QUERY	->	CancelQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_METADATA	->	UpdateElementMatchingStats
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REQUEST_CONSISTENCY	->	RequestConsistency
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_DATASET	->	UpdateDataset
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_DICTIONARY	->	UpdateDictionary
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_CONCEPT	->	UpdateConcept
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_MATCHING_STATS	->	UpdateMatchingStatsMessage
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ADD_IMPORT	->	AddImport
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REPORT_CONSISTENCY	->	ReportConsistency
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_TABLE	->	UpdateTable
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COLLECT_QUERY_RESULT	->	CollectQueryResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EXECUTE_QUERY	->	ExecuteQuery
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_SECONDARYID	->	UpdateSecondaryId
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class NetworkMessage
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORWARD_TO_WORKER	->	ForwardToWorker
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				UPDATE_JOB_MANAGER_STATUS	->	UpdateJobManagerStatus
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				CANCEL_JOB	->	CancelJobMessage
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORWARD_TO_NAMESPACE	->	ForwardToNamespace
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_SHARD_NODE	->	RemoveShardNode
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REMOVE_WORKER	->	RemoveWorker
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ADD_SHARD_NODE	->	AddShardNode
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				REGISTER_SHARD_WORKER_IDENTITY	->	RegisterWorker
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ADD_WORKER	->	AddWorker
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class OutputDescription
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COMPOUND_DATE_RANGE	->	CompoundDateRangeOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				COPY	->	CopyOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				LINE	->	LineOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				DATE_RANGE	->	DateRangeOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EPOCH_DATE_RANGE	->	EpochDateRangeOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				EPOCH	->	EpochOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				NULL	->	NullOutput
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class EntityResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SINGLE_LINE	->	SinglelineEntityResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				MULTI_LINE	->	MultilineEntityResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ShardResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORM_SHARD_RESULT	->	FormShardResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				SHARD_RESULT	->	ShardResult
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver			Base Class ResourcesProvider
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				ApiV1	->	ApiV1
[INFO] [TEST] [2023-01-17 00:49:45]	c.b.c.i.c.CPSTypeIdResolver				FORM_RESOURCES	->	FormResourceProvider
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.434 s - in com.bakdata.conquery.models.execution.DefaultLabelTest
[INFO] Running com.bakdata.conquery.models.events.stores.primitive.BooleanStoreTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.104 s - in com.bakdata.conquery.models.events.stores.primitive.BooleanStoreTest
[INFO] Running com.bakdata.conquery.models.events.stores.primitive.IntArrayStoreTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.models.events.stores.primitive.IntArrayStoreTest
[INFO] Running com.bakdata.conquery.models.events.stores.types.ColumnStoreSerializationTests
ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...
[INFO] Tests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.828 s - in com.bakdata.conquery.models.events.stores.types.ColumnStoreSerializationTests
[INFO] Running com.bakdata.conquery.models.events.stores.types.MajorTypesTest
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 s - in com.bakdata.conquery.models.events.stores.types.MajorTypesTest
[INFO] Running com.bakdata.conquery.models.events.stores.types.StringEncodingTest
[INFO] Tests run: 101, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.057 s - in com.bakdata.conquery.models.events.stores.types.StringEncodingTest
[INFO] Running com.bakdata.conquery.models.events.CBlockTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.058 s - in com.bakdata.conquery.models.events.CBlockTest
[INFO] Running com.bakdata.conquery.models.error.ConqueryErrorTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 s - in com.bakdata.conquery.models.error.ConqueryErrorTest
[INFO] Running com.bakdata.conquery.models.query.UniqueNameTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.query.UniqueNameTest
[INFO] Running com.bakdata.conquery.models.query.DefaultColumnNameTest
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.084 s - in com.bakdata.conquery.models.query.DefaultColumnNameTest
[INFO] Running com.bakdata.conquery.models.SerializationTests
[INFO] [TEST] [2023-01-17 00:49:46]	c.b.c.m.SerializationTests		Beware, this test will print an ERROR message.
[ERROR] [TEST] [2023-01-17 00:49:46]	c.b.c.m.e.ConqueryError$UnknownError		Encountered unknown Error[63f1265d-c9ed-4a80-bbce-73cc6e4c9ab4]
[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.727 s - in com.bakdata.conquery.models.SerializationTests
[INFO] Running com.bakdata.conquery.models.auth.ApiTokenTest
[INFO] [TEST] [2023-01-17 00:49:47]	c.b.c.m.a.ApiTokenTest		Testing token: cq_44TZxFytw2tnEbJTa0OpX9gmBZENIzOWE2l47
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.auth.ApiTokenTest
[INFO] Running com.bakdata.conquery.models.auth.LocalAuthRealmTest
[INFO] [TEST] [2023-01-17 00:49:47]	c.b.c.m.a.b.PasswordHasher		Using the following settings to generate password hashes:
	Algorithm: PBKDF2WithHmacSHA1
	Iterations: 10000
	Key length: 256
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.609 s - in com.bakdata.conquery.models.auth.LocalAuthRealmTest
[INFO] Running com.bakdata.conquery.models.auth.CopyUserTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.bakdata.conquery.models.auth.CopyUserTest
[INFO] Running com.bakdata.conquery.models.auth.InstancePermissionImplificationTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.models.auth.InstancePermissionImplificationTest
[INFO] Running com.bakdata.conquery.models.auth.IdpDelegatingAccessTokenCreatorTest
Loading JavaScript to validate ECMA262 regular expression in JsonSchema because java.util.regex package in Java does not match ECMA262
Warning: Nashorn engine is planned to be removed from a future JDK release
[INFO] [TEST] [2023-01-17 00:49:49]	c.b.c.m.a.IdpDelegatingAccessTokenCreatorTest		This test will print an Error below.
[ERROR] [TEST] [2023-01-17 00:49:49]	c.b.c.m.a.o.p.IdpDelegatingAccessTokenCreator		Received the following error from the auth server while validating username and password:
	Path: http://localhost:1080/realms/test_relam/protocol/openid-connect/token
	Status code: 403
	Status message: null
	Content: {"error":null}
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.563 s - in com.bakdata.conquery.models.auth.IdpDelegatingAccessTokenCreatorTest
[INFO] Running com.bakdata.conquery.models.auth.PermissionCreationTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.auth.PermissionCreationTest
[INFO] Running com.bakdata.conquery.models.auth.oidc.JwtPkceVerifyingRealmTest
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.724 s - in com.bakdata.conquery.models.auth.oidc.JwtPkceVerifyingRealmTest
[INFO] Running com.bakdata.conquery.models.auth.IntrospectionDelegatingRealmTest
[INFO] [TEST] [2023-01-17 00:49:53]	c.b.c.m.a.o.IntrospectionDelegatingRealm		Created new group: Group[group.group2]
[INFO] [TEST] [2023-01-17 00:49:53]	c.b.c.m.a.o.IntrospectionDelegatingRealm		Created new user: User[user.test_name1]
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.43 s - in com.bakdata.conquery.models.auth.IntrospectionDelegatingRealmTest
[INFO] Running com.bakdata.conquery.models.common.CQuarterTest
[INFO] Tests run: 616, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.169 s - in com.bakdata.conquery.models.common.CQuarterTest
[INFO] Running com.bakdata.conquery.models.common.daterange.CDateRangeTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.common.daterange.CDateRangeTest
[INFO] Running com.bakdata.conquery.models.common.RangeTest
[INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in com.bakdata.conquery.models.common.RangeTest
[INFO] Running com.bakdata.conquery.models.common.QuarterUtilsTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.common.QuarterUtilsTest
[INFO] Running com.bakdata.conquery.models.externalservice.ResultTypeTest
[INFO] Tests run: 62, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.044 s - in com.bakdata.conquery.models.externalservice.ResultTypeTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.frontend.FEValueTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.datasets.concepts.frontend.FEValueTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.temporal.TemporalSamplerTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.models.datasets.concepts.temporal.TemporalSamplerTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.tree.GroovyIndexedTest
[INFO] Tests run: 51, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.834 s - in com.bakdata.conquery.models.datasets.concepts.tree.GroovyIndexedTest
[INFO] Running com.bakdata.conquery.models.datasets.concepts.tree.MatchingStatsTests
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.datasets.concepts.tree.MatchingStatsTests
[INFO] Running com.bakdata.conquery.models.identifiable.ids.IdTests
[INFO] Tests run: 57, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.04 s - in com.bakdata.conquery.models.identifiable.ids.IdTests
[INFO] Running com.bakdata.conquery.models.identifiable.mapping.PseudomizationTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 s - in com.bakdata.conquery.models.identifiable.mapping.PseudomizationTest
[INFO] Running com.bakdata.conquery.models.identifiable.IdMapTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.identifiable.IdMapTest
[INFO] Running com.bakdata.conquery.models.preproc.PreprocessorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.PreprocessorTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.IntegerParserTest
[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 s - in com.bakdata.conquery.models.preproc.parser.specific.IntegerParserTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.DateRangeParserTest
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.parser.specific.DateRangeParserTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.DecimalParserTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.parser.specific.DecimalParserTest
[INFO] Running com.bakdata.conquery.models.preproc.parser.specific.RealParserTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.models.preproc.parser.specific.RealParserTest
[INFO] Running com.bakdata.conquery.models.dictionary.MapDictionaryTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 s - in com.bakdata.conquery.models.dictionary.MapDictionaryTest
[INFO] Running com.bakdata.conquery.api.StoredQueriesProcessorTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.038 s - in com.bakdata.conquery.api.StoredQueriesProcessorTest
[INFO] Running com.bakdata.conquery.api.form.config.FormConfigTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.086 s - in com.bakdata.conquery.api.form.config.FormConfigTest
[INFO] Running com.bakdata.conquery.util.dict.SuccinctTrieTest
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		structure build
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		trie compressed
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		forward lookup done
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		reverse lookup done
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		structure build
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		trie compressed
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		forward lookup done
[INFO] [TEST] [2023-01-17 00:49:56]	c.b.c.u.d.SuccinctTrieTest		reverse lookup done
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.254 s - in com.bakdata.conquery.util.dict.SuccinctTrieTest
[INFO] Running com.bakdata.conquery.util.dict.TernaryTreeBalancerTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.171 s - in com.bakdata.conquery.util.dict.TernaryTreeBalancerTest
[INFO] Running com.bakdata.conquery.util.ConqueryEscapeTest
[INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.bakdata.conquery.util.ConqueryEscapeTest
[INFO] Running com.bakdata.conquery.util.progressreporter.ProgressReporterTest
[INFO] [TEST] [2023-01-17 00:49:59]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 09s 
[INFO] [TEST] [2023-01-17 00:49:59]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 09s 
[INFO] [TEST] [2023-01-17 00:49:59]	c.b.c.u.p.ProgressReporterTest		
[INFO] [TEST] [2023-01-17 00:49:59]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 10s 
[INFO] [TEST] [2023-01-17 00:49:59]	c.b.c.u.p.ProgressReporterTest		waited 00h 00m 00s  -   0% - est. 00h 00m 20s 
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.309 s - in com.bakdata.conquery.util.progressreporter.ProgressReporterTest
[INFO] Running com.bakdata.conquery.util.progressreporter.ProgressReporterUtilTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.util.progressreporter.ProgressReporterUtilTest
[INFO] Running com.bakdata.conquery.util.search.QuickSearchTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.util.search.QuickSearchTest
[INFO] Running com.bakdata.conquery.integration.common.CDateSetTest
[INFO] Tests run: 22, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 s - in com.bakdata.conquery.integration.common.CDateSetTest
[INFO] Running com.bakdata.conquery.integration.ConqueryIntegrationTests
[INFO] [TEST] [2023-01-17 00:49:59]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:49:59]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[WARN] [2023-01-17 00:49:59]	c.b.c.m.c.XodusStoreFactory	ManagerNode	Had to create Storage Dir at `/tmp/conqueryIntegrationTest17490523365462791536/manager`
[INFO] [2023-01-17 00:49:59]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: []
[INFO] [2023-01-17 00:49:59]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 0 entries, 0 B within 6.689 ms
[DEBUG] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 406.9 μs
[DEBUG] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 298.8 μs
[DEBUG] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 0 entries, 0 B within 303.2 μs
[DEBUG] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:00]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 348.0 μs
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@6f23187b
[DEBUG] [2023-01-17 00:50:00]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:00]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:00]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_8
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_9
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_10
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:00]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:00]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:00]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[WARN] [2023-01-17 00:50:00]	c.b.c.m.c.XodusStoreFactory	shard-node0	Had to create Storage Dir at `/tmp/conqueryIntegrationTest17490523365462791536/shard-node0`
[INFO] [2023-01-17 00:50:00]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: []
[WARN] [2023-01-17 00:50:00]	c.b.c.m.c.XodusStoreFactory	shard-node1	Had to create Storage Dir at `/tmp/conqueryIntegrationTest17490523365462791536/shard-node1`
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 0
[INFO] [2023-01-17 00:50:00]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: []
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 0
[DEBUG] [2023-01-17 00:50:00]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:55236 connected, waiting for identity
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ShardNode	/127.0.0.1:55236	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:55238 connected, waiting for identity
[INFO] [2023-01-17 00:50:00]	c.b.c.c.ShardNode	/127.0.0.1:55238	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:00]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:55238` registered.
[INFO] [2023-01-17 00:50:00]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:55236` registered.
[WARN] [2023-01-17 00:50:00]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:00]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:01]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:01]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:01]	c.b.c.i.IntegrationTest$Wrapper	DownloadLinkGeneration	STARTING integration test DownloadLinkGeneration
[INFO] [2023-01-17 00:50:01]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Setting up dataset
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:DATASET}): 0 entries, 0 B within 352.0 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:SECONDARY_IDS}): 0 entries, 0 B within 218.9 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:TABLES}): 0 entries, 0 B within 229.2 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 732.7 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:IMPORTS}): 0 entries, 0 B within 188.2 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:CONCEPTS}): 0 entries, 0 B within 178.5 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.NamespacedStorage	DownloadLinkGeneration	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 199.0 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:STRUCTURE}): 0 entries, 0 B within 137.1 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:WORKER_TO_BUCKETS}): 0 entries, 0 B within 134.7 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	DownloadLinkGeneration	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	DownloadLinkGeneration		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration:PRIMARY_DICTIONARY}): 0 entries, 0 B within 129.5 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:DATASET}): 0 entries, 0 B within 254.1 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:DATASET}): 0 entries, 0 B within 266.2 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:SECONDARY_IDS}): 0 entries, 0 B within 139.5 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:SECONDARY_IDS}): 0 entries, 0 B within 174.0 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:TABLES}): 0 entries, 0 B within 201.0 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:TABLES}): 0 entries, 0 B within 134.8 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 130.3 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 147.5 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:IMPORTS}): 0 entries, 0 B within 130.4 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:IMPORTS}): 0 entries, 0 B within 122.2 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:CONCEPTS}): 0 entries, 0 B within 143.3 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:CONCEPTS}): 0 entries, 0 B within 124.2 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:WORKER}): 0 entries, 0 B within 121.2 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:WORKER}): 0 entries, 0 B within 120.5 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:BUCKETS}): 0 entries, 0 B within 117.1 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:BUCKETS}): 0 entries, 0 B within 130.8 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614:C_BLOCKS}): 0 entries, 0 B within 121.2 μs
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:01]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab:C_BLOCKS}): 0 entries, 0 B within 103.9 μs
[INFO] [2023-01-17 00:50:01]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Imports of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Buckets of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Consistency check was successful
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Imports of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Buckets of worker DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DownloadLinkGeneration]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:01]	c.b.c.m.a.AuthorizationController	DownloadLinkGeneration	Security manager registered
[INFO] [2023-01-17 00:50:01]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:01]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.UpdateTable	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Received update of Table DownloadLinkGeneration.test_table
[INFO] [2023-01-17 00:50:01]	c.b.c.m.m.n.s.UpdateTable	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Received update of Table DownloadLinkGeneration.test_table
[INFO] [2023-01-17 00:50:01]	c.b.c.u.s.TestConquery	DownloadLinkGeneration	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Updating Concept[DownloadLinkGeneration.test_tree]
[DEBUG] [2023-01-17 00:50:01]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Updating Concept[DownloadLinkGeneration.test_tree]
[INFO] [2023-01-17 00:50:01]	c.b.c.c.PreprocessorCommand	DownloadLinkGeneration	Preprocessing from command line config.
[INFO] [2023-01-17 00:50:01]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:01]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)	Required to preprocess 94 B in total
[INFO] [2023-01-17 00:50:01]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000686187s[INFO] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:02]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7f5fe2ad)
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7f5fe2ad) -> IntegerDateStore(store=ShortArrayStore())
[INFO] [2023-01-17 00:50:02]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Chosen encoding is Base16UpperCase
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@34e28d3d(est. 64 B)
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		test_column: StringParser(super=Parser(lines=4, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing header
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing data
[INFO] [2023-01-17 00:50:02]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:02]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 1 Jobs:
[INFO] [2023-01-17 00:50:02]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DownloadLinkGeneration/test_table.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 4 Entities.
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into DownloadLinkGeneration.test_table
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Mapped 4 new ids
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.w.Namespace	Job Manager slow DownloadLinkGeneration	Assigning Bucket[0] to Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab]
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.w.Namespace	Job Manager slow DownloadLinkGeneration	Assigning Bucket[1] to Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614]
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:50:02 +0000] "POST /admin/datasets/DownloadLinkGeneration/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_DownloadLinkGeneration%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 100
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Received new WorkerInformation(size = 1,dataset = DownloadLinkGeneration)
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Received new WorkerInformation(size = 1,dataset = DownloadLinkGeneration)
[INFO] [2023-01-17 00:50:02]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:02]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DownloadLinkGeneration	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Received Dictionary[DownloadLinkGeneration.test_table#DownloadLinkGeneration$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Received Dictionary[DownloadLinkGeneration.test_table#DownloadLinkGeneration$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.ImportJob	Job Manager slow DownloadLinkGeneration	Start sending 2 Buckets
[INFO] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.AddImport	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Received Import[DownloadLinkGeneration.test_table.test_table], containing 4 entries.
[WARN] [2023-01-17 00:50:02]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DownloadLinkGeneration	One or more Children are not done yet
[INFO] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.ImportBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Received DownloadLinkGeneration.test_table.test_table.1
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614, /127.0.0.1:55238]	Adding Bucket[DownloadLinkGeneration.test_table.test_table.1]
[INFO] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.AddImport	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Received Import[DownloadLinkGeneration.test_table.test_table], containing 4 entries.
[INFO] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.ImportBucket	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Received DownloadLinkGeneration.test_table.test_table.0
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.WorkerStorage	Worker[DownloadLinkGeneration.worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab, /127.0.0.1:55236]	Adding Bucket[DownloadLinkGeneration.test_table.test_table.0]
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.WorkerStorage		Adding CBlock[DownloadLinkGeneration.test_table.test_table.1.DownloadLinkGeneration.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.WorkerStorage		Adding CBlock[DownloadLinkGeneration.test_table.test_table.0.DownloadLinkGeneration.test_tree.test_column]
[INFO] [2023-01-17 00:50:02]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:02]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:02 +0000] "GET /api/datasets/DownloadLinkGeneration/queries/DownloadLinkGeneration.52c08334-d8e3-458a-83dc-802af83842cd HTTP/1.1" 200 752 "-" "Conquery (test client)" 42
127.0.0.1 - - [17/Jan/2023:00:50:02 +0000] "GET /api/datasets/DownloadLinkGeneration/queries/DownloadLinkGeneration.52c08334-d8e3-458a-83dc-802af83842cd HTTP/1.1" 200 753 "-" "Conquery (test client)" 7
127.0.0.1 - - [17/Jan/2023:00:50:02 +0000] "GET /api/datasets/DownloadLinkGeneration/queries/DownloadLinkGeneration.52c08334-d8e3-458a-83dc-802af83842cd HTTP/1.1" 200 1017 "-" "Conquery (test client)" 7
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DownloadLinkGeneration
[INFO] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[INFO] [2023-01-17 00:50:02]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=DownloadLinkGeneration, name=DownloadLinkGeneration]
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DownloadLinkGeneration
[INFO] [2023-01-17 00:50:02]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[INFO] [2023-01-17 00:50:02]	c.b.c.m.w.Namespace		Removing namespace storage of DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[INFO] [2023-01-17 00:50:02]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DownloadLinkGeneration
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[DEBUG] [2023-01-17 00:50:02]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[INFO] [2023-01-17 00:50:02]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DownloadLinkGeneration_a0f879cf-eab8-4949-a380-a0e8edd6e614
[INFO] [2023-01-17 00:50:02]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DownloadLinkGeneration_f7b5b6e2-d92a-4ea7-b903-b2b79219f4ab
[INFO] [2023-01-17 00:50:02]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	DownloadLinkGeneration	SUCCESS integration test DownloadLinkGeneration
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	AdminEndpointTest	STARTING integration test AdminEndpointTest
[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	AdminEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	AdminEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:03]	c.b.c.u.s.TestConquery	AdminEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	AdminEndpointTest	SUCCESS integration test AdminEndpointTest
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	AdminUIEndpointTest	STARTING integration test AdminUIEndpointTest
[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	AdminUIEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	AdminUIEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:03]	c.b.c.u.s.TestConquery	AdminUIEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	AdminUIEndpointTest	SUCCESS integration test AdminUIEndpointTest
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	ApiEndpointTest	STARTING integration test ApiEndpointTest
[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:03]	o.g.j.i.Errors	ApiEndpointTest	The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:03]	c.b.c.u.s.TestConquery	ApiEndpointTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:03]	c.b.c.i.IntegrationTest$Wrapper	ApiEndpointTest	SUCCESS integration test ApiEndpointTest
[INFO] [2023-01-17 00:50:03]	c.b.c.u.s.TestConquery	ApiEndpointTest	Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
INFO  [2023-01-17 00:50:03,147] io.dropwizard.server.DefaultServerFactory: Registering jersey handler with root path prefix: /
INFO  [2023-01-17 00:50:03,147] io.dropwizard.server.DefaultServerFactory: Registering admin handler with root path prefix: /
WARN  [2023-01-17 00:50:03,151] com.bakdata.conquery.models.config.XodusStoreFactory: Had to create Storage Dir at `/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager`
INFO  [2023-01-17 00:50:03,151] com.bakdata.conquery.models.config.XodusStoreFactory: All WorkerStores loaded: []
INFO  [2023-01-17 00:50:03,152] com.bakdata.conquery.commands.ManagerNode: Opening MetaStorage
INFO  [2023-01-17 00:50:03,170] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/users
INFO  [2023-01-17 00:50:03,205] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/roles
INFO  [2023-01-17 00:50:03,226] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/groups
INFO  [2023-01-17 00:50:03,248] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/executions
INFO  [2023-01-17 00:50:03,271] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/formConfigs
INFO  [2023-01-17 00:50:03,274] com.bakdata.conquery.commands.ManagerNode: Loading MetaStorage
INFO  [2023-01-17 00:50:03,275] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/users:AUTH_USER}): 0 entries, 0 B within 219.5 μs
INFO  [2023-01-17 00:50:03,275] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 172.5 μs
INFO  [2023-01-17 00:50:03,275] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 138.2 μs
INFO  [2023-01-17 00:50:03,276] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/executions:EXECUTIONS}): 0 entries, 0 B within 132.8 μs
INFO  [2023-01-17 00:50:03,276] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 134.4 μs
INFO  [2023-01-17 00:50:03,276] com.bakdata.conquery.commands.ManagerNode: MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@213e6c37
WARN  [2023-01-17 00:50:03,280] com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm: 
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
INFO  [2023-01-17 00:50:03,283] com.bakdata.conquery.models.auth.AuthorizationController: Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_11
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_12
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_13
	com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm_14
INFO  [2023-01-17 00:50:03,283] com.bakdata.conquery.commands.ManagerNode: Registering ResourcesProvider
INFO  [2023-01-17 00:50:03,288] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
INFO  [2023-01-17 00:50:03,288] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
WARN  [2023-01-17 00:50:03,334] com.bakdata.conquery.models.config.XodusStoreFactory: Had to create Storage Dir at `/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0`
INFO  [2023-01-17 00:50:03,334] com.bakdata.conquery.models.config.XodusStoreFactory: All WorkerStores loaded: []
INFO  [2023-01-17 00:50:03,334] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
WARN  [2023-01-17 00:50:03,336] com.bakdata.conquery.models.config.XodusStoreFactory: Had to create Storage Dir at `/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1`
INFO  [2023-01-17 00:50:03,336] com.bakdata.conquery.models.config.XodusStoreFactory: All WorkerStores loaded: []
INFO  [2023-01-17 00:50:03,336] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-17 00:50:03,337] io.dropwizard.server.ServerFactory: Starting Conquery


 ▄████████  ▄██████▄  ███▄▄▄▄   ████████▄   ███    █▄     ▄████████    ▄████████ ▄██   ▄   
███    ███ ███    ███ ███▀▀▀██▄ ███    ███  ███    ███   ███    ███   ███    ███ ███   ██▄ 
███    █▀  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▀    ███    ███ ███▄▄▄███ 
███        ███    ███ ███   ███ ███    ███  ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀ ▀▀▀▀▀▀███ 
███        ███    ███ ███   ███ ███    ███  ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀   ▄██   ███ 
███    █▄  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▄  ▀███████████ ███   ███ 
███    ███ ███    ███ ███   ███ ███  ▀ ███  ███    ███   ███    ███   ███    ███ ███   ███ 
████████▀   ▀██████▀   ▀█   █▀   ▀██████▀▄█ ████████▀    ██████████   ███    ███  ▀█████▀  
                                                                      ███    ███           
Version: 0.0.0-SNAPSHOT(${git.commit.id.describe})

           
INFO  [2023-01-17 00:50:03,341] org.eclipse.jetty.setuid.SetUIDListener: Opened application@710ee5b1{HTTP/1.1, (http/1.1)}{0.0.0.0:42273}
INFO  [2023-01-17 00:50:03,342] org.eclipse.jetty.setuid.SetUIDListener: Opened admin@23968461{HTTP/1.1, (http/1.1)}{0.0.0.0:36369}
INFO  [2023-01-17 00:50:03,342] org.eclipse.jetty.server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.17+8-post-Ubuntu-1ubuntu218.04
INFO  [2023-01-17 00:50:03,346] com.bakdata.conquery.commands.ManagerNode: Started ManagerNode @ /0:0:0:0:0:0:0:0:44697
INFO  [2023-01-17 00:50:03,366] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/api-token
INFO  [2023-01-17 00:50:03,384] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:44697
INFO  [2023-01-17 00:50:03,385] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:47748 connected, waiting for identity
INFO  [2023-01-17 00:50:03,385] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:44697
INFO  [2023-01-17 00:50:03,386] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:44697
INFO  [2023-01-17 00:50:03,387] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:47750 connected, waiting for identity
INFO  [2023-01-17 00:50:03,388] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:44697
INFO  [2023-01-17 00:50:03,397] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:47750` registered.
INFO  [2023-01-17 00:50:03,398] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:47748` registered.
INFO  [2023-01-17 00:50:03,479] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /api/config/frontend (com.bakdata.conquery.resources.api.ConfigResource)
    GET     /api/datasets (com.bakdata.conquery.resources.api.DatasetsResource)
    GET     /api/datasets/{dataset}/concepts (com.bakdata.conquery.resources.api.DatasetResource)
    GET     /api/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/resolve (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/autocomplete (com.bakdata.conquery.resources.api.FilterResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/resolve (com.bakdata.conquery.resources.api.FilterResource)
    GET     /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    POST    /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    DELETE  /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    PATCH   /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-queries (com.bakdata.conquery.resources.api.FormResource)
    GET     /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/upload (com.bakdata.conquery.resources.api.QueryResource)
    DELETE  /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    PATCH   /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/cancel (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/reexecute (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/result/{query}.arrf (com.bakdata.conquery.resources.api.ResultArrowFileResource)
    GET     /api/datasets/{dataset}/result/{query}.arrs (com.bakdata.conquery.resources.api.ResultArrowStreamResource)
    GET     /api/datasets/{dataset}/result/{query}.csv (com.bakdata.conquery.resources.api.ResultCsvResource)
    GET     /api/datasets/{dataset}/result/{query}.xlsx (com.bakdata.conquery.resources.api.ResultExcelResource)
    GET     /api/me (com.bakdata.conquery.resources.api.MeResource)
    GET     /api/token (com.bakdata.conquery.resources.api.ApiTokenResource)
    POST    /api/token (com.bakdata.conquery.resources.api.ApiTokenResource)
    DELETE  /api/token/{token} (com.bakdata.conquery.resources.api.ApiTokenResource)

WARN  [2023-01-17 00:50:03,479] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:03,511] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-17 00:50:03,511] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@60786bcf{/,null,AVAILABLE}
INFO  [2023-01-17 00:50:03,511] io.dropwizard.setup.AdminEnvironment: tasks = 

    POST    /tasks/permission-cleanup (com.bakdata.conquery.tasks.PermissionCleanupTask)
    POST    /tasks/report-consistency (com.bakdata.conquery.tasks.ReportConsistencyTask)
    POST    /tasks/log-level (io.dropwizard.servlets.tasks.LogConfigurationTask)
    POST    /tasks/form-scanner (com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner)
    POST    /tasks/clear-filter-source-search (com.bakdata.conquery.tasks.ClearFilterSourceSearch)
    POST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)
    POST    /tasks/query-cleanup (com.bakdata.conquery.tasks.QueryCleanupTask)
    POST    /tasks/shutdown (com.bakdata.conquery.resources.admin.ShutdownTask)

WARN  [2023-01-17 00:50:03,511] io.dropwizard.setup.AdminEnvironment: 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
INFO  [2023-01-17 00:50:03,580] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin/auth-overview/csv (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/auth-overview/csv/group/{groupId} (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    POST    /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    DELETE  /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    GET     /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    POST    /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/label (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/secondaryId (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/secondaryId/{secondaryId} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/structure (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    DELETE  /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    POST    /admin/datasets/{dataset}/update-matching-stats (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/weight (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/jobs (com.bakdata.conquery.resources.admin.rest.AdminResource)
    POST    /admin/jobs/{jobId}/cancel (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/logout (com.bakdata.conquery.resources.admin.rest.AdminResource)
    DELETE  /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    POST    /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    GET     /admin/queries (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    DELETE  /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    GET     /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/script (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/upload (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    GET     /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)

WARN  [2023-01-17 00:50:03,581] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:03,617] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin-ui/ (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/auth-overview (com.bakdata.conquery.resources.admin.ui.AuthOverviewUIResource)
    GET     /admin-ui/datasets (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset} (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.ui.ConceptsUIResource)
    GET     /admin-ui/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table}/import/{importId} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/groups (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/groups/{groupId} (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/jobs (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/queries (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/roles (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/roles/{roleId} (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/script (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/users (com.bakdata.conquery.resources.admin.ui.UserUIResource)
    GET     /admin-ui/users/{userId} (com.bakdata.conquery.resources.admin.ui.UserUIResource)

WARN  [2023-01-17 00:50:03,618] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:03,649] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-17 00:50:03,649] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@4d8f7bfe{/,null,AVAILABLE}
INFO  [2023-01-17 00:50:03,666] org.eclipse.jetty.server.AbstractConnector: Started application@710ee5b1{HTTP/1.1, (http/1.1)}{0.0.0.0:42273}
INFO  [2023-01-17 00:50:03,669] org.eclipse.jetty.server.AbstractConnector: Started admin@23968461{HTTP/1.1, (http/1.1)}{0.0.0.0:36369}
INFO  [2023-01-17 00:50:03,669] org.eclipse.jetty.server.Server: Started @21029ms
INFO  [2023-01-17 00:50:03,676] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ApiTokenRealmTest
INFO  [2023-01-17 00:50:03,678] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:03,701] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest
INFO  [2023-01-17 00:50:03,736] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:DATASET}): 0 entries, 0 B within 161.4 μs
INFO  [2023-01-17 00:50:03,736] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:SECONDARY_IDS}): 0 entries, 0 B within 114.7 μs
INFO  [2023-01-17 00:50:03,736] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:TABLES}): 0 entries, 0 B within 117.9 μs
INFO  [2023-01-17 00:50:03,737] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 93.06 μs
INFO  [2023-01-17 00:50:03,737] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:IMPORTS}): 0 entries, 0 B within 132.5 μs
INFO  [2023-01-17 00:50:03,737] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:CONCEPTS}): 0 entries, 0 B within 167.7 μs
INFO  [2023-01-17 00:50:03,737] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:03,737] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 96.56 μs
INFO  [2023-01-17 00:50:03,737] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:STRUCTURE}): 0 entries, 0 B within 93.96 μs
INFO  [2023-01-17 00:50:03,738] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 94.22 μs
INFO  [2023-01-17 00:50:03,738] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 113.8 μs
INFO  [2023-01-17 00:50:03,754] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-17 00:50:03,754] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-17 00:50:03,782] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664
INFO  [2023-01-17 00:50:03,787] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0
INFO  [2023-01-17 00:50:03,809] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:DATASET}): 0 entries, 0 B within 174.5 μs
INFO  [2023-01-17 00:50:03,809] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:SECONDARY_IDS}): 0 entries, 0 B within 88.22 μs
INFO  [2023-01-17 00:50:03,809] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:TABLES}): 0 entries, 0 B within 75.74 μs
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 74.56 μs
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:IMPORTS}): 0 entries, 0 B within 70.28 μs
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:CONCEPTS}): 0 entries, 0 B within 120.5 μs
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:WORKER}): 0 entries, 0 B within 70.72 μs
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:BUCKETS}): 0 entries, 0 B within 64.41 μs
INFO  [2023-01-17 00:50:03,810] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664:C_BLOCKS}): 0 entries, 0 B within 64.27 μs
INFO  [2023-01-17 00:50:03,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:DATASET}): 0 entries, 0 B within 135.3 μs
INFO  [2023-01-17 00:50:03,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:SECONDARY_IDS}): 0 entries, 0 B within 93.89 μs
INFO  [2023-01-17 00:50:03,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:TABLES}): 0 entries, 0 B within 77.18 μs
INFO  [2023-01-17 00:50:03,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 101.1 μs
INFO  [2023-01-17 00:50:03,819] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:IMPORTS}): 0 entries, 0 B within 75.44 μs
INFO  [2023-01-17 00:50:03,820] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:CONCEPTS}): 0 entries, 0 B within 80.51 μs
INFO  [2023-01-17 00:50:03,820] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:03,820] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:WORKER}): 0 entries, 0 B within 83.90 μs
INFO  [2023-01-17 00:50:03,821] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:BUCKETS}): 0 entries, 0 B within 90.29 μs
INFO  [2023-01-17 00:50:03,821] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0:C_BLOCKS}): 0 entries, 0 B within 76.24 μs
INFO  [2023-01-17 00:50:03,833] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:03,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:03,839] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:03,840] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:03,840] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:03,840] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ApiTokenRealmTest.worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:03,840] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:04,017] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Creating new api token data for user user.SUPERUSER@SUPERUSER with id: 00cbcea3-e450-44d2-96ae-07f7a6297436
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "POST /api/token HTTP/1.1" 200 96 "-" "Conquery (test client)" 64
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /api/token HTTP/1.1" 200 178 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:04,138] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Creating new api token data for user user.SUPERUSER@SUPERUSER with id: dba247ba-bca9-44f0-bb6d-d5bbb9d44e5d
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "POST /api/token HTTP/1.1" 200 96 "-" "Conquery (test client)" 54
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /api/token HTTP/1.1" 200 353 "-" "Conquery (test client)" 5
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /api/datasets HTTP/1.1" 200 56 "-" "Conquery (test client)" 46
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /api/datasets HTTP/1.1" 200 2 "-" "Conquery (test client)" 42
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /admin/datasets HTTP/1.1" 200 21 "-" "Conquery (test client)" 63
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "DELETE /api/token/dba247ba-bca9-44f0-bb6d-d5bbb9d44e5d HTTP/1.1" 403 63 "-" "Conquery (test client)" 42
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "DELETE /api/token/dba247ba-bca9-44f0-bb6d-d5bbb9d44e5d HTTP/1.1" 200 0 "-" "Conquery (test client)" 5
WARN  [2023-01-17 00:50:04,578] com.bakdata.conquery.models.auth.web.DefaultAuthFilter: Non of the configured realms was able to successfully authenticate the extracted token(s).
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /admin/datasets HTTP/1.1" 401 46 "-" "Conquery (test client)" 48
WARN  [2023-01-17 00:50:04,607] com.bakdata.conquery.models.auth.web.AuthorizationExceptionMapper: Shiro failed to authorize the request. Reason: Subject does not have permission [admin]
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "DELETE /api/token/00cbcea3-e450-44d2-96ae-07f7a6297436 HTTP/1.1" 403 86 "-" "Conquery (test client)" 5
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "POST /api/token HTTP/1.1" 422 27 "-" "Conquery (test client)" 34
INFO  [2023-01-17 00:50:04,749] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Creating new api token data for user user.TestUser2 with id: 4f8aef4c-9cbb-4fb4-b87a-77f148ebb3bc
INFO  [2023-01-17 00:50:04,813] com.bakdata.conquery.models.auth.apitoken.ApiTokenRealm: Supplied token expired on: 2023-01-16
WARN  [2023-01-17 00:50:04,813] com.bakdata.conquery.models.auth.web.DefaultAuthFilter: Non of the configured realms was able to successfully authenticate the extracted token(s).
127.0.0.1 - - [17/Jan/2023:00:50:04 +0000] "GET /api/datasets HTTP/1.1" 401 46 "-" "Conquery (test client)" 37
INFO  [2023-01-17 00:50:04,818] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ApiTokenRealmTest
INFO  [2023-01-17 00:50:04,820] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-17 00:50:04,820] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664
INFO  [2023-01-17 00:50:04,820] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ApiTokenRealmTest, name=ApiTokenRealmTest]
INFO  [2023-01-17 00:50:04,820] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0
INFO  [2023-01-17 00:50:04,822] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664
INFO  [2023-01-17 00:50:04,826] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0
INFO  [2023-01-17 00:50:04,847] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ApiTokenRealmTest
INFO  [2023-01-17 00:50:04,925] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node1/worker_worker_ApiTokenRealmTest_077c4f68-6dae-4f7b-bdfe-a432fb9d8664
INFO  [2023-01-17 00:50:04,927] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/shard-node0/worker_worker_ApiTokenRealmTest_54bf17c2-4da5-42a7-8956-aed2173fdef0
INFO  [2023-01-17 00:50:04,947] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ApiTokenRealmTest
INFO  [2023-01-17 00:50:04,950] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/dataset_ApiTokenRealmTest
INFO  [2023-01-17 00:50:04,963] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:05,042] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ApiTokenRealmTest
INFO  [2023-01-17 00:50:05,049] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ConceptPermissionTest
INFO  [2023-01-17 00:50:05,051] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:05,072] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest
INFO  [2023-01-17 00:50:05,095] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:DATASET}): 0 entries, 0 B within 237.1 μs
INFO  [2023-01-17 00:50:05,096] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:SECONDARY_IDS}): 0 entries, 0 B within 168.7 μs
INFO  [2023-01-17 00:50:05,096] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:TABLES}): 0 entries, 0 B within 203.2 μs
INFO  [2023-01-17 00:50:05,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 141.6 μs
INFO  [2023-01-17 00:50:05,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:IMPORTS}): 0 entries, 0 B within 138.7 μs
INFO  [2023-01-17 00:50:05,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:CONCEPTS}): 0 entries, 0 B within 122.5 μs
INFO  [2023-01-17 00:50:05,097] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:05,097] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 144.2 μs
INFO  [2023-01-17 00:50:05,098] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:STRUCTURE}): 0 entries, 0 B within 91.58 μs
INFO  [2023-01-17 00:50:05,098] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 82.33 μs
INFO  [2023-01-17 00:50:05,098] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 80.79 μs
INFO  [2023-01-17 00:50:05,106] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-17 00:50:05,106] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-17 00:50:05,135] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312
INFO  [2023-01-17 00:50:05,139] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d
INFO  [2023-01-17 00:50:05,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:DATASET}): 0 entries, 0 B within 150.3 μs
INFO  [2023-01-17 00:50:05,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:SECONDARY_IDS}): 0 entries, 0 B within 91.13 μs
INFO  [2023-01-17 00:50:05,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:TABLES}): 0 entries, 0 B within 72.52 μs
INFO  [2023-01-17 00:50:05,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 73.72 μs
INFO  [2023-01-17 00:50:05,168] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:IMPORTS}): 0 entries, 0 B within 78.32 μs
INFO  [2023-01-17 00:50:05,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:CONCEPTS}): 0 entries, 0 B within 68.16 μs
INFO  [2023-01-17 00:50:05,169] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:05,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:WORKER}): 0 entries, 0 B within 111.7 μs
INFO  [2023-01-17 00:50:05,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:BUCKETS}): 0 entries, 0 B within 98.61 μs
INFO  [2023-01-17 00:50:05,169] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312:C_BLOCKS}): 0 entries, 0 B within 78.61 μs
INFO  [2023-01-17 00:50:05,173] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:DATASET}): 0 entries, 0 B within 181.7 μs
INFO  [2023-01-17 00:50:05,174] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptPermissionTest.worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:05,174] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptPermissionTest.worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:05,174] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:05,174] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:SECONDARY_IDS}): 0 entries, 0 B within 1.408 ms
INFO  [2023-01-17 00:50:05,175] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:TABLES}): 0 entries, 0 B within 181.3 μs
INFO  [2023-01-17 00:50:05,175] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 181.9 μs
INFO  [2023-01-17 00:50:05,175] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:IMPORTS}): 0 entries, 0 B within 152.2 μs
INFO  [2023-01-17 00:50:05,176] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:CONCEPTS}): 0 entries, 0 B within 153.1 μs
INFO  [2023-01-17 00:50:05,176] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:05,176] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:WORKER}): 0 entries, 0 B within 196.6 μs
INFO  [2023-01-17 00:50:05,176] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:BUCKETS}): 0 entries, 0 B within 135.3 μs
INFO  [2023-01-17 00:50:05,177] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d:C_BLOCKS}): 0 entries, 0 B within 139.4 μs
INFO  [2023-01-17 00:50:05,179] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:05,181] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptPermissionTest.worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:05,181] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptPermissionTest.worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:05,181] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:05,285] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:05,295] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:05,296] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptPermissionTest.test_table
INFO  [2023-01-17 00:50:05,296] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptPermissionTest.test_table
INFO  [2023-01-17 00:50:05,425] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:05,541] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:05,542] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:05,542] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-17 00:50:05,542] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptPermissionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000353092sINFO  [2023-01-17 00:50:05,578] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:05,578] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:05,578] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@33ca9dc4)
INFO  [2023-01-17 00:50:05,583] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptPermissionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:05,583] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:05,583] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptPermissionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:05,607] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into ConceptPermissionTest.test_table
127.0.0.1 - - [17/Jan/2023:00:50:05 +0000] "POST /admin/datasets/ConceptPermissionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ConceptPermissionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:50:05,609] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:05,613] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:05,624] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:05,624] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:05,696] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptPermissionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:05,696] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptPermissionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:05,698] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:05,701] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptPermissionTest.test_table.test_table.0
WARN  [2023-01-17 00:50:05,701] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:05,703] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptPermissionTest.test_table.test_table.1
INFO  [2023-01-17 00:50:05,861] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ConceptPermissionTest] by User[{user.testUser].
WARN  [2023-01-17 00:50:05,867] com.bakdata.conquery.models.auth.web.AuthorizationExceptionMapper: Shiro failed to authorize the request. Reason: Subject does not have permission [concepts:read:conceptpermissiontest.test_tree]
INFO  [2023-01-17 00:50:05,868] com.bakdata.conquery.integration.tests.ConceptPermissionTest: Adding the Permission[concepts:read:conceptpermissiontest.test_tree] to User[User[user.testUser]]
127.0.0.1 - - [17/Jan/2023:00:50:05 +0000] "POST /api/datasets/ConceptPermissionTest/queries HTTP/1.1" 403 126 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:50:05,885] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ConceptPermissionTest] by User[{user.testUser].
INFO  [2023-01-17 00:50:05,889] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b9b46506-7e7d-41b8-b5f2-67414eb86bf1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptPermissionTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:05 +0000] "POST /api/datasets/ConceptPermissionTest/queries HTTP/1.1" 201 1118 "-" "Conquery (test client)" 16
INFO  [2023-01-17 00:50:05,907] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1
INFO  [2023-01-17 00:50:05,907] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1
INFO  [2023-01-17 00:50:05,939] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1] with 0 results within PT0.031294S
INFO  [2023-01-17 00:50:05,940] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1] with 2 results within PT0.032714S
INFO  [2023-01-17 00:50:05,953] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1, workerId=ConceptPermissionTest.worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d, startTime=2023-01-17T00:50:05.907915, finishTime=2023-01-17T00:50:05.939209) of size 0
INFO  [2023-01-17 00:50:05,954] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1, workerId=ConceptPermissionTest.worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312, startTime=2023-01-17T00:50:05.907920, finishTime=2023-01-17T00:50:05.940634) of size 2
INFO  [2023-01-17 00:50:05,956] com.bakdata.conquery.models.execution.ManagedExecution: DONE b9b46506-7e7d-41b8-b5f2-67414eb86bf1 ManagedQuery within PT0.065456S
127.0.0.1 - - [17/Jan/2023:00:50:05 +0000] "GET /api/datasets/ConceptPermissionTest/queries/ConceptPermissionTest.b9b46506-7e7d-41b8-b5f2-67414eb86bf1 HTTP/1.1" 200 1133 "-" "Conquery (test client)" 40
INFO  [2023-01-17 00:50:05,961] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,061] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.testUser
INFO  [2023-01-17 00:50:06,062] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ConceptPermissionTest
INFO  [2023-01-17 00:50:06,063] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-17 00:50:06,063] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptPermissionTest, name=ConceptPermissionTest]
INFO  [2023-01-17 00:50:06,063] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312
INFO  [2023-01-17 00:50:06,063] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d
INFO  [2023-01-17 00:50:06,089] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312
INFO  [2023-01-17 00:50:06,089] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d
INFO  [2023-01-17 00:50:06,106] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ConceptPermissionTest
INFO  [2023-01-17 00:50:06,109] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptPermissionTest_cc99b06d-3146-4faf-ae1d-afb98b905312
INFO  [2023-01-17 00:50:06,114] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptPermissionTest_446515a4-e611-40f7-b32d-7e7a2d5c043d
INFO  [2023-01-17 00:50:06,202] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ConceptPermissionTest
INFO  [2023-01-17 00:50:06,205] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptPermissionTest
INFO  [2023-01-17 00:50:06,215] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,244] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ConceptPermissionTest
INFO  [2023-01-17 00:50:06,252] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ConceptResolutionTest
INFO  [2023-01-17 00:50:06,253] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:06,273] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest
INFO  [2023-01-17 00:50:06,295] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:DATASET}): 0 entries, 0 B within 201.2 μs
INFO  [2023-01-17 00:50:06,295] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:SECONDARY_IDS}): 0 entries, 0 B within 160.5 μs
INFO  [2023-01-17 00:50:06,296] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:TABLES}): 0 entries, 0 B within 111.6 μs
INFO  [2023-01-17 00:50:06,296] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 120.1 μs
INFO  [2023-01-17 00:50:06,296] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:IMPORTS}): 0 entries, 0 B within 101.5 μs
INFO  [2023-01-17 00:50:06,296] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:CONCEPTS}): 0 entries, 0 B within 119.7 μs
INFO  [2023-01-17 00:50:06,296] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:06,296] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 106.7 μs
INFO  [2023-01-17 00:50:06,297] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:STRUCTURE}): 0 entries, 0 B within 112.6 μs
INFO  [2023-01-17 00:50:06,297] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 98.04 μs
INFO  [2023-01-17 00:50:06,297] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 103.5 μs
INFO  [2023-01-17 00:50:06,305] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-17 00:50:06,305] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-17 00:50:06,324] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d
INFO  [2023-01-17 00:50:06,325] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae
INFO  [2023-01-17 00:50:06,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:DATASET}): 0 entries, 0 B within 120.9 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:SECONDARY_IDS}): 0 entries, 0 B within 80.18 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:DATASET}): 0 entries, 0 B within 129.2 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:TABLES}): 0 entries, 0 B within 56.54 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:SECONDARY_IDS}): 0 entries, 0 B within 64.49 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 63.35 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:TABLES}): 0 entries, 0 B within 75.07 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:IMPORTS}): 0 entries, 0 B within 58.06 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 69.98 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:CONCEPTS}): 0 entries, 0 B within 57.77 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:IMPORTS}): 0 entries, 0 B within 57.76 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:WORKER}): 0 entries, 0 B within 54.80 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:CONCEPTS}): 0 entries, 0 B within 55.64 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:BUCKETS}): 0 entries, 0 B within 53.86 μs
INFO  [2023-01-17 00:50:06,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:WORKER}): 0 entries, 0 B within 154.2 μs
INFO  [2023-01-17 00:50:06,358] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d:C_BLOCKS}): 0 entries, 0 B within 134.4 μs
INFO  [2023-01-17 00:50:06,358] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:BUCKETS}): 0 entries, 0 B within 94.16 μs
INFO  [2023-01-17 00:50:06,358] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae:C_BLOCKS}): 0 entries, 0 B within 89.09 μs
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptResolutionTest.worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptResolutionTest.worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ConceptResolutionTest.worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ConceptResolutionTest.worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:06,363] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:06,465] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,475] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,476] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptResolutionTest.test_table
INFO  [2023-01-17 00:50:06,476] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ConceptResolutionTest.test_table
INFO  [2023-01-17 00:50:06,603] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,722] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:06,723] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:06,723] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-17 00:50:06,723] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptResolutionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000352038sINFO  [2023-01-17 00:50:06,759] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:06,759] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:06,759] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7e9b53aa)
INFO  [2023-01-17 00:50:06,763] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptResolutionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:06,763] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:06,763] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptResolutionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:06,795] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into ConceptResolutionTest.test_table
INFO  [2023-01-17 00:50:06,797] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:06 +0000] "POST /admin/datasets/ConceptResolutionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ConceptResolutionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:50:06,800] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:06,809] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:06,809] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:06,817] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:06,817] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptResolutionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:06,817] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ConceptResolutionTest.test_table.test_table], containing 4 entries.
WARN  [2023-01-17 00:50:06,819] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:06,819] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptResolutionTest.test_table.test_table.0
INFO  [2023-01-17 00:50:06,820] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ConceptResolutionTest.test_table.test_table.1
INFO  [2023-01-17 00:50:06,927] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,932] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,952] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:06,959] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ConceptResolutionTest
INFO  [2023-01-17 00:50:06,960] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-17 00:50:06,960] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae
INFO  [2023-01-17 00:50:06,960] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ConceptResolutionTest, name=ConceptResolutionTest]
INFO  [2023-01-17 00:50:06,960] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d
INFO  [2023-01-17 00:50:07,007] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ConceptResolutionTest
INFO  [2023-01-17 00:50:07,019] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ConceptResolutionTest
INFO  [2023-01-17 00:50:07,022] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptResolutionTest
INFO  [2023-01-17 00:50:07,033] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:07,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d
INFO  [2023-01-17 00:50:07,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae
INFO  [2023-01-17 00:50:07,124] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptResolutionTest_ee9cdf94-e123-490e-8d92-a8d7b85036ae
INFO  [2023-01-17 00:50:07,125] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptResolutionTest_db3cc57c-72df-4a43-a9ef-05d324a8da3d
INFO  [2023-01-17 00:50:07,241] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ConceptResolutionTest
INFO  [2023-01-17 00:50:07,247] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FilterResolutionContainsTest
INFO  [2023-01-17 00:50:07,248] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:07,269] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest
INFO  [2023-01-17 00:50:07,289] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:DATASET}): 0 entries, 0 B within 208.4 μs
INFO  [2023-01-17 00:50:07,289] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:SECONDARY_IDS}): 0 entries, 0 B within 109.0 μs
INFO  [2023-01-17 00:50:07,289] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:TABLES}): 0 entries, 0 B within 89.16 μs
INFO  [2023-01-17 00:50:07,290] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 88.09 μs
INFO  [2023-01-17 00:50:07,290] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:IMPORTS}): 0 entries, 0 B within 84.38 μs
INFO  [2023-01-17 00:50:07,290] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:CONCEPTS}): 0 entries, 0 B within 108.3 μs
INFO  [2023-01-17 00:50:07,290] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:07,290] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 92.08 μs
INFO  [2023-01-17 00:50:07,290] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:STRUCTURE}): 0 entries, 0 B within 81.42 μs
INFO  [2023-01-17 00:50:07,291] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 82.43 μs
INFO  [2023-01-17 00:50:07,291] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 94.64 μs
INFO  [2023-01-17 00:50:07,299] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-17 00:50:07,299] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-17 00:50:07,328] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60
INFO  [2023-01-17 00:50:07,333] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:DATASET}): 0 entries, 0 B within 150.9 μs
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:SECONDARY_IDS}): 0 entries, 0 B within 62.16 μs
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:TABLES}): 0 entries, 0 B within 47.59 μs
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 49.39 μs
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:IMPORTS}): 0 entries, 0 B within 45.22 μs
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:CONCEPTS}): 0 entries, 0 B within 43.58 μs
INFO  [2023-01-17 00:50:07,356] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:07,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:WORKER}): 0 entries, 0 B within 45.87 μs
INFO  [2023-01-17 00:50:07,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:BUCKETS}): 0 entries, 0 B within 44.65 μs
INFO  [2023-01-17 00:50:07,357] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60:C_BLOCKS}): 0 entries, 0 B within 49.57 μs
INFO  [2023-01-17 00:50:07,362] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:07,362] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:07,362] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:DATASET}): 0 entries, 0 B within 96.43 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:SECONDARY_IDS}): 0 entries, 0 B within 66.80 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:TABLES}): 0 entries, 0 B within 47.42 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 48.31 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:IMPORTS}): 0 entries, 0 B within 58.72 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:CONCEPTS}): 0 entries, 0 B within 44.84 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:WORKER}): 0 entries, 0 B within 43.52 μs
INFO  [2023-01-17 00:50:07,364] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:BUCKETS}): 0 entries, 0 B within 51.25 μs
INFO  [2023-01-17 00:50:07,365] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8:C_BLOCKS}): 0 entries, 0 B within 44.25 μs
INFO  [2023-01-17 00:50:07,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:07,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionContainsTest.worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:07,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:07,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:07,472] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:07,484] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:07,485] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionContainsTest.table1
INFO  [2023-01-17 00:50:07,485] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionContainsTest.table1
INFO  [2023-01-17 00:50:07,638] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:07,769] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:07,769] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:07,769] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:50:07,770] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionContainsTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000410609sINFO  [2023-01-17 00:50:07,812] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:07,812] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:07,812] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@285ab4d5)
INFO  [2023-01-17 00:50:07,816] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionContainsTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:07,816] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:07,816] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionContainsTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:07,866] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into FilterResolutionContainsTest.table1
127.0.0.1 - - [17/Jan/2023:00:50:07 +0000] "POST /admin/datasets/FilterResolutionContainsTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_FilterResolutionContainsTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:50:07,868] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:07,871] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:07,887] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:07,887] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:07,892] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
WARN  [2023-01-17 00:50:07,895] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:07,898] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionContainsTest.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:50:07,902] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionContainsTest.table1.table1.1
INFO  [2023-01-17 00:50:07,933] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionContainsTest.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:50:07,935] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionContainsTest.table1.table1.0
INFO  [2023-01-17 00:50:07,936] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionContainsTest.table1.table1.2
INFO  [2023-01-17 00:50:08,042] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,047] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,075] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,084] com.bakdata.conquery.apiv1.FilterSearch: Processing reference list '/tmp/conquery_search2397408326902087670.csv' ...
INFO  [2023-01-17 00:50:08,110] com.bakdata.conquery.apiv1.FilterSearch: Processed reference list '/tmp/conquery_search2397408326902087670.csv' in 25 ms (5 Items in 6 Lines)
INFO  [2023-01-17 00:50:08,114] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FilterResolutionContainsTest
INFO  [2023-01-17 00:50:08,115] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-17 00:50:08,115] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionContainsTest, name=FilterResolutionContainsTest]
INFO  [2023-01-17 00:50:08,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60
INFO  [2023-01-17 00:50:08,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8
INFO  [2023-01-17 00:50:08,159] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60
INFO  [2023-01-17 00:50:08,166] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8
INFO  [2023-01-17 00:50:08,199] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FilterResolutionContainsTest
INFO  [2023-01-17 00:50:08,233] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionContainsTest_b21252b1-5edb-4cdb-b612-e20d8cae59d8
INFO  [2023-01-17 00:50:08,241] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionContainsTest_a42207fb-7567-48bf-a8de-40ab1e32fc60
INFO  [2023-01-17 00:50:08,295] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FilterResolutionContainsTest
INFO  [2023-01-17 00:50:08,297] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionContainsTest
INFO  [2023-01-17 00:50:08,306] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,353] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FilterResolutionContainsTest
INFO  [2023-01-17 00:50:08,358] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FilterResolutionExactTest
INFO  [2023-01-17 00:50:08,359] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:08,379] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:DATASET}): 0 entries, 0 B within 117.7 μs
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:SECONDARY_IDS}): 0 entries, 0 B within 62.66 μs
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:TABLES}): 0 entries, 0 B within 50.76 μs
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 59.62 μs
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:IMPORTS}): 0 entries, 0 B within 48.50 μs
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:CONCEPTS}): 0 entries, 0 B within 48.18 μs
INFO  [2023-01-17 00:50:08,396] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:08,397] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 62.94 μs
INFO  [2023-01-17 00:50:08,397] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:STRUCTURE}): 0 entries, 0 B within 47.33 μs
INFO  [2023-01-17 00:50:08,397] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 61.45 μs
INFO  [2023-01-17 00:50:08,397] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 61.86 μs
INFO  [2023-01-17 00:50:08,404] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-17 00:50:08,404] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-17 00:50:08,423] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8
INFO  [2023-01-17 00:50:08,428] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e
INFO  [2023-01-17 00:50:08,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:DATASET}): 0 entries, 0 B within 148.9 μs
INFO  [2023-01-17 00:50:08,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:SECONDARY_IDS}): 0 entries, 0 B within 66.84 μs
INFO  [2023-01-17 00:50:08,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:TABLES}): 0 entries, 0 B within 50.64 μs
INFO  [2023-01-17 00:50:08,451] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 53.80 μs
INFO  [2023-01-17 00:50:08,452] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:IMPORTS}): 0 entries, 0 B within 48.58 μs
INFO  [2023-01-17 00:50:08,452] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:CONCEPTS}): 0 entries, 0 B within 47.57 μs
INFO  [2023-01-17 00:50:08,452] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:08,452] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:WORKER}): 0 entries, 0 B within 48.67 μs
INFO  [2023-01-17 00:50:08,452] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:BUCKETS}): 0 entries, 0 B within 47.78 μs
INFO  [2023-01-17 00:50:08,452] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8:C_BLOCKS}): 0 entries, 0 B within 52.24 μs
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:DATASET}): 0 entries, 0 B within 592.2 μs
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:SECONDARY_IDS}): 0 entries, 0 B within 63.59 μs
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:TABLES}): 0 entries, 0 B within 49.72 μs
INFO  [2023-01-17 00:50:08,457] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.93 μs
INFO  [2023-01-17 00:50:08,458] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:IMPORTS}): 0 entries, 0 B within 51.59 μs
INFO  [2023-01-17 00:50:08,458] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:CONCEPTS}): 0 entries, 0 B within 42.03 μs
INFO  [2023-01-17 00:50:08,458] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:08,458] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:WORKER}): 0 entries, 0 B within 48.58 μs
INFO  [2023-01-17 00:50:08,458] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:BUCKETS}): 0 entries, 0 B within 46.85 μs
INFO  [2023-01-17 00:50:08,458] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e:C_BLOCKS}): 0 entries, 0 B within 40.05 μs
INFO  [2023-01-17 00:50:08,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:08,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionExactTest.worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:08,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:08,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,565] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,573] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,574] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionExactTest.table1
INFO  [2023-01-17 00:50:08,575] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionExactTest.table1
INFO  [2023-01-17 00:50:08,701] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:08,818] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:08,819] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:08,819] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:50:08,819] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionExactTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000409049sINFO  [2023-01-17 00:50:08,861] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:08,861] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:08,861] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@281071f9)
INFO  [2023-01-17 00:50:08,865] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionExactTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:08,866] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:08,866] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionExactTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:08,891] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into FilterResolutionExactTest.table1
INFO  [2023-01-17 00:50:08,892] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:08 +0000] "POST /admin/datasets/FilterResolutionExactTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_FilterResolutionExactTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:50:08,896] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:08,906] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:08,906] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:08,909] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
WARN  [2023-01-17 00:50:08,911] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:08,918] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionExactTest.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:50:08,918] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionExactTest.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:50:08,920] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionExactTest.table1.table1.0
INFO  [2023-01-17 00:50:08,920] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionExactTest.table1.table1.1
INFO  [2023-01-17 00:50:08,922] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionExactTest.table1.table1.2
INFO  [2023-01-17 00:50:09,028] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,033] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,048] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,053] com.bakdata.conquery.apiv1.FilterSearch: Processing reference list '/tmp/conquery_search5284426562223699337csv' ...
INFO  [2023-01-17 00:50:09,077] com.bakdata.conquery.apiv1.FilterSearch: Processed reference list '/tmp/conquery_search5284426562223699337csv' in 24 ms (4 Items in 5 Lines)
INFO  [2023-01-17 00:50:09,079] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FilterResolutionExactTest
INFO  [2023-01-17 00:50:09,084] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-17 00:50:09,084] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionExactTest, name=FilterResolutionExactTest]
INFO  [2023-01-17 00:50:09,084] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e
INFO  [2023-01-17 00:50:09,084] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8
INFO  [2023-01-17 00:50:09,104] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FilterResolutionExactTest
INFO  [2023-01-17 00:50:09,111] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FilterResolutionExactTest
INFO  [2023-01-17 00:50:09,113] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionExactTest
INFO  [2023-01-17 00:50:09,120] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,177] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e
INFO  [2023-01-17 00:50:09,177] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8
INFO  [2023-01-17 00:50:09,227] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionExactTest_042cf586-4510-4fe3-995e-8197fd40d10e
INFO  [2023-01-17 00:50:09,228] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionExactTest_2ce1cc39-7ca5-4de0-8990-ae3a9b9b04e8
INFO  [2023-01-17 00:50:09,346] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FilterResolutionExactTest
INFO  [2023-01-17 00:50:09,354] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:09,355] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:09,376] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:09,394] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:DATASET}): 0 entries, 0 B within 165.1 μs
INFO  [2023-01-17 00:50:09,394] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:SECONDARY_IDS}): 0 entries, 0 B within 159.0 μs
INFO  [2023-01-17 00:50:09,394] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:TABLES}): 0 entries, 0 B within 88.89 μs
INFO  [2023-01-17 00:50:09,394] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 94.75 μs
INFO  [2023-01-17 00:50:09,394] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:IMPORTS}): 0 entries, 0 B within 73.83 μs
INFO  [2023-01-17 00:50:09,395] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:CONCEPTS}): 0 entries, 0 B within 73.18 μs
INFO  [2023-01-17 00:50:09,395] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:09,395] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 76.16 μs
INFO  [2023-01-17 00:50:09,395] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:STRUCTURE}): 0 entries, 0 B within 71.70 μs
INFO  [2023-01-17 00:50:09,395] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 71.66 μs
INFO  [2023-01-17 00:50:09,395] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 75.30 μs
INFO  [2023-01-17 00:50:09,405] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-17 00:50:09,405] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-17 00:50:09,426] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07
INFO  [2023-01-17 00:50:09,427] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858
INFO  [2023-01-17 00:50:09,455] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:DATASET}): 0 entries, 0 B within 101.0 μs
INFO  [2023-01-17 00:50:09,455] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:SECONDARY_IDS}): 0 entries, 0 B within 53.01 μs
INFO  [2023-01-17 00:50:09,455] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:TABLES}): 0 entries, 0 B within 40.06 μs
INFO  [2023-01-17 00:50:09,455] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.19 μs
INFO  [2023-01-17 00:50:09,455] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:IMPORTS}): 0 entries, 0 B within 49.25 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:CONCEPTS}): 0 entries, 0 B within 38.38 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:WORKER}): 0 entries, 0 B within 38.99 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:BUCKETS}): 0 entries, 0 B within 36.83 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858:C_BLOCKS}): 0 entries, 0 B within 37.73 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:DATASET}): 0 entries, 0 B within 93.25 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:SECONDARY_IDS}): 0 entries, 0 B within 49.80 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:TABLES}): 0 entries, 0 B within 74.06 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.39 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:IMPORTS}): 0 entries, 0 B within 39.65 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:CONCEPTS}): 0 entries, 0 B within 38.38 μs
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:09,456] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:WORKER}): 0 entries, 0 B within 39.00 μs
INFO  [2023-01-17 00:50:09,457] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:BUCKETS}): 0 entries, 0 B within 50.73 μs
INFO  [2023-01-17 00:50:09,457] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07:C_BLOCKS}): 0 entries, 0 B within 39.64 μs
INFO  [2023-01-17 00:50:09,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:09,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:09,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:09,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:09,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FilterResolutionPrefixTest.worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:09,461] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:09,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,584] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,592] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,593] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionPrefixTest.table1
INFO  [2023-01-17 00:50:09,593] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FilterResolutionPrefixTest.table1
INFO  [2023-01-17 00:50:09,716] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,835] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:09,835] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:09,836] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:50:09,836] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionPrefixTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000221021sINFO  [2023-01-17 00:50:09,859] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:09,859] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:09,859] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4f53ed82)
INFO  [2023-01-17 00:50:09,866] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionPrefixTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:09,866] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:09,866] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_FilterResolutionPrefixTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:09,893] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into FilterResolutionPrefixTest.table1
127.0.0.1 - - [17/Jan/2023:00:50:09 +0000] "POST /admin/datasets/FilterResolutionPrefixTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_FilterResolutionPrefixTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:50:09,894] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:09,897] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:09,927] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:09,927] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:09,951] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionPrefixTest.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:50:09,951] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FilterResolutionPrefixTest.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:50:09,953] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:50:09,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionPrefixTest.table1.table1.0
WARN  [2023-01-17 00:50:09,956] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:09,957] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionPrefixTest.table1.table1.1
INFO  [2023-01-17 00:50:09,959] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FilterResolutionPrefixTest.table1.table1.2
INFO  [2023-01-17 00:50:10,086] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:10,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:10,110] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:10,115] com.bakdata.conquery.apiv1.FilterSearch: Processing reference list '/tmp/conquery_search17259375253443029530csv' ...
INFO  [2023-01-17 00:50:10,140] com.bakdata.conquery.apiv1.FilterSearch: Processed reference list '/tmp/conquery_search17259375253443029530csv' in 24 ms (4 Items in 5 Lines)
INFO  [2023-01-17 00:50:10,141] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:10,142] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-17 00:50:10,142] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FilterResolutionPrefixTest, name=FilterResolutionPrefixTest]
INFO  [2023-01-17 00:50:10,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07
INFO  [2023-01-17 00:50:10,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858
INFO  [2023-01-17 00:50:10,182] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858
INFO  [2023-01-17 00:50:10,182] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07
INFO  [2023-01-17 00:50:10,221] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:10,257] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:10,259] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:10,273] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:10,284] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_FilterResolutionPrefixTest_41d1ccba-cf72-42c0-a678-b902881eb858
INFO  [2023-01-17 00:50:10,284] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_FilterResolutionPrefixTest_a73b6254-3ce5-424e-8821-cc20db69ab07
INFO  [2023-01-17 00:50:10,401] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FilterResolutionPrefixTest
INFO  [2023-01-17 00:50:10,405] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test GroupHandlingTest
INFO  [2023-01-17 00:50:10,406] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:10,426] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest
INFO  [2023-01-17 00:50:10,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:DATASET}): 0 entries, 0 B within 143.7 μs
INFO  [2023-01-17 00:50:10,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 98.13 μs
INFO  [2023-01-17 00:50:10,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:TABLES}): 0 entries, 0 B within 81.32 μs
INFO  [2023-01-17 00:50:10,445] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 82.66 μs
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:IMPORTS}): 0 entries, 0 B within 76.42 μs
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:CONCEPTS}): 0 entries, 0 B within 76.40 μs
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 80.94 μs
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:STRUCTURE}): 0 entries, 0 B within 90.41 μs
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 77.09 μs
INFO  [2023-01-17 00:50:10,446] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 75.34 μs
INFO  [2023-01-17 00:50:10,455] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-17 00:50:10,455] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-17 00:50:10,507] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b
INFO  [2023-01-17 00:50:10,512] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268
INFO  [2023-01-17 00:50:10,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:DATASET}): 0 entries, 0 B within 101.3 μs
INFO  [2023-01-17 00:50:10,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:SECONDARY_IDS}): 0 entries, 0 B within 48.28 μs
INFO  [2023-01-17 00:50:10,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:TABLES}): 0 entries, 0 B within 34.15 μs
INFO  [2023-01-17 00:50:10,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.01 μs
INFO  [2023-01-17 00:50:10,527] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:IMPORTS}): 0 entries, 0 B within 32.48 μs
INFO  [2023-01-17 00:50:10,527] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:CONCEPTS}): 0 entries, 0 B within 34.74 μs
INFO  [2023-01-17 00:50:10,527] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:10,527] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:WORKER}): 0 entries, 0 B within 31.66 μs
INFO  [2023-01-17 00:50:10,527] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:BUCKETS}): 0 entries, 0 B within 31.91 μs
INFO  [2023-01-17 00:50:10,527] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b:C_BLOCKS}): 0 entries, 0 B within 31.83 μs
INFO  [2023-01-17 00:50:10,531] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker GroupHandlingTest.worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:10,531] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker GroupHandlingTest.worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:10,531] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:DATASET}): 0 entries, 0 B within 122.0 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:SECONDARY_IDS}): 0 entries, 0 B within 49.87 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:TABLES}): 0 entries, 0 B within 37.55 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 35.49 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:IMPORTS}): 0 entries, 0 B within 33.90 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:CONCEPTS}): 0 entries, 0 B within 45.58 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:WORKER}): 0 entries, 0 B within 35.22 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:BUCKETS}): 0 entries, 0 B within 34.39 μs
INFO  [2023-01-17 00:50:10,535] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268:C_BLOCKS}): 0 entries, 0 B within 33.08 μs
INFO  [2023-01-17 00:50:10,537] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:10,538] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker GroupHandlingTest.worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:10,538] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker GroupHandlingTest.worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:10,538] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:10,649] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user
INFO  [2023-01-17 00:50:10,649] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user2
INFO  [2023-01-17 00:50:10,650] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast GroupHandlingTest
INFO  [2023-01-17 00:50:10,651] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-17 00:50:10,651] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268
INFO  [2023-01-17 00:50:10,651] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=GroupHandlingTest, name=GroupHandlingTest]
INFO  [2023-01-17 00:50:10,652] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b
INFO  [2023-01-17 00:50:10,654] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow GroupHandlingTest
INFO  [2023-01-17 00:50:10,728] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b
INFO  [2023-01-17 00:50:10,736] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268
INFO  [2023-01-17 00:50:10,754] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of GroupHandlingTest
INFO  [2023-01-17 00:50:10,755] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_GroupHandlingTest
INFO  [2023-01-17 00:50:10,763] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:10,829] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_GroupHandlingTest_d9712580-062c-4b6f-b191-bb83e50beb4b
INFO  [2023-01-17 00:50:10,837] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_GroupHandlingTest_8e42d330-55e2-4900-9444-00e019d9e268
INFO  [2023-01-17 00:50:10,950] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test GroupHandlingTest
INFO  [2023-01-17 00:50:10,953] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ImportUpdateTest
INFO  [2023-01-17 00:50:10,953] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:10,972] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest
INFO  [2023-01-17 00:50:10,986] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:DATASET}): 0 entries, 0 B within 152.2 μs
INFO  [2023-01-17 00:50:10,986] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:SECONDARY_IDS}): 0 entries, 0 B within 87.64 μs
INFO  [2023-01-17 00:50:10,986] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:TABLES}): 0 entries, 0 B within 74.76 μs
INFO  [2023-01-17 00:50:10,987] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 75.19 μs
INFO  [2023-01-17 00:50:10,987] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:IMPORTS}): 0 entries, 0 B within 84.27 μs
INFO  [2023-01-17 00:50:10,987] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:CONCEPTS}): 0 entries, 0 B within 81.88 μs
INFO  [2023-01-17 00:50:10,987] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:10,987] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 88.23 μs
INFO  [2023-01-17 00:50:10,987] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:STRUCTURE}): 0 entries, 0 B within 72.63 μs
INFO  [2023-01-17 00:50:10,988] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 122.5 μs
INFO  [2023-01-17 00:50:10,988] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 104.2 μs
INFO  [2023-01-17 00:50:10,996] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-17 00:50:10,996] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-17 00:50:11,016] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e
INFO  [2023-01-17 00:50:11,021] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:DATASET}): 0 entries, 0 B within 184.2 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:SECONDARY_IDS}): 0 entries, 0 B within 68.66 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:TABLES}): 0 entries, 0 B within 45.18 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 38.20 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:IMPORTS}): 0 entries, 0 B within 35.83 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:CONCEPTS}): 0 entries, 0 B within 34.31 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:WORKER}): 0 entries, 0 B within 37.16 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:BUCKETS}): 0 entries, 0 B within 34.06 μs
INFO  [2023-01-17 00:50:11,040] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e:C_BLOCKS}): 0 entries, 0 B within 33.74 μs
INFO  [2023-01-17 00:50:11,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ImportUpdateTest.worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:11,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ImportUpdateTest.worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:11,045] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:11,047] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:DATASET}): 0 entries, 0 B within 67.58 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:SECONDARY_IDS}): 0 entries, 0 B within 62.27 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:TABLES}): 0 entries, 0 B within 38.98 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 33.09 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:IMPORTS}): 0 entries, 0 B within 28.51 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:CONCEPTS}): 0 entries, 0 B within 30.74 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:WORKER}): 0 entries, 0 B within 30.49 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:BUCKETS}): 0 entries, 0 B within 28.73 μs
INFO  [2023-01-17 00:50:11,048] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a:C_BLOCKS}): 0 entries, 0 B within 42.21 μs
INFO  [2023-01-17 00:50:11,050] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ImportUpdateTest.worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:11,050] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ImportUpdateTest.worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:11,050] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:11,053] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,175] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,183] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table1
INFO  [2023-01-17 00:50:11,184] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,184] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table1
INFO  [2023-01-17 00:50:11,184] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table2
INFO  [2023-01-17 00:50:11,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ImportUpdateTest.table2
INFO  [2023-01-17 00:50:11,301] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,413] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:11,413] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:11,413] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:11,413] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 151 B in total
INFO  [2023-01-17 00:50:11,414] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
███████████████████████████████                   ▌  62%	est. time remaining: 0.024670269sINFO  [2023-01-17 00:50:11,454] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:11,454] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:11,454] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=15655, maxValue=16021), dateReader=com.bakdata.conquery.util.DateReader@6bf1e1c1)
INFO  [2023-01-17 00:50:11,457] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:11,457] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000595325sINFO  [2023-01-17 00:50:11,474] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:11,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:11,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@16f01c76)
INFO  [2023-01-17 00:50:11,477] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:11,477] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:50:11,477] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:11,477] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:11,477] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,518] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into ImportUpdateTest.table1
127.0.0.1 - - [17/Jan/2023:00:50:11 +0000] "POST /admin/datasets/ImportUpdateTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ImportUpdateTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:11,520] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,521] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:11,533] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:11,533] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:11,574] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 4 entries.
INFO  [2023-01-17 00:50:11,575] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-17 00:50:11,577] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:11,577] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 4 entries.
INFO  [2023-01-17 00:50:11,577] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.0
INFO  [2023-01-17 00:50:11,578] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.1
INFO  [2023-01-17 00:50:11,692] com.bakdata.conquery.integration.tests.ImportUpdateTest: Checking state before update
INFO  [2023-01-17 00:50:11,716] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ImportUpdateTest] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:11,723] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3ad7d7d0-b074-4aed-84e7-79c4ef01465b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportUpdateTest))]]
INFO  [2023-01-17 00:50:11,728] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b
INFO  [2023-01-17 00:50:11,728] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b
INFO  [2023-01-17 00:50:11,729] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b] with 0 results within PT0.00056S
INFO  [2023-01-17 00:50:11,729] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b] with 2 results within PT0.000904S
INFO  [2023-01-17 00:50:11,730] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b, workerId=ImportUpdateTest.worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a, startTime=2023-01-17T00:50:11.728528, finishTime=2023-01-17T00:50:11.729088) of size 0
INFO  [2023-01-17 00:50:11,730] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b, workerId=ImportUpdateTest.worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e, startTime=2023-01-17T00:50:11.728293, finishTime=2023-01-17T00:50:11.729197) of size 2
INFO  [2023-01-17 00:50:11,731] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3ad7d7d0-b074-4aed-84e7-79c4ef01465b ManagedQuery within PT0.007224S
127.0.0.1 - - [17/Jan/2023:00:50:11 +0000] "POST /api/datasets/ImportUpdateTest/queries HTTP/1.1" 201 1163 "-" "Conquery (test client)" 21
127.0.0.1 - - [17/Jan/2023:00:50:11 +0000] "GET /api/datasets/ImportUpdateTest/queries/ImportUpdateTest.3ad7d7d0-b074-4aed-84e7-79c4ef01465b HTTP/1.1" 200 1418 "-" "Conquery (test client)" 5
127.0.0.1 - - [17/Jan/2023:00:50:11 +0000] "PUT /admin/datasets/ImportUpdateTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ImportUpdateTest%2Ftable2.cqpp HTTP/1.1" 404 79 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:50:11,802] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,836] com.bakdata.conquery.integration.tests.ImportUpdateTest: Manually loading new data for import
INFO  [2023-01-17 00:50:11,838] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:11,839] com.bakdata.conquery.commands.PreprocessorCommand: EXISTS ALREADY
INFO  [2023-01-17 00:50:11,840] com.bakdata.conquery.commands.PreprocessorCommand: 	HASH OUTDATED
INFO  [2023-01-17 00:50:11,840] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 124 B in total
INFO  [2023-01-17 00:50:11,841] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000322859sINFO  [2023-01-17 00:50:11,873] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=5, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:11,873] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:11,873] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=15929), dateReader=com.bakdata.conquery.util.DateReader@3dc438e8)
INFO  [2023-01-17 00:50:11,877] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:11,877] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:11,877] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportUpdateTest/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:11,877] com.bakdata.conquery.integration.tests.ImportUpdateTest: updating import
INFO  [2023-01-17 00:50:11,896] com.bakdata.conquery.models.messages.namespaces.specific.RemoveImportJob: Deleting Import[NamedImpl(name=table1)]
INFO  [2023-01-17 00:50:11,897] com.bakdata.conquery.models.messages.namespaces.specific.RemoveImportJob: Deleting Import[NamedImpl(name=table1)]
INFO  [2023-01-17 00:50:11,902] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into ImportUpdateTest.table1
127.0.0.1 - - [17/Jan/2023:00:50:11 +0000] "PUT /admin/datasets/ImportUpdateTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ImportUpdateTest%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:50:11,903] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:11,903] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:11,942] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:11,942] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:11,955] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:11,955] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 5 entries.
WARN  [2023-01-17 00:50:11,956] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:11,956] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ImportUpdateTest.table1.table1], containing 5 entries.
INFO  [2023-01-17 00:50:11,956] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.2
INFO  [2023-01-17 00:50:11,957] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ImportUpdateTest.table1.table1.1
INFO  [2023-01-17 00:50:12,063] com.bakdata.conquery.integration.tests.ImportUpdateTest: Checking state after update
INFO  [2023-01-17 00:50:12,087] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ImportUpdateTest] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:12,088] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1970e5d3-466f-4097-bffe-a20a7aaf225c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportUpdateTest))]]
INFO  [2023-01-17 00:50:12,090] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c
INFO  [2023-01-17 00:50:12,090] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c
127.0.0.1 - - [17/Jan/2023:00:50:12 +0000] "POST /api/datasets/ImportUpdateTest/queries HTTP/1.1" 201 1164 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:12,105] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c] with 2 results within PT0.014976S
INFO  [2023-01-17 00:50:12,107] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c] with 2 results within PT0.017513S
INFO  [2023-01-17 00:50:12,109] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c, workerId=ImportUpdateTest.worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e, startTime=2023-01-17T00:50:12.090423, finishTime=2023-01-17T00:50:12.107936) of size 2
INFO  [2023-01-17 00:50:12,109] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c, workerId=ImportUpdateTest.worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a, startTime=2023-01-17T00:50:12.090413, finishTime=2023-01-17T00:50:12.105389) of size 2
INFO  [2023-01-17 00:50:12,110] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1970e5d3-466f-4097-bffe-a20a7aaf225c ManagedQuery within PT0.021985S
127.0.0.1 - - [17/Jan/2023:00:50:12 +0000] "GET /api/datasets/ImportUpdateTest/queries/ImportUpdateTest.1970e5d3-466f-4097-bffe-a20a7aaf225c HTTP/1.1" 200 1420 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:50:12,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ImportUpdateTest
INFO  [2023-01-17 00:50:12,115] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-17 00:50:12,115] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ImportUpdateTest, name=ImportUpdateTest]
INFO  [2023-01-17 00:50:12,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a
INFO  [2023-01-17 00:50:12,116] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e
INFO  [2023-01-17 00:50:12,149] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e
INFO  [2023-01-17 00:50:12,156] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a
INFO  [2023-01-17 00:50:12,160] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportUpdateTest_b2709c24-ae24-47d2-9525-7b48a65be30e
INFO  [2023-01-17 00:50:12,160] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportUpdateTest_a09a4ef0-c1d9-400a-a7a0-6dd7563be01a
INFO  [2023-01-17 00:50:12,208] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ImportUpdateTest
INFO  [2023-01-17 00:50:12,257] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ImportUpdateTest
INFO  [2023-01-17 00:50:12,258] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportUpdateTest
INFO  [2023-01-17 00:50:12,267] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,273] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ImportUpdateTest
INFO  [2023-01-17 00:50:12,279] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MetadataCollectionTest
INFO  [2023-01-17 00:50:12,280] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:12,300] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest
INFO  [2023-01-17 00:50:12,319] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:DATASET}): 0 entries, 0 B within 161.6 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:SECONDARY_IDS}): 0 entries, 0 B within 84.51 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:TABLES}): 0 entries, 0 B within 81.35 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 80.70 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:IMPORTS}): 0 entries, 0 B within 72.30 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:CONCEPTS}): 0 entries, 0 B within 69.66 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 72.01 μs
INFO  [2023-01-17 00:50:12,320] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:STRUCTURE}): 0 entries, 0 B within 66.88 μs
INFO  [2023-01-17 00:50:12,321] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 67.10 μs
INFO  [2023-01-17 00:50:12,321] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 74.69 μs
INFO  [2023-01-17 00:50:12,329] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-17 00:50:12,329] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-17 00:50:12,349] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46
INFO  [2023-01-17 00:50:12,349] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1
INFO  [2023-01-17 00:50:12,377] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:DATASET}): 0 entries, 0 B within 81.97 μs
INFO  [2023-01-17 00:50:12,377] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:SECONDARY_IDS}): 0 entries, 0 B within 40.20 μs
INFO  [2023-01-17 00:50:12,377] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:TABLES}): 0 entries, 0 B within 23.58 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 25.11 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:IMPORTS}): 0 entries, 0 B within 24.01 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:CONCEPTS}): 0 entries, 0 B within 21.90 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:WORKER}): 0 entries, 0 B within 23.04 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:BUCKETS}): 0 entries, 0 B within 32.67 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:DATASET}): 0 entries, 0 B within 76.64 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1:C_BLOCKS}): 0 entries, 0 B within 24.98 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:SECONDARY_IDS}): 0 entries, 0 B within 38.24 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:TABLES}): 0 entries, 0 B within 31.57 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 37.51 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:IMPORTS}): 0 entries, 0 B within 31.50 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:CONCEPTS}): 0 entries, 0 B within 27.43 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:WORKER}): 0 entries, 0 B within 28.91 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:BUCKETS}): 0 entries, 0 B within 26.98 μs
INFO  [2023-01-17 00:50:12,378] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46:C_BLOCKS}): 0 entries, 0 B within 26.31 μs
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MetadataCollectionTest.worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MetadataCollectionTest.worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MetadataCollectionTest.worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MetadataCollectionTest.worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:12,382] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:12,486] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,495] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MetadataCollectionTest.test_table
INFO  [2023-01-17 00:50:12,495] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MetadataCollectionTest.test_table
INFO  [2023-01-17 00:50:12,618] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,732] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:12,733] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:12,733] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-17 00:50:12,733] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_MetadataCollectionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000325324sINFO  [2023-01-17 00:50:12,766] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:12,766] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:12,766] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@6d2bed2f)
INFO  [2023-01-17 00:50:12,769] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_MetadataCollectionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:12,769] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:12,769] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_MetadataCollectionTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:12,793] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into MetadataCollectionTest.test_table
INFO  [2023-01-17 00:50:12,794] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:12 +0000] "POST /admin/datasets/MetadataCollectionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_MetadataCollectionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:50:12,797] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:12,810] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:12,811] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:12,816] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-17 00:50:12,818] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:12,822] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MetadataCollectionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:12,822] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MetadataCollectionTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:12,824] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MetadataCollectionTest.test_table.test_table.0
INFO  [2023-01-17 00:50:12,824] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MetadataCollectionTest.test_table.test_table.1
INFO  [2023-01-17 00:50:12,932] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,937] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,953] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:12,956] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:50:12,956] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:50:13,081] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MetadataCollectionTest
INFO  [2023-01-17 00:50:13,081] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-17 00:50:13,081] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MetadataCollectionTest, name=MetadataCollectionTest]
INFO  [2023-01-17 00:50:13,081] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46
INFO  [2023-01-17 00:50:13,081] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1
INFO  [2023-01-17 00:50:13,129] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MetadataCollectionTest
INFO  [2023-01-17 00:50:13,179] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1
INFO  [2023-01-17 00:50:13,180] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46
INFO  [2023-01-17 00:50:13,219] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MetadataCollectionTest
INFO  [2023-01-17 00:50:13,221] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_MetadataCollectionTest
INFO  [2023-01-17 00:50:13,231] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:13,268] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_MetadataCollectionTest_30929144-c703-49e0-b6b0-a862a5542d46
INFO  [2023-01-17 00:50:13,268] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_MetadataCollectionTest_e8aaa2e8-18eb-433f-9556-daa9f432c4a1
INFO  [2023-01-17 00:50:13,392] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MetadataCollectionTest
INFO  [2023-01-17 00:50:13,397] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,399] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:13,418] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,436] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:DATASET}): 0 entries, 0 B within 146.2 μs
INFO  [2023-01-17 00:50:13,436] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 81.28 μs
INFO  [2023-01-17 00:50:13,436] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:TABLES}): 0 entries, 0 B within 105.3 μs
INFO  [2023-01-17 00:50:13,437] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 99.53 μs
INFO  [2023-01-17 00:50:13,437] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:IMPORTS}): 0 entries, 0 B within 92.36 μs
INFO  [2023-01-17 00:50:13,437] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:CONCEPTS}): 0 entries, 0 B within 92.43 μs
INFO  [2023-01-17 00:50:13,437] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:13,437] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 117.8 μs
INFO  [2023-01-17 00:50:13,437] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:STRUCTURE}): 0 entries, 0 B within 89.03 μs
INFO  [2023-01-17 00:50:13,438] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 89.05 μs
INFO  [2023-01-17 00:50:13,438] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 96.20 μs
INFO  [2023-01-17 00:50:13,445] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-17 00:50:13,445] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-17 00:50:13,474] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5
INFO  [2023-01-17 00:50:13,479] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91
INFO  [2023-01-17 00:50:13,501] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:DATASET}): 0 entries, 0 B within 76.50 μs
INFO  [2023-01-17 00:50:13,501] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:SECONDARY_IDS}): 0 entries, 0 B within 41.25 μs
INFO  [2023-01-17 00:50:13,501] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:TABLES}): 0 entries, 0 B within 31.65 μs
INFO  [2023-01-17 00:50:13,501] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 48.17 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:IMPORTS}): 0 entries, 0 B within 40.09 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:CONCEPTS}): 0 entries, 0 B within 30.60 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:WORKER}): 0 entries, 0 B within 26.07 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:BUCKETS}): 0 entries, 0 B within 25.46 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5:C_BLOCKS}): 0 entries, 0 B within 23.13 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:DATASET}): 0 entries, 0 B within 54.65 μs
INFO  [2023-01-17 00:50:13,502] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:SECONDARY_IDS}): 0 entries, 0 B within 33.52 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:TABLES}): 0 entries, 0 B within 29.31 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 30.89 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:IMPORTS}): 0 entries, 0 B within 25.81 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:CONCEPTS}): 0 entries, 0 B within 25.31 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:WORKER}): 0 entries, 0 B within 26.83 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:BUCKETS}): 0 entries, 0 B within 25.08 μs
INFO  [2023-01-17 00:50:13,503] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91:C_BLOCKS}): 0 entries, 0 B within 24.46 μs
INFO  [2023-01-17 00:50:13,506] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:13,506] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:13,506] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:13,506] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:13,507] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionGroupHandlingTest.worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:13,507] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:13,508] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:13,618] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user
INFO  [2023-01-17 00:50:13,618] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,619] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-17 00:50:13,619] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91
INFO  [2023-01-17 00:50:13,619] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionGroupHandlingTest, name=PermissionGroupHandlingTest]
INFO  [2023-01-17 00:50:13,619] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5
INFO  [2023-01-17 00:50:13,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,703] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5
INFO  [2023-01-17 00:50:13,704] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91
INFO  [2023-01-17 00:50:13,745] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,747] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,755] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:13,804] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionGroupHandlingTest_f3d781c7-8428-4521-9669-fa6aa78039a5
INFO  [2023-01-17 00:50:13,804] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionGroupHandlingTest_dbe31c28-06f8-431f-b5b9-5be2cb3bee91
INFO  [2023-01-17 00:50:13,917] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test PermissionGroupHandlingTest
INFO  [2023-01-17 00:50:13,921] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:13,922] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:13,941] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:13,956] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:DATASET}): 0 entries, 0 B within 121.2 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 75.58 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:TABLES}): 0 entries, 0 B within 98.40 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 66.67 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:IMPORTS}): 0 entries, 0 B within 60.61 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:CONCEPTS}): 0 entries, 0 B within 59.81 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 69.71 μs
INFO  [2023-01-17 00:50:13,957] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:STRUCTURE}): 0 entries, 0 B within 58.41 μs
INFO  [2023-01-17 00:50:13,958] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 74.49 μs
INFO  [2023-01-17 00:50:13,958] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 59.32 μs
INFO  [2023-01-17 00:50:13,966] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-17 00:50:13,966] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-17 00:50:13,995] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b
INFO  [2023-01-17 00:50:14,000] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600
INFO  [2023-01-17 00:50:14,021] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:DATASET}): 0 entries, 0 B within 110.8 μs
INFO  [2023-01-17 00:50:14,021] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:SECONDARY_IDS}): 0 entries, 0 B within 48.41 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:TABLES}): 0 entries, 0 B within 35.48 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 38.32 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:IMPORTS}): 0 entries, 0 B within 34.78 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:CONCEPTS}): 0 entries, 0 B within 33.53 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:WORKER}): 0 entries, 0 B within 47.04 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:BUCKETS}): 0 entries, 0 B within 32.39 μs
INFO  [2023-01-17 00:50:14,022] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b:C_BLOCKS}): 0 entries, 0 B within 32.86 μs
INFO  [2023-01-17 00:50:14,024] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:DATASET}): 0 entries, 0 B within 57.81 μs
INFO  [2023-01-17 00:50:14,024] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:SECONDARY_IDS}): 0 entries, 0 B within 31.57 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:TABLES}): 0 entries, 0 B within 26.08 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 29.12 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:IMPORTS}): 0 entries, 0 B within 23.95 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:CONCEPTS}): 0 entries, 0 B within 23.56 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:WORKER}): 0 entries, 0 B within 32.59 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:BUCKETS}): 0 entries, 0 B within 23.41 μs
INFO  [2023-01-17 00:50:14,025] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600:C_BLOCKS}): 0 entries, 0 B within 22.48 μs
INFO  [2023-01-17 00:50:14,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:14,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:14,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:14,029] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:14,029] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:14,029] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PermissionRoleHandlingTest.worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:14,029] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:14,136] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.user
INFO  [2023-01-17 00:50:14,137] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:14,137] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-17 00:50:14,137] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600
INFO  [2023-01-17 00:50:14,138] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PermissionRoleHandlingTest, name=PermissionRoleHandlingTest]
INFO  [2023-01-17 00:50:14,138] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b
INFO  [2023-01-17 00:50:14,166] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:14,223] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b
INFO  [2023-01-17 00:50:14,226] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600
INFO  [2023-01-17 00:50:14,266] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:14,267] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:14,275] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:14,325] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_PermissionRoleHandlingTest_70f1d72b-b3dd-4381-a35d-769705b1666b
INFO  [2023-01-17 00:50:14,327] com.bakdata.conquery.models.config.XodusStoreFactory: Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_PermissionRoleHandlingTest_d6bccc50-2953-4c4b-bba9-01f07ff50600
INFO  [2023-01-17 00:50:14,442] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test PermissionRoleHandlingTest
INFO  [2023-01-17 00:50:14,446] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test RestartTest
INFO  [2023-01-17 00:50:14,452] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:14,471] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
INFO  [2023-01-17 00:50:14,485] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:DATASET}): 0 entries, 0 B within 102.6 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:SECONDARY_IDS}): 0 entries, 0 B within 70.40 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:TABLES}): 0 entries, 0 B within 55.03 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 56.69 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:IMPORTS}): 0 entries, 0 B within 50.67 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:CONCEPTS}): 0 entries, 0 B within 50.41 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 54.52 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:STRUCTURE}): 0 entries, 0 B within 50.74 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 49.23 μs
INFO  [2023-01-17 00:50:14,486] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 59.75 μs
INFO  [2023-01-17 00:50:14,494] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RestartTest, name=RestartTest]
INFO  [2023-01-17 00:50:14,494] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RestartTest, name=RestartTest]
INFO  [2023-01-17 00:50:14,520] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
INFO  [2023-01-17 00:50:14,525] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:DATASET}): 0 entries, 0 B within 78.47 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:SECONDARY_IDS}): 0 entries, 0 B within 33.71 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:TABLES}): 0 entries, 0 B within 26.74 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 36.63 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:IMPORTS}): 0 entries, 0 B within 26.19 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:CONCEPTS}): 0 entries, 0 B within 23.98 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:WORKER}): 0 entries, 0 B within 23.76 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:BUCKETS}): 0 entries, 0 B within 24.30 μs
INFO  [2023-01-17 00:50:14,546] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:C_BLOCKS}): 0 entries, 0 B within 23.75 μs
INFO  [2023-01-17 00:50:14,551] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:14,551] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:14,551] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:14,553] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:DATASET}): 0 entries, 0 B within 88.51 μs
INFO  [2023-01-17 00:50:14,553] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:SECONDARY_IDS}): 0 entries, 0 B within 47.72 μs
INFO  [2023-01-17 00:50:14,553] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:TABLES}): 0 entries, 0 B within 38.92 μs
INFO  [2023-01-17 00:50:14,553] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 43.03 μs
INFO  [2023-01-17 00:50:14,553] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:IMPORTS}): 0 entries, 0 B within 40.48 μs
INFO  [2023-01-17 00:50:14,554] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:CONCEPTS}): 0 entries, 0 B within 34.59 μs
INFO  [2023-01-17 00:50:14,554] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:14,554] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:WORKER}): 0 entries, 0 B within 35.86 μs
INFO  [2023-01-17 00:50:14,554] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:BUCKETS}): 0 entries, 0 B within 33.95 μs
INFO  [2023-01-17 00:50:14,554] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:C_BLOCKS}): 0 entries, 0 B within 33.11 μs
INFO  [2023-01-17 00:50:14,556] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:14,556] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:14,556] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:14,559] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:14,688] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[RestartTest.secondary]
INFO  [2023-01-17 00:50:14,690] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:14,691] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId RestartTest.secondary
INFO  [2023-01-17 00:50:14,692] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId RestartTest.secondary
INFO  [2023-01-17 00:50:14,800] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:14,801] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RestartTest.test_table
INFO  [2023-01-17 00:50:14,801] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RestartTest.test_table
INFO  [2023-01-17 00:50:14,931] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:15,044] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:15,045] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:15,045] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-17 00:50:15,045] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_RestartTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000314177sINFO  [2023-01-17 00:50:15,077] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:15,077] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:15,077] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@9b0f6a2)
INFO  [2023-01-17 00:50:15,080] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_RestartTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:15,080] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:15,080] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_RestartTest/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:15,106] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into RestartTest.test_table
INFO  [2023-01-17 00:50:15,108] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:15 +0000] "POST /admin/datasets/RestartTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_RestartTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:50:15,110] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:15,122] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:15,122] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:15,127] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-17 00:50:15,129] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:15,133] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RestartTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:15,133] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RestartTest.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:50:15,135] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RestartTest.test_table.test_table.0
INFO  [2023-01-17 00:50:15,136] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RestartTest.test_table.test_table.1
INFO  [2023-01-17 00:50:15,243] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:15,248] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:15,264] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:50:15,283] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[RestartTest] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:15,284] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest))]]
INFO  [2023-01-17 00:50:15,288] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3
INFO  [2023-01-17 00:50:15,288] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3
INFO  [2023-01-17 00:50:15,290] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3] with 0 results within PT0.001152S
INFO  [2023-01-17 00:50:15,290] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3] with 2 results within PT0.001251S
INFO  [2023-01-17 00:50:15,291] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3, workerId=RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a, startTime=2023-01-17T00:50:15.288816, finishTime=2023-01-17T00:50:15.289968) of size 0
INFO  [2023-01-17 00:50:15,291] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3, workerId=RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36, startTime=2023-01-17T00:50:15.288745, finishTime=2023-01-17T00:50:15.289996) of size 2
127.0.0.1 - - [17/Jan/2023:00:50:15 +0000] "POST /api/datasets/RestartTest/queries HTTP/1.1" 201 1116 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:15,293] com.bakdata.conquery.models.execution.ManagedExecution: DONE d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3 ManagedQuery within PT0.007649S
127.0.0.1 - - [17/Jan/2023:00:50:15 +0000] "GET /api/datasets/RestartTest/queries/RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3 HTTP/1.1" 200 1351 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:50:15,357] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RestartTest], queryId=d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:50:15.283927, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@705e81ed[Count = 0], startTime=2023-01-17T00:50:15.284830, finishTime=2023-01-17T00:50:15.292479, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5c52cfc5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@b78b225], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e2e8239], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@58d16be2, com.bakdata.conquery.models.query.ColumnDescriptor@5c3db630]) download on dataset Dataset[label=null, name=RestartTest] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:15,395] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RestartTest], queryId=d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:50:15.283927, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@705e81ed[Count = 0], startTime=2023-01-17T00:50:15.284830, finishTime=2023-01-17T00:50:15.292479, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5c52cfc5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@b78b225], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4e2e8239], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@58d16be2, com.bakdata.conquery.models.query.ColumnDescriptor@5c3db630]) on dataset Dataset[label=null, name=RestartTest]
127.0.0.1 - - [17/Jan/2023:00:50:15 +0000] "GET /api/datasets/RestartTest/result/RestartTest.d2ce4a9b-b72b-49fd-aac8-cffe7ab3ddf3.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 60
INFO  [2023-01-17 00:50:15,408] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:50:15,438] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
INFO  [2023-01-17 00:50:15,453] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:DATASET}): 0 entries, 0 B within 142.8 μs
INFO  [2023-01-17 00:50:15,453] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:SECONDARY_IDS}): 0 entries, 0 B within 86.84 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:TABLES}): 0 entries, 0 B within 63.77 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 66.58 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:IMPORTS}): 0 entries, 0 B within 60.67 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:CONCEPTS}): 0 entries, 0 B within 58.90 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 61.85 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:STRUCTURE}): 0 entries, 0 B within 58.44 μs
INFO  [2023-01-17 00:50:15,454] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:WORKER_TO_BUCKETS}): 0 entries, 0 B within 58.37 μs
INFO  [2023-01-17 00:50:15,455] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:PRIMARY_DICTIONARY}): 0 entries, 0 B within 75.13 μs
INFO  [2023-01-17 00:50:15,463] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset1, name=testDataset1]
INFO  [2023-01-17 00:50:15,463] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset1, name=testDataset1]
INFO  [2023-01-17 00:50:15,491] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
INFO  [2023-01-17 00:50:15,496] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
INFO  [2023-01-17 00:50:15,496] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
INFO  [2023-01-17 00:50:15,505] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:DATASET}): 0 entries, 0 B within 203.7 μs
INFO  [2023-01-17 00:50:15,505] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:SECONDARY_IDS}): 0 entries, 0 B within 62.10 μs
INFO  [2023-01-17 00:50:15,506] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:TABLES}): 0 entries, 0 B within 52.89 μs
INFO  [2023-01-17 00:50:15,507] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 1.630 ms
INFO  [2023-01-17 00:50:15,507] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:IMPORTS}): 0 entries, 0 B within 57.17 μs
INFO  [2023-01-17 00:50:15,508] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:CONCEPTS}): 0 entries, 0 B within 48.71 μs
INFO  [2023-01-17 00:50:15,508] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:15,508] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 53.12 μs
INFO  [2023-01-17 00:50:15,508] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:STRUCTURE}): 0 entries, 0 B within 47.78 μs
INFO  [2023-01-17 00:50:15,508] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:WORKER_TO_BUCKETS}): 0 entries, 0 B within 47.08 μs
INFO  [2023-01-17 00:50:15,508] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:PRIMARY_DICTIONARY}): 0 entries, 0 B within 46.09 μs
INFO  [2023-01-17 00:50:15,518] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:DATASET}): 0 entries, 0 B within 76.28 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:SECONDARY_IDS}): 0 entries, 0 B within 48.40 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:TABLES}): 0 entries, 0 B within 31.39 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 34.13 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:IMPORTS}): 0 entries, 0 B within 29.72 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:CONCEPTS}): 0 entries, 0 B within 29.20 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:WORKER}): 0 entries, 0 B within 29.68 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:BUCKETS}): 0 entries, 0 B within 30.57 μs
INFO  [2023-01-17 00:50:15,519] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:C_BLOCKS}): 0 entries, 0 B within 38.93 μs
INFO  [2023-01-17 00:50:15,520] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset2, name=testDataset2]
INFO  [2023-01-17 00:50:15,523] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset1.worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,523] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset1.worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,523] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,525] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:DATASET}): 0 entries, 0 B within 64.89 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:SECONDARY_IDS}): 0 entries, 0 B within 46.64 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:TABLES}): 0 entries, 0 B within 38.99 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 36.92 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:IMPORTS}): 0 entries, 0 B within 32.98 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:CONCEPTS}): 0 entries, 0 B within 33.13 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:WORKER}): 0 entries, 0 B within 33.22 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:BUCKETS}): 0 entries, 0 B within 32.26 μs
INFO  [2023-01-17 00:50:15,526] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:C_BLOCKS}): 0 entries, 0 B within 31.56 μs
INFO  [2023-01-17 00:50:15,527] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset2, name=testDataset2]
INFO  [2023-01-17 00:50:15,529] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset1.worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,529] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset1.worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,529] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,544] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
INFO  [2023-01-17 00:50:15,549] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
INFO  [2023-01-17 00:50:15,554] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
INFO  [2023-01-17 00:50:15,559] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:DATASET}): 0 entries, 0 B within 106.7 μs
INFO  [2023-01-17 00:50:15,559] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:SECONDARY_IDS}): 0 entries, 0 B within 69.67 μs
INFO  [2023-01-17 00:50:15,559] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:TABLES}): 0 entries, 0 B within 58.49 μs
INFO  [2023-01-17 00:50:15,559] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 65.44 μs
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:IMPORTS}): 0 entries, 0 B within 56.28 μs
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:CONCEPTS}): 0 entries, 0 B within 56.11 μs
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 59.07 μs
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:STRUCTURE}): 0 entries, 0 B within 63.26 μs
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:WORKER_TO_BUCKETS}): 0 entries, 0 B within 59.25 μs
INFO  [2023-01-17 00:50:15,560] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:PRIMARY_DICTIONARY}): 0 entries, 0 B within 57.15 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:DATASET}): 0 entries, 0 B within 66.38 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:SECONDARY_IDS}): 0 entries, 0 B within 29.74 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:TABLES}): 0 entries, 0 B within 22.71 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 26.30 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:IMPORTS}): 0 entries, 0 B within 25.04 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:CONCEPTS}): 0 entries, 0 B within 22.82 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:WORKER}): 0 entries, 0 B within 21.85 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:BUCKETS}): 0 entries, 0 B within 21.16 μs
INFO  [2023-01-17 00:50:15,574] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:C_BLOCKS}): 0 entries, 0 B within 23.23 μs
INFO  [2023-01-17 00:50:15,575] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset3, name=testDataset3]
INFO  [2023-01-17 00:50:15,578] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset2.worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,578] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset2.worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,578] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,581] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:DATASET}): 0 entries, 0 B within 72.94 μs
INFO  [2023-01-17 00:50:15,581] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:SECONDARY_IDS}): 0 entries, 0 B within 64.24 μs
INFO  [2023-01-17 00:50:15,581] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:TABLES}): 0 entries, 0 B within 41.46 μs
INFO  [2023-01-17 00:50:15,581] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 56.27 μs
INFO  [2023-01-17 00:50:15,582] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:IMPORTS}): 0 entries, 0 B within 36.43 μs
INFO  [2023-01-17 00:50:15,582] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:CONCEPTS}): 0 entries, 0 B within 43.78 μs
INFO  [2023-01-17 00:50:15,582] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,582] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:WORKER}): 0 entries, 0 B within 38.62 μs
INFO  [2023-01-17 00:50:15,582] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:BUCKETS}): 0 entries, 0 B within 36.27 μs
INFO  [2023-01-17 00:50:15,582] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:C_BLOCKS}): 0 entries, 0 B within 38.09 μs
INFO  [2023-01-17 00:50:15,583] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset3, name=testDataset3]
INFO  [2023-01-17 00:50:15,584] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset2.worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,585] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset2.worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,585] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,593] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:DATASET}): 0 entries, 0 B within 90.53 μs
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:SECONDARY_IDS}): 0 entries, 0 B within 62.99 μs
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:TABLES}): 0 entries, 0 B within 46.26 μs
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 55.22 μs
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:IMPORTS}): 0 entries, 0 B within 58.97 μs
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:CONCEPTS}): 0 entries, 0 B within 45.53 μs
INFO  [2023-01-17 00:50:15,607] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:15,608] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 48.86 μs
INFO  [2023-01-17 00:50:15,608] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:STRUCTURE}): 0 entries, 0 B within 47.17 μs
INFO  [2023-01-17 00:50:15,608] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:WORKER_TO_BUCKETS}): 0 entries, 0 B within 50.33 μs
INFO  [2023-01-17 00:50:15,608] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:PRIMARY_DICTIONARY}): 0 entries, 0 B within 43.60 μs
INFO  [2023-01-17 00:50:15,608] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
INFO  [2023-01-17 00:50:15,624] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
INFO  [2023-01-17 00:50:15,639] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:DATASET}): 0 entries, 0 B within 64.04 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:SECONDARY_IDS}): 0 entries, 0 B within 26.52 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:TABLES}): 0 entries, 0 B within 20.24 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 30.07 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:IMPORTS}): 0 entries, 0 B within 22.84 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:CONCEPTS}): 0 entries, 0 B within 19.17 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:WORKER}): 0 entries, 0 B within 20.00 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:BUCKETS}): 0 entries, 0 B within 19.59 μs
INFO  [2023-01-17 00:50:15,640] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:C_BLOCKS}): 0 entries, 0 B within 18.55 μs
INFO  [2023-01-17 00:50:15,641] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset4, name=testDataset4]
INFO  [2023-01-17 00:50:15,644] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset3.worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,644] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset3.worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,644] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,646] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:DATASET}): 0 entries, 0 B within 80.40 μs
INFO  [2023-01-17 00:50:15,646] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:SECONDARY_IDS}): 0 entries, 0 B within 51.77 μs
INFO  [2023-01-17 00:50:15,646] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:TABLES}): 0 entries, 0 B within 30.11 μs
INFO  [2023-01-17 00:50:15,646] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.37 μs
INFO  [2023-01-17 00:50:15,646] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:IMPORTS}): 0 entries, 0 B within 32.01 μs
INFO  [2023-01-17 00:50:15,647] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:CONCEPTS}): 0 entries, 0 B within 31.84 μs
INFO  [2023-01-17 00:50:15,647] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,647] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
INFO  [2023-01-17 00:50:15,647] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:WORKER}): 0 entries, 0 B within 35.75 μs
INFO  [2023-01-17 00:50:15,647] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:BUCKETS}): 0 entries, 0 B within 40.28 μs
INFO  [2023-01-17 00:50:15,647] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:C_BLOCKS}): 0 entries, 0 B within 32.05 μs
INFO  [2023-01-17 00:50:15,648] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset4, name=testDataset4]
INFO  [2023-01-17 00:50:15,649] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset3.worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,649] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset3.worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,649] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:DATASET}): 0 entries, 0 B within 75.01 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:SECONDARY_IDS}): 0 entries, 0 B within 41.52 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:TABLES}): 0 entries, 0 B within 35.71 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 43.12 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:IMPORTS}): 0 entries, 0 B within 36.43 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:CONCEPTS}): 0 entries, 0 B within 33.86 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 38.63 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:STRUCTURE}): 0 entries, 0 B within 35.28 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:WORKER_TO_BUCKETS}): 0 entries, 0 B within 33.23 μs
INFO  [2023-01-17 00:50:15,657] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:PRIMARY_DICTIONARY}): 0 entries, 0 B within 32.60 μs
INFO  [2023-01-17 00:50:15,682] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
INFO  [2023-01-17 00:50:15,687] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
INFO  [2023-01-17 00:50:15,698] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:DATASET}): 0 entries, 0 B within 82.61 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:SECONDARY_IDS}): 0 entries, 0 B within 29.67 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:TABLES}): 0 entries, 0 B within 21.32 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 25.51 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:IMPORTS}): 0 entries, 0 B within 21.76 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:CONCEPTS}): 0 entries, 0 B within 21.23 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:WORKER}): 0 entries, 0 B within 18.54 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:BUCKETS}): 0 entries, 0 B within 18.37 μs
INFO  [2023-01-17 00:50:15,708] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:C_BLOCKS}): 0 entries, 0 B within 18.19 μs
INFO  [2023-01-17 00:50:15,709] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset5, name=testDataset5]
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:DATASET}): 0 entries, 0 B within 61.93 μs
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:SECONDARY_IDS}): 0 entries, 0 B within 55.42 μs
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:TABLES}): 0 entries, 0 B within 34.79 μs
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.92 μs
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:IMPORTS}): 0 entries, 0 B within 33.45 μs
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:CONCEPTS}): 0 entries, 0 B within 61.67 μs
INFO  [2023-01-17 00:50:15,710] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 41.39 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:STRUCTURE}): 0 entries, 0 B within 36.11 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:WORKER_TO_BUCKETS}): 0 entries, 0 B within 33.58 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:PRIMARY_DICTIONARY}): 0 entries, 0 B within 35.03 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:DATASET}): 0 entries, 0 B within 46.99 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:SECONDARY_IDS}): 0 entries, 0 B within 26.29 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:TABLES}): 0 entries, 0 B within 19.10 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.15 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:IMPORTS}): 0 entries, 0 B within 19.56 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:CONCEPTS}): 0 entries, 0 B within 23.19 μs
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,711] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:WORKER}): 0 entries, 0 B within 18.41 μs
INFO  [2023-01-17 00:50:15,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset4.worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset4.worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,718] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:BUCKETS}): 0 entries, 0 B within 80.58 μs
INFO  [2023-01-17 00:50:15,718] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:C_BLOCKS}): 0 entries, 0 B within 31.39 μs
INFO  [2023-01-17 00:50:15,719] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset5, name=testDataset5]
INFO  [2023-01-17 00:50:15,720] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset4.worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,720] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset4.worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,720] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,728] com.bakdata.conquery.io.storage.MetaStorage: Remove User = user.userDelete@test$2eemail
INFO  [2023-01-17 00:50:15,728] com.bakdata.conquery.resources.admin.rest.AdminProcessor: Deleting Role[role.roleDelete]
INFO  [2023-01-17 00:50:15,728] com.bakdata.conquery.integration.tests.RestartTest: Shutting down for restart
INFO  [2023-01-17 00:50:15,734] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
INFO  [2023-01-17 00:50:15,735] org.eclipse.jetty.server.AbstractConnector: Stopped application@65a3ca0{HTTP/1.1, (http/1.1)}{0.0.0.0:38705}
INFO  [2023-01-17 00:50:15,736] org.eclipse.jetty.server.AbstractConnector: Stopped admin@659b8aaa{HTTP/1.1, (http/1.1)}{0.0.0.0:44953}
INFO  [2023-01-17 00:50:15,744] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@7f20bf32{/,null,STOPPED}
INFO  [2023-01-17 00:50:15,744] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
INFO  [2023-01-17 00:50:15,746] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@510ba0cf{/,null,STOPPED}
INFO  [2023-01-17 00:50:15,747] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:DATASET}): 0 entries, 0 B within 85.97 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:SECONDARY_IDS}): 0 entries, 0 B within 47.62 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:TABLES}): 0 entries, 0 B within 33.26 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 38.04 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:IMPORTS}): 0 entries, 0 B within 31.82 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:CONCEPTS}): 0 entries, 0 B within 30.79 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:WORKER}): 0 entries, 0 B within 32.03 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:BUCKETS}): 0 entries, 0 B within 30.45 μs
INFO  [2023-01-17 00:50:15,756] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:C_BLOCKS}): 0 entries, 0 B within 30.00 μs
INFO  [2023-01-17 00:50:15,757] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset6, name=testDataset6]
WARN  [2023-01-17 00:50:15,759] com.bakdata.conquery.models.jobs.JobExecutor: Tried to add a job to a closed JobManager: reacting to ForwardToWorker(workerId=testDataset5.worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed, text=RequestConsistency)
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:DATASET}): 0 entries, 0 B within 59.25 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:SECONDARY_IDS}): 0 entries, 0 B within 27.63 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:TABLES}): 0 entries, 0 B within 20.41 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 24.91 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:IMPORTS}): 0 entries, 0 B within 20.23 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:CONCEPTS}): 0 entries, 0 B within 18.54 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:WORKER}): 0 entries, 0 B within 19.74 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:BUCKETS}): 0 entries, 0 B within 19.26 μs
INFO  [2023-01-17 00:50:15,767] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:C_BLOCKS}): 0 entries, 0 B within 18.86 μs
INFO  [2023-01-17 00:50:15,768] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=testDataset6, name=testDataset6]
INFO  [2023-01-17 00:50:15,771] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset5.worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,771] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset5.worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,771] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,780] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
INFO  [2023-01-17 00:50:15,791] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:DATASET}): 0 entries, 0 B within 80.40 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:SECONDARY_IDS}): 0 entries, 0 B within 37.93 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:TABLES}): 0 entries, 0 B within 31.84 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 34.77 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:IMPORTS}): 0 entries, 0 B within 31.13 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:CONCEPTS}): 0 entries, 0 B within 29.22 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:WORKER}): 0 entries, 0 B within 30.52 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:BUCKETS}): 0 entries, 0 B within 29.32 μs
INFO  [2023-01-17 00:50:15,807] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:C_BLOCKS}): 0 entries, 0 B within 28.80 μs
WARN  [2023-01-17 00:50:15,810] com.bakdata.conquery.models.jobs.JobExecutor: Tried to add a job to a closed JobManager: reacting to ForwardToWorker(workerId=testDataset6.worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6, text=RequestConsistency)
INFO  [2023-01-17 00:50:15,811] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:DATASET}): 0 entries, 0 B within 74.08 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:SECONDARY_IDS}): 0 entries, 0 B within 35.93 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:TABLES}): 0 entries, 0 B within 27.82 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 31.23 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:IMPORTS}): 0 entries, 0 B within 42.16 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:CONCEPTS}): 0 entries, 0 B within 26.42 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:WORKER}): 0 entries, 0 B within 27.11 μs
INFO  [2023-01-17 00:50:15,814] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:BUCKETS}): 0 entries, 0 B within 26.45 μs
INFO  [2023-01-17 00:50:15,815] com.bakdata.conquery.io.storage.xodus.stores.CachedStore: 	loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:C_BLOCKS}): 0 entries, 0 B within 27.33 μs
INFO  [2023-01-17 00:50:15,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker testDataset6.worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:15,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker testDataset6.worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:15,824] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:15,909] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
INFO  [2023-01-17 00:50:15,909] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:DATASET}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:TABLES}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:IMPORTS}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:CONCEPTS}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:WORKER}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:BUCKETS}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:C_BLOCKS}
INFO  [2023-01-17 00:50:16,010] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
INFO  [2023-01-17 00:50:16,017] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
INFO  [2023-01-17 00:50:16,058] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:DATASET}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:TABLES}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:IMPORTS}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:CONCEPTS}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:WORKER}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:BUCKETS}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:C_BLOCKS}
INFO  [2023-01-17 00:50:16,158] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
INFO  [2023-01-17 00:50:16,165] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
INFO  [2023-01-17 00:50:16,209] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:DATASET}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:TABLES}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:IMPORTS}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:CONCEPTS}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:WORKER}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:BUCKETS}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:C_BLOCKS}
INFO  [2023-01-17 00:50:16,309] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
INFO  [2023-01-17 00:50:16,316] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
INFO  [2023-01-17 00:50:16,348] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:DATASET}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:TABLES}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:IMPORTS}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:CONCEPTS}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:WORKER}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:BUCKETS}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:C_BLOCKS}
INFO  [2023-01-17 00:50:16,448] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
INFO  [2023-01-17 00:50:16,462] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
INFO  [2023-01-17 00:50:16,549] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:DATASET}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:TABLES}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:IMPORTS}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:CONCEPTS}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:WORKER}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:BUCKETS}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:C_BLOCKS}
INFO  [2023-01-17 00:50:16,641] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
INFO  [2023-01-17 00:50:16,648] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
INFO  [2023-01-17 00:50:16,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:DATASET}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:TABLES}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:IMPORTS}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:CONCEPTS}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:WORKER}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:BUCKETS}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:C_BLOCKS}
INFO  [2023-01-17 00:50:16,783] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
INFO  [2023-01-17 00:50:16,790] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:DATASET}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:SECONDARY_IDS}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:TABLES}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:DICTIONARIES_META}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:IMPORTS}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:CONCEPTS}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:WORKER}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:BUCKETS}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:C_BLOCKS}
INFO  [2023-01-17 00:50:16,825] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
INFO  [2023-01-17 00:50:16,833] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:50:16,833] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:50:16,833] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-17 00:50:16,834] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:50:16,919] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-17 00:50:17,017] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
INFO  [2023-01-17 00:50:17,062] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:DATASET}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:SECONDARY_IDS}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:TABLES}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:DICTIONARIES_META}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:IMPORTS}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:CONCEPTS}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:WORKER}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:BUCKETS}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:C_BLOCKS}
INFO  [2023-01-17 00:50:17,142] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
INFO  [2023-01-17 00:50:17,154] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
INFO  [2023-01-17 00:50:17,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:DATASET}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:SECONDARY_IDS}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:TABLES}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:DICTIONARIES_META}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:IMPORTS}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:CONCEPTS}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:WORKER}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:BUCKETS}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:C_BLOCKS}
INFO  [2023-01-17 00:50:17,329] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
INFO  [2023-01-17 00:50:17,340] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
INFO  [2023-01-17 00:50:17,370] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:DATASET}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:SECONDARY_IDS}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:TABLES}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:DICTIONARIES_META}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:IMPORTS}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:CONCEPTS}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:WORKER}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:BUCKETS}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:C_BLOCKS}
INFO  [2023-01-17 00:50:17,470] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
INFO  [2023-01-17 00:50:17,482] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
INFO  [2023-01-17 00:50:17,543] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:DATASET}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:SECONDARY_IDS}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:TABLES}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:DICTIONARIES_META}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:IMPORTS}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:CONCEPTS}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:WORKER}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:BUCKETS}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:C_BLOCKS}
INFO  [2023-01-17 00:50:17,643] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
INFO  [2023-01-17 00:50:17,659] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
INFO  [2023-01-17 00:50:17,718] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:DATASET}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:SECONDARY_IDS}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:TABLES}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:DICTIONARIES_META}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:IMPORTS}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:CONCEPTS}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:WORKER}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:BUCKETS}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:C_BLOCKS}
INFO  [2023-01-17 00:50:17,818] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
INFO  [2023-01-17 00:50:17,829] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
INFO  [2023-01-17 00:50:17,888] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:DATASET}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:SECONDARY_IDS}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:TABLES}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:DICTIONARIES_META}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:IMPORTS}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:CONCEPTS}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:WORKER}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:BUCKETS}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:C_BLOCKS}
INFO  [2023-01-17 00:50:17,988] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
INFO  [2023-01-17 00:50:17,999] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
INFO  [2023-01-17 00:50:18,023] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:DATASET}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:TABLES}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:IMPORTS}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:CONCEPTS}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:WORKER}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:BUCKETS}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:C_BLOCKS}
INFO  [2023-01-17 00:50:18,024] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
INFO  [2023-01-17 00:50:18,042] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:50:18,042] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:50:18,042] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-17 00:50:18,042] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:50:18,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-17 00:50:18,196] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset1
INFO  [2023-01-17 00:50:18,269] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset1
INFO  [2023-01-17 00:50:18,369] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset1
INFO  [2023-01-17 00:50:18,369] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:DATASET}
INFO  [2023-01-17 00:50:18,369] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,369] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:TABLES}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:IMPORTS}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:CONCEPTS}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:ID_MAPPING_META}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:STRUCTURE}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:18,370] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
INFO  [2023-01-17 00:50:18,380] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset2
INFO  [2023-01-17 00:50:18,430] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset2
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset2
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:DATASET}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:TABLES}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:IMPORTS}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:CONCEPTS}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:ID_MAPPING_META}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:STRUCTURE}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:18,527] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
INFO  [2023-01-17 00:50:18,543] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast RestartTest
INFO  [2023-01-17 00:50:18,602] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow RestartTest
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of RestartTest
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:DATASET}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:TABLES}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:IMPORTS}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:CONCEPTS}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:ID_MAPPING_META}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:STRUCTURE}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:18,633] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
INFO  [2023-01-17 00:50:18,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset3
INFO  [2023-01-17 00:50:18,689] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset3
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset3
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:DATASET}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:TABLES}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:IMPORTS}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:CONCEPTS}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:ID_MAPPING_META}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:STRUCTURE}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:18,788] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
INFO  [2023-01-17 00:50:18,797] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset4
INFO  [2023-01-17 00:50:18,833] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset4
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset4
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:DATASET}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:TABLES}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:IMPORTS}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:CONCEPTS}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:ID_MAPPING_META}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:STRUCTURE}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:18,933] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
INFO  [2023-01-17 00:50:18,942] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset5
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset5
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset5
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:DATASET}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:SECONDARY_IDS}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:TABLES}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:DICTIONARIES_META}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:IMPORTS}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:CONCEPTS}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:ID_MAPPING_META}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:STRUCTURE}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:18,970] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
INFO  [2023-01-17 00:50:18,979] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast testDataset6
INFO  [2023-01-17 00:50:19,025] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow testDataset6
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.worker.Namespace: Closing namespace storage of testDataset6
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:DATASET}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:SECONDARY_IDS}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:TABLES}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:DICTIONARIES_META}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:DICTIONARIES_DATA}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:IMPORTS}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:CONCEPTS}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:ID_MAPPING_META}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:ID_MAPPING_DATA}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:STRUCTURE}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:WORKER_TO_BUCKETS}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:PRIMARY_DICTIONARY}
INFO  [2023-01-17 00:50:19,027] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
INFO  [2023-01-17 00:50:19,039] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
INFO  [2023-01-17 00:50:19,039] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
INFO  [2023-01-17 00:50:19,049] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
INFO  [2023-01-17 00:50:19,049] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
INFO  [2023-01-17 00:50:19,067] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
INFO  [2023-01-17 00:50:19,067] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
INFO  [2023-01-17 00:50:19,077] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
INFO  [2023-01-17 00:50:19,077] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
INFO  [2023-01-17 00:50:19,087] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
INFO  [2023-01-17 00:50:19,087] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
INFO  [2023-01-17 00:50:19,099] com.bakdata.conquery.integration.tests.RestartTest: Restarting
INFO  [2023-01-17 00:50:19,099] com.bakdata.conquery.util.support.TestConquery: Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:50:19]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:DATASET}): 1 entries, 51 B within 1.701 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:SECONDARY_IDS}): 0 entries, 0 B within 84.68 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:TABLES}): 0 entries, 0 B within 72.31 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 67.59 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:IMPORTS}): 0 entries, 0 B within 63.66 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:CONCEPTS}): 0 entries, 0 B within 57.20 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	Done reading Dataset[label=testDataset5, name=testDataset5] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:DATASET}): 1 entries, 51 B within 179.8 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:SECONDARY_IDS}): 0 entries, 0 B within 61.28 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:TABLES}): 0 entries, 0 B within 51.30 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 53.85 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:IMPORTS}): 0 entries, 0 B within 49.77 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:CONCEPTS}): 0 entries, 0 B within 47.04 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	Done reading Dataset[label=testDataset3, name=testDataset3] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:DATASET}): 1 entries, 51 B within 106.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:SECONDARY_IDS}): 0 entries, 0 B within 29.39 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:TABLES}): 0 entries, 0 B within 23.68 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 25.66 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:IMPORTS}): 0 entries, 0 B within 23.13 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:CONCEPTS}): 0 entries, 0 B within 22.49 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	Done reading Dataset[label=testDataset6, name=testDataset6] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 4.117 ms
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 5.996 ms
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 8.773 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:STRUCTURE}): 0 entries, 0 B within 68.78 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:STRUCTURE}): 0 entries, 0 B within 67.60 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:STRUCTURE}): 0 entries, 0 B within 74.09 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:WORKER_TO_BUCKETS}): 1 entries, 12 B within 147.3 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:WORKER_TO_BUCKETS}): 1 entries, 12 B within 116.4 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:WORKER_TO_BUCKETS}): 1 entries, 12 B within 110.9 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6:PRIMARY_DICTIONARY}): 0 entries, 0 B within 53.38 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3:PRIMARY_DICTIONARY}): 0 entries, 0 B within 95.63 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3	DONE reading Storage
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5:PRIMARY_DICTIONARY}): 0 entries, 0 B within 109.1 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:DATASET}): 1 entries, 49 B within 161.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store SECONDARY_IDS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:SECONDARY_IDS}): 1 entries, 70 B within 258.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:DATASET}): 1 entries, 51 B within 128.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:SECONDARY_IDS}): 0 entries, 0 B within 47.67 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:TABLES}): 1 entries, 187 B within 317.6 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:TABLES}): 0 entries, 0 B within 47.97 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 53.31 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:IMPORTS}): 0 entries, 0 B within 38.85 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:CONCEPTS}): 0 entries, 0 B within 37.03 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	Done reading Dataset[label=testDataset4, name=testDataset4] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 241.4 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:STRUCTURE}): 0 entries, 0 B within 40.37 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:WORKER_TO_BUCKETS}): 1 entries, 12 B within 69.99 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4:PRIMARY_DICTIONARY}): 0 entries, 0 B within 40.49 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 1.528 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:DATASET}): 1 entries, 51 B within 200.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:SECONDARY_IDS}): 0 entries, 0 B within 79.53 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:TABLES}): 0 entries, 0 B within 62.04 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 63.67 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:IMPORTS}): 0 entries, 0 B within 56.51 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	BEGIN reading Storage
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:CONCEPTS}): 0 entries, 0 B within 59.71 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	Done reading Dataset[label=testDataset2, name=testDataset2] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:IMPORTS}): 1 entries, 466 B within 5.292 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:DATASET}): 1 entries, 51 B within 196.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:SECONDARY_IDS}): 0 entries, 0 B within 73.37 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 472.2 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:TABLES}): 0 entries, 0 B within 86.12 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:STRUCTURE}): 0 entries, 0 B within 71.55 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 89.14 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:WORKER_TO_BUCKETS}): 1 entries, 12 B within 114.1 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2:PRIMARY_DICTIONARY}): 0 entries, 0 B within 75.30 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:IMPORTS}): 0 entries, 0 B within 81.65 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:CONCEPTS}): 0 entries, 0 B within 98.93 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	Done reading Dataset[label=testDataset1, name=testDataset1] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 305.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:STRUCTURE}): 0 entries, 0 B within 109.4 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:WORKER_TO_BUCKETS}): 1 entries, 12 B within 113.2 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1:PRIMARY_DICTIONARY}): 0 entries, 0 B within 65.50 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:CONCEPTS}): 1 entries, 452 B within 5.638 ms
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	Done reading Dataset[label=RestartTest, name=RestartTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 260.7 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:STRUCTURE}): 0 entries, 0 B within 33.30 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:WORKER_TO_BUCKETS}): 1 entries, 226 B within 194.9 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest:PRIMARY_DICTIONARY}): 1 entries, 97 B within 660.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest	DONE reading Storage
[INFO] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_testDataset6), NamespacedStorage(pathName=dataset_testDataset3), NamespacedStorage(pathName=dataset_testDataset5), NamespacedStorage(pathName=dataset_testDataset4), NamespacedStorage(pathName=dataset_testDataset2), NamespacedStorage(pathName=dataset_testDataset1), NamespacedStorage(pathName=dataset_RestartTest)]
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 2 entries, 328 B within 1.784 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 1 entries, 150 B within 452.7 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 1 entries, 203 B within 430.9 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 1 entries, 523 B within 11.42 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 87.07 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@18b86ce8
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:19]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:19]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_15
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_16
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_17
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:19]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:19]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:19]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:DATASET}): 1 entries, 49 B within 179.9 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store SECONDARY_IDS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:SECONDARY_IDS}): 1 entries, 70 B within 172.2 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:TABLES}): 1 entries, 187 B within 226.5 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 992.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:IMPORTS}): 1 entries, 466 B within 4.705 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:DATASET}): 1 entries, 51 B within 240.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:SECONDARY_IDS}): 0 entries, 0 B within 83.37 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:TABLES}): 0 entries, 0 B within 74.69 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 61.95 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:IMPORTS}): 0 entries, 0 B within 54.70 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:CONCEPTS}): 0 entries, 0 B within 59.12 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	Done reading Dataset[label=testDataset6, name=testDataset6] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:WORKER}): 1 entries, 125 B within 157.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:BUCKETS}): 0 entries, 0 B within 57.15 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6:C_BLOCKS}): 0 entries, 0 B within 53.67 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:CONCEPTS}): 1 entries, 452 B within 6.637 ms
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	Done reading Dataset[label=RestartTest, name=RestartTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:WORKER}): 1 entries, 124 B within 109.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:BUCKETS}): 1 entries, 358 B within 1.505 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36:C_BLOCKS}): 1 entries, 213 B within 183.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:DATASET}): 1 entries, 51 B within 131.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:SECONDARY_IDS}): 0 entries, 0 B within 46.22 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:TABLES}): 0 entries, 0 B within 66.16 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 39.59 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:IMPORTS}): 0 entries, 0 B within 34.90 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:CONCEPTS}): 0 entries, 0 B within 34.29 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	Done reading Dataset[label=testDataset4, name=testDataset4] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:WORKER}): 1 entries, 125 B within 105.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:BUCKETS}): 0 entries, 0 B within 36.15 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593:C_BLOCKS}): 0 entries, 0 B within 45.75 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:DATASET}): 1 entries, 51 B within 143.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:SECONDARY_IDS}): 0 entries, 0 B within 53.36 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:TABLES}): 0 entries, 0 B within 42.15 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.58 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:IMPORTS}): 0 entries, 0 B within 60.40 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:CONCEPTS}): 0 entries, 0 B within 40.07 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	Done reading Dataset[label=testDataset3, name=testDataset3] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:WORKER}): 1 entries, 125 B within 111.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:BUCKETS}): 0 entries, 0 B within 42.26 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3:C_BLOCKS}): 0 entries, 0 B within 39.15 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:DATASET}): 1 entries, 51 B within 141.8 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:SECONDARY_IDS}): 0 entries, 0 B within 45.64 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:TABLES}): 0 entries, 0 B within 38.29 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 39.42 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:IMPORTS}): 0 entries, 0 B within 36.86 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:CONCEPTS}): 0 entries, 0 B within 35.54 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	Done reading Dataset[label=testDataset5, name=testDataset5] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:WORKER}): 1 entries, 125 B within 96.03 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:BUCKETS}): 0 entries, 0 B within 46.38 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516:C_BLOCKS}): 0 entries, 0 B within 36.17 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:DATASET}): 1 entries, 51 B within 154.5 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:SECONDARY_IDS}): 0 entries, 0 B within 45.23 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:TABLES}): 0 entries, 0 B within 36.70 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.11 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:IMPORTS}): 0 entries, 0 B within 34.71 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:CONCEPTS}): 0 entries, 0 B within 49.31 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	Done reading Dataset[label=testDataset2, name=testDataset2] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:WORKER}): 1 entries, 125 B within 99.60 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:BUCKETS}): 0 entries, 0 B within 36.22 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19:C_BLOCKS}): 0 entries, 0 B within 34.95 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:DATASET}): 1 entries, 51 B within 164.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:SECONDARY_IDS}): 0 entries, 0 B within 60.18 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:TABLES}): 0 entries, 0 B within 48.37 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 50.92 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:IMPORTS}): 0 entries, 0 B within 51.01 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:CONCEPTS}): 0 entries, 0 B within 80.00 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	Done reading Dataset[label=testDataset5, name=testDataset5] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:WORKER}): 1 entries, 125 B within 130.9 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:BUCKETS}): 0 entries, 0 B within 48.82 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed:C_BLOCKS}): 0 entries, 0 B within 46.72 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:DATASET}): 1 entries, 51 B within 173.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:SECONDARY_IDS}): 0 entries, 0 B within 50.17 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:TABLES}): 0 entries, 0 B within 88.44 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.91 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:DATASET}): 1 entries, 51 B within 154.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:IMPORTS}): 0 entries, 0 B within 49.40 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:CONCEPTS}): 0 entries, 0 B within 47.33 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	Done reading Dataset[label=testDataset6, name=testDataset6] / com.bakdata.conquery.io.storage.WorkerStorage
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:SECONDARY_IDS}): 0 entries, 0 B within 78.49 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:TABLES}): 0 entries, 0 B within 66.39 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:WORKER}): 1 entries, 125 B within 121.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 62.54 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:BUCKETS}): 0 entries, 0 B within 44.73 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:IMPORTS}): 0 entries, 0 B within 64.83 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db:C_BLOCKS}): 0 entries, 0 B within 58.10 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:CONCEPTS}): 0 entries, 0 B within 48.53 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	Done reading Dataset[label=testDataset4, name=testDataset4] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:WORKER}): 1 entries, 125 B within 126.2 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:BUCKETS}): 0 entries, 0 B within 46.30 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7:C_BLOCKS}): 0 entries, 0 B within 43.85 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:DATASET}): 1 entries, 51 B within 196.4 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:SECONDARY_IDS}): 0 entries, 0 B within 60.14 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:TABLES}): 0 entries, 0 B within 49.02 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 52.85 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:IMPORTS}): 0 entries, 0 B within 45.60 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:CONCEPTS}): 0 entries, 0 B within 47.98 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	Done reading Dataset[label=testDataset1, name=testDataset1] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:WORKER}): 1 entries, 125 B within 133.1 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:BUCKETS}): 0 entries, 0 B within 48.64 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb:C_BLOCKS}): 0 entries, 0 B within 57.27 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:DATASET}): 1 entries, 51 B within 217.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:SECONDARY_IDS}): 0 entries, 0 B within 76.57 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:TABLES}): 0 entries, 0 B within 60.51 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 62.53 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:IMPORTS}): 0 entries, 0 B within 56.82 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:CONCEPTS}): 0 entries, 0 B within 39.95 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	Done reading Dataset[label=testDataset1, name=testDataset1] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:WORKER}): 1 entries, 125 B within 108.7 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:BUCKETS}): 0 entries, 0 B within 39.07 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f:C_BLOCKS}): 0 entries, 0 B within 36.29 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:DATASET}): 1 entries, 51 B within 144.5 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:SECONDARY_IDS}): 0 entries, 0 B within 51.31 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:TABLES}): 0 entries, 0 B within 41.71 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 46.27 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:IMPORTS}): 0 entries, 0 B within 41.93 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:CONCEPTS}): 0 entries, 0 B within 58.89 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	Done reading Dataset[label=testDataset2, name=testDataset2] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:WORKER}): 1 entries, 125 B within 146.0 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:BUCKETS}): 0 entries, 0 B within 44.50 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585:C_BLOCKS}): 0 entries, 0 B within 38.52 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585	DONE reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:DATASET}): 1 entries, 51 B within 147.3 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:SECONDARY_IDS}): 0 entries, 0 B within 50.89 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:TABLES}): 0 entries, 0 B within 43.30 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 44.61 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:IMPORTS}): 0 entries, 0 B within 42.41 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:CONCEPTS}): 0 entries, 0 B within 39.78 μs
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	Done reading Dataset[label=testDataset3, name=testDataset3] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:WORKER}): 1 entries, 125 B within 111.2 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:BUCKETS}): 0 entries, 0 B within 40.35 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377:C_BLOCKS}): 0 entries, 0 B within 37.80 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377	DONE reading Storage
[INFO] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36)), WorkerStorage(worker=NamedImpl(name=worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593)), WorkerStorage(worker=NamedImpl(name=worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516)), WorkerStorage(worker=NamedImpl(name=worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19)), WorkerStorage(worker=NamedImpl(name=worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db)), WorkerStorage(worker=NamedImpl(name=worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f)), WorkerStorage(worker=NamedImpl(name=worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377))]
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 7
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:DATASET}): 1 entries, 49 B within 189.7 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store SECONDARY_IDS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:SECONDARY_IDS}): 1 entries, 70 B within 169.8 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:TABLES}): 1 entries, 187 B within 220.8 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 1.004 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:IMPORTS}): 1 entries, 466 B within 4.582 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:CONCEPTS}): 1 entries, 452 B within 6.055 ms
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	Done reading Dataset[label=RestartTest, name=RestartTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:WORKER}): 1 entries, 124 B within 124.6 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:BUCKETS}): 1 entries, 346 B within 13.42 ms
[DEBUG] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:19]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a:C_BLOCKS}): 1 entries, 213 B within 264.8 μs
[DEBUG] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a	DONE reading Storage
[INFO] [2023-01-17 00:50:19]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6)), WorkerStorage(worker=NamedImpl(name=worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3)), WorkerStorage(worker=NamedImpl(name=worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed)), WorkerStorage(worker=NamedImpl(name=worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7)), WorkerStorage(worker=NamedImpl(name=worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb)), WorkerStorage(worker=NamedImpl(name=worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585)), WorkerStorage(worker=NamedImpl(name=worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a))]
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 7
[DEBUG] [2023-01-17 00:50:19]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:56814 connected, waiting for identity
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:56818 connected, waiting for identity
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56814	Sending worker identity 'worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593'
[INFO] [2023-01-17 00:50:19]	c.b.c.c.ShardNode	/127.0.0.1:56818	Sending worker identity 'worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb'
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:56818` registered.
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:56814` registered.
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Imports of worker testDataset5.worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Buckets of worker testDataset5.worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Imports of worker RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36 are consistent with the manager: 1 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Buckets of worker RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36 are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Imports of worker testDataset4.worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Buckets of worker testDataset4.worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Imports of worker testDataset6.worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Buckets of worker testDataset6.worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Imports of worker testDataset3.worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Buckets of worker testDataset3.worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Imports of worker testDataset2.worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Buckets of worker testDataset2.worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Imports of worker RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a are consistent with the manager: 1 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Buckets of worker RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=RestartTest, name=RestartTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Imports of worker testDataset1.worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Buckets of worker testDataset1.worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Imports of worker testDataset5.worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Buckets of worker testDataset5.worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset5, name=testDataset5]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Imports of worker testDataset1.worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Buckets of worker testDataset1.worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset1, name=testDataset1]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Imports of worker testDataset3.worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Buckets of worker testDataset3.worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset3, name=testDataset3]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Imports of worker testDataset6.worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Buckets of worker testDataset6.worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset6, name=testDataset6]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Imports of worker testDataset2.worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Buckets of worker testDataset2.worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset2, name=testDataset2]	Consistency check was successful
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Imports of worker testDataset4.worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Buckets of worker testDataset4.worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:19]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=testDataset4, name=testDataset4]	Consistency check was successful
[WARN] [2023-01-17 00:50:19]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:20]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:20]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:20]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:20]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-17 00:50:20]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest17490523365462791536/tmp_RestartTest for Support
[INFO] [2023-01-17 00:50:20]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:20]	c.b.c.i.t.RestartTest		Restart complete
[INFO] [2023-01-17 00:50:20]	c.b.c.i.j.AbstractQueryEngineTest		SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
[INFO] [2023-01-17 00:50:20]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[RestartTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:20]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[5f1f2e1d-37fe-45eb-976a-c6b4c118d002] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:20 +0000] "POST /api/datasets/RestartTest/queries HTTP/1.1" 201 1116 "-" "Conquery (test client)" 68
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.ExecuteQuery	Worker[RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36, /127.0.0.1:56814]	Started ConceptQuery RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.ExecuteQuery	Worker[RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a, /127.0.0.1:56818]	Started ConceptQuery RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002
[DEBUG] [2023-01-17 00:50:20]	c.b.c.m.q.QueryExecutor	Worker[RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36, /127.0.0.1:56814]	QueryPlan for Query[RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = RestartTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=connector, name=connector], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:20]	c.b.c.m.q.QueryExecutor	Worker[RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a, /127.0.0.1:56818]	QueryPlan for Query[RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = RestartTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=connector, name=connector], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:20]	c.b.c.m.q.r.ShardResult		FINISHED Query[RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002] with 0 results within PT0.001004S
[INFO] [2023-01-17 00:50:20]	c.b.c.m.q.r.ShardResult		FINISHED Query[RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002] with 2 results within PT0.00285S
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=RestartTest, name=RestartTest]	Received ShardResult(queryId=RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002, workerId=RestartTest.worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a, startTime=2023-01-17T00:50:20.292832, finishTime=2023-01-17T00:50:20.293836) of size 0
[DEBUG] [2023-01-17 00:50:20]	c.b.c.m.q.ManagedQuery	Dataset[label=RestartTest, name=RestartTest]	Received Result[size=0] for Query[RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=RestartTest, name=RestartTest]	Received ShardResult(queryId=RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002, workerId=RestartTest.worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36, startTime=2023-01-17T00:50:20.291299, finishTime=2023-01-17T00:50:20.294149) of size 2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.m.q.ManagedQuery	Dataset[label=RestartTest, name=RestartTest]	Received Result[size=2] for Query[RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.e.ManagedExecution	Dataset[label=RestartTest, name=RestartTest]	DONE 5f1f2e1d-37fe-45eb-976a-c6b4c118d002 ManagedQuery within PT0.059794S
127.0.0.1 - - [17/Jan/2023:00:50:20 +0000] "GET /api/datasets/RestartTest/queries/RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002 HTTP/1.1" 200 1352 "-" "Conquery (test client)" 7
[INFO] [2023-01-17 00:50:20]	c.b.c.r.a.ResultCsvResource	user.SUPERUSER@SUPERUSER	Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=RestartTest, name=RestartTest], queryId=5f1f2e1d-37fe-45eb-976a-c6b4c118d002, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:50:20.234782, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7a2ad647[Count = 0], startTime=2023-01-17T00:50:20.239670, finishTime=2023-01-17T00:50:20.299464, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@52acaaec), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@16a1a755], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4d1ac6f0], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@612eb9fa, com.bakdata.conquery.models.query.ColumnDescriptor@56e294f9]) download on dataset Dataset[label=RestartTest, name=RestartTest] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
[INFO] [2023-01-17 00:50:20]	c.b.c.i.r.c.ResultCsvProcessor	SUPERUSER@SUPERUSER	Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=RestartTest, name=RestartTest], queryId=5f1f2e1d-37fe-45eb-976a-c6b4c118d002, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:50:20.234782, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7a2ad647[Count = 0], startTime=2023-01-17T00:50:20.239670, finishTime=2023-01-17T00:50:20.299464, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@52acaaec), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RestartTest)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@16a1a755], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4d1ac6f0], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@612eb9fa, com.bakdata.conquery.models.query.ColumnDescriptor@56e294f9]) on dataset Dataset[label=RestartTest, name=RestartTest]
127.0.0.1 - - [17/Jan/2023:00:50:20 +0000] "GET /api/datasets/RestartTest/result/RestartTest.5f1f2e1d-37fe-45eb-976a-c6b4c118d002.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 6
[INFO] [2023-01-17 00:50:20]	c.b.c.i.j.AbstractQueryEngineTest		INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset1
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset1, name=testDataset1]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset1, name=testDataset1]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset1
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[INFO] [2023-01-17 00:50:20]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset1_2953193e-d649-4956-a4b8-621cf7cd4bbb
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset1_cb5cfadf-1d4e-4200-85ad-792fb5d8d76f
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset1
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset2
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset2, name=testDataset2]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset2, name=testDataset2]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[INFO] [2023-01-17 00:50:20]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset2_3fc9b51e-fadf-4b31-a2b9-96ab05b39e19
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset2_c0c59d0b-de95-45e0-ac47-1302784d3585
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset2
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset3, name=testDataset3]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset3, name=testDataset3]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[INFO] [2023-01-17 00:50:20]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset3_9cfcbb20-8c90-46dc-86b8-712d9383b377
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset3_9d6a1983-0099-4b9c-8339-c3b36ff7dee3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset3
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset4
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset4, name=testDataset4]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset4, name=testDataset4]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[INFO] [2023-01-17 00:50:20]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset4_b4383dd5-9c8e-4ad6-ace6-56dd250c3593
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset4_6b42c923-9fb6-4823-8d7d-9e95c7ac72f7
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset4
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset5
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset5, name=testDataset5]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[INFO] [2023-01-17 00:50:20]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset5, name=testDataset5]
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[INFO] [2023-01-17 00:50:20]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset5_2d73f6b1-1fe5-4c9c-bbb6-a2e04d182516
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset5_bfc99602-1646-4a1e-affb-0f618eafc4ed
[INFO] [2023-01-17 00:50:20]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[DEBUG] [2023-01-17 00:50:20]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[INFO] [2023-01-17 00:50:20]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset5
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor		Closing Job Manager fast testDataset6
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=testDataset6, name=testDataset6]
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=testDataset6, name=testDataset6]
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor		Closing Job Manager slow testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[INFO] [2023-01-17 00:50:21]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_testDataset6_40fcf188-5f08-4204-945d-f1d86a654da6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[INFO] [2023-01-17 00:50:21]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_testDataset6_b56cf2df-5a69-4578-93f9-1a89b81c64db
[INFO] [2023-01-17 00:50:21]	c.b.c.m.w.Namespace		Removing namespace storage of testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[INFO] [2023-01-17 00:50:21]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_testDataset6
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor		Closing Job Manager fast RestartTest
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RestartTest, name=RestartTest]
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RestartTest, name=RestartTest]
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor		Closing Job Manager slow RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[INFO] [2023-01-17 00:50:21]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RestartTest_a51ce307-d72d-4bfa-bbb4-c2d2166bfa1a
[INFO] [2023-01-17 00:50:21]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[INFO] [2023-01-17 00:50:21]	c.b.c.m.w.Namespace		Removing namespace storage of RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[INFO] [2023-01-17 00:50:21]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RestartTest_a90a3ade-4ce3-452d-92a9-c7b8ce4f1d36
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[INFO] [2023-01-17 00:50:21]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RestartTest
[INFO] [2023-01-17 00:50:21]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:21]	c.b.c.i.IntegrationTest$Wrapper	RestartTest	SUCCESS integration test RestartTest
[INFO] [2023-01-17 00:50:21]	c.b.c.i.IntegrationTest$Wrapper	ReusedQueryTest	STARTING integration test ReusedQueryTest
[INFO] [2023-01-17 00:50:21]	c.b.c.u.s.TestConquery	ReusedQueryTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:DATASET}): 0 entries, 0 B within 182.4 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:SECONDARY_IDS}): 0 entries, 0 B within 100.3 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:TABLES}): 0 entries, 0 B within 80.85 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 86.33 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:IMPORTS}): 0 entries, 0 B within 80.93 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:CONCEPTS}): 0 entries, 0 B within 79.67 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.NamespacedStorage	ReusedQueryTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 83.63 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:STRUCTURE}): 0 entries, 0 B within 79.70 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 77.62 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	ReusedQueryTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	ReusedQueryTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 53.22 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:DATASET}): 0 entries, 0 B within 108.3 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:SECONDARY_IDS}): 0 entries, 0 B within 35.21 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:DATASET}): 0 entries, 0 B within 208.2 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:TABLES}): 0 entries, 0 B within 31.24 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:SECONDARY_IDS}): 0 entries, 0 B within 38.60 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 50.97 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:TABLES}): 0 entries, 0 B within 48.15 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:IMPORTS}): 0 entries, 0 B within 34.19 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.72 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:CONCEPTS}): 0 entries, 0 B within 40.36 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:IMPORTS}): 0 entries, 0 B within 42.99 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:WORKER}): 0 entries, 0 B within 45.12 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:CONCEPTS}): 0 entries, 0 B within 42.52 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:BUCKETS}): 0 entries, 0 B within 42.15 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:WORKER}): 0 entries, 0 B within 42.51 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8:C_BLOCKS}): 0 entries, 0 B within 41.54 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:BUCKETS}): 0 entries, 0 B within 32.92 μs
[DEBUG] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:21]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae:C_BLOCKS}): 0 entries, 0 B within 23.65 μs
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Imports of worker ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Buckets of worker ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Imports of worker ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Buckets of worker ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ReusedQueryTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:21]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:21]	c.b.c.r.a.r.AdminDatasetProcessor	ReusedQueryTest	Received new SecondaryId[ReusedQueryTest.secondary]
[INFO] [2023-01-17 00:50:21]	c.b.c.r.a.r.AdminDatasetProcessor	ReusedQueryTest	Received new SecondaryId[ReusedQueryTest.ignored]
[INFO] [2023-01-17 00:50:21]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received update of SecondaryId ReusedQueryTest.secondary
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received update of SecondaryId ReusedQueryTest.secondary
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received update of SecondaryId ReusedQueryTest.ignored
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received update of SecondaryId ReusedQueryTest.ignored
[INFO] [2023-01-17 00:50:21]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received update of Table ReusedQueryTest.table
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received update of Table ReusedQueryTest.table
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received update of Table ReusedQueryTest.table2
[INFO] [2023-01-17 00:50:21]	c.b.c.m.m.n.s.UpdateTable	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received update of Table ReusedQueryTest.table2
[INFO] [2023-01-17 00:50:22]	c.b.c.u.s.TestConquery	ReusedQueryTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Updating Concept[ReusedQueryTest.concept]
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Updating Concept[ReusedQueryTest.concept]
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand	ReusedQueryTest	Preprocessing from command line config.
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)	Required to preprocess 465 B in total
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)
██████████████████████████▌                       ▌  53%	est. time remaining: 0.053508256s[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[ignored] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=a, suffix=a)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn			ignored: StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=a, suffix=a) -> StringTypeSingleton(singleValue=a)
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.StringParser		Reduced strings by the 'f_' prefix and '' suffix
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.StringParser			Chosen encoding is Base16LowerCase
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.StringParser			Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@77775ded(est. 100 B)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn			sid: StringParser(super=Parser(lines=6, nullLines=1), encoding=Base16LowerCase, prefix=f_, suffix=) -> StringTypePrefixSuffix(subType=StringTypeEncoded(encoding=Base16LowerCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=4], numberType=ByteArrayStore())), prefix=f_, suffix=)
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.RealParser		Max ULP = 1.1920928955078125E-7
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn			value: RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7) -> DoubleArrayStore()
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@61b390e3), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@39dff81a), dateReader=com.bakdata.conquery.util.DateReader@317e08f9, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table, name=table]:table[0/content.csv]		datum: DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@61b390e3), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@39dff81a), dateReader=com.bakdata.conquery.util.DateReader@317e08f9, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false) -> DateRangeTypeDateRange(minStore=IntegerDateStore(store=ShortArrayStore()), maxStore=IntegerDateStore(store=ShortArrayStore()))
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing header
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table, name=table]:table[0/content.csv]	Writing data
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=table, name=table]:table[0/content.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000929205s[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.RealParser	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Max ULP = 9.5367431640625E-7
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]		value: RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7) -> DoubleArrayStore()
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.StringParser		Reduced strings by the 'f_' prefix and '' suffix
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@58ea108e), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@3e69212b), dateReader=com.bakdata.conquery.util.DateReader@99bb13c, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.StringParser			Chosen encoding is Base16LowerCase
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn			datum: DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@58ea108e), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@3e69212b), dateReader=com.bakdata.conquery.util.DateReader@99bb13c, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false) -> DateRangeTypeDateRange(minStore=IntegerDateStore(store=ShortArrayStore()), maxStore=IntegerDateStore(store=ShortArrayStore()))
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.p.s.StringParser			Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@155d6480(est. 83 B)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.PPColumn			sid: StringParser(super=Parser(lines=6, nullLines=1), encoding=Base16LowerCase, prefix=f_, suffix=) -> StringTypePrefixSuffix(subType=StringTypeEncoded(encoding=Base16LowerCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=3], numberType=ByteArrayStore())), prefix=f_, suffix=)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	Writing data
[INFO] [2023-01-17 00:50:22]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=table2, name=table2]:table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:22]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ReusedQueryTest/table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing table into ReusedQueryTest.table
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /admin/datasets/ReusedQueryTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ReusedQueryTest%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 20
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.w.Namespace	Job Manager slow ReusedQueryTest	Assigning Bucket[0] to Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received new WorkerInformation(size = 0,dataset = ReusedQueryTest)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received new WorkerInformation(size = 1,dataset = ReusedQueryTest)
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:22]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received Dictionary[ReusedQueryTest.table#ReusedQueryTest$2etable$2esid] of size 4.
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received Dictionary[ReusedQueryTest.table#ReusedQueryTest$2etable$2esid] of size 4.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Start sending 1 Buckets
[WARN] [2023-01-17 00:50:22]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	One or more Children are not done yet
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing table2 into ReusedQueryTest.table2
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received Import[ReusedQueryTest.table.table], containing 6 entries.
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Mapped 0 new ids
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Updating bucket assignments.
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /admin/datasets/ReusedQueryTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ReusedQueryTest%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Importing Dictionaries
[INFO] [2023-01-17 00:50:22]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received new WorkerInformation(size = 1,dataset = ReusedQueryTest)
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received new WorkerInformation(size = 0,dataset = ReusedQueryTest)
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:22]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	Max cannot be decreased.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received Import[ReusedQueryTest.table.table], containing 6 entries.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.j.ImportJob	Job Manager slow ReusedQueryTest	Start sending 1 Buckets
[WARN] [2023-01-17 00:50:22]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ReusedQueryTest	One or more Children are not done yet
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received Dictionary[ReusedQueryTest.table2#ReusedQueryTest$2etable2$2esid] of size 3.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ImportBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received ReusedQueryTest.table.table.0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Adding Bucket[ReusedQueryTest.table.table.0]
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received Dictionary[ReusedQueryTest.table2#ReusedQueryTest$2etable2$2esid] of size 3.
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.s.WorkerStorage		Adding CBlock[ReusedQueryTest.table.table.0.ReusedQueryTest.concept.connector1]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received Import[ReusedQueryTest.table2.table2], containing 6 entries.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.AddImport	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Received Import[ReusedQueryTest.table2.table2], containing 6 entries.
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ImportBucket	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Received ReusedQueryTest.table2.table2.0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.s.WorkerStorage	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Adding Bucket[ReusedQueryTest.table2.table2.0]
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.s.WorkerStorage		Adding CBlock[ReusedQueryTest.table2.table2.0.ReusedQueryTest.concept.connector2]
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[67336252-01f8-410c-bae2-e305fe97a3e8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1594 "-" "Conquery (test client)" 22
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@7276ae75`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@1e073ead`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 0 results within PT0.005936S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.582683, finishTime=2023-01-17T00:50:22.588619) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 2 results within PT0.01465S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.582594, finishTime=2023-01-17T00:50:22.597244) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 67336252-01f8-410c-bae2-e305fe97a3e8 ManagedQuery within PT0.029874S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8 HTTP/1.1" 200 1846 "-" "Conquery (test client)" 10
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[b0fffbe4-567e-4c5f-8dd3-9ec932724a7c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started ConceptQuery ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started ConceptQuery ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1013 "-" "Conquery (test client)" 13
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c] with 0 results within PT0.003316S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.640587, finishTime=2023-01-17T00:50:22.643903) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c] with 2 results within PT0.004582S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.640655, finishTime=2023-01-17T00:50:22.645237) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE b0fffbe4-567e-4c5f-8dd3-9ec932724a7c ManagedQuery within PT0.010308S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.b0fffbe4-567e-4c5f-8dd3-9ec932724a7c HTTP/1.1" 200 1265 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	User[user.SUPERUSER@SUPERUSER] reexecuted Query[ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ReusedQueryTest], queryId=67336252-01f8-410c-bae2-e305fe97a3e8, label=concept	@§$, creationTime=2023-01-17T00:50:22.568387, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@283e1649[Count = 0], startTime=2023-01-17T00:50:22.572021, finishTime=2023-01-17T00:50:22.601895, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@723f021e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@16a1a755], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@4d1ac6f0], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=null, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@50dd4adc, com.bakdata.conquery.models.query.ColumnDescriptor@2319ff7e, com.bakdata.conquery.models.query.ColumnDescriptor@7bcd390a])]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[67336252-01f8-410c-bae2-e305fe97a3e8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@66bb92ec`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 0 results within PT0.000544S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.687650, finishTime=2023-01-17T00:50:22.688194) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@3b98ce3`
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8/reexecute HTTP/1.1" 200 1614 "-" "Conquery (test client)" 8
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 2 results within PT0.003662S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.688341, finishTime=2023-01-17T00:50:22.692003) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 67336252-01f8-410c-bae2-e305fe97a3e8 ManagedQuery within PT0.010818S
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[67336252-01f8-410c-bae2-e305fe97a3e8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@2737286b`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@61fa15da`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 0 results within PT0.000539S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1613 "-" "Conquery (test client)" 7
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.712379, finishTime=2023-01-17T00:50:22.712918) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 2 results within PT0.003257S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.712197, finishTime=2023-01-17T00:50:22.715454) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 67336252-01f8-410c-bae2-e305fe97a3e8 ManagedQuery within PT0.006794S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8 HTTP/1.1" 200 1845 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[7710ea47-ae7b-4324-9209-95e73ba75380] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1607 "-" "Conquery (test client)" 8
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@390cd08c`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@31a17046`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380] with 0 results within PT0.001593S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.753970, finishTime=2023-01-17T00:50:22.755563) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380] with 1 results within PT0.004967S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.754223, finishTime=2023-01-17T00:50:22.759190) of size 1
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=1] for Query[ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 7710ea47-ae7b-4324-9209-95e73ba75380 ManagedQuery within PT0.013465S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.7710ea47-ae7b-4324-9209-95e73ba75380 HTTP/1.1" 200 1859 "-" "Conquery (test client)" 4
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[67336252-01f8-410c-bae2-e305fe97a3e8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@5aea3d62`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@645d90d7`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 0 results within PT0.000558S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1614 "-" "Conquery (test client)" 25
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.813084, finishTime=2023-01-17T00:50:22.813642) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 2 results within PT0.002807S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.812985, finishTime=2023-01-17T00:50:22.815792) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 67336252-01f8-410c-bae2-e305fe97a3e8 ManagedQuery within PT0.025298S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8 HTTP/1.1" 200 1846 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[67336252-01f8-410c-bae2-e305fe97a3e8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@285e7c8`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@69ad9de9`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 0 results within PT0.000622S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1613 "-" "Conquery (test client)" 7
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.860572, finishTime=2023-01-17T00:50:22.861194) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8] with 2 results within PT0.002952S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.860385, finishTime=2023-01-17T00:50:22.863337) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 67336252-01f8-410c-bae2-e305fe97a3e8 ManagedQuery within PT0.016036S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.67336252-01f8-410c-bae2-e305fe97a3e8 HTTP/1.1" 200 1845 "-" "Conquery (test client)" 3
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started ConceptQuery ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started ConceptQuery ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table, name=table]])])), validityDateColumn=Column(id = ReusedQueryTest.table.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector1, name=connector1], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[RealFilterNode(super=NumberFilterNode(super=FilterNode(filterValue=[1.0]), column=Column(id = ReusedQueryTest.table2.value, type = REAL)))], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=table2, name=table2]])])), validityDateColumn=Column(id = ReusedQueryTest.table2.datum, type = DATE_RANGE))), table=CQTable(filters=[FilterValue.CQRealRangeFilter(super=FilterValue(value=[1]))], selects=[], connector=ConceptTreeConnector[label=connector2, name=connector2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d] with 0 results within PT0.000734S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1014 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d] with 2 results within PT0.001476S
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.915821, finishTime=2023-01-17T00:50:22.916555) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.915395, finishTime=2023-01-17T00:50:22.916871) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d ManagedQuery within PT0.004885S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.4fa4914e-e42a-41d0-a57a-07ac7c6c5e3d HTTP/1.1" 200 1265 "-" "Conquery (test client)" 3
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ReusedQueryTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[a684507a-b8ff-4cc4-b4f8-46a3dd45d842] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@872d299`
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@41fd58d6`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842] with 0 results within PT0.000578S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 1182 "-" "Conquery (test client)" 9
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.951105, finishTime=2023-01-17T00:50:22.951683) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842] with 2 results within PT0.002102S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.950771, finishTime=2023-01-17T00:50:22.952873) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE a684507a-b8ff-4cc4-b4f8-46a3dd45d842 ManagedQuery within PT0.006616S
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.a684507a-b8ff-4cc4-b4f8-46a3dd45d842 HTTP/1.1" 200 1432 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:22]	c.b.c.a.QueryProcessor	user.shareholder	Query posted on Dataset[ReusedQueryTest] by User[{user.shareholder].
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.ExecutionManager	user.shareholder	Executing Query[547edda6-685d-4c33-8f9a-cbfce7150851] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ReusedQueryTest))]]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	Started SecondaryIdQuery ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Started SecondaryIdQuery ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851
[WARN] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	Entities for query are empty
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, /127.0.0.1:56818]	QueryPlan for Query[ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@6ca7e616`
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.QueryExecutor	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	QueryPlan for Query[ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851] = `com.bakdata.conquery.models.query.queryplan.SecondaryIdQueryPlan@96026d5`
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult	Worker[ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, /127.0.0.1:56814]	FINISHED Query[ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851] with 0 results within PT0.000664S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851, workerId=ReusedQueryTest.worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae, startTime=2023-01-17T00:50:22.991250, finishTime=2023-01-17T00:50:22.991914) of size 0
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=0] for Query[ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851]
127.0.0.1 - - [17/Jan/2023:00:50:22 +0000] "POST /api/datasets/ReusedQueryTest/queries HTTP/1.1" 201 953 "-" "Conquery (test client)" 8
[DEBUG] [2023-01-17 00:50:22]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:22]	c.b.c.m.q.r.ShardResult		FINISHED Query[ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851] with 2 results within PT0.002201S
[INFO] [2023-01-17 00:50:22]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ReusedQueryTest]	Received ShardResult(queryId=ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851, workerId=ReusedQueryTest.worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8, startTime=2023-01-17T00:50:22.991204, finishTime=2023-01-17T00:50:22.993405) of size 2
[DEBUG] [2023-01-17 00:50:22]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ReusedQueryTest]	Received Result[size=2] for Query[ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851]
[INFO] [2023-01-17 00:50:22]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ReusedQueryTest]	DONE 547edda6-685d-4c33-8f9a-cbfce7150851 ManagedQuery within PT0.005373S
127.0.0.1 - - [17/Jan/2023:00:50:23 +0000] "GET /api/datasets/ReusedQueryTest/queries/ReusedQueryTest.547edda6-685d-4c33-8f9a-cbfce7150851 HTTP/1.1" 200 968 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ReusedQueryTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ReusedQueryTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=ReusedQueryTest, name=ReusedQueryTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[INFO] [2023-01-17 00:50:23]	c.b.c.m.w.Namespace		Removing namespace storage of ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ReusedQueryTest_76120659-53a9-42da-9154-e514dd8922ae
[INFO] [2023-01-17 00:50:23]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ReusedQueryTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[INFO] [2023-01-17 00:50:23]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ReusedQueryTest_dab62d02-0564-40cb-a2da-6d1e2a584bc8
[INFO] [2023-01-17 00:50:23]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:23]	c.b.c.i.IntegrationTest$Wrapper	ReusedQueryTest	SUCCESS integration test ReusedQueryTest
[INFO] [2023-01-17 00:50:23]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingOnGroupTest	STARTING integration test RoleHandlingOnGroupTest
[INFO] [2023-01-17 00:50:23]	c.b.c.u.s.TestConquery	RoleHandlingOnGroupTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:DATASET}): 0 entries, 0 B within 151.4 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:SECONDARY_IDS}): 0 entries, 0 B within 77.49 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:TABLES}): 0 entries, 0 B within 67.02 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 69.43 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:IMPORTS}): 0 entries, 0 B within 64.67 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:CONCEPTS}): 0 entries, 0 B within 64.67 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.NamespacedStorage	RoleHandlingOnGroupTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 86.92 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:STRUCTURE}): 0 entries, 0 B within 64.70 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 62.75 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingOnGroupTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingOnGroupTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 65.22 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:DATASET}): 0 entries, 0 B within 76.22 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:SECONDARY_IDS}): 0 entries, 0 B within 23.71 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:TABLES}): 0 entries, 0 B within 19.64 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.32 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:IMPORTS}): 0 entries, 0 B within 19.69 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:CONCEPTS}): 0 entries, 0 B within 33.74 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:WORKER}): 0 entries, 0 B within 34.12 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:BUCKETS}): 0 entries, 0 B within 32.15 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0:C_BLOCKS}): 0 entries, 0 B within 41.93 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:DATASET}): 0 entries, 0 B within 67.74 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:SECONDARY_IDS}): 0 entries, 0 B within 35.93 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:TABLES}): 0 entries, 0 B within 29.60 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 34.08 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:IMPORTS}): 0 entries, 0 B within 38.20 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:CONCEPTS}): 0 entries, 0 B within 32.29 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:WORKER}): 0 entries, 0 B within 30.18 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:BUCKETS}): 0 entries, 0 B within 30.45 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f:C_BLOCKS}): 0 entries, 0 B within 29.09 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Imports of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Buckets of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Imports of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Buckets of worker RoleHandlingOnGroupTest.worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingOnGroupTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:23]	c.b.c.u.s.TestConquery	RoleHandlingOnGroupTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:23]	c.b.c.m.a.AuthorizationController	RoleHandlingOnGroupTest	Security manager registered
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.MetaStorage	RoleHandlingOnGroupTest	Remove User = user.user
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	RoleHandlingOnGroupTest	Closing Job Manager fast RoleHandlingOnGroupTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RoleHandlingOnGroupTest, name=RoleHandlingOnGroupTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	RoleHandlingOnGroupTest	Closing Job Manager slow RoleHandlingOnGroupTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[INFO] [2023-01-17 00:50:23]	c.b.c.m.w.Namespace	RoleHandlingOnGroupTest	Removing namespace storage of RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	RoleHandlingOnGroupTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.c.XodusStoreFactory	RoleHandlingOnGroupTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingOnGroupTest
[INFO] [2023-01-17 00:50:23]	c.b.c.u.s.TestConquery	RoleHandlingOnGroupTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[INFO] [2023-01-17 00:50:23]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingOnGroupTest_59d7cf93-8894-4aac-84f7-1e076e3cf2b0
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[INFO] [2023-01-17 00:50:23]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingOnGroupTest_458640b6-d5e7-464c-8bcb-e1a1d06f4d9f
[INFO] [2023-01-17 00:50:23]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingOnGroupTest	SUCCESS integration test RoleHandlingOnGroupTest
[INFO] [2023-01-17 00:50:23]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingTest	STARTING integration test RoleHandlingTest
[INFO] [2023-01-17 00:50:23]	c.b.c.u.s.TestConquery	RoleHandlingTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:DATASET}): 0 entries, 0 B within 129.6 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:SECONDARY_IDS}): 0 entries, 0 B within 60.30 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:TABLES}): 0 entries, 0 B within 51.34 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 56.69 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:IMPORTS}): 0 entries, 0 B within 50.01 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:CONCEPTS}): 0 entries, 0 B within 69.88 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.NamespacedStorage	RoleHandlingTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 56.92 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:STRUCTURE}): 0 entries, 0 B within 50.00 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 56.36 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	RoleHandlingTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	RoleHandlingTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 48.81 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:DATASET}): 0 entries, 0 B within 73.39 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:SECONDARY_IDS}): 0 entries, 0 B within 24.67 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:TABLES}): 0 entries, 0 B within 22.02 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 25.29 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:IMPORTS}): 0 entries, 0 B within 20.95 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:CONCEPTS}): 0 entries, 0 B within 20.00 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:WORKER}): 0 entries, 0 B within 20.67 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:BUCKETS}): 0 entries, 0 B within 18.57 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6:C_BLOCKS}): 0 entries, 0 B within 19.06 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Imports of worker RoleHandlingTest.worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Buckets of worker RoleHandlingTest.worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:DATASET}): 0 entries, 0 B within 91.13 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:SECONDARY_IDS}): 0 entries, 0 B within 39.96 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:TABLES}): 0 entries, 0 B within 34.74 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 38.09 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:IMPORTS}): 0 entries, 0 B within 33.13 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:CONCEPTS}): 0 entries, 0 B within 31.84 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:WORKER}): 0 entries, 0 B within 33.16 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:BUCKETS}): 0 entries, 0 B within 63.92 μs
[DEBUG] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8:C_BLOCKS}): 0 entries, 0 B within 34.47 μs
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Imports of worker RoleHandlingTest.worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Buckets of worker RoleHandlingTest.worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleHandlingTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:23]	c.b.c.u.s.TestConquery	RoleHandlingTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:23]	c.b.c.m.a.AuthorizationController	RoleHandlingTest	Security manager registered
[INFO] [2023-01-17 00:50:23]	c.b.c.i.s.MetaStorage	RoleHandlingTest	Remove User = user.user
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	RoleHandlingTest	Closing Job Manager fast RoleHandlingTest
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RoleHandlingTest, name=RoleHandlingTest]
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[INFO] [2023-01-17 00:50:23]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	RoleHandlingTest	Closing Job Manager slow RoleHandlingTest
[INFO] [2023-01-17 00:50:24]	c.b.c.m.w.Namespace	RoleHandlingTest	Removing namespace storage of RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleHandlingTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[INFO] [2023-01-17 00:50:24]	c.b.c.m.c.XodusStoreFactory	RoleHandlingTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleHandlingTest
[INFO] [2023-01-17 00:50:24]	c.b.c.u.s.TestConquery	RoleHandlingTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[INFO] [2023-01-17 00:50:24]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleHandlingTest_b491a041-0f64-4bc2-a88f-ee9a254d8de6
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[INFO] [2023-01-17 00:50:24]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleHandlingTest_99b36e7e-6b24-4042-8d41-111f620770d8
[INFO] [2023-01-17 00:50:24]	c.b.c.i.IntegrationTest$Wrapper	RoleHandlingTest	SUCCESS integration test RoleHandlingTest
[INFO] [2023-01-17 00:50:24]	c.b.c.i.IntegrationTest$Wrapper	RoleUITest	STARTING integration test RoleUITest
[INFO] [2023-01-17 00:50:24]	c.b.c.u.s.TestConquery	RoleUITest	Setting up dataset
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:DATASET}): 0 entries, 0 B within 221.7 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:SECONDARY_IDS}): 0 entries, 0 B within 78.76 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:TABLES}): 0 entries, 0 B within 91.51 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 75.59 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:IMPORTS}): 0 entries, 0 B within 64.35 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:CONCEPTS}): 0 entries, 0 B within 67.27 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.NamespacedStorage	RoleUITest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 64.77 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:STRUCTURE}): 0 entries, 0 B within 72.75 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 66.81 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	RoleUITest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	RoleUITest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 64.72 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=RoleUITest, name=RoleUITest]
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=RoleUITest, name=RoleUITest]
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:DATASET}): 0 entries, 0 B within 78.73 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:SECONDARY_IDS}): 0 entries, 0 B within 26.36 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:TABLES}): 0 entries, 0 B within 22.07 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 35.78 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:IMPORTS}): 0 entries, 0 B within 86.87 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:CONCEPTS}): 0 entries, 0 B within 71.61 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:WORKER}): 0 entries, 0 B within 36.00 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:BUCKETS}): 0 entries, 0 B within 32.30 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648:C_BLOCKS}): 0 entries, 0 B within 31.66 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Imports of worker RoleUITest.worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Buckets of worker RoleUITest.worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:DATASET}): 0 entries, 0 B within 100.7 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:SECONDARY_IDS}): 0 entries, 0 B within 66.04 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:TABLES}): 0 entries, 0 B within 72.23 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 54.41 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:IMPORTS}): 0 entries, 0 B within 44.96 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:CONCEPTS}): 0 entries, 0 B within 33.14 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:WORKER}): 0 entries, 0 B within 24.67 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:BUCKETS}): 0 entries, 0 B within 28.78 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea:C_BLOCKS}): 0 entries, 0 B within 37.83 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Imports of worker RoleUITest.worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Buckets of worker RoleUITest.worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=RoleUITest]	Consistency check was successful
[INFO] [2023-01-17 00:50:24]	c.b.c.u.s.TestConquery	RoleUITest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:24]	c.b.c.m.a.AuthorizationController	RoleUITest	Security manager registered
127.0.0.1 - - [17/Jan/2023:00:50:24 +0000] "GET /admin/roles/role.testMandatorName HTTP/1.1" 200 197 "-" "Conquery (test client)" 9
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.MetaStorage	RoleUITest	Remove User = user.testUser@test$2ede
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	RoleUITest	Closing Job Manager fast RoleUITest
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=RoleUITest, name=RoleUITest]
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=RoleUITest, name=RoleUITest]
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	RoleUITest	Closing Job Manager slow RoleUITest
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[INFO] [2023-01-17 00:50:24]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[INFO] [2023-01-17 00:50:24]	c.b.c.m.w.Namespace	RoleUITest	Removing namespace storage of RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	RoleUITest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[INFO] [2023-01-17 00:50:24]	c.b.c.m.c.XodusStoreFactory	RoleUITest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_RoleUITest
[INFO] [2023-01-17 00:50:24]	c.b.c.u.s.TestConquery	RoleUITest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[INFO] [2023-01-17 00:50:24]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_RoleUITest_272996c2-54c3-4a28-9fdf-1b4d5178a648
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[INFO] [2023-01-17 00:50:24]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_RoleUITest_b048ab4f-553f-4dd1-8d67-a1e34eb783ea
[INFO] [2023-01-17 00:50:24]	c.b.c.i.IntegrationTest$Wrapper	RoleUITest	SUCCESS integration test RoleUITest
[INFO] [2023-01-17 00:50:24]	c.b.c.i.IntegrationTest$Wrapper	SecondaryIdEndpointTest	STARTING integration test SecondaryIdEndpointTest
[INFO] [2023-01-17 00:50:24]	c.b.c.u.s.TestConquery	SecondaryIdEndpointTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:DATASET}): 0 entries, 0 B within 230.6 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:SECONDARY_IDS}): 0 entries, 0 B within 94.15 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:TABLES}): 0 entries, 0 B within 76.83 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 83.09 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:IMPORTS}): 0 entries, 0 B within 76.84 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:CONCEPTS}): 0 entries, 0 B within 67.64 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.NamespacedStorage	SecondaryIdEndpointTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 69.43 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:STRUCTURE}): 0 entries, 0 B within 63.97 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 63.38 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	SecondaryIdEndpointTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	SecondaryIdEndpointTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 63.80 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:DATASET}): 0 entries, 0 B within 307.5 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:SECONDARY_IDS}): 0 entries, 0 B within 124.2 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:TABLES}): 0 entries, 0 B within 100.2 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 138.3 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:IMPORTS}): 0 entries, 0 B within 85.18 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:CONCEPTS}): 0 entries, 0 B within 89.81 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:WORKER}): 0 entries, 0 B within 87.08 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:BUCKETS}): 0 entries, 0 B within 85.95 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694:C_BLOCKS}): 0 entries, 0 B within 83.70 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:DATASET}): 0 entries, 0 B within 293.2 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:SECONDARY_IDS}): 0 entries, 0 B within 126.4 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:TABLES}): 0 entries, 0 B within 117.0 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 112.7 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:IMPORTS}): 0 entries, 0 B within 95.81 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:CONCEPTS}): 0 entries, 0 B within 84.11 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:WORKER}): 0 entries, 0 B within 85.33 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:BUCKETS}): 0 entries, 0 B within 94.85 μs
[DEBUG] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:24]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5:C_BLOCKS}): 0 entries, 0 B within 91.98 μs
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Imports of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Buckets of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Imports of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Buckets of worker SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:24]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SecondaryIdEndpointTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:24]	c.b.c.u.s.TestConquery	SecondaryIdEndpointTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:25]	c.b.c.m.a.AuthorizationController	SecondaryIdEndpointTest	Security manager registered
[INFO] [2023-01-17 00:50:25]	c.b.c.r.a.r.AdminDatasetProcessor	user.SUPERUSER@SUPERUSER	Received new SecondaryId[SecondaryIdEndpointTest.description-NAME]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694, /127.0.0.1:56814]	Received update of SecondaryId SecondaryIdEndpointTest.description-NAME
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.UpdateSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5, /127.0.0.1:56818]	Received update of SecondaryId SecondaryIdEndpointTest.description-NAME
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "POST /admin/datasets/SecondaryIdEndpointTest/secondaryId HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
[INFO] [2023-01-17 00:50:25]	c.b.c.i.t.SecondaryIdEndpointTest	SecondaryIdEndpointTest	InboundJaxrsResponse{context=ClientResponse{method=POST, uri=http://localhost:44953/admin/datasets/SecondaryIdEndpointTest/secondaryId, status=204, reason=No Content}}
[WARN] [2023-01-17 00:50:25]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	There are no displayable concepts in the dataset SecondaryIdEndpointTest
[WARN] [2023-01-17 00:50:25]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	No concepts could be collected for user.SUPERUSER@SUPERUSER on dataset SecondaryIdEndpointTest. The subject is possibly lacking the permission to use them.
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "GET /api/datasets/SecondaryIdEndpointTest/concepts HTTP/1.1" 200 150 "-" "Conquery (test client)" 22
[INFO] [2023-01-17 00:50:25]	c.b.c.i.t.SecondaryIdEndpointTest	SecondaryIdEndpointTest	[FESecondaryId(id=SecondaryIdEndpointTest.description-NAME, label=description-LABEL, description=description-DESCRIPTION)]
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "POST /admin/datasets/SecondaryIdEndpointTest/tables HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.UpdateTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5, /127.0.0.1:56818]	Received update of Table SecondaryIdEndpointTest.table
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.UpdateTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694, /127.0.0.1:56814]	Received update of Table SecondaryIdEndpointTest.table
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "GET /admin/datasets/SecondaryIdEndpointTest HTTP/1.1" 200 79 "-" "Conquery (test client)" 3
[ERROR] [2023-01-17 00:50:25]	c.b.c.r.a.r.AdminDatasetProcessor	user.SUPERUSER@SUPERUSER	SecondaryId[SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )] still present on [SecondaryIdEndpointTest.table]
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "DELETE /admin/datasets/SecondaryIdEndpointTest/secondaryId/SecondaryIdEndpointTest.description-NAME HTTP/1.1" 403 92 "-" "Conquery (test client)" 6
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694, /127.0.0.1:56814]	Received update of Table Table[label=table, name=table]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveTable	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5, /127.0.0.1:56818]	Received update of Table Table[label=table, name=table]
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "DELETE /admin/datasets/SecondaryIdEndpointTest/tables/SecondaryIdEndpointTest.table HTTP/1.1" 200 2 "-" "Conquery (test client)" 11
[INFO] [2023-01-17 00:50:25]	c.b.c.r.a.r.AdminDatasetProcessor	user.SUPERUSER@SUPERUSER	Deleting SecondaryId[SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )]
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "DELETE /admin/datasets/SecondaryIdEndpointTest/secondaryId/SecondaryIdEndpointTest.description-NAME HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5, /127.0.0.1:56818]	Received Deletion of SecondaryId SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveSecondaryId	Worker[SecondaryIdEndpointTest.worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694, /127.0.0.1:56814]	Received Deletion of SecondaryId SecondaryIdDescription(id = SecondaryIdEndpointTest.description-NAME, label = description-LABEL )
[WARN] [2023-01-17 00:50:25]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	There are no displayable concepts in the dataset SecondaryIdEndpointTest
[WARN] [2023-01-17 00:50:25]	c.b.c.m.d.c.FrontEndConceptBuilder	user.SUPERUSER@SUPERUSER	No concepts could be collected for user.SUPERUSER@SUPERUSER on dataset SecondaryIdEndpointTest. The subject is possibly lacking the permission to use them.
127.0.0.1 - - [17/Jan/2023:00:50:25 +0000] "GET /api/datasets/SecondaryIdEndpointTest/concepts HTTP/1.1" 200 33 "-" "Conquery (test client)" 4
[INFO] [2023-01-17 00:50:25]	c.b.c.i.t.SecondaryIdEndpointTest	SecondaryIdEndpointTest	[]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	SecondaryIdEndpointTest	Closing Job Manager fast SecondaryIdEndpointTest
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=SecondaryIdEndpointTest, name=SecondaryIdEndpointTest]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	SecondaryIdEndpointTest	Closing Job Manager slow SecondaryIdEndpointTest
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[INFO] [2023-01-17 00:50:25]	c.b.c.m.w.Namespace	SecondaryIdEndpointTest	Removing namespace storage of SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SecondaryIdEndpointTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[INFO] [2023-01-17 00:50:25]	c.b.c.m.c.XodusStoreFactory	SecondaryIdEndpointTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SecondaryIdEndpointTest
[INFO] [2023-01-17 00:50:25]	c.b.c.u.s.TestConquery	SecondaryIdEndpointTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[INFO] [2023-01-17 00:50:25]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SecondaryIdEndpointTest_d7092244-edb5-4b11-99a4-3d23a427b694
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[INFO] [2023-01-17 00:50:25]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SecondaryIdEndpointTest_f71d6037-a2c3-4aeb-858e-bb15f2745bd5
[INFO] [2023-01-17 00:50:25]	c.b.c.i.IntegrationTest$Wrapper	SecondaryIdEndpointTest	SUCCESS integration test SecondaryIdEndpointTest
[INFO] [2023-01-17 00:50:25]	c.b.c.i.IntegrationTest$Wrapper	SuperPermissionTest	STARTING integration test SuperPermissionTest
[INFO] [2023-01-17 00:50:25]	c.b.c.u.s.TestConquery	SuperPermissionTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:DATASET}): 0 entries, 0 B within 147.6 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:SECONDARY_IDS}): 0 entries, 0 B within 126.3 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:TABLES}): 0 entries, 0 B within 83.36 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 79.21 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:IMPORTS}): 0 entries, 0 B within 78.43 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:CONCEPTS}): 0 entries, 0 B within 73.27 μs
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.NamespacedStorage	SuperPermissionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 78.98 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:STRUCTURE}): 0 entries, 0 B within 73.08 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 72.03 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	SuperPermissionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	SuperPermissionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 63.88 μs
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:DATASET}): 0 entries, 0 B within 108.5 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:SECONDARY_IDS}): 0 entries, 0 B within 35.52 μs
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:DATASET}): 0 entries, 0 B within 92.06 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:TABLES}): 0 entries, 0 B within 29.87 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:SECONDARY_IDS}): 0 entries, 0 B within 42.52 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 34.16 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:TABLES}): 0 entries, 0 B within 31.30 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:IMPORTS}): 0 entries, 0 B within 30.34 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 36.95 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:CONCEPTS}): 0 entries, 0 B within 40.53 μs
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:IMPORTS}): 0 entries, 0 B within 30.20 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:WORKER}): 0 entries, 0 B within 30.93 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:CONCEPTS}): 0 entries, 0 B within 30.06 μs
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:BUCKETS}): 0 entries, 0 B within 38.47 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:WORKER}): 0 entries, 0 B within 37.32 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd:C_BLOCKS}): 0 entries, 0 B within 29.74 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:BUCKETS}): 0 entries, 0 B within 30.53 μs
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8:C_BLOCKS}): 0 entries, 0 B within 22.26 μs
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Imports of worker SuperPermissionTest.worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Buckets of worker SuperPermissionTest.worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Imports of worker SuperPermissionTest.worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Buckets of worker SuperPermissionTest.worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=SuperPermissionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:25]	c.b.c.u.s.TestConquery	SuperPermissionTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:25]	c.b.c.m.a.AuthorizationController	SuperPermissionTest	Security manager registered
[INFO] [2023-01-17 00:50:25]	c.b.c.i.s.MetaStorage	SuperPermissionTest	Remove User = user.user
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	SuperPermissionTest	Closing Job Manager fast SuperPermissionTest
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=SuperPermissionTest, name=SuperPermissionTest]
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	SuperPermissionTest	Closing Job Manager slow SuperPermissionTest
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[INFO] [2023-01-17 00:50:25]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[INFO] [2023-01-17 00:50:25]	c.b.c.m.w.Namespace	SuperPermissionTest	Removing namespace storage of SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	SuperPermissionTest	Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[INFO] [2023-01-17 00:50:25]	c.b.c.m.c.XodusStoreFactory	SuperPermissionTest	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_SuperPermissionTest
[INFO] [2023-01-17 00:50:25]	c.b.c.u.s.TestConquery	SuperPermissionTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[DEBUG] [2023-01-17 00:50:25]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[INFO] [2023-01-17 00:50:25]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_SuperPermissionTest_2af386fc-b035-458f-a072-503b733200bd
[INFO] [2023-01-17 00:50:25]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_SuperPermissionTest_c5677a73-0833-4547-93d5-fa80ad0cdaf8
[INFO] [2023-01-17 00:50:26]	c.b.c.i.IntegrationTest$Wrapper	SuperPermissionTest	SUCCESS integration test SuperPermissionTest
[INFO] [2023-01-17 00:50:26]	c.b.c.i.IntegrationTest$Wrapper	ConceptUpdateAndDeletionTest	STARTING integration test ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}): 0 entries, 0 B within 151.9 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 94.41 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}): 0 entries, 0 B within 87.68 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 96.56 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}): 0 entries, 0 B within 83.92 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}): 0 entries, 0 B within 86.91 μs
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.NamespacedStorage	ConceptUpdateAndDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 89.36 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}): 0 entries, 0 B within 87.98 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 56.83 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	ConceptUpdateAndDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 55.57 μs
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DATASET}): 0 entries, 0 B within 96.73 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:SECONDARY_IDS}): 0 entries, 0 B within 37.36 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:TABLES}): 0 entries, 0 B within 36.43 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 45.95 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:IMPORTS}): 0 entries, 0 B within 34.96 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:CONCEPTS}): 0 entries, 0 B within 40.40 μs
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:WORKER}): 0 entries, 0 B within 33.60 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:BUCKETS}): 0 entries, 0 B within 32.63 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:C_BLOCKS}): 0 entries, 0 B within 31.54 μs
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DATASET}): 0 entries, 0 B within 132.3 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:SECONDARY_IDS}): 0 entries, 0 B within 68.92 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:TABLES}): 0 entries, 0 B within 54.56 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 66.10 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:IMPORTS}): 0 entries, 0 B within 54.92 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:CONCEPTS}): 0 entries, 0 B within 45.98 μs
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:WORKER}): 0 entries, 0 B within 54.12 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:BUCKETS}): 0 entries, 0 B within 51.40 μs
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:26]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:C_BLOCKS}): 0 entries, 0 B within 51.83 μs
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.UpdateTable	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Received update of Table ConceptUpdateAndDeletionTest.test_table
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.UpdateTable	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Received update of Table ConceptUpdateAndDeletionTest.test_table
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery	ConceptUpdateAndDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:26]	c.b.c.c.PreprocessorCommand	ConceptUpdateAndDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-17 00:50:26]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:26]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)	Required to preprocess 94 B in total
[INFO] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000461709s[INFO] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:26]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:26]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1b9a885b)
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1b9a885b) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@337316bb(est. 81 B)
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]		test_column: StringParser(super=Parser(lines=4, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=3], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing header
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	Writing data
[INFO] [2023-01-17 00:50:26]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:26]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 1 Jobs:
[INFO] [2023-01-17 00:50:26]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest/test_table.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 4 Entities.
[INFO] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into ConceptUpdateAndDeletionTest.test_table
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Mapped 4 new ids
127.0.0.1 - - [17/Jan/2023:00:50:26 +0000] "POST /admin/datasets/ConceptUpdateAndDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ConceptUpdateAndDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.w.Namespace	Job Manager slow ConceptUpdateAndDeletionTest	Assigning Bucket[0] to Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.w.Namespace	Job Manager slow ConceptUpdateAndDeletionTest	Assigning Bucket[1] to Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5]
[INFO] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Received new WorkerInformation(size = 1,dataset = ConceptUpdateAndDeletionTest)
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Received new WorkerInformation(size = 1,dataset = ConceptUpdateAndDeletionTest)
[INFO] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:26]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ConceptUpdateAndDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Received Dictionary[ConceptUpdateAndDeletionTest.test_table#ConceptUpdateAndDeletionTest$2etest_table$2etest_column] of size 3.
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Received Dictionary[ConceptUpdateAndDeletionTest.test_table#ConceptUpdateAndDeletionTest$2etest_table$2etest_column] of size 3.
[INFO] [2023-01-17 00:50:26]	c.b.c.m.j.ImportJob	Job Manager slow ConceptUpdateAndDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-17 00:50:26]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ConceptUpdateAndDeletionTest	One or more Children are not done yet
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.AddImport	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Received Import[ConceptUpdateAndDeletionTest.test_table.test_table], containing 4 entries.
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ImportBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Received ConceptUpdateAndDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Adding Bucket[ConceptUpdateAndDeletionTest.test_table.test_table.0]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.AddImport	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Received Import[ConceptUpdateAndDeletionTest.test_table.test_table], containing 4 entries.
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ImportBucket	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Received ConceptUpdateAndDeletionTest.test_table.test_table.1
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Adding Bucket[ConceptUpdateAndDeletionTest.test_table.test_table.1]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[INFO] [2023-01-17 00:50:26]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state before update
[INFO] [2023-01-17 00:50:26]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query before update
[INFO] [2023-01-17 00:50:26]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ConceptUpdateAndDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:26]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[597a8fe4-2261-4629-bd2f-ca4b58174fcd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest))]]
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Started ConceptQuery ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Started ConceptQuery ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	QueryPlan for Query[ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	QueryPlan for Query[ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:26]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd] with 0 results within PT0.001027S
[INFO] [2023-01-17 00:50:26]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd] with 1 results within PT0.001436S
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, startTime=2023-01-17T00:50:26.766856, finishTime=2023-01-17T00:50:26.767883) of size 0
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=0] for Query[ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd]
127.0.0.1 - - [17/Jan/2023:00:50:26 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 201 1147 "-" "Conquery (test client)" 10
[INFO] [2023-01-17 00:50:26]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, startTime=2023-01-17T00:50:26.766831, finishTime=2023-01-17T00:50:26.768267) of size 1
[DEBUG] [2023-01-17 00:50:26]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:26]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	DONE 597a8fe4-2261-4629-bd2f-ca4b58174fcd ManagedQuery within PT0.006818S
127.0.0.1 - - [17/Jan/2023:00:50:26 +0000] "GET /api/datasets/ConceptUpdateAndDeletionTest/queries/ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd HTTP/1.1" 200 1450 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:26]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Query before update executed
[INFO] [2023-01-17 00:50:26]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing  update
127.0.0.1 - - [17/Jan/2023:00:50:26 +0000] "PUT /admin/datasets/ConceptUpdateAndDeletionTest/concepts HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
[INFO] [2023-01-17 00:50:26]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Updating Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:26]	c.b.c.i.s.WorkerStorage		Adding CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[INFO] [2023-01-17 00:50:27]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Update executed
[INFO] [2023-01-17 00:50:27]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after update
[INFO] [2023-01-17 00:50:27]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after update
[INFO] [2023-01-17 00:50:27]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ConceptUpdateAndDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:27]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[aa463847-6aee-4b15-8da9-427c3840bdfc] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest))]]
[INFO] [2023-01-17 00:50:27]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	Started ConceptQuery ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:56814]	QueryPlan for Query[ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:27]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	Started ConceptQuery ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:56818]	QueryPlan for Query[ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:27]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc] with 1 results within PT0.000555S
[INFO] [2023-01-17 00:50:27]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc] with 1 results within PT0.001222S
[INFO] [2023-01-17 00:50:27]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, startTime=2023-01-17T00:50:27.104878, finishTime=2023-01-17T00:50:27.105433) of size 1
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc]
127.0.0.1 - - [17/Jan/2023:00:50:27 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 201 1147 "-" "Conquery (test client)" 9
[INFO] [2023-01-17 00:50:27]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, startTime=2023-01-17T00:50:27.105035, finishTime=2023-01-17T00:50:27.106257) of size 1
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc]
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:27]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ConceptUpdateAndDeletionTest]	DONE aa463847-6aee-4b15-8da9-427c3840bdfc ManagedQuery within PT0.00391S
127.0.0.1 - - [17/Jan/2023:00:50:27 +0000] "GET /api/datasets/ConceptUpdateAndDeletionTest/queries/ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc HTTP/1.1" 200 1450 "-" "Conquery (test client)" 3
[INFO] [2023-01-17 00:50:27]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:27]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Query after update executed
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DATASET}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:TABLES}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:IMPORTS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:CONCEPTS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:WORKER}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:BUCKETS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:C_BLOCKS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:56818	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DATASET}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:TABLES}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:IMPORTS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:CONCEPTS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:WORKER}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:BUCKETS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:C_BLOCKS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:56814	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:27]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:27]	c.b.c.m.w.Namespace		Closing namespace storage of ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
[INFO] [2023-01-17 00:50:27]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:50:27]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}): 1 entries, 83 B within 300.8 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 52.11 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}): 1 entries, 183 B within 361.0 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 1.418 ms
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}): 1 entries, 553 B within 3.883 ms
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}): 1 entries, 508 B within 4.912 ms
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 371.6 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}): 0 entries, 0 B within 24.49 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 328 B within 109.7 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 114 B within 505.6 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	DONE reading Storage
[INFO] [2023-01-17 00:50:27]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest)]
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.050 ms
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 98.23 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 79.28 μs
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 2 entries, 1.1 KiB within 11.17 ms
[DEBUG] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:27]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 73.51 μs
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@f6c7e90
[DEBUG] [2023-01-17 00:50:27]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:27]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:27]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_18
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_19
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_20
[INFO] [2023-01-17 00:50:27]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:27]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:27]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:27]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DATASET}): 1 entries, 83 B within 115.9 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DATASET}): 1 entries, 83 B within 118.4 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:SECONDARY_IDS}): 0 entries, 0 B within 30.45 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:SECONDARY_IDS}): 0 entries, 0 B within 32.05 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:TABLES}): 1 entries, 183 B within 160.5 μs
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:TABLES}): 1 entries, 183 B within 141.3 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 769.8 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 836.3 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:IMPORTS}): 1 entries, 553 B within 3.029 ms
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:IMPORTS}): 1 entries, 553 B within 3.023 ms
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:CONCEPTS}): 1 entries, 508 B within 4.067 ms
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:WORKER}): 1 entries, 159 B within 117.8 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:CONCEPTS}): 1 entries, 508 B within 4.950 ms
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:WORKER}): 1 entries, 159 B within 159.2 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:BUCKETS}): 1 entries, 398 B within 1.348 ms
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:C_BLOCKS}): 1 entries, 249 B within 151.0 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	DONE reading Storage
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5))]
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:BUCKETS}): 1 entries, 410 B within 2.296 ms
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:28]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:C_BLOCKS}): 1 entries, 248 B within 278.5 μs
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	DONE reading Storage
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c))]
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:28]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:57516 connected, waiting for identity
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	/127.0.0.1:57516	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:57518 connected, waiting for identity
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	/127.0.0.1:57518	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	/127.0.0.1:57518	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c'
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	/127.0.0.1:57516	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5'
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:57518` registered.
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:57516` registered.
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c are consistent with the manager: 1 Imports
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5 are consistent with the manager: 1 Imports
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5 are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[WARN] [2023-01-17 00:50:28]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:28]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:28]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:28]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:28]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-17 00:50:28]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest for Support
[INFO] [2023-01-17 00:50:28]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:28]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after re-start
[INFO] [2023-01-17 00:50:28]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after restart.
[INFO] [2023-01-17 00:50:28]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ConceptUpdateAndDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:28]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[16bef84b-7ffa-498b-a2f3-461214938711] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:28 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 201 1147 "-" "Conquery (test client)" 57
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:57516]	Started ConceptQuery ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:57518]	Started ConceptQuery ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:57516]	QueryPlan for Query[ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.q.QueryExecutor	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:57518]	QueryPlan for Query[ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711] = `ConceptQueryPlan(child=ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ConceptUpdateAndDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:28]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711] with 1 results within PT0.001109S
[INFO] [2023-01-17 00:50:28]	c.b.c.m.q.r.ShardResult		FINISHED Query[ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711] with 1 results within PT0.001346S
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, startTime=2023-01-17T00:50:28.386522, finishTime=2023-01-17T00:50:28.387868) of size 1
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.q.ManagedQuery	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711]
[INFO] [2023-01-17 00:50:28]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received ShardResult(queryId=ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711, workerId=ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, startTime=2023-01-17T00:50:28.386285, finishTime=2023-01-17T00:50:28.387394) of size 1
[DEBUG] [2023-01-17 00:50:28]	c.b.c.m.q.ManagedQuery	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Received Result[size=1] for Query[ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711]
[INFO] [2023-01-17 00:50:28]	c.b.c.m.e.ManagedExecution	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	DONE 16bef84b-7ffa-498b-a2f3-461214938711 ManagedQuery within PT0.030209S
127.0.0.1 - - [17/Jan/2023:00:50:28 +0000] "GET /api/datasets/ConceptUpdateAndDeletionTest/queries/ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711 HTTP/1.1" 200 1451 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:28]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:28]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Issuing deletion of import ConceptUpdateAndDeletionTest.test_tree
[INFO] [2023-01-17 00:50:28]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:57516]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.1.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5, /127.0.0.1:57516]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:57518]	Removing CBlock[ConceptUpdateAndDeletionTest.test_table.test_table.0.ConceptUpdateAndDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.s.WorkerStorage	Worker[ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c, /127.0.0.1:57518]	Removing Concept[ConceptUpdateAndDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:28]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after deletion
[INFO] [2023-01-17 00:50:28]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after deletion (EXPECTING AN EXCEPTION IN THE LOGS!)
[DEBUG] [2023-01-17 00:50:28]	c.b.c.i.j.ConqueryJsonExceptionMapper	user.SUPERUSER@SUPERUSER	Unable to process JSON in request 'http://localhost:38705/api/datasets/ConceptUpdateAndDeletionTest/queries'
com.fasterxml.jackson.databind.JsonMappingException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector (through reference chain: com.bakdata.conquery.apiv1.query.ConceptQuery["root"]->com.bakdata.conquery.apiv1.query.concept.specific.CQConcept["tables"]->java.util.ArrayList[0]->com.bakdata.conquery.apiv1.query.concept.filter.CQTable["id"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:254)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:155)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:286)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.impl.ManagedReferenceProperty.deserializeAndSet(ManagedReferenceProperty.java:62)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:295)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:138)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:278)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:282)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:65)
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:26)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:252)
	... 96 common frames omitted
Caused by: com.bakdata.conquery.io.jackson.serializer.IdReferenceResolvingException: Could not find entry `ConceptUpdateAndDeletionTest.test_tree.test_column` of type com.bakdata.conquery.models.datasets.concepts.Connector
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 123]
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:55)
	... 99 common frames omitted
127.0.0.1 - - [17/Jan/2023:00:50:28 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 400 208 "-" "Conquery (test client)" 14
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DATASET}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:TABLES}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:IMPORTS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:CONCEPTS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:WORKER}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:BUCKETS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:C_BLOCKS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:57518	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DATASET}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:TABLES}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:IMPORTS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:CONCEPTS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:WORKER}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:BUCKETS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:C_BLOCKS}
[INFO] [2023-01-17 00:50:28]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:57516	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-17 00:50:28]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:28]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.w.Namespace		Closing namespace storage of ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
[INFO] [2023-01-17 00:50:29]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:50:29]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:DATASET}): 1 entries, 83 B within 222.3 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 48.29 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:TABLES}): 1 entries, 183 B within 212.9 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 946.6 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:IMPORTS}): 1 entries, 553 B within 4.518 ms
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:CONCEPTS}): 0 entries, 0 B within 48.64 μs
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 293.2 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:STRUCTURE}): 0 entries, 0 B within 56.52 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 328 B within 183.3 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 114 B within 717.9 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest	DONE reading Storage
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_ConceptUpdateAndDeletionTest)]
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 644.7 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 58.72 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 48.48 μs
[WARN] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [ConceptUpdateAndDeletionTest.16bef84b-7ffa-498b-a2f3-461214938711]
[WARN] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [ConceptUpdateAndDeletionTest.597a8fe4-2261-4629-bd2f-ca4b58174fcd]
[WARN] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [ConceptUpdateAndDeletionTest.aa463847-6aee-4b15-8da9-427c3840bdfc]
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	3
	Key read failure:	0 (0.00%)
	Value read failure:	3 (100.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 0 entries, 0 B within 10.99 ms
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 708.8 μs
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@7cf89b42
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:29]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:29]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_21
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_22
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_23
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:29]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:29]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:29]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:DATASET}): 1 entries, 83 B within 164.8 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:SECONDARY_IDS}): 0 entries, 0 B within 48.21 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:TABLES}): 1 entries, 183 B within 218.0 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 952.8 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:DATASET}): 1 entries, 83 B within 141.0 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:SECONDARY_IDS}): 0 entries, 0 B within 45.26 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store TABLES:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:TABLES}): 1 entries, 183 B within 172.7 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store DICTIONARIES_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached big DICTIONARIES(Dictionary): 1 entries, 43 B within 917.5 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:IMPORTS}): 1 entries, 553 B within 3.152 ms
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:CONCEPTS}): 0 entries, 0 B within 32.90 μs
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:WORKER}): 1 entries, 159 B within 89.05 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:BUCKETS}): 1 entries, 398 B within 1.432 ms
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5:C_BLOCKS}): 0 entries, 0 B within 37.28 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5	DONE reading Storage
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5))]
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store IMPORTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:IMPORTS}): 1 entries, 553 B within 4.272 ms
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:CONCEPTS}): 0 entries, 0 B within 46.62 μs
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	Done reading Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:WORKER}): 1 entries, 159 B within 120.2 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:BUCKETS}): 1 entries, 410 B within 1.996 ms
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c:C_BLOCKS}): 0 entries, 0 B within 33.44 μs
[DEBUG] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c	DONE reading Storage
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c))]
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:29]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:57648 connected, waiting for identity
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode	/127.0.0.1:57648	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:57650 connected, waiting for identity
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode	/127.0.0.1:57650	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode	/127.0.0.1:57648	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5'
[INFO] [2023-01-17 00:50:29]	c.b.c.c.ShardNode	/127.0.0.1:57650	Sending worker identity 'worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c'
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:57648` registered.
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:57650` registered.
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c are consistent with the manager: 1 Imports
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Imports of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5 are consistent with the manager: 1 Imports
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Buckets of worker ConceptUpdateAndDeletionTest.worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5 are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]	Consistency check was successful
[WARN] [2023-01-17 00:50:29]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:29]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:29]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:29]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:29]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-17 00:50:29]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest17490523365462791536/tmp_ConceptUpdateAndDeletionTest for Support
[INFO] [2023-01-17 00:50:29]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:29]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Checking state after restart
[INFO] [2023-01-17 00:50:29]	c.b.c.i.t.d.ConceptUpdateAndDeletionTest		Executing query after restart (EXPECTING AN EXCEPTION IN THE LOGS!)
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.j.ConqueryJsonExceptionMapper	user.SUPERUSER@SUPERUSER	Unable to process JSON in request 'http://localhost:38705/api/datasets/ConceptUpdateAndDeletionTest/queries'
com.fasterxml.jackson.databind.JsonMappingException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector (through reference chain: com.bakdata.conquery.apiv1.query.ConceptQuery["root"]->com.bakdata.conquery.apiv1.query.concept.specific.CQConcept["tables"]->java.util.ArrayList[0]->com.bakdata.conquery.apiv1.query.concept.filter.CQTable["id"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:254)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:155)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:286)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.impl.ManagedReferenceProperty.deserializeAndSet(ManagedReferenceProperty.java:62)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:295)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:138)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:278)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:282)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while resolving entry ConceptUpdateAndDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:65)
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:26)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:252)
	... 92 common frames omitted
Caused by: com.bakdata.conquery.io.jackson.serializer.IdReferenceResolvingException: Could not find entry `ConceptUpdateAndDeletionTest.test_tree.test_column` of type com.bakdata.conquery.models.datasets.concepts.Connector
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 123]
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:55)
	... 95 common frames omitted
127.0.0.1 - - [17/Jan/2023:00:50:29 +0000] "POST /api/datasets/ConceptUpdateAndDeletionTest/queries HTTP/1.1" 400 208 "-" "Conquery (test client)" 10
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=ConceptUpdateAndDeletionTest, name=ConceptUpdateAndDeletionTest]
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[INFO] [2023-01-17 00:50:29]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ConceptUpdateAndDeletionTest_3834cf66-87e8-464d-a63f-ec0b015378e5
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ConceptUpdateAndDeletionTest_eae6e8d4-a6e6-4646-a760-48caa744430c
[INFO] [2023-01-17 00:50:29]	c.b.c.m.w.Namespace		Removing namespace storage of ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[DEBUG] [2023-01-17 00:50:29]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:29]	c.b.c.i.IntegrationTest$Wrapper	ConceptUpdateAndDeletionTest	SUCCESS integration test ConceptUpdateAndDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.i.IntegrationTest$Wrapper	DatasetDeletionTest	STARTING integration test DatasetDeletionTest
[INFO] [2023-01-17 00:50:29]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:DATASET}): 0 entries, 0 B within 121.0 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 72.54 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:TABLES}): 0 entries, 0 B within 98.20 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 113.5 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:IMPORTS}): 0 entries, 0 B within 90.25 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:CONCEPTS}): 0 entries, 0 B within 91.85 μs
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.NamespacedStorage	DatasetDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 93.27 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:STRUCTURE}): 0 entries, 0 B within 98.93 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 69.00 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	DatasetDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	DatasetDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 59.62 μs
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:DATASET}): 0 entries, 0 B within 111.6 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:SECONDARY_IDS}): 0 entries, 0 B within 24.47 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:TABLES}): 0 entries, 0 B within 19.86 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 28.48 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:IMPORTS}): 0 entries, 0 B within 32.93 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:CONCEPTS}): 0 entries, 0 B within 18.58 μs
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:WORKER}): 0 entries, 0 B within 18.60 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:BUCKETS}): 0 entries, 0 B within 23.91 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6:C_BLOCKS}): 0 entries, 0 B within 23.96 μs
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Imports of worker DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Buckets of worker DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:DATASET}): 0 entries, 0 B within 61.44 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:SECONDARY_IDS}): 0 entries, 0 B within 25.13 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:TABLES}): 0 entries, 0 B within 21.24 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.82 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:IMPORTS}): 0 entries, 0 B within 18.27 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:CONCEPTS}): 0 entries, 0 B within 18.27 μs
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:WORKER}): 0 entries, 0 B within 18.99 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:BUCKETS}): 0 entries, 0 B within 21.17 μs
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:30]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8:C_BLOCKS}): 0 entries, 0 B within 20.48 μs
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Imports of worker DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Buckets of worker DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:30]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:30]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:30]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received update of Table DatasetDeletionTest.test_table
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received update of Table DatasetDeletionTest.test_table
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received update of Table DatasetDeletionTest.test_table2
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received update of Table DatasetDeletionTest.test_table2
[INFO] [2023-01-17 00:50:30]	c.b.c.u.s.TestConquery	DatasetDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Updating Concept[DatasetDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Updating Concept[DatasetDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand	DatasetDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.043286602s[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1fa50ff0)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1fa50ff0) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@5fdea267(est. 62 B)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000677635s[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@5aa78260)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@5aa78260) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@5cf373b1(est. 62 B)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-17 00:50:30]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:30]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into DatasetDeletionTest.test_table
127.0.0.1 - - [17/Jan/2023:00:50:30 +0000] "POST /admin/datasets/DatasetDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_DatasetDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest	Assigning Bucket[0] to Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received new WorkerInformation(size = 0,dataset = DatasetDeletionTest)
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:30]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received Dictionary[DatasetDeletionTest.test_table#DatasetDeletionTest$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received Dictionary[DatasetDeletionTest.test_table#DatasetDeletionTest$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Start sending 1 Buckets
[WARN] [2023-01-17 00:50:30]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	One or more Children are not done yet
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received Import[DatasetDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received Import[DatasetDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received DatasetDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Adding Bucket[DatasetDeletionTest.test_table.test_table.0]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest.test_table.test_table.0.DatasetDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into DatasetDeletionTest.test_table2
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest	Assigning Bucket[1] to Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest)
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest)
127.0.0.1 - - [17/Jan/2023:00:50:30 +0000] "POST /admin/datasets/DatasetDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_DatasetDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
[INFO] [2023-01-17 00:50:30]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:30]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received Dictionary[DatasetDeletionTest.test_table2#DatasetDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received Dictionary[DatasetDeletionTest.test_table2#DatasetDeletionTest$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-17 00:50:30]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest	One or more Children are not done yet
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received Import[DatasetDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received DatasetDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Adding Bucket[DatasetDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest.test_table2.test_table2.1.DatasetDeletionTest.test_tree.test_column2]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received Import[DatasetDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received DatasetDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Adding Bucket[DatasetDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest.test_table2.test_table2.0.DatasetDeletionTest.test_tree.test_column2]
[INFO] [2023-01-17 00:50:30]	c.b.c.i.t.d.DatasetDeletionTest		Checking state before deletion
[INFO] [2023-01-17 00:50:30]	c.b.c.i.t.d.DatasetDeletionTest		Executing query before deletion
[INFO] [2023-01-17 00:50:30]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[DatasetDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:30]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[9e95cb08-814f-4de2-9890-ca1620636b94] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DatasetDeletionTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:30 +0000] "POST /api/datasets/DatasetDeletionTest/queries HTTP/1.1" 201 1215 "-" "Conquery (test client)" 18
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Started ConceptQuery DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Started ConceptQuery DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	QueryPlan for Query[DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	QueryPlan for Query[DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:30]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94] with 0 results within PT0.00178S
[INFO] [2023-01-17 00:50:30]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94] with 2 results within PT0.002137S
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest]	Received ShardResult(queryId=DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94, workerId=DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, startTime=2023-01-17T00:50:30.878586, finishTime=2023-01-17T00:50:30.880366) of size 0
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest]	Received Result[size=0] for Query[DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest]	Received ShardResult(queryId=DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94, workerId=DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, startTime=2023-01-17T00:50:30.878594, finishTime=2023-01-17T00:50:30.880731) of size 2
[DEBUG] [2023-01-17 00:50:30]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest]	Received Result[size=2] for Query[DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=DatasetDeletionTest]	DONE 9e95cb08-814f-4de2-9890-ca1620636b94 ManagedQuery within PT0.017076S
127.0.0.1 - - [17/Jan/2023:00:50:30 +0000] "GET /api/datasets/DatasetDeletionTest/queries/DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94 HTTP/1.1" 200 1483 "-" "Conquery (test client)" 4
[INFO] [2023-01-17 00:50:30]	c.b.c.i.t.d.DatasetDeletionTest		Issuing deletion of import Dataset[label=null, name=DatasetDeletionTest]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Removing CBlock[DatasetDeletionTest.test_table2.test_table2.1.DatasetDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Removing CBlock[DatasetDeletionTest.test_table.test_table.0.DatasetDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Removing Concept[DatasetDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Removing CBlock[DatasetDeletionTest.test_table2.test_table2.0.DatasetDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Removing Concept[DatasetDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:30]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Deleting Import[NamedImpl(name=test_table)]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Deleting Import[NamedImpl(name=test_table)]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Removing Bucket[DatasetDeletionTest.test_table.test_table.0]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received update of Table Table[label=test_table, name=test_table]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Deleting Import[NamedImpl(name=test_table2)]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received update of Table Table[label=test_table, name=test_table]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Removing Bucket[DatasetDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6, /127.0.0.1:57650]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveImportJob	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-17 00:50:30]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Removing Bucket[DatasetDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-17 00:50:30]	c.b.c.m.m.n.s.RemoveTable	Worker[DatasetDeletionTest.worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8, /127.0.0.1:57648]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DatasetDeletionTest
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=DatasetDeletionTest, name=DatasetDeletionTest]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DatasetDeletionTest
[INFO] [2023-01-17 00:50:31]	c.b.c.m.w.Namespace		Removing namespace storage of DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[INFO] [2023-01-17 00:50:31]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest_aa2a5c33-4793-43e2-8158-49d95a0f22f6
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[INFO] [2023-01-17 00:50:31]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest_095b1cb3-ca32-4121-9f2d-ce4817ba96b8
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[INFO] [2023-01-17 00:50:31]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest
[INFO] [2023-01-17 00:50:31]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:31]	c.b.c.i.t.d.DatasetDeletionTest		Checking state after deletion
[INFO] [2023-01-17 00:50:31]	c.b.c.u.s.TestConquery		Setting up dataset
[WARN] [2023-01-17 00:50:31]	o.g.j.i.Errors	user.SUPERUSER@SUPERUSER	The following warnings have been detected: WARNING: Unknown HK2 failure detected:
MultiException stack 1 of 3
org.glassfish.jersey.server.ParamException$PathParamException: HTTP 404 Not Found
	at org.glassfish.jersey.server.internal.inject.PathParamValueParamProvider$PathParamValueProvider.apply(PathParamValueParamProvider.java:94)
	at org.glassfish.jersey.server.internal.inject.PathParamValueParamProvider$PathParamValueProvider.apply(PathParamValueParamProvider.java:79)
	at org.glassfish.jersey.server.internal.inject.ParamInjectionResolver.resolve(ParamInjectionResolver.java:97)
	at org.glassfish.jersey.inject.hk2.InjectionResolverWrapper.resolve(InjectionResolverWrapper.java:62)
	at org.jvnet.hk2.internal.ClazzCreator.resolve(ClazzCreator.java:188)
	at org.jvnet.hk2.internal.ClazzCreator.resolveAllDependencies(ClazzCreator.java:211)
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:334)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
	at org.glassfish.jersey.inject.hk2.RequestContext.findOrCreate(RequestContext.java:59)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.NoSuchElementException: Did not find Dataset[DatasetDeletionTest] in [[]]
	at com.bakdata.conquery.models.worker.DatasetRegistry.findRegistry(DatasetRegistry.java:80)
	at com.bakdata.conquery.models.worker.IdResolveContext.resolve(IdResolveContext.java:43)
	at com.bakdata.conquery.io.jackson.NamespacedIdRefParamConverter.fromString(NamespacedIdRefParamConverter.java:23)
	at com.bakdata.conquery.io.jackson.NamespacedIdRefParamConverter.fromString(NamespacedIdRefParamConverter.java:12)
	at org.glassfish.jersey.server.internal.inject.AbstractParamValueExtractor.convert(AbstractParamValueExtractor.java:116)
	at org.glassfish.jersey.server.internal.inject.AbstractParamValueExtractor.fromString(AbstractParamValueExtractor.java:107)
	at org.glassfish.jersey.server.internal.inject.SingleValueExtractor.extract(SingleValueExtractor.java:61)
	at org.glassfish.jersey.server.internal.inject.PathParamValueParamProvider$PathParamValueProvider.apply(PathParamValueParamProvider.java:92)
	... 77 more
MultiException stack 2 of 3
java.lang.IllegalArgumentException: While attempting to resolve the dependencies of com.bakdata.conquery.resources.api.QueryResource errors were found
	at org.jvnet.hk2.internal.ClazzCreator.resolveAllDependencies(ClazzCreator.java:224)
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:334)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
	at org.glassfish.jersey.inject.hk2.RequestContext.findOrCreate(RequestContext.java:59)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
MultiException stack 3 of 3
java.lang.IllegalStateException: Unable to perform operation: resolve on com.bakdata.conquery.resources.api.QueryResource
	at org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:363)
	at org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)
	at org.glassfish.jersey.inject.hk2.RequestContext.findOrCreate(RequestContext.java:59)
	at org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:758)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetService(ServiceLocatorImpl.java:721)
	at org.jvnet.hk2.internal.ServiceLocatorImpl.getService(ServiceLocatorImpl.java:691)
	at org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getInstance(AbstractHk2InjectionManager.java:160)
	at org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getInstance(ImmediateHk2InjectionManager.java:30)
	at org.glassfish.jersey.internal.inject.Injections.getOrCreate(Injections.java:105)
	at org.glassfish.jersey.server.model.MethodHandler$ClassBasedMethodHandler.getInstance(MethodHandler.java:260)
	at org.glassfish.jersey.server.internal.routing.PushMethodHandlerRouter.apply(PushMethodHandlerRouter.java:51)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)


127.0.0.1 - - [17/Jan/2023:00:50:31 +0000] "POST /api/datasets/DatasetDeletionTest/queries HTTP/1.1" 404 43 "-" "Conquery (test client)" 13
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:DATASET}): 0 entries, 0 B within 124.3 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:SECONDARY_IDS}): 0 entries, 0 B within 65.49 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:TABLES}): 0 entries, 0 B within 71.33 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 67.31 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:IMPORTS}): 0 entries, 0 B within 70.41 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:CONCEPTS}): 0 entries, 0 B within 65.36 μs
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.NamespacedStorage		Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 64.18 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:STRUCTURE}): 0 entries, 0 B within 59.33 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:WORKER_TO_BUCKETS}): 0 entries, 0 B within 59.88 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore		While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore			loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:PRIMARY_DICTIONARY}): 0 entries, 0 B within 57.24 μs
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:DATASET}): 0 entries, 0 B within 112.9 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:SECONDARY_IDS}): 0 entries, 0 B within 52.57 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:TABLES}): 0 entries, 0 B within 43.91 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 48.55 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:IMPORTS}): 0 entries, 0 B within 52.64 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:CONCEPTS}): 0 entries, 0 B within 33.10 μs
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:WORKER}): 0 entries, 0 B within 24.51 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:BUCKETS}): 0 entries, 0 B within 31.13 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:C_BLOCKS}): 0 entries, 0 B within 34.55 μs
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:DATASET}): 0 entries, 0 B within 81.30 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:SECONDARY_IDS}): 0 entries, 0 B within 44.70 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:TABLES}): 0 entries, 0 B within 35.95 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 40.85 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:IMPORTS}): 0 entries, 0 B within 34.52 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:CONCEPTS}): 0 entries, 0 B within 46.26 μs
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:WORKER}): 0 entries, 0 B within 34.10 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:BUCKETS}): 0 entries, 0 B within 47.70 μs
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:31]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:C_BLOCKS}): 0 entries, 0 B within 28.57 μs
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Consistency check was successful
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=DatasetDeletionTest[1]]	Consistency check was successful
[INFO] [2023-01-17 00:50:31]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received update of Table DatasetDeletionTest[1].test_table
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received update of Table DatasetDeletionTest[1].test_table
[INFO] [2023-01-17 00:50:31]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received update of Table DatasetDeletionTest[1].test_table2
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateTable	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received update of Table DatasetDeletionTest[1].test_table2
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand		Preprocessing from command line config.
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.067252587s[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@24a094d0)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@24a094d0) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@3f23df8d(est. 62 B)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000906271s[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6bb7184f)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@41faa8e1(est. 62 B)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6bb7184f) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-17 00:50:31]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:31]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1]/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into DatasetDeletionTest[1].test_table
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Mapped 2 new ids
127.0.0.1 - - [17/Jan/2023:00:50:31 +0000] "POST /admin/datasets/DatasetDeletionTest%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_DatasetDeletionTest%5B1%5D%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest[1]	Assigning Bucket[0] to Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest[1])
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received new WorkerInformation(size = 0,dataset = DatasetDeletionTest[1])
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:31]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received Dictionary[DatasetDeletionTest[1].test_table#DatasetDeletionTest[1]$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received Dictionary[DatasetDeletionTest[1].test_table#DatasetDeletionTest[1]$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Start sending 1 Buckets
[WARN] [2023-01-17 00:50:31]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	One or more Children are not done yet
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into DatasetDeletionTest[1].test_table2
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.w.Namespace	Job Manager slow DatasetDeletionTest[1]	Assigning Bucket[1] to Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Importing Dictionaries
[INFO] [2023-01-17 00:50:31]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:31 +0000] "POST /admin/datasets/DatasetDeletionTest%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_DatasetDeletionTest%5B1%5D%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest[1])
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received Import[DatasetDeletionTest[1].test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received Import[DatasetDeletionTest[1].test_table.test_table], containing 2 entries.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received new WorkerInformation(size = 1,dataset = DatasetDeletionTest[1])
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received DatasetDeletionTest[1].test_table.test_table.0
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Adding Bucket[DatasetDeletionTest[1].test_table.test_table.0]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:31]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received Dictionary[DatasetDeletionTest[1].test_table2#DatasetDeletionTest[1]$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.UpdateDictionary	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received Dictionary[DatasetDeletionTest[1].test_table2#DatasetDeletionTest[1]$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.j.ImportJob	Job Manager slow DatasetDeletionTest[1]	Start sending 2 Buckets
[WARN] [2023-01-17 00:50:31]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow DatasetDeletionTest[1]	One or more Children are not done yet
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received Import[DatasetDeletionTest[1].test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Received DatasetDeletionTest[1].test_table2.test_table2.1
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Adding Bucket[DatasetDeletionTest[1].test_table2.test_table2.1]
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.AddImport	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received Import[DatasetDeletionTest[1].test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:31]	c.b.c.m.m.n.s.ImportBucket	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Received DatasetDeletionTest[1].test_table2.test_table2.0
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Adding Bucket[DatasetDeletionTest[1].test_table2.test_table2.0]
[INFO] [2023-01-17 00:50:31]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Updating Concept[DatasetDeletionTest[1].test_tree]
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Updating Concept[DatasetDeletionTest[1].test_tree]
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest[1].test_table2.test_table2.1.DatasetDeletionTest[1].test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest[1].test_table.test_table.0.DatasetDeletionTest[1].test_tree.test_column]
[DEBUG] [2023-01-17 00:50:31]	c.b.c.i.s.WorkerStorage		Adding CBlock[DatasetDeletionTest[1].test_table2.test_table2.0.DatasetDeletionTest[1].test_tree.test_column2]
[INFO] [2023-01-17 00:50:32]	c.b.c.i.t.d.DatasetDeletionTest		Executing query after re-import
[INFO] [2023-01-17 00:50:32]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[DatasetDeletionTest[1]] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:32]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[47b2a3d6-22a5-41b0-a255-64ec15e9f20d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DatasetDeletionTest[1]))]]
[INFO] [2023-01-17 00:50:32]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	Started ConceptQuery DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d
[INFO] [2023-01-17 00:50:32]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	Started ConceptQuery DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d
[DEBUG] [2023-01-17 00:50:32]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57648]	QueryPlan for Query[DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:32]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57650]	QueryPlan for Query[DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
127.0.0.1 - - [17/Jan/2023:00:50:32 +0000] "POST /api/datasets/DatasetDeletionTest%5B1%5D/queries HTTP/1.1" 201 1227 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:32]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d] with 0 results within PT0.00104S
[INFO] [2023-01-17 00:50:32]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d] with 2 results within PT0.001498S
[INFO] [2023-01-17 00:50:32]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, startTime=2023-01-17T00:50:32.035094, finishTime=2023-01-17T00:50:32.036134) of size 0
[DEBUG] [2023-01-17 00:50:32]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest[1]]	Received Result[size=0] for Query[DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d]
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:32]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, startTime=2023-01-17T00:50:32.035157, finishTime=2023-01-17T00:50:32.036655) of size 2
[DEBUG] [2023-01-17 00:50:32]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=DatasetDeletionTest[1]]	Received Result[size=2] for Query[DatasetDeletionTest[1].47b2a3d6-22a5-41b0-a255-64ec15e9f20d]
[INFO] [2023-01-17 00:50:32]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=DatasetDeletionTest[1]]	DONE 47b2a3d6-22a5-41b0-a255-64ec15e9f20d ManagedQuery within PT0.005617S
127.0.0.1 - - [17/Jan/2023:00:50:32 +0000] "GET /api/datasets/DatasetDeletionTest%5B1%5D/queries/DatasetDeletionTest%5B1%5D.47b2a3d6-22a5-41b0-a255-64ec15e9f20d HTTP/1.1" 200 1746 "-" "Conquery (test client)" 7
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:DATASET}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:TABLES}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:IMPORTS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:CONCEPTS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:WORKER}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:BUCKETS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:C_BLOCKS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:57650	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:DATASET}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:TABLES}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:IMPORTS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:CONCEPTS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:WORKER}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:BUCKETS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:C_BLOCKS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:57648	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:32]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:32]	c.b.c.m.w.Namespace		Closing namespace storage of DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:DATASET}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:TABLES}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:IMPORTS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:CONCEPTS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:ID_MAPPING_META}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:ID_MAPPING_DATA}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:STRUCTURE}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:WORKER_TO_BUCKETS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:PRIMARY_DICTIONARY}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
[INFO] [2023-01-17 00:50:32]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:50:32]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:DATASET}): 1 entries, 71 B within 220.4 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:SECONDARY_IDS}): 0 entries, 0 B within 69.51 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:TABLES}): 2 entries, 356 B within 358.1 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.251 ms
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:IMPORTS}): 2 entries, 1 KiB within 4.515 ms
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:CONCEPTS}): 1 entries, 642 B within 6.388 ms
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	Done reading Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 240.7 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:STRUCTURE}): 0 entries, 0 B within 36.07 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:WORKER_TO_BUCKETS}): 1 entries, 343 B within 122.8 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:32]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]:PRIMARY_DICTIONARY}): 1 entries, 108 B within 557.2 μs
[DEBUG] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]	DONE reading Storage
[INFO] [2023-01-17 00:50:32]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_DatasetDeletionTest[1])]
[INFO] [2023-01-17 00:50:32]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.186 ms
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 149.1 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 78.96 μs
[WARN] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	Could not parse value for key [DatasetDeletionTest.9e95cb08-814f-4de2-9890-ca1620636b94]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	1 (50.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 1 entries, 614 B within 12.00 ms
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 68.87 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@592a8804
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:33]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:33]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_24
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_25
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_26
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:33]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:DATASET}): 1 entries, 71 B within 118.6 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:SECONDARY_IDS}): 0 entries, 0 B within 30.17 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:TABLES}): 2 entries, 356 B within 205.0 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:DATASET}): 1 entries, 71 B within 91.46 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:SECONDARY_IDS}): 0 entries, 0 B within 31.52 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:TABLES}): 2 entries, 356 B within 170.0 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 815.4 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 750.0 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:IMPORTS}): 2 entries, 1 KiB within 3.011 ms
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:IMPORTS}): 2 entries, 1 KiB within 2.997 ms
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:CONCEPTS}): 1 entries, 642 B within 3.833 ms
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	Done reading Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:WORKER}): 1 entries, 147 B within 82.52 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:CONCEPTS}): 1 entries, 642 B within 3.803 ms
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	Done reading Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:WORKER}): 1 entries, 147 B within 90.18 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store BUCKETS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:BUCKETS}): 2 entries, 770 B within 1.429 ms
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	While processing store C_BLOCKS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:BUCKETS}): 1 entries, 384 B within 1.217 ms
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0:C_BLOCKS}): 2 entries, 477 B within 203.1 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0	DONE reading Storage
[INFO] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0))]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89:C_BLOCKS}): 1 entries, 240 B within 124.9 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89	DONE reading Storage
[INFO] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89))]
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:33]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:57948 connected, waiting for identity
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode	/127.0.0.1:57948	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:57950 connected, waiting for identity
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode	/127.0.0.1:57950	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode	/127.0.0.1:57948	Sending worker identity 'worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0'
[INFO] [2023-01-17 00:50:33]	c.b.c.c.ShardNode	/127.0.0.1:57950	Sending worker identity 'worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89'
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:57950` registered.
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:57948` registered.
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89 are consistent with the manager: 2 Imports
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89 are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Consistency check was successful
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Imports of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0 are consistent with the manager: 2 Imports
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Buckets of worker DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0 are consistent with the manager: 2 Buckets
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Consistency check was successful
[WARN] [2023-01-17 00:50:33]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:33]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:33]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:33]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest17490523365462791536/tmp_DatasetDeletionTest[1] for Support
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:33]	c.b.c.i.t.d.DatasetDeletionTest		Checking state after re-start
[INFO] [2023-01-17 00:50:33]	c.b.c.i.t.d.DatasetDeletionTest		Executing query after restart
[INFO] [2023-01-17 00:50:33]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[DatasetDeletionTest[1]] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:33]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[dfc5b896-2abf-4d77-b4c3-8bf05dace797] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DatasetDeletionTest[1]))]]
127.0.0.1 - - [17/Jan/2023:00:50:33 +0000] "POST /api/datasets/DatasetDeletionTest%5B1%5D/queries HTTP/1.1" 201 1228 "-" "Conquery (test client)" 51
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57950]	Started ConceptQuery DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ExecuteQuery	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57948]	Started ConceptQuery DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, /127.0.0.1:57950]	QueryPlan for Query[DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.q.QueryExecutor	Worker[DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, /127.0.0.1:57948]	QueryPlan for Query[DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = DatasetDeletionTest[1].test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:33]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797] with 0 results within PT0.00131S
[INFO] [2023-01-17 00:50:33]	c.b.c.m.q.r.ShardResult		FINISHED Query[DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797] with 2 results within PT0.00138S
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89, startTime=2023-01-17T00:50:33.521313, finishTime=2023-01-17T00:50:33.522623) of size 0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.q.ManagedQuery	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received Result[size=0] for Query[DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received ShardResult(queryId=DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797, workerId=DatasetDeletionTest[1].worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0, startTime=2023-01-17T00:50:33.521584, finishTime=2023-01-17T00:50:33.522964) of size 2
[DEBUG] [2023-01-17 00:50:33]	c.b.c.m.q.ManagedQuery	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	Received Result[size=2] for Query[DatasetDeletionTest[1].dfc5b896-2abf-4d77-b4c3-8bf05dace797]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.e.ManagedExecution	Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]	DONE dfc5b896-2abf-4d77-b4c3-8bf05dace797 ManagedQuery within PT0.040073S
127.0.0.1 - - [17/Jan/2023:00:50:33 +0000] "GET /api/datasets/DatasetDeletionTest%5B1%5D/queries/DatasetDeletionTest%5B1%5D.dfc5b896-2abf-4d77-b4c3-8bf05dace797 HTTP/1.1" 200 1747 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:33]	c.b.c.m.j.JobExecutor		Closing Job Manager fast DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=DatasetDeletionTest[1], name=DatasetDeletionTest[1]]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[INFO] [2023-01-17 00:50:33]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:33]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[INFO] [2023-01-17 00:50:33]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_DatasetDeletionTest[1]_79f693a9-5dd4-4190-b093-f1ab1788cf89
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_DatasetDeletionTest[1]_81eaf4a0-1368-4270-9bfc-712ed7f44bd0
[INFO] [2023-01-17 00:50:33]	c.b.c.m.j.JobExecutor		Closing Job Manager slow DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.w.Namespace		Removing namespace storage of DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_DatasetDeletionTest[1]
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:33]	c.b.c.i.IntegrationTest$Wrapper	DatasetDeletionTest	SUCCESS integration test DatasetDeletionTest
[INFO] [2023-01-17 00:50:33]	c.b.c.i.IntegrationTest$Wrapper	ImportDeletionTest	STARTING integration test ImportDeletionTest
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery	ImportDeletionTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:DATASET}): 0 entries, 0 B within 148.0 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 91.58 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:TABLES}): 0 entries, 0 B within 87.71 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 80.89 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:IMPORTS}): 0 entries, 0 B within 72.24 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:CONCEPTS}): 0 entries, 0 B within 100.1 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.NamespacedStorage	ImportDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 111.2 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:STRUCTURE}): 0 entries, 0 B within 79.65 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 64.32 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	ImportDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 79.55 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:DATASET}): 0 entries, 0 B within 83.97 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:SECONDARY_IDS}): 0 entries, 0 B within 29.32 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:TABLES}): 0 entries, 0 B within 21.92 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 25.48 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:IMPORTS}): 0 entries, 0 B within 27.04 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:CONCEPTS}): 0 entries, 0 B within 20.74 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:WORKER}): 0 entries, 0 B within 21.38 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:BUCKETS}): 0 entries, 0 B within 20.39 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:C_BLOCKS}): 0 entries, 0 B within 26.02 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0 are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0 are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:DATASET}): 0 entries, 0 B within 84.20 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:SECONDARY_IDS}): 0 entries, 0 B within 45.94 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:TABLES}): 0 entries, 0 B within 24.56 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 26.82 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:IMPORTS}): 0 entries, 0 B within 22.84 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:CONCEPTS}): 0 entries, 0 B within 25.49 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:WORKER}): 0 entries, 0 B within 23.36 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:BUCKETS}): 0 entries, 0 B within 35.72 μs
[DEBUG] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:33]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:C_BLOCKS}): 0 entries, 0 B within 22.71 μs
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=ImportDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:33]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received update of Table ImportDeletionTest.test_table
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received update of Table ImportDeletionTest.test_table
[INFO] [2023-01-17 00:50:33]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received update of Table ImportDeletionTest.test_table2
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateTable	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received update of Table ImportDeletionTest.test_table2
[INFO] [2023-01-17 00:50:34]	c.b.c.u.s.TestConquery	ImportDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Updating Concept[ImportDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Updating Concept[ImportDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	ImportDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.058841481s[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5c5a5046)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@4bcd3ed6(est. 62 B)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5c5a5046) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000817442s[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@3a887d2f)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@3a887d2f) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@785142f0(est. 62 B)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into ImportDeletionTest.test_table
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "POST /admin/datasets/ImportDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ImportDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.w.Namespace	Job Manager slow ImportDeletionTest	Assigning Bucket[0] to Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received new WorkerInformation(size = 0,dataset = ImportDeletionTest)
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:34]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received Dictionary[ImportDeletionTest.test_table#ImportDeletionTest$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received Dictionary[ImportDeletionTest.test_table#ImportDeletionTest$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Start sending 1 Buckets
[WARN] [2023-01-17 00:50:34]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	One or more Children are not done yet
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into ImportDeletionTest.test_table2
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.w.Namespace	Job Manager slow ImportDeletionTest	Assigning Bucket[1] to Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Importing Dictionaries
[INFO] [2023-01-17 00:50:34]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "POST /admin/datasets/ImportDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_ImportDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:34]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	Max cannot be decreased.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-17 00:50:34]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	One or more Children are not done yet
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received Import[ImportDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received Import[ImportDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received ImportDeletionTest.test_table.test_table.0
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Adding Bucket[ImportDeletionTest.test_table.test_table.0]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table.test_table.0.ImportDeletionTest.test_tree.test_column]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received ImportDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.1.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received ImportDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.0.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-17 00:50:34]	c.b.c.i.t.d.ImportDeletionTest		Checking state before deletion
[INFO] [2023-01-17 00:50:34]	c.b.c.i.t.d.ImportDeletionTest		Executing query before deletion
[INFO] [2023-01-17 00:50:34]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:34]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[767d9a93-1a8a-4d55-9c47-9186d232532c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Started ConceptQuery ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Started ConceptQuery ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	QueryPlan for Query[ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	QueryPlan for Query[ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:34]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c] with 0 results within PT0.001218S
[INFO] [2023-01-17 00:50:34]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c] with 2 results within PT0.001898S
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c, workerId=ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, startTime=2023-01-17T00:50:34.595262, finishTime=2023-01-17T00:50:34.596480) of size 0
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c]
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 9
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c, workerId=ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, startTime=2023-01-17T00:50:34.595048, finishTime=2023-01-17T00:50:34.596946) of size 2
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=2] for Query[ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ImportDeletionTest]	DONE 767d9a93-1a8a-4d55-9c47-9186d232532c ManagedQuery within PT0.006419S
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.767d9a93-1a8a-4d55-9c47-9186d232532c HTTP/1.1" 200 1475 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:34]	c.b.c.i.t.d.ImportDeletionTest		Issuing deletion of import ImportDeletionTest.test_table2.test_table2
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "DELETE /admin/datasets/ImportDeletionTest/tables/ImportDeletionTest.test_table2/imports/ImportDeletionTest.test_table2.test_table2 HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
[INFO] [2023-01-17 00:50:34]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.RemoveImportJob	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Deleting Import[NamedImpl(name=test_table2)]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.RemoveImportJob	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Removing CBlock[ImportDeletionTest.test_table2.test_table2.1.ImportDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Removing CBlock[ImportDeletionTest.test_table2.test_table2.0.ImportDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Removing Bucket[ImportDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Removing Bucket[ImportDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-17 00:50:34]	c.b.c.i.t.d.ImportDeletionTest		Checking state after deletion
[INFO] [2023-01-17 00:50:34]	c.b.c.i.t.d.ImportDeletionTest		Executing query after deletion
[INFO] [2023-01-17 00:50:34]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:34]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[eeeab917-264c-4b4d-b284-6d0589f57e77] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Started ConceptQuery ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Started ConceptQuery ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	QueryPlan for Query[ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	QueryPlan for Query[ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:34]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77] with 0 results within PT0.001139S
[INFO] [2023-01-17 00:50:34]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77] with 1 results within PT0.001709S
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 7
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77, workerId=ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, startTime=2023-01-17T00:50:34.773738, finishTime=2023-01-17T00:50:34.774877) of size 0
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77, workerId=ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, startTime=2023-01-17T00:50:34.773771, finishTime=2023-01-17T00:50:34.775480) of size 1
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=1] for Query[ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ImportDeletionTest]	DONE eeeab917-264c-4b4d-b284-6d0589f57e77 ManagedQuery within PT0.007507S
127.0.0.1 - - [17/Jan/2023:00:50:34 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.eeeab917-264c-4b4d-b284-6d0589f57e77 HTTP/1.1" 200 1475 "-" "Conquery (test client)" 4
[INFO] [2023-01-17 00:50:34]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand		Preprocessing from command line config.
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	EXISTS ALREADY
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)		HASH OUTDATED
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 75 B in total
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000556998s[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=3, min=1, average=1.500000, max=2}
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=3, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1aae78ba)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@2179fc60(est. 80 B)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1aae78ba) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=3, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=3], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-17 00:50:34]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 1 Jobs:
[INFO] [2023-01-17 00:50:34]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob		Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob		Importing test_table2 into ImportDeletionTest.test_table2
[INFO] [2023-01-17 00:50:34]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Mapped 0 new ids
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Updating bucket assignments.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received new WorkerInformation(size = 1,dataset = ImportDeletionTest)
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:34]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 3.
[DEBUG] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.UpdateDictionary	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received Dictionary[ImportDeletionTest.test_table2#ImportDeletionTest$2etest_table2$2etest_column] of size 3.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.j.ImportJob	Job Manager slow ImportDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-17 00:50:34]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow ImportDeletionTest	One or more Children are not done yet
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 3 entries.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.AddImport	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received Import[ImportDeletionTest.test_table2.test_table2], containing 3 entries.
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Received ImportDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-17 00:50:34]	c.b.c.m.m.n.s.ImportBucket	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Received ImportDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Adding Bucket[ImportDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.1.ImportDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:34]	c.b.c.i.s.WorkerStorage		Adding CBlock[ImportDeletionTest.test_table2.test_table2.0.ImportDeletionTest.test_tree.test_column2]
[INFO] [2023-01-17 00:50:35]	c.b.c.i.t.d.ImportDeletionTest		Checking state after re-import
[INFO] [2023-01-17 00:50:35]	c.b.c.i.t.d.ImportDeletionTest		Executing query after re-import
[INFO] [2023-01-17 00:50:35]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:35]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[c1521cb2-e383-4835-8c23-5eae7de15a2d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
[INFO] [2023-01-17 00:50:35]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	Started ConceptQuery ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d
[INFO] [2023-01-17 00:50:35]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	Started ConceptQuery ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d
[DEBUG] [2023-01-17 00:50:35]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:57948]	QueryPlan for Query[ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:35]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:57950]	QueryPlan for Query[ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:35]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d] with 2 results within PT0.001284S
[INFO] [2023-01-17 00:50:35]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d] with 0 results within PT0.001318S
127.0.0.1 - - [17/Jan/2023:00:50:35 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 7
[INFO] [2023-01-17 00:50:35]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d, workerId=ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, startTime=2023-01-17T00:50:35.096653, finishTime=2023-01-17T00:50:35.097971) of size 0
[DEBUG] [2023-01-17 00:50:35]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[DEBUG] [2023-01-17 00:50:35]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d]
[INFO] [2023-01-17 00:50:35]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d, workerId=ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, startTime=2023-01-17T00:50:35.096514, finishTime=2023-01-17T00:50:35.097798) of size 2
[DEBUG] [2023-01-17 00:50:35]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=ImportDeletionTest]	Received Result[size=2] for Query[ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d]
[INFO] [2023-01-17 00:50:35]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=ImportDeletionTest]	DONE c1521cb2-e383-4835-8c23-5eae7de15a2d ManagedQuery within PT0.004059S
127.0.0.1 - - [17/Jan/2023:00:50:35 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.c1521cb2-e383-4835-8c23-5eae7de15a2d HTTP/1.1" 200 1475 "-" "Conquery (test client)" 4
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:DATASET}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:TABLES}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:IMPORTS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:CONCEPTS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:WORKER}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:BUCKETS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:C_BLOCKS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[INFO] [2023-01-17 00:50:35]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:35]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:57950	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:35]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:DATASET}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:TABLES}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:IMPORTS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:CONCEPTS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:WORKER}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:BUCKETS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:C_BLOCKS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:35]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:35]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:57948	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-17 00:50:35]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ImportDeletionTest
[INFO] [2023-01-17 00:50:35]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ImportDeletionTest
[INFO] [2023-01-17 00:50:35]	c.b.c.m.w.Namespace		Closing namespace storage of ImportDeletionTest
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:DATASET}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:TABLES}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:IMPORTS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:CONCEPTS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:STRUCTURE}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-17 00:50:35]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
[INFO] [2023-01-17 00:50:35]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:50:35]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:DATASET}): 1 entries, 63 B within 152.7 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 56.08 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:TABLES}): 2 entries, 348 B within 308.4 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.033 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:IMPORTS}): 2 entries, 1,012 B within 4.572 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:CONCEPTS}): 1 entries, 622 B within 5.330 ms
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	Done reading Dataset[label=ImportDeletionTest, name=ImportDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 215.6 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:STRUCTURE}): 0 entries, 0 B within 59.77 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 315 B within 128.8 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 104 B within 565.2 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest	DONE reading Storage
[INFO] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_ImportDeletionTest)]
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.427 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 91.43 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 74.74 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	3
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 3 entries, 1.8 KiB within 11.04 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 62.87 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@13d8ccaa
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:36]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:36]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_27
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_28
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_29
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:36]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:36]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:36]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:DATASET}): 1 entries, 63 B within 110.0 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:SECONDARY_IDS}): 0 entries, 0 B within 26.24 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:TABLES}): 2 entries, 348 B within 174.0 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 832.4 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:IMPORTS}): 2 entries, 1,012 B within 2.746 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:CONCEPTS}): 1 entries, 622 B within 3.703 ms
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	Done reading Dataset[label=ImportDeletionTest, name=ImportDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:WORKER}): 1 entries, 138 B within 73.91 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:BUCKETS}): 1 entries, 377 B within 1.319 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0:C_BLOCKS}): 1 entries, 235 B within 180.8 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0	DONE reading Storage
[INFO] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0))]
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:DATASET}): 1 entries, 63 B within 103.8 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:SECONDARY_IDS}): 0 entries, 0 B within 37.87 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:TABLES}): 2 entries, 348 B within 211.1 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 934.4 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:IMPORTS}): 2 entries, 1,012 B within 3.605 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:CONCEPTS}): 1 entries, 622 B within 3.841 ms
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	Done reading Dataset[label=ImportDeletionTest, name=ImportDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:WORKER}): 1 entries, 138 B within 74.20 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store BUCKETS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:BUCKETS}): 2 entries, 746 B within 1.624 ms
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	While processing store C_BLOCKS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba:C_BLOCKS}): 2 entries, 461 B within 207.0 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba	DONE reading Storage
[INFO] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba))]
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:36]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:58212 connected, waiting for identity
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode	/127.0.0.1:58212	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode	/127.0.0.1:58214	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:58214 connected, waiting for identity
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode	/127.0.0.1:58214	Sending worker identity 'worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba'
[INFO] [2023-01-17 00:50:36]	c.b.c.c.ShardNode	/127.0.0.1:58212	Sending worker identity 'worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0'
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:58212` registered.
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:58214` registered.
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba are consistent with the manager: 2 Imports
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba are consistent with the manager: 2 Buckets
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Imports of worker ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0 are consistent with the manager: 2 Imports
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Buckets of worker ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0 are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Consistency check was successful
[WARN] [2023-01-17 00:50:36]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:36]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:36]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:36]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:36]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-17 00:50:36]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest17490523365462791536/tmp_ImportDeletionTest for Support
[INFO] [2023-01-17 00:50:36]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:36]	c.b.c.i.t.d.ImportDeletionTest		Checking state after re-start
[INFO] [2023-01-17 00:50:36]	c.b.c.i.t.d.ImportDeletionTest		Executing query after re-import
[INFO] [2023-01-17 00:50:36]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[ImportDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:36]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ImportDeletionTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:36 +0000] "POST /api/datasets/ImportDeletionTest/queries HTTP/1.1" 201 1212 "-" "Conquery (test client)" 75
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:58214]	Started ConceptQuery ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, /127.0.0.1:58214]	QueryPlan for Query[ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:36]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0] with 2 results within PT0.001692S
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ExecuteQuery	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:58212]	Started ConceptQuery ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.q.QueryExecutor	Worker[ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, /127.0.0.1:58212]	QueryPlan for Query[ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = ImportDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:36]	c.b.c.m.q.r.ShardResult		FINISHED Query[ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0] with 0 results within PT0.001083S
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0, workerId=ImportDeletionTest.worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0, startTime=2023-01-17T00:50:36.584846, finishTime=2023-01-17T00:50:36.585929) of size 0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.q.ManagedQuery	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received Result[size=0] for Query[ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0]
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received ShardResult(queryId=ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0, workerId=ImportDeletionTest.worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba, startTime=2023-01-17T00:50:36.582897, finishTime=2023-01-17T00:50:36.584589) of size 2
[DEBUG] [2023-01-17 00:50:36]	c.b.c.m.q.ManagedQuery	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	Received Result[size=2] for Query[ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0]
[INFO] [2023-01-17 00:50:36]	c.b.c.m.e.ManagedExecution	Dataset[label=ImportDeletionTest, name=ImportDeletionTest]	DONE ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0 ManagedQuery within PT0.0476S
127.0.0.1 - - [17/Jan/2023:00:50:36 +0000] "GET /api/datasets/ImportDeletionTest/queries/ImportDeletionTest.ef00823e-4d4b-40c3-9fa5-9ee3ab3f71f0 HTTP/1.1" 200 1476 "-" "Conquery (test client)" 6
[INFO] [2023-01-17 00:50:36]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ImportDeletionTest
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[INFO] [2023-01-17 00:50:36]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=ImportDeletionTest, name=ImportDeletionTest]
[INFO] [2023-01-17 00:50:36]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:36]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ImportDeletionTest
[INFO] [2023-01-17 00:50:36]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[INFO] [2023-01-17 00:50:36]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_ImportDeletionTest_91599955-b769-4238-b53c-e9ba6dd419e0
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_ImportDeletionTest_db7e771f-7e15-4494-a08d-e457afcdb1ba
[INFO] [2023-01-17 00:50:36]	c.b.c.m.w.Namespace		Removing namespace storage of ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[INFO] [2023-01-17 00:50:36]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_ImportDeletionTest
[INFO] [2023-01-17 00:50:36]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:36]	c.b.c.i.IntegrationTest$Wrapper	ImportDeletionTest	SUCCESS integration test ImportDeletionTest
[INFO] [2023-01-17 00:50:36]	c.b.c.i.IntegrationTest$Wrapper	TableDeletionTest	STARTING integration test TableDeletionTest
[INFO] [2023-01-17 00:50:36]	c.b.c.u.s.TestConquery	TableDeletionTest	Setting up dataset
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:DATASET}): 0 entries, 0 B within 155.0 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 106.6 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:TABLES}): 0 entries, 0 B within 77.05 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 83.03 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:IMPORTS}): 0 entries, 0 B within 70.83 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:CONCEPTS}): 0 entries, 0 B within 83.59 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.NamespacedStorage	TableDeletionTest	Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 0 entries, 0 B within 90.07 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:STRUCTURE}): 0 entries, 0 B within 85.70 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:WORKER_TO_BUCKETS}): 0 entries, 0 B within 74.65 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	TableDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:PRIMARY_DICTIONARY}): 0 entries, 0 B within 74.15 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node1	creating a new worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.AddWorker	Job Manager slow shard-node0	creating a new worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:DATASET}): 0 entries, 0 B within 86.39 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:SECONDARY_IDS}): 0 entries, 0 B within 29.19 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:TABLES}): 0 entries, 0 B within 19.36 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.84 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:IMPORTS}): 0 entries, 0 B within 18.05 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:CONCEPTS}): 0 entries, 0 B within 17.94 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node1	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:WORKER}): 0 entries, 0 B within 21.86 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:BUCKETS}): 0 entries, 0 B within 35.08 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node1	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node1		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:C_BLOCKS}): 0 entries, 0 B within 24.24 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Consistency check was successful
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DATASET:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:DATASET}): 0 entries, 0 B within 79.86 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:SECONDARY_IDS}): 0 entries, 0 B within 21.79 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store TABLES:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:TABLES}): 0 entries, 0 B within 17.41 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store DICTIONARIES_META:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached big DICTIONARIES(Dictionary): 0 entries, 0 B within 21.68 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store IMPORTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:IMPORTS}): 0 entries, 0 B within 16.24 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store CONCEPTS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:CONCEPTS}): 0 entries, 0 B within 21.46 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.NamespacedStorage	Job Manager slow shard-node0	Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store WORKER:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:WORKER}): 0 entries, 0 B within 16.40 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store BUCKETS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:BUCKETS}): 0 entries, 0 B within 20.91 μs
[DEBUG] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.SerializingStore	Job Manager slow shard-node0	While processing store C_BLOCKS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:36]	c.b.c.i.s.x.s.CachedStore	Job Manager slow shard-node0		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:C_BLOCKS}): 0 entries, 0 B within 15.74 μs
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d are consistent with the manager: 0 Imports
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d are consistent with the manager: 0 Buckets
[INFO] [2023-01-17 00:50:36]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=null, name=TableDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:36]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received update of Table TableDeletionTest.test_table
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received update of Table TableDeletionTest.test_table
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery	TableDeletionTest	Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Updating Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Updating Concept[TableDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand	TableDeletionTest	Preprocessing from command line config.
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	DOES NOT EXIST
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 115 B in total
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.05585013s[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@49887ec0)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@49887ec0) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@669ed646(est. 62 B)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing header
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	Writing data
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table, name=test_table]:test_table[0/content1.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessor	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000786842s[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Chosen encoding is Base16UpperCase
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn		Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@475edfd7)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.p.s.StringParser	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		Using com.bakdata.conquery.models.preproc.parser.specific.string.TrieTypeGuesser@7deeed85(est. 62 B)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn			datum: DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@475edfd7) -> IntegerDateStore(store=ShortArrayStore())
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.PPColumn	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]		test_column: StringParser(super=Parser(lines=2, nullLines=0), encoding=Base16UpperCase, prefix=, suffix=) -> StringTypeEncoded(encoding=Base16UpperCase, subType=StringTypeDictionary(dictionary=SuccinctTrie[size=2], numberType=ByteArrayStore()))
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Encode primary Dictionary
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Headers
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing header
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing Dictionaries
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessed	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	Writing data
[INFO] [2023-01-17 00:50:37]	c.b.c.m.p.Preprocessor	ImportDescriptor [table=test_table2, name=test_table2]:test_table2[0/content2.csv]	PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand		Successfully Preprocess 2 Jobs:
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table.import.json, tag=Optional.empty)
[INFO] [2023-01-17 00:50:37]	c.b.c.c.PreprocessorCommand			Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table into TableDeletionTest.test_table
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Updating bucket assignments.
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "POST /admin/datasets/TableDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_TableDeletionTest%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.w.Namespace	Job Manager slow TableDeletionTest	Assigning Bucket[0] to Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Importing Dictionaries
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received new WorkerInformation(size = 0,dataset = TableDeletionTest)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:37]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received Dictionary[TableDeletionTest.test_table#TableDeletionTest$2etest_table$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received Dictionary[TableDeletionTest.test_table#TableDeletionTest$2etest_table$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Start sending 1 Buckets
[WARN] [2023-01-17 00:50:37]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	One or more Children are not done yet
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into TableDeletionTest.test_table2
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Mapped 2 new ids
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Updating bucket assignments.
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.w.Namespace	Job Manager slow TableDeletionTest	Assigning Bucket[1] to Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "POST /admin/datasets/TableDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_TableDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received Import[TableDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received Import[TableDeletionTest.test_table.test_table], containing 2 entries.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received TableDeletionTest.test_table.test_table.0
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Remapping Dictionaries []
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Adding Bucket[TableDeletionTest.test_table.test_table.0]
[WARN] [2023-01-17 00:50:37]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	Max cannot be decreased.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Start sending 2 Buckets
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[WARN] [2023-01-17 00:50:37]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	One or more Children are not done yet
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table.test_table.0.TableDeletionTest.test_tree.test_column]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received TableDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Adding Bucket[TableDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received TableDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Adding Bucket[TableDeletionTest.test_table2.test_table2.1]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.0.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.1.TableDeletionTest.test_tree.test_column2]
[INFO] [2023-01-17 00:50:37]	c.b.c.i.t.d.TableDeletionTest		Checking state before deletion
[INFO] [2023-01-17 00:50:37]	c.b.c.i.t.d.TableDeletionTest		Executing query before deletion
[INFO] [2023-01-17 00:50:37]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[TableDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:37]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[adf4b730-e8ef-4c76-a638-e2818f4b046c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TableDeletionTest))]]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Started ConceptQuery TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Started ConceptQuery TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	QueryPlan for Query[TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	QueryPlan for Query[TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 201 1208 "-" "Conquery (test client)" 7
[INFO] [2023-01-17 00:50:37]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c] with 0 results within PT0.001255S
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:37]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c] with 2 results within PT0.001799S
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c, workerId=TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, startTime=2023-01-17T00:50:37.693802, finishTime=2023-01-17T00:50:37.695057) of size 0
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=0] for Query[TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c, workerId=TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, startTime=2023-01-17T00:50:37.693647, finishTime=2023-01-17T00:50:37.695446) of size 2
[DEBUG] [2023-01-17 00:50:37]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=2] for Query[TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=TableDeletionTest]	DONE adf4b730-e8ef-4c76-a638-e2818f4b046c ManagedQuery within PT0.005932S
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "GET /api/datasets/TableDeletionTest/queries/TableDeletionTest.adf4b730-e8ef-4c76-a638-e2818f4b046c HTTP/1.1" 200 1467 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:37]	c.b.c.i.t.d.TableDeletionTest		Issuing deletion of import TableDeletionTest.test_table2
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "DELETE /admin/datasets/TableDeletionTest/tables/TableDeletionTest.test_table2 HTTP/1.1" 409 31 "-" "Conquery (test client)" 8
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Removing CBlock[TableDeletionTest.test_table2.test_table2.1.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Removing CBlock[TableDeletionTest.test_table.test_table.0.TableDeletionTest.test_tree.test_column]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Removing Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Removing CBlock[TableDeletionTest.test_table2.test_table2.0.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Removing Concept[TableDeletionTest.test_tree]
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "DELETE /admin/datasets/TableDeletionTest/tables/TableDeletionTest.test_table2 HTTP/1.1" 200 2 "-" "Conquery (test client)" 5
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.RemoveImportJob	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Deleting Import[NamedImpl(name=test_table2)]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.RemoveImportJob	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Deleting Import[NamedImpl(name=test_table2)]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Removing Bucket[TableDeletionTest.test_table2.test_table2.0]
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Removing Bucket[TableDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.RemoveTable	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.RemoveTable	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received update of Table Table[label=test_table2, name=test_table2]
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:37]	c.b.c.i.t.d.TableDeletionTest		Checking state after deletion
[INFO] [2023-01-17 00:50:37]	c.b.c.i.t.d.TableDeletionTest		Executing query after deletion
[DEBUG] [2023-01-17 00:50:37]	c.b.c.i.j.ConqueryJsonExceptionMapper	user.SUPERUSER@SUPERUSER	Unable to process JSON in request 'http://localhost:38705/api/datasets/TableDeletionTest/queries'
com.fasterxml.jackson.databind.JsonMappingException: Error while resolving entry TableDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector (through reference chain: com.bakdata.conquery.apiv1.query.ConceptQuery["root"]->com.bakdata.conquery.apiv1.query.concept.specific.CQConcept["tables"]->java.util.ArrayList[0]->com.bakdata.conquery.apiv1.query.concept.filter.CQTable["id"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:254)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:155)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:286)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245)
	at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.impl.ManagedReferenceProperty.deserializeAndSet(ManagedReferenceProperty.java:62)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:295)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:138)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserializeFromObject(SuperSonicBeanDeserializer.java:278)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161)
	at com.fasterxml.jackson.module.afterburner.deser.SuperSonicBeanDeserializer.deserialize(SuperSonicBeanDeserializer.java:116)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130)
	at com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97)
	at com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254)
	at com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:282)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)
	at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:47)
	at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:41)
	at org.eclipse.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)
	at org.eclipse.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:313)
	at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:713)
	at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:54)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Error while resolving entry TableDeletionTest.test_tree.test_column of type class com.bakdata.conquery.models.datasets.concepts.Connector
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:65)
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:26)
	at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:252)
	... 96 common frames omitted
Caused by: com.bakdata.conquery.io.jackson.serializer.IdReferenceResolvingException: Could not find entry `TableDeletionTest.test_tree.test_column` of type com.bakdata.conquery.models.datasets.concepts.Connector
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 123]
	at com.bakdata.conquery.io.jackson.serializer.NsIdReferenceDeserializer.deserialize(NsIdReferenceDeserializer.java:55)
	... 99 common frames omitted
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:50:37 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 400 197 "-" "Conquery (test client)" 6
[INFO] [2023-01-17 00:50:37]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-17 00:50:37]	c.b.c.m.m.n.s.UpdateTable	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received update of Table TableDeletionTest.test_table2
[INFO] [2023-01-17 00:50:38]	c.b.c.c.PreprocessorCommand		Preprocessing from command line config.
[INFO] [2023-01-17 00:50:38]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	EXISTS ALREADY
[INFO] [2023-01-17 00:50:38]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)		HASH STILL VALID
[INFO] [2023-01-17 00:50:38]	c.b.c.c.PreprocessorCommand	PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest/test_table2.import.json, tag=Optional.empty)	Required to preprocess 0 B in total
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Done reading data. Contains 2 Entities.
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	user.SUPERUSER@SUPERUSER	Importing test_table2 into TableDeletionTest.test_table2
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Mapped 0 new ids
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Updating bucket assignments.
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:50:38 +0000] "POST /admin/datasets/TableDeletionTest/imports?file=%2Ftmp%2FconqueryIntegrationTest17490523365462791536%2Ftmp_TableDeletionTest%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
[INFO] [2023-01-17 00:50:38]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.UpdateWorkerBucket	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received new WorkerInformation(size = 1,dataset = TableDeletionTest)
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Remapping Dictionaries []
[WARN] [2023-01-17 00:50:38]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	Max cannot be decreased.
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.UpdateDictionary	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received Dictionary[TableDeletionTest.test_table2#TableDeletionTest$2etest_table2$2etest_column] of size 2.
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.ImportJob	Job Manager slow TableDeletionTest	Start sending 2 Buckets
[WARN] [2023-01-17 00:50:38]	c.b.c.u.p.ProgressReporterImpl	Job Manager slow TableDeletionTest	One or more Children are not done yet
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Received TableDeletionTest.test_table2.test_table2.0
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Adding Bucket[TableDeletionTest.test_table2.test_table2.0]
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.AddImport	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received Import[TableDeletionTest.test_table2.test_table2], containing 2 entries.
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.ImportBucket	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Received TableDeletionTest.test_table2.test_table2.1
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Adding Bucket[TableDeletionTest.test_table2.test_table2.1]
[INFO] [2023-01-17 00:50:38]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Updating Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Updating Concept[TableDeletionTest.test_tree]
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.1.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table2.test_table2.0.TableDeletionTest.test_tree.test_column2]
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.s.WorkerStorage		Adding CBlock[TableDeletionTest.test_table.test_table.0.TableDeletionTest.test_tree.test_column]
[INFO] [2023-01-17 00:50:38]	c.b.c.i.t.d.TableDeletionTest		Checking state after re-import
[INFO] [2023-01-17 00:50:38]	c.b.c.i.t.d.TableDeletionTest		Executing query after re-import
[INFO] [2023-01-17 00:50:38]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[TableDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:38]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[e84f8b75-b277-4a78-88ee-3fa08966c541] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TableDeletionTest))]]
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	Started ConceptQuery TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	Started ConceptQuery TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58214]	QueryPlan for Query[TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58212]	QueryPlan for Query[TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:38]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541] with 0 results within PT0.001298S
[INFO] [2023-01-17 00:50:38]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541] with 2 results within PT0.001879S
127.0.0.1 - - [17/Jan/2023:00:50:38 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 201 1208 "-" "Conquery (test client)" 6
[DEBUG] [2023-01-17 00:50:38]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541, workerId=TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, startTime=2023-01-17T00:50:38.420790, finishTime=2023-01-17T00:50:38.422088) of size 0
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=0] for Query[TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541]
[INFO] [2023-01-17 00:50:38]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=null, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541, workerId=TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, startTime=2023-01-17T00:50:38.420632, finishTime=2023-01-17T00:50:38.422511) of size 2
[DEBUG] [2023-01-17 00:50:38]	c.b.c.m.q.ManagedQuery	Dataset[label=null, name=TableDeletionTest]	Received Result[size=2] for Query[TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541]
[INFO] [2023-01-17 00:50:38]	c.b.c.m.e.ManagedExecution	Dataset[label=null, name=TableDeletionTest]	DONE e84f8b75-b277-4a78-88ee-3fa08966c541 ManagedQuery within PT0.004473S
127.0.0.1 - - [17/Jan/2023:00:50:38 +0000] "GET /api/datasets/TableDeletionTest/queries/TableDeletionTest.e84f8b75-b277-4a78-88ee-3fa08966c541 HTTP/1.1" 200 1467 "-" "Conquery (test client)" 3
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node1
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node1
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:DATASET}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:TABLES}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:IMPORTS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:CONCEPTS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:WORKER}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:BUCKETS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:C_BLOCKS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[INFO] [2023-01-17 00:50:38]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:38]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:58214	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager fast shard-node0
[INFO] [2023-01-17 00:50:38]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager slow shard-node0
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager fast worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager slow worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:DATASET}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:TABLES}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:IMPORTS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:CONCEPTS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:WORKER}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:BUCKETS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:C_BLOCKS}
[INFO] [2023-01-17 00:50:38]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[INFO] [2023-01-17 00:50:38]	c.b.c.c.ShardNode		Connection was closed by ManagerNode
[INFO] [2023-01-17 00:50:38]	c.b.c.c.ShardNode	0.0.0.0/0.0.0.0:58212	Disconnected from ManagerNode
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager fast ManagerNode
[INFO] [2023-01-17 00:50:38]	c.b.c.c.ManagerNode	ManagerNode[0.0.0.0/0.0.0.0:46141]	Client 'null' disconnected 
[INFO] [2023-01-17 00:50:38]	c.b.c.m.j.JobExecutor		Closing Job Manager slow ManagerNode
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor		Closing Job Manager fast TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor		Closing Job Manager slow TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.w.Namespace		Closing namespace storage of TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:DATASET}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:SECONDARY_IDS}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:TABLES}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:DICTIONARIES_META}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:DICTIONARIES_DATA}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:IMPORTS}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:CONCEPTS}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:ID_MAPPING_META}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:ID_MAPPING_DATA}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:STRUCTURE}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:WORKER_TO_BUCKETS}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:PRIMARY_DICTIONARY}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
[INFO] [2023-01-17 00:50:39]	c.b.c.u.s.TestConquery		Working in temporary directory /tmp/conqueryIntegrationTest17490523365462791536
[DEBUG] [2023-01-17 00:50:39]	c.b.c.c.StandaloneCommand	ManagerNode	Starting ManagerNode
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:DATASET}): 1 entries, 61 B within 207.5 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:SECONDARY_IDS}): 0 entries, 0 B within 61.87 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:TABLES}): 2 entries, 346 B within 324.3 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.230 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:IMPORTS}): 2 entries, 1,002 B within 4.419 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:CONCEPTS}): 1 entries, 617 B within 4.125 ms
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	Done reading Dataset[label=TableDeletionTest, name=TableDeletionTest] / com.bakdata.conquery.io.storage.NamespaceStorage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store ID_MAPPING_META:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached big ID_MAPPING(EntityIdMap): 1 entries, 43 B within 206.7 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store STRUCTURE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:STRUCTURE}): 0 entries, 0 B within 25.58 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store WORKER_TO_BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:WORKER_TO_BUCKETS}): 1 entries, 308 B within 113.5 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	While processing store PRIMARY_DICTIONARY:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest:PRIMARY_DICTIONARY}): 1 entries, 103 B within 525.7 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest	DONE reading Storage
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	ManagerNode	All WorkerStores loaded: [NamespacedStorage(pathName=dataset_TableDeletionTest)]
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode	ManagerNode	Opening MetaStorage
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode	ManagerNode	Loading MetaStorage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_USER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}): 1 entries, 149 B within 1.084 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_ROLE:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}): 0 entries, 0 B within 93.47 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store AUTH_GROUP:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}): 0 entries, 0 B within 78.61 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store EXECUTIONS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}): 2 entries, 1.2 KiB within 11.47 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	ManagerNode	While processing store FORM_CONFIG:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	ManagerNode		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}): 0 entries, 0 B within 71.27 μs
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode	ManagerNode	MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@cba648f
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.a.AuthorizationController	ManagerNode	Security manager registered
[WARN] [2023-01-17 00:50:39]	c.b.c.m.a.d.DefaultInitialUserRealm	ManagerNode	
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
[INFO] [2023-01-17 00:50:39]	c.b.c.m.a.AuthorizationController	ManagerNode	Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_30
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_31
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_32
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode	ManagerNode	Registering ResourcesProvider
[INFO] [2023-01-17 00:50:39]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
[INFO] [2023-01-17 00:50:39]	c.b.c.m.f.f.FormScanner	ManagerNode	Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
[DEBUG] [2023-01-17 00:50:39]	c.b.c.c.StandaloneCommand	ManagerNode	Waiting for ShardNodes to start
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:DATASET}): 1 entries, 61 B within 104.5 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:SECONDARY_IDS}): 0 entries, 0 B within 27.25 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:TABLES}): 2 entries, 346 B within 194.8 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 689.8 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:IMPORTS}): 2 entries, 1,002 B within 2.675 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:CONCEPTS}): 1 entries, 617 B within 3.532 ms
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	Done reading Dataset[label=TableDeletionTest, name=TableDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:WORKER}): 1 entries, 136 B within 69.70 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store BUCKETS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:BUCKETS}): 2 entries, 740 B within 1.278 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	While processing store C_BLOCKS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b:C_BLOCKS}): 2 entries, 457 B within 188.3 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b	DONE reading Storage
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	shard-node1	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b))]
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode	shard-node1	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	BEGIN reading Storage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store DATASET:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:DATASET}): 1 entries, 61 B within 109.9 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store SECONDARY_IDS:
	Entries processed:	0
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:SECONDARY_IDS}): 0 entries, 0 B within 37.58 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store TABLES:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:TABLES}): 2 entries, 346 B within 196.2 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store DICTIONARIES_META:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached big DICTIONARIES(Dictionary): 2 entries, 86 B within 1.157 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store IMPORTS:
	Entries processed:	2
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:IMPORTS}): 2 entries, 1,002 B within 3.537 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store CONCEPTS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:CONCEPTS}): 1 entries, 617 B within 3.911 ms
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.NamespacedStorage	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	Done reading Dataset[label=TableDeletionTest, name=TableDeletionTest] / com.bakdata.conquery.io.storage.WorkerStorage
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store WORKER:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:WORKER}): 1 entries, 136 B within 78.74 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store BUCKETS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:BUCKETS}): 1 entries, 369 B within 2.538 ms
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.SerializingStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	While processing store C_BLOCKS:
	Entries processed:	1
	Key read failure:	0 (0.00%)
	Value read failure:	0 (0.00%)
[INFO] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.CachedStore	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d		loaded store cached SerializingStore(store=XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d:C_BLOCKS}): 1 entries, 230 B within 171.2 μs
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	/tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d	DONE reading Storage
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	shard-node0	All WorkerStores loaded: [WorkerStorage(worker=NamedImpl(name=worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d))]
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode	shard-node0	All Worker Storages loaded: 1
[DEBUG] [2023-01-17 00:50:39]	c.b.c.c.StandaloneCommand	ManagerNode	Starting REST Server
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode		Started ManagerNode @ /0:0:0:0:0:0:0:0:46141
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:58492 connected, waiting for identity
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode	/127.0.0.1:58492	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode		Trying to connect to /127.0.0.1:46141
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ManagerNode	ManagerNode[/127.0.0.1:46141]	New client /127.0.0.1:58494 connected, waiting for identity
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode	/127.0.0.1:58494	Connected to ManagerNode @ /127.0.0.1:46141
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode	/127.0.0.1:58492	Sending worker identity 'worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d'
[INFO] [2023-01-17 00:50:39]	c.b.c.c.ShardNode	/127.0.0.1:58494	Sending worker identity 'worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b'
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:58494` registered.
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.AddShardNode	Job Manager fast ManagerNode	ShardNode `/127.0.0.1:58492` registered.
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b are consistent with the manager: 2 Imports
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b are consistent with the manager: 2 Buckets
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Consistency check was successful
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Imports of worker TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d are consistent with the manager: 2 Imports
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Buckets of worker TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d are consistent with the manager: 1 Buckets
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ReportConsistency	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Consistency check was successful
[WARN] [2023-01-17 00:50:39]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:39]	i.d.s.AdminEnvironment		
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[WARN] [2023-01-17 00:50:39]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

[WARN] [2023-01-17 00:50:39]	o.g.j.i.Errors		The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

[INFO] [2023-01-17 00:50:39]	c.b.c.u.s.TestConquery		loading dataset
[INFO] [2023-01-17 00:50:39]	c.b.c.u.s.TestConquery		Reusing existing folder /tmp/conqueryIntegrationTest17490523365462791536/tmp_TableDeletionTest for Support
[INFO] [2023-01-17 00:50:39]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:39]	c.b.c.i.t.d.TableDeletionTest		Checking state after re-start
[INFO] [2023-01-17 00:50:39]	c.b.c.i.t.d.TableDeletionTest		Executing query after re-import
[INFO] [2023-01-17 00:50:39]	c.b.c.a.QueryProcessor	user.SUPERUSER@SUPERUSER	Query posted on Dataset[TableDeletionTest] by User[{user.SUPERUSER@SUPERUSER].
[INFO] [2023-01-17 00:50:39]	c.b.c.m.q.ExecutionManager	user.SUPERUSER@SUPERUSER	Executing Query[1b23ee14-413d-4182-847d-14953dc3689f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TableDeletionTest))]]
127.0.0.1 - - [17/Jan/2023:00:50:39 +0000] "POST /api/datasets/TableDeletionTest/queries HTTP/1.1" 201 1208 "-" "Conquery (test client)" 56
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.c.IntegrationUtils		Trying to get Query result
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58494]	Started ConceptQuery TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, /127.0.0.1:58494]	QueryPlan for Query[TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.ExecuteQuery	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58492]	Started ConceptQuery TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.q.QueryExecutor	Worker[TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, /127.0.0.1:58492]	QueryPlan for Query[TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f] = `ConceptQueryPlan(child=QPParentNode(children=[ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table, name=test_table]])])), validityDateColumn=Column(id = TableDeletionTest.test_table.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column], dateColumn=null), selectedSecondaryId=null), ConceptNode(super=QPChainNode(child=ValidityDateNode(super=QPChainNode(child=FiltersNode(filters=[], aggregators=[EventDateUnionAggregator(requiredTables=[Table[label=test_table2, name=test_table2]])])), validityDateColumn=Column(id = TableDeletionTest.test_table2.datum, type = DATE))), table=CQTable(filters=[], selects=[], connector=ConceptTreeConnector[label=tree_label, name=test_column2], dateColumn=null), selectedSecondaryId=null)]), dateAggregator=DateAggregator(action=MERGE))`
[INFO] [2023-01-17 00:50:39]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f] with 2 results within PT0.001626S
[INFO] [2023-01-17 00:50:39]	c.b.c.m.q.r.ShardResult		FINISHED Query[TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f] with 0 results within PT0.000895S
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f, workerId=TableDeletionTest.worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d, startTime=2023-01-17T00:50:39.739366, finishTime=2023-01-17T00:50:39.740261) of size 0
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.q.ManagedQuery	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received Result[size=0] for Query[TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f]
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.CollectQueryResult	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received ShardResult(queryId=TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f, workerId=TableDeletionTest.worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b, startTime=2023-01-17T00:50:39.738328, finishTime=2023-01-17T00:50:39.739954) of size 2
[DEBUG] [2023-01-17 00:50:39]	c.b.c.m.q.ManagedQuery	Dataset[label=TableDeletionTest, name=TableDeletionTest]	Received Result[size=2] for Query[TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f]
[INFO] [2023-01-17 00:50:39]	c.b.c.m.e.ManagedExecution	Dataset[label=TableDeletionTest, name=TableDeletionTest]	DONE 1b23ee14-413d-4182-847d-14953dc3689f ManagedQuery within PT0.037438S
127.0.0.1 - - [17/Jan/2023:00:50:39 +0000] "GET /api/datasets/TableDeletionTest/queries/TableDeletionTest.1b23ee14-413d-4182-847d-14953dc3689f HTTP/1.1" 200 1468 "-" "Conquery (test client)" 4
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor		Closing Job Manager fast TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node1	removing worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager fast worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[INFO] [2023-01-17 00:50:39]	c.b.c.m.m.n.s.RemoveWorker	Job Manager slow shard-node0	removing worker for Dataset[label=TableDeletionTest, name=TableDeletionTest]
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager fast worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node1	Closing Job Manager slow worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor		Closing Job Manager slow TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.j.JobExecutor	Job Manager slow shard-node0	Closing Job Manager slow worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store WORKER from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node1	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore	Job Manager slow shard-node0	Deleting store C_BLOCKS from environment /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node1	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node1/worker_worker_TableDeletionTest_252db35e-5586-470e-962e-a82c5451d89b
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory	Job Manager slow shard-node0	Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/shard-node0/worker_worker_TableDeletionTest_b34ddc0d-94c6-46ef-b79b-74ac5a31269d
[INFO] [2023-01-17 00:50:39]	c.b.c.m.w.Namespace		Removing namespace storage of TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store DATASET from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store SECONDARY_IDS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store TABLES from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store DICTIONARIES_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store IMPORTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store CONCEPTS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_META from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store ID_MAPPING_DATA from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store STRUCTURE from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store WORKER_TO_BUCKETS from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[DEBUG] [2023-01-17 00:50:39]	c.b.c.i.s.x.s.XodusStore		Deleting store PRIMARY_DICTIONARY from environment /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.m.c.XodusStoreFactory		Removed last XodusStore in Environment. Removing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/dataset_TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.u.s.TestConquery		Waiting for jobs to finish
[INFO] [2023-01-17 00:50:39]	c.b.c.i.IntegrationTest$Wrapper	TableDeletionTest	SUCCESS integration test TableDeletionTest
[INFO] [2023-01-17 00:50:39]	c.b.c.u.s.TestConquery	TableDeletionTest	Working in temporary directory /tmp/conqueryIntegrationTest7483997657802395183
INFO  [2023-01-17 00:50:39,945] io.dropwizard.server.DefaultServerFactory: Registering jersey handler with root path prefix: /
INFO  [2023-01-17 00:50:39,945] io.dropwizard.server.DefaultServerFactory: Registering admin handler with root path prefix: /
INFO  [2023-01-17 00:50:39,948] com.bakdata.conquery.commands.ManagerNode: Opening MetaStorage
INFO  [2023-01-17 00:50:39,949] com.bakdata.conquery.commands.ManagerNode: Loading MetaStorage
INFO  [2023-01-17 00:50:39,949] com.bakdata.conquery.commands.ManagerNode: MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@25863e4b
WARN  [2023-01-17 00:50:39,949] com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm: 
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
INFO  [2023-01-17 00:50:39,949] com.bakdata.conquery.models.auth.AuthorizationController: Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_33
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_34
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_35
INFO  [2023-01-17 00:50:39,949] com.bakdata.conquery.commands.ManagerNode: Registering ResourcesProvider
INFO  [2023-01-17 00:50:39,953] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
INFO  [2023-01-17 00:50:39,953] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
INFO  [2023-01-17 00:50:39,967] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-17 00:50:39,968] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-17 00:50:39,968] io.dropwizard.server.ServerFactory: Starting Conquery


 ▄████████  ▄██████▄  ███▄▄▄▄   ████████▄   ███    █▄     ▄████████    ▄████████ ▄██   ▄   
███    ███ ███    ███ ███▀▀▀██▄ ███    ███  ███    ███   ███    ███   ███    ███ ███   ██▄ 
███    █▀  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▀    ███    ███ ███▄▄▄███ 
███        ███    ███ ███   ███ ███    ███  ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀ ▀▀▀▀▀▀███ 
███        ███    ███ ███   ███ ███    ███  ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀   ▄██   ███ 
███    █▄  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▄  ▀███████████ ███   ███ 
███    ███ ███    ███ ███   ███ ███  ▀ ███  ███    ███   ███    ███   ███    ███ ███   ███ 
████████▀   ▀██████▀   ▀█   █▀   ▀██████▀▄█ ████████▀    ██████████   ███    ███  ▀█████▀  
                                                                      ███    ███           
Version: 0.0.0-SNAPSHOT(${git.commit.id.describe})

           
INFO  [2023-01-17 00:50:39,971] org.eclipse.jetty.setuid.SetUIDListener: Opened application@2fd2b877{HTTP/1.1, (http/1.1)}{0.0.0.0:34263}
INFO  [2023-01-17 00:50:39,971] org.eclipse.jetty.setuid.SetUIDListener: Opened admin@71cf415f{HTTP/1.1, (http/1.1)}{0.0.0.0:40167}
INFO  [2023-01-17 00:50:39,971] org.eclipse.jetty.server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.17+8-post-Ubuntu-1ubuntu218.04
INFO  [2023-01-17 00:50:39,973] com.bakdata.conquery.commands.ManagerNode: Started ManagerNode @ /0:0:0:0:0:0:0:0:37903
INFO  [2023-01-17 00:50:39,974] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:37903
INFO  [2023-01-17 00:50:39,975] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:42922 connected, waiting for identity
INFO  [2023-01-17 00:50:39,975] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:37903
INFO  [2023-01-17 00:50:39,977] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:37903
INFO  [2023-01-17 00:50:39,977] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:42924 connected, waiting for identity
INFO  [2023-01-17 00:50:39,978] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:37903
INFO  [2023-01-17 00:50:40,000] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:42922` registered.
INFO  [2023-01-17 00:50:40,000] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:42924` registered.
INFO  [2023-01-17 00:50:40,066] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /api/config/frontend (com.bakdata.conquery.resources.api.ConfigResource)
    GET     /api/datasets (com.bakdata.conquery.resources.api.DatasetsResource)
    GET     /api/datasets/{dataset}/concepts (com.bakdata.conquery.resources.api.DatasetResource)
    GET     /api/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/resolve (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/autocomplete (com.bakdata.conquery.resources.api.FilterResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/resolve (com.bakdata.conquery.resources.api.FilterResource)
    GET     /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    POST    /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    DELETE  /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    PATCH   /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-queries (com.bakdata.conquery.resources.api.FormResource)
    GET     /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/upload (com.bakdata.conquery.resources.api.QueryResource)
    DELETE  /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    PATCH   /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/cancel (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/reexecute (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/result/{query}.arrf (com.bakdata.conquery.resources.api.ResultArrowFileResource)
    GET     /api/datasets/{dataset}/result/{query}.arrs (com.bakdata.conquery.resources.api.ResultArrowStreamResource)
    GET     /api/datasets/{dataset}/result/{query}.csv (com.bakdata.conquery.resources.api.ResultCsvResource)
    GET     /api/datasets/{dataset}/result/{query}.xlsx (com.bakdata.conquery.resources.api.ResultExcelResource)
    GET     /api/me (com.bakdata.conquery.resources.api.MeResource)

WARN  [2023-01-17 00:50:40,067] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:40,101] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-17 00:50:40,101] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@52836725{/,null,AVAILABLE}
INFO  [2023-01-17 00:50:40,101] io.dropwizard.setup.AdminEnvironment: tasks = 

    POST    /tasks/permission-cleanup (com.bakdata.conquery.tasks.PermissionCleanupTask)
    POST    /tasks/report-consistency (com.bakdata.conquery.tasks.ReportConsistencyTask)
    POST    /tasks/log-level (io.dropwizard.servlets.tasks.LogConfigurationTask)
    POST    /tasks/form-scanner (com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner)
    POST    /tasks/clear-filter-source-search (com.bakdata.conquery.tasks.ClearFilterSourceSearch)
    POST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)
    POST    /tasks/query-cleanup (com.bakdata.conquery.tasks.QueryCleanupTask)
    POST    /tasks/shutdown (com.bakdata.conquery.resources.admin.ShutdownTask)

WARN  [2023-01-17 00:50:40,101] io.dropwizard.setup.AdminEnvironment: 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
INFO  [2023-01-17 00:50:40,155] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin/auth-overview/csv (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/auth-overview/csv/group/{groupId} (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    POST    /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    DELETE  /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    GET     /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    POST    /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/label (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/secondaryId (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/secondaryId/{secondaryId} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/structure (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    DELETE  /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    POST    /admin/datasets/{dataset}/update-matching-stats (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/weight (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/jobs (com.bakdata.conquery.resources.admin.rest.AdminResource)
    POST    /admin/jobs/{jobId}/cancel (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/logout (com.bakdata.conquery.resources.admin.rest.AdminResource)
    DELETE  /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    POST    /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    GET     /admin/queries (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    DELETE  /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    GET     /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/script (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/upload (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    GET     /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)

WARN  [2023-01-17 00:50:40,156] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:40,189] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin-ui/ (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/auth-overview (com.bakdata.conquery.resources.admin.ui.AuthOverviewUIResource)
    GET     /admin-ui/datasets (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset} (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.ui.ConceptsUIResource)
    GET     /admin-ui/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table}/import/{importId} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/groups (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/groups/{groupId} (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/jobs (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/queries (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/roles (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/roles/{roleId} (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/script (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/users (com.bakdata.conquery.resources.admin.ui.UserUIResource)
    GET     /admin-ui/users/{userId} (com.bakdata.conquery.resources.admin.ui.UserUIResource)

WARN  [2023-01-17 00:50:40,189] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:40,232] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-17 00:50:40,233] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@1449d690{/,null,AVAILABLE}
INFO  [2023-01-17 00:50:40,238] org.eclipse.jetty.server.AbstractConnector: Started application@2fd2b877{HTTP/1.1, (http/1.1)}{0.0.0.0:34263}
INFO  [2023-01-17 00:50:40,240] org.eclipse.jetty.server.AbstractConnector: Started admin@71cf415f{HTTP/1.1, (http/1.1)}{0.0.0.0:40167}
INFO  [2023-01-17 00:50:40,240] org.eclipse.jetty.server.Server: Started @57600ms
INFO  [2023-01-17 00:50:40,348] com.bakdata.conquery.util.support.TestConquery: Working in temporary directory /tmp/conqueryIntegrationTest7483997657802395183
INFO  [2023-01-17 00:50:40,358] io.dropwizard.server.DefaultServerFactory: Registering jersey handler with root path prefix: /
INFO  [2023-01-17 00:50:40,358] io.dropwizard.server.DefaultServerFactory: Registering admin handler with root path prefix: /
INFO  [2023-01-17 00:50:40,361] com.bakdata.conquery.commands.ManagerNode: Opening MetaStorage
INFO  [2023-01-17 00:50:40,361] com.bakdata.conquery.commands.ManagerNode: Loading MetaStorage
INFO  [2023-01-17 00:50:40,361] com.bakdata.conquery.commands.ManagerNode: MetaStorage loaded com.bakdata.conquery.io.storage.MetaStorage@7aa9b025
WARN  [2023-01-17 00:50:40,362] com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm: 
           §§
          §  §
         §    §
        §      §
       §  §§§§  §       You instantiated and are probably using a Shiro realm
      §   §§§§   §      that does not do any permission checks or authentication.
     §     §§     §     Access to all resources is granted to everyone.
    §      §§      §    DO NOT USE THIS REALM IN PRODUCTION
   $                §
  §        §§        §
 §                    §
 §§§§§§§§§§§§§§§§§§§§§§
INFO  [2023-01-17 00:50:40,362] com.bakdata.conquery.models.auth.AuthorizationController: Registering the following realms to Shiro:
	com.bakdata.conquery.models.auth.conquerytoken.ConqueryTokenRealm_36
	com.bakdata.conquery.models.auth.ConqueryAuthorizationRealm_37
	com.bakdata.conquery.models.auth.develop.DefaultInitialUserRealm_38
INFO  [2023-01-17 00:50:40,362] com.bakdata.conquery.commands.ManagerNode: Registering ResourcesProvider
INFO  [2023-01-17 00:50:40,366] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.ExportForm]
INFO  [2023-01-17 00:50:40,366] com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner: Form[FULL_EXPORT_FORM] from `Resource com/bakdata/conquery/frontend/forms/table_export_form.frontend_conf.json` of Type[com.bakdata.conquery.apiv1.forms.export_form.FullExportForm]
INFO  [2023-01-17 00:50:40,405] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-17 00:50:40,406] com.bakdata.conquery.commands.ShardNode: All Worker Storages loaded: 0
INFO  [2023-01-17 00:50:40,406] io.dropwizard.server.ServerFactory: Starting Conquery


 ▄████████  ▄██████▄  ███▄▄▄▄   ████████▄   ███    █▄     ▄████████    ▄████████ ▄██   ▄   
███    ███ ███    ███ ███▀▀▀██▄ ███    ███  ███    ███   ███    ███   ███    ███ ███   ██▄ 
███    █▀  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▀    ███    ███ ███▄▄▄███ 
███        ███    ███ ███   ███ ███    ███  ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀ ▀▀▀▀▀▀███ 
███        ███    ███ ███   ███ ███    ███  ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀   ▄██   ███ 
███    █▄  ███    ███ ███   ███ ███    ███  ███    ███   ███    █▄  ▀███████████ ███   ███ 
███    ███ ███    ███ ███   ███ ███  ▀ ███  ███    ███   ███    ███   ███    ███ ███   ███ 
████████▀   ▀██████▀   ▀█   █▀   ▀██████▀▄█ ████████▀    ██████████   ███    ███  ▀█████▀  
                                                                      ███    ███           
Version: 0.0.0-SNAPSHOT(${git.commit.id.describe})

           
INFO  [2023-01-17 00:50:40,409] org.eclipse.jetty.setuid.SetUIDListener: Opened application@3239a0b6{HTTP/1.1, (http/1.1)}{0.0.0.0:45089}
INFO  [2023-01-17 00:50:40,409] org.eclipse.jetty.setuid.SetUIDListener: Opened admin@1c699f7{HTTP/1.1, (http/1.1)}{0.0.0.0:46479}
INFO  [2023-01-17 00:50:40,409] org.eclipse.jetty.server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.17+8-post-Ubuntu-1ubuntu218.04
INFO  [2023-01-17 00:50:40,413] com.bakdata.conquery.commands.ManagerNode: Started ManagerNode @ /0:0:0:0:0:0:0:0:42563
INFO  [2023-01-17 00:50:40,436] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:42563
INFO  [2023-01-17 00:50:40,437] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:43222 connected, waiting for identity
INFO  [2023-01-17 00:50:40,438] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:42563
INFO  [2023-01-17 00:50:40,440] com.bakdata.conquery.commands.ShardNode: Trying to connect to /127.0.0.1:42563
INFO  [2023-01-17 00:50:40,451] com.bakdata.conquery.commands.ManagerNode: New client /127.0.0.1:43226 connected, waiting for identity
INFO  [2023-01-17 00:50:40,451] com.bakdata.conquery.commands.ShardNode: Connected to ManagerNode @ /127.0.0.1:42563
INFO  [2023-01-17 00:50:40,452] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:43222` registered.
INFO  [2023-01-17 00:50:40,455] com.bakdata.conquery.models.messages.network.specific.AddShardNode: ShardNode `/127.0.0.1:43226` registered.
INFO  [2023-01-17 00:50:40,531] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /api/config/frontend (com.bakdata.conquery.resources.api.ConfigResource)
    GET     /api/datasets (com.bakdata.conquery.resources.api.DatasetsResource)
    GET     /api/datasets/{dataset}/concepts (com.bakdata.conquery.resources.api.DatasetResource)
    GET     /api/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/resolve (com.bakdata.conquery.resources.api.ConceptResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/autocomplete (com.bakdata.conquery.resources.api.FilterResource)
    POST    /api/datasets/{dataset}/concepts/{concept}/tables/{table}/filters/{filter}/resolve (com.bakdata.conquery.resources.api.FilterResource)
    GET     /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    POST    /api/datasets/{dataset}/form-configs (com.bakdata.conquery.resources.api.FormConfigResource)
    DELETE  /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    PATCH   /api/datasets/{dataset}/form-configs/{form-config} (com.bakdata.conquery.resources.api.FormConfigResource)
    GET     /api/datasets/{dataset}/form-queries (com.bakdata.conquery.resources.api.FormResource)
    GET     /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/upload (com.bakdata.conquery.resources.api.QueryResource)
    DELETE  /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    PATCH   /api/datasets/{dataset}/queries/{query} (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/cancel (com.bakdata.conquery.resources.api.QueryResource)
    POST    /api/datasets/{dataset}/queries/{query}/reexecute (com.bakdata.conquery.resources.api.QueryResource)
    GET     /api/datasets/{dataset}/result/{query}.arrf (com.bakdata.conquery.resources.api.ResultArrowFileResource)
    GET     /api/datasets/{dataset}/result/{query}.arrs (com.bakdata.conquery.resources.api.ResultArrowStreamResource)
    GET     /api/datasets/{dataset}/result/{query}.csv (com.bakdata.conquery.resources.api.ResultCsvResource)
    GET     /api/datasets/{dataset}/result/{query}.xlsx (com.bakdata.conquery.resources.api.ResultExcelResource)
    GET     /api/me (com.bakdata.conquery.resources.api.MeResource)

WARN  [2023-01-17 00:50:40,531] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.getStatus(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) throws java.lang.InterruptedException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.patchQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>,com.bakdata.conquery.apiv1.MetaDataPatch) throws com.bakdata.conquery.models.exceptions.JSONException is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.deleteQuery(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public void com.bakdata.conquery.resources.api.QueryResource.cancel(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>) is not resolvable to a concrete type.
WARNING: Parameter 2 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public com.bakdata.conquery.apiv1.FullExecutionStatus com.bakdata.conquery.resources.api.QueryResource.reexecute(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultExcelResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.identifiable.ids.specific.DatasetId,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowStreamResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultCsvResource.getAsCsv(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.lang.String,java.lang.String,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 3 of type com.bakdata.conquery.models.execution.ManagedExecution<?> from public javax.ws.rs.core.Response com.bakdata.conquery.resources.api.ResultArrowFileResource.get(com.bakdata.conquery.models.auth.entities.Subject,com.bakdata.conquery.models.datasets.Dataset,com.bakdata.conquery.models.execution.ManagedExecution<?>,java.util.Optional<java.lang.Boolean>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.filters.Filter<?> from public void com.bakdata.conquery.resources.hierarchies.HFilters.setFilter(com.bakdata.conquery.models.datasets.concepts.filters.Filter<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.hierarchies.HConcepts.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:40,560] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-17 00:50:40,560] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@536093ae{/,null,AVAILABLE}
INFO  [2023-01-17 00:50:40,560] io.dropwizard.setup.AdminEnvironment: tasks = 

    POST    /tasks/permission-cleanup (com.bakdata.conquery.tasks.PermissionCleanupTask)
    POST    /tasks/report-consistency (com.bakdata.conquery.tasks.ReportConsistencyTask)
    POST    /tasks/log-level (io.dropwizard.servlets.tasks.LogConfigurationTask)
    POST    /tasks/form-scanner (com.bakdata.conquery.models.forms.frontendconfiguration.FormScanner)
    POST    /tasks/clear-filter-source-search (com.bakdata.conquery.tasks.ClearFilterSourceSearch)
    POST    /tasks/gc (io.dropwizard.servlets.tasks.GarbageCollectionTask)
    POST    /tasks/query-cleanup (com.bakdata.conquery.tasks.QueryCleanupTask)
    POST    /tasks/shutdown (com.bakdata.conquery.resources.admin.ShutdownTask)

WARN  [2023-01-17 00:50:40,560] io.dropwizard.setup.AdminEnvironment: 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    THIS APPLICATION HAS NO HEALTHCHECKS. THIS MEANS YOU WILL NEVER KNOW      !
!     IF IT DIES IN PRODUCTION, WHICH MEANS YOU WILL NEVER KNOW IF YOU'RE      !
!    LETTING YOUR USERS DOWN. YOU SHOULD ADD A HEALTHCHECK FOR EACH OF YOUR    !
!         APPLICATION'S DEPENDENCIES WHICH FULLY (BUT LIGHTLY) TESTS IT.       !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
INFO  [2023-01-17 00:50:40,611] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin/auth-overview/csv (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/auth-overview/csv/group/{groupId} (com.bakdata.conquery.resources.admin.rest.AuthOverviewResource)
    GET     /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    POST    /admin/datasets (com.bakdata.conquery.resources.admin.rest.AdminDatasetsResource)
    DELETE  /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/concepts (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    GET     /admin/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.rest.AdminConceptsResource)
    POST    /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/cqpp (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    PUT     /admin/datasets/{dataset}/imports (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/label (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/secondaryId (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/secondaryId/{secondaryId} (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/structure (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/tables (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    DELETE  /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    DELETE  /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    GET     /admin/datasets/{dataset}/tables/{table}/imports/{importId} (com.bakdata.conquery.resources.admin.rest.AdminTablesResource)
    POST    /admin/datasets/{dataset}/update-matching-stats (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    POST    /admin/datasets/{dataset}/weight (com.bakdata.conquery.resources.admin.rest.AdminDatasetResource)
    GET     /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/groups/{groupId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    DELETE  /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    POST    /admin/groups/{groupId}/users/{userId} (com.bakdata.conquery.resources.admin.rest.GroupResource)
    GET     /admin/jobs (com.bakdata.conquery.resources.admin.rest.AdminResource)
    POST    /admin/jobs/{jobId}/cancel (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/logout (com.bakdata.conquery.resources.admin.rest.AdminResource)
    DELETE  /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    POST    /admin/permissions/{ownerId} (com.bakdata.conquery.resources.admin.rest.PermissionResource)
    GET     /admin/queries (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/roles (com.bakdata.conquery.resources.admin.rest.RoleResource)
    DELETE  /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    GET     /admin/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.RoleResource)
    POST    /admin/script (com.bakdata.conquery.resources.admin.rest.AdminResource)
    GET     /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/upload (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    GET     /admin/users/{userId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    DELETE  /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)
    POST    /admin/users/{userId}/roles/{roleId} (com.bakdata.conquery.resources.admin.rest.UserResource)

WARN  [2023-01-17 00:50:40,611] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.auth.entities.PermissionOwner<?> from public void com.bakdata.conquery.resources.hierarchies.HPermissions.setOwner(com.bakdata.conquery.models.auth.entities.PermissionOwner<?>) is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:40,638] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    GET     /admin-ui/ (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/auth-overview (com.bakdata.conquery.resources.admin.ui.AuthOverviewUIResource)
    GET     /admin-ui/datasets (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset} (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/concepts/{concept} (com.bakdata.conquery.resources.admin.ui.ConceptsUIResource)
    GET     /admin-ui/datasets/{dataset}/mapping (com.bakdata.conquery.resources.admin.ui.DatasetsUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/datasets/{dataset}/tables/{table}/import/{importId} (com.bakdata.conquery.resources.admin.ui.TablesUIResource)
    GET     /admin-ui/groups (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/groups/{groupId} (com.bakdata.conquery.resources.admin.ui.GroupUIResource)
    GET     /admin-ui/jobs (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/queries (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/roles (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/roles/{roleId} (com.bakdata.conquery.resources.admin.ui.RoleUIResource)
    GET     /admin-ui/script (com.bakdata.conquery.resources.admin.ui.AdminUIResource)
    GET     /admin-ui/users (com.bakdata.conquery.resources.admin.ui.UserUIResource)
    GET     /admin-ui/users/{userId} (com.bakdata.conquery.resources.admin.ui.UserUIResource)

WARN  [2023-01-17 00:50:40,638] org.glassfish.jersey.internal.Errors: The following warnings have been detected: WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.
WARNING: Parameter 1 of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from public void com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.setConcept(com.bakdata.conquery.models.datasets.concepts.Concept<?>) is not resolvable to a concrete type.
WARNING: Parameter concept of type com.bakdata.conquery.models.datasets.concepts.Concept<?> from protected com.bakdata.conquery.models.datasets.concepts.Concept<?> com.bakdata.conquery.resources.admin.ui.ConceptsUIResource.concept is not resolvable to a concrete type.

INFO  [2023-01-17 00:50:40,664] io.dropwizard.jersey.DropwizardResourceConfig: The following paths were found for the configured resources:

    NONE

INFO  [2023-01-17 00:50:40,664] org.eclipse.jetty.server.handler.ContextHandler: Started i.d.j.MutableServletContextHandler@ac8b0e5{/,null,AVAILABLE}
INFO  [2023-01-17 00:50:40,680] org.eclipse.jetty.server.AbstractConnector: Started application@3239a0b6{HTTP/1.1, (http/1.1)}{0.0.0.0:45089}
INFO  [2023-01-17 00:50:40,682] org.eclipse.jetty.server.AbstractConnector: Started admin@1c699f7{HTTP/1.1, (http/1.1)}{0.0.0.0:46479}
INFO  [2023-01-17 00:50:40,682] org.eclipse.jetty.server.Server: Started @58043ms
INFO  [2023-01-17 00:50:40,777] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT Test
INFO  [2023-01-17 00:50:40,778] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:40,778] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:40,815] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-17 00:50:40,815] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:40,815] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-17 00:50:40,815] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:40,827] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:40,828] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_6fa8df22-694c-4f39-adca-0213cf7c33ad are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:40,828] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_6fa8df22-694c-4f39-adca-0213cf7c33ad are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:40,828] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:40,828] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_1315b4e3-f8e0-4513-90d0-9a67408b1dcf are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:40,828] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_1315b4e3-f8e0-4513-90d0-9a67408b1dcf are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:40,828] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:40,940] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT$20Test.table
INFO  [2023-01-17 00:50:40,940] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:40,941] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT$20Test.table
INFO  [2023-01-17 00:50:41,085] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:41,236] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:41,241] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:41,241] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 76 B in total
INFO  [2023-01-17 00:50:41,241] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000249976sINFO  [2023-01-17 00:50:41,267] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:41,267] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@6c9479ad)
INFO  [2023-01-17 00:50:41,267] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:41,270] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:41,270] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:41,270] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:41,297] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:41 +0000] "POST /admin/datasets/BIG_MULTI_SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:41,299] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:41,301] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:41,302] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:41,302] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:41,304] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-17 00:50:41,305] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:41,317] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-17 00:50:41,318] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT$20Test.table.table.0
INFO  [2023-01-17 00:50:41,320] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-17 00:50:41,321] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT$20Test.table.table.1
INFO  [2023-01-17 00:50:41,447] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT Test QUERY INIT
INFO  [2023-01-17 00:50:41,483] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:41,484] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ac72fa97-f78a-4e5b-a76a-0f780a5dd29f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT Test))]]
127.0.0.1 - - [17/Jan/2023:00:50:41 +0000] "POST /api/datasets/BIG_MULTI_SELECT$20Test/queries HTTP/1.1" 201 1196 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:50:41,498] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f
INFO  [2023-01-17 00:50:41,499] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f
INFO  [2023-01-17 00:50:41,500] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f] with 0 results within PT0.001821S
INFO  [2023-01-17 00:50:41,500] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f] with 2 results within PT0.001303S
INFO  [2023-01-17 00:50:41,504] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f, workerId=BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_6fa8df22-694c-4f39-adca-0213cf7c33ad, startTime=2023-01-17T00:50:41.498560, finishTime=2023-01-17T00:50:41.500381) of size 0
INFO  [2023-01-17 00:50:41,504] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f, workerId=BIG_MULTI_SELECT$20Test.worker_BIG_MULTI_SELECT$20Test_1315b4e3-f8e0-4513-90d0-9a67408b1dcf, startTime=2023-01-17T00:50:41.499537, finishTime=2023-01-17T00:50:41.500840) of size 2
INFO  [2023-01-17 00:50:41,505] com.bakdata.conquery.models.execution.ManagedExecution: DONE ac72fa97-f78a-4e5b-a76a-0f780a5dd29f ManagedQuery within PT0.021181S
127.0.0.1 - - [17/Jan/2023:00:50:41 +0000] "GET /api/datasets/BIG_MULTI_SELECT$20Test/queries/BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f HTTP/1.1" 200 1480 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:50:41,534] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT Test], queryId=ac72fa97-f78a-4e5b-a76a-0f780a5dd29f, label=concept	@§$, creationTime=2023-01-17T00:50:41.484138, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7da092e6[Count = 0], startTime=2023-01-17T00:50:41.484410, finishTime=2023-01-17T00:50:41.505591, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5bcc5a78), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1f6a6a9a, com.bakdata.conquery.models.query.ColumnDescriptor@66a4fc33]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:41,534] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT Test], queryId=ac72fa97-f78a-4e5b-a76a-0f780a5dd29f, label=concept	@§$, creationTime=2023-01-17T00:50:41.484138, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7da092e6[Count = 0], startTime=2023-01-17T00:50:41.484410, finishTime=2023-01-17T00:50:41.505591, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5bcc5a78), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1f6a6a9a, com.bakdata.conquery.models.query.ColumnDescriptor@66a4fc33]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT Test]
127.0.0.1 - - [17/Jan/2023:00:50:41 +0000] "GET /api/datasets/BIG_MULTI_SELECT%20Test/result/BIG_MULTI_SELECT$20Test.ac72fa97-f78a-4e5b-a76a-0f780a5dd29f.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 37
INFO  [2023-01-17 00:50:41,568] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT Test on 3 rows
INFO  [2023-01-17 00:50:41,569] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT Test
INFO  [2023-01-17 00:50:41,570] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-17 00:50:41,570] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT Test_6fa8df22-694c-4f39-adca-0213cf7c33ad
INFO  [2023-01-17 00:50:41,572] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT Test, name=BIG_MULTI_SELECT Test]
INFO  [2023-01-17 00:50:41,572] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT Test_1315b4e3-f8e0-4513-90d0-9a67408b1dcf
INFO  [2023-01-17 00:50:41,579] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT Test
INFO  [2023-01-17 00:50:41,606] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT$20Test
INFO  [2023-01-17 00:50:41,606] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:41,632] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT Test_6fa8df22-694c-4f39-adca-0213cf7c33ad
INFO  [2023-01-17 00:50:41,633] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT Test_1315b4e3-f8e0-4513-90d0-9a67408b1dcf
INFO  [2023-01-17 00:50:41,838] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT Test
INFO  [2023-01-17 00:50:41,839] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-17 00:50:41,839] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:41,839] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:41,840] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-17 00:50:41,840] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-17 00:50:41,840] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:41,840] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:41,842] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_41e4678b-a05d-4ddc-9011-c1e3cd4a44c7 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:41,842] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_41e4678b-a05d-4ddc-9011-c1e3cd4a44c7 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:41,842] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:41,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_8dac1856-2ff7-4e5a-983e-d645ff4e23c5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:41,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_8dac1856-2ff7-4e5a-983e-d645ff4e23c5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:41,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:41,847] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:41,950] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:41,951] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_2VALUES$20Test.table
INFO  [2023-01-17 00:50:41,951] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_2VALUES$20Test.table
INFO  [2023-01-17 00:50:42,069] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:42,183] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:42,183] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:42,183] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 91 B in total
INFO  [2023-01-17 00:50:42,184] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_2VALUES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000582356sINFO  [2023-01-17 00:50:42,243] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:50:42,243] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@28ebc038)
INFO  [2023-01-17 00:50:42,243] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:42,249] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_2VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:42,250] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:42,250] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_2VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:42,278] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_2VALUES$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:42 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_2VALUES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT_2VALUES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-17 00:50:42,281] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:42,281] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:42,282] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:42,282] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:42,292] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:42,292] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_2VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:50:42,292] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_2VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:50:42,294] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_2VALUES$20Test.table.table.0
WARN  [2023-01-17 00:50:42,294] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:42,294] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_2VALUES$20Test.table.table.1
INFO  [2023-01-17 00:50:42,410] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_2VALUES Test QUERY INIT
INFO  [2023-01-17 00:50:42,428] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_2VALUES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:42,428] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9aae75ed-1d10-4216-ad22-02de116ef0bf] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_2VALUES Test))]]
INFO  [2023-01-17 00:50:42,433] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf
INFO  [2023-01-17 00:50:42,433] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf
INFO  [2023-01-17 00:50:42,434] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf] with 0 results within PT0.000665S
INFO  [2023-01-17 00:50:42,435] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf] with 3 results within PT0.002027S
INFO  [2023-01-17 00:50:42,435] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf, workerId=BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_41e4678b-a05d-4ddc-9011-c1e3cd4a44c7, startTime=2023-01-17T00:50:42.433552, finishTime=2023-01-17T00:50:42.434217) of size 0
127.0.0.1 - - [17/Jan/2023:00:50:42 +0000] "POST /api/datasets/BIG_MULTI_SELECT_2VALUES$20Test/queries HTTP/1.1" 201 1233 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:50:42,436] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf, workerId=BIG_MULTI_SELECT_2VALUES$20Test.worker_BIG_MULTI_SELECT_2VALUES$20Test_8dac1856-2ff7-4e5a-983e-d645ff4e23c5, startTime=2023-01-17T00:50:42.433394, finishTime=2023-01-17T00:50:42.435421) of size 3
INFO  [2023-01-17 00:50:42,437] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9aae75ed-1d10-4216-ad22-02de116ef0bf ManagedQuery within PT0.00877S
127.0.0.1 - - [17/Jan/2023:00:50:42 +0000] "GET /api/datasets/BIG_MULTI_SELECT_2VALUES$20Test/queries/BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf HTTP/1.1" 200 1548 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:50:42,468] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test], queryId=9aae75ed-1d10-4216-ad22-02de116ef0bf, label=concept	@§$, creationTime=2023-01-17T00:50:42.428738, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@63669fa7[Count = 0], startTime=2023-01-17T00:50:42.429028, finishTime=2023-01-17T00:50:42.437798, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2c467a32), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_2VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@524ffd53, com.bakdata.conquery.models.query.ColumnDescriptor@54854678]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:42,469] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test], queryId=9aae75ed-1d10-4216-ad22-02de116ef0bf, label=concept	@§$, creationTime=2023-01-17T00:50:42.428738, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@63669fa7[Count = 0], startTime=2023-01-17T00:50:42.429028, finishTime=2023-01-17T00:50:42.437798, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2c467a32), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_2VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@524ffd53, com.bakdata.conquery.models.query.ColumnDescriptor@54854678]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_2VALUES Test]
127.0.0.1 - - [17/Jan/2023:00:50:42 +0000] "GET /api/datasets/BIG_MULTI_SELECT_2VALUES%20Test/result/BIG_MULTI_SELECT_2VALUES$20Test.9aae75ed-1d10-4216-ad22-02de116ef0bf.csv?pretty=false HTTP/1.1" 200 115 "-" "Conquery (test client)" 34
INFO  [2023-01-17 00:50:42,501] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_2VALUES Test on 4 rows
INFO  [2023-01-17 00:50:42,501] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-17 00:50:42,501] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-17 00:50:42,501] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_2VALUES Test, name=BIG_MULTI_SELECT_2VALUES Test]
INFO  [2023-01-17 00:50:42,501] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_2VALUES Test_41e4678b-a05d-4ddc-9011-c1e3cd4a44c7
INFO  [2023-01-17 00:50:42,501] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_2VALUES Test_8dac1856-2ff7-4e5a-983e-d645ff4e23c5
INFO  [2023-01-17 00:50:42,540] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-17 00:50:42,541] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_2VALUES Test_41e4678b-a05d-4ddc-9011-c1e3cd4a44c7
INFO  [2023-01-17 00:50:42,542] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_2VALUES Test_8dac1856-2ff7-4e5a-983e-d645ff4e23c5
INFO  [2023-01-17 00:50:42,598] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_2VALUES$20Test
INFO  [2023-01-17 00:50:42,598] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:42,710] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_2VALUES Test
INFO  [2023-01-17 00:50:42,710] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:50:42,711] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:42,711] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:42,712] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:50:42,712] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:50:42,712] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:42,712] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:42,714] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_38030dac-7135-4bf4-88d7-415f94dbf1cc are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:42,714] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_38030dac-7135-4bf4-88d7-415f94dbf1cc are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:42,714] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:42,714] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_1ea5ce3d-afd8-4305-8ce5-60d26631965e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:42,714] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_1ea5ce3d-afd8-4305-8ce5-60d26631965e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:42,715] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:42,717] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:42,822] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:42,822] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-17 00:50:42,823] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-17 00:50:42,938] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,051] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:43,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:43,052] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 90 B in total
INFO  [2023-01-17 00:50:43,052] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000509261sINFO  [2023-01-17 00:50:43,103] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:50:43,103] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:43,103] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@32df0ff6)
INFO  [2023-01-17 00:50:43,107] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:43,107] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:43,107] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:43,131] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:43 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_EMPTY_VALUES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT_EMPTY_VALUES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:50:43,133] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,133] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:43,134] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:43,134] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:43,137] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:43,137] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:50:43,138] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:50:43,139] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table.0
WARN  [2023-01-17 00:50:43,139] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:43,139] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_EMPTY_VALUES$20Test.table.table.1
INFO  [2023-01-17 00:50:43,245] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_EMPTY_VALUES Test QUERY INIT
INFO  [2023-01-17 00:50:43,259] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_EMPTY_VALUES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:43,259] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[845a1da6-00be-4648-92ce-c7120416ec27] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_EMPTY_VALUES Test))]]
INFO  [2023-01-17 00:50:43,262] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27
INFO  [2023-01-17 00:50:43,262] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27
127.0.0.1 - - [17/Jan/2023:00:50:43 +0000] "POST /api/datasets/BIG_MULTI_SELECT_EMPTY_VALUES$20Test/queries HTTP/1.1" 201 1249 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:50:43,308] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27] with 0 results within PT0.045904S
INFO  [2023-01-17 00:50:43,308] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27] with 2 results within PT0.046109S
INFO  [2023-01-17 00:50:43,309] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27, workerId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_38030dac-7135-4bf4-88d7-415f94dbf1cc, startTime=2023-01-17T00:50:43.262852, finishTime=2023-01-17T00:50:43.308756) of size 0
INFO  [2023-01-17 00:50:43,309] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27, workerId=BIG_MULTI_SELECT_EMPTY_VALUES$20Test.worker_BIG_MULTI_SELECT_EMPTY_VALUES$20Test_1ea5ce3d-afd8-4305-8ce5-60d26631965e, startTime=2023-01-17T00:50:43.262862, finishTime=2023-01-17T00:50:43.308971) of size 2
INFO  [2023-01-17 00:50:43,311] com.bakdata.conquery.models.execution.ManagedExecution: DONE 845a1da6-00be-4648-92ce-c7120416ec27 ManagedQuery within PT0.051711S
127.0.0.1 - - [17/Jan/2023:00:50:43 +0000] "GET /api/datasets/BIG_MULTI_SELECT_EMPTY_VALUES$20Test/queries/BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27 HTTP/1.1" 200 1585 "-" "Conquery (test client)" 40
INFO  [2023-01-17 00:50:43,326] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test], queryId=845a1da6-00be-4648-92ce-c7120416ec27, label=concept	@§$, creationTime=2023-01-17T00:50:43.259504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@57a21b07[Count = 0], startTime=2023-01-17T00:50:43.259762, finishTime=2023-01-17T00:50:43.311473, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@8476f70), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@66bf89b5, com.bakdata.conquery.models.query.ColumnDescriptor@70b0a37a]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:43,327] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test], queryId=845a1da6-00be-4648-92ce-c7120416ec27, label=concept	@§$, creationTime=2023-01-17T00:50:43.259504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@57a21b07[Count = 0], startTime=2023-01-17T00:50:43.259762, finishTime=2023-01-17T00:50:43.311473, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@8476f70), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@66bf89b5, com.bakdata.conquery.models.query.ColumnDescriptor@70b0a37a]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
127.0.0.1 - - [17/Jan/2023:00:50:43 +0000] "GET /api/datasets/BIG_MULTI_SELECT_EMPTY_VALUES%20Test/result/BIG_MULTI_SELECT_EMPTY_VALUES$20Test.845a1da6-00be-4648-92ce-c7120416ec27.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:43,344] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_EMPTY_VALUES Test on 3 rows
INFO  [2023-01-17 00:50:43,344] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:50:43,344] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:50:43,344] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_EMPTY_VALUES Test, name=BIG_MULTI_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:50:43,344] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_38030dac-7135-4bf4-88d7-415f94dbf1cc
INFO  [2023-01-17 00:50:43,344] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_1ea5ce3d-afd8-4305-8ce5-60d26631965e
INFO  [2023-01-17 00:50:43,412] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:50:43,413] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_38030dac-7135-4bf4-88d7-415f94dbf1cc
INFO  [2023-01-17 00:50:43,413] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_EMPTY_VALUES Test_1ea5ce3d-afd8-4305-8ce5-60d26631965e
INFO  [2023-01-17 00:50:43,441] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_EMPTY_VALUES$20Test
INFO  [2023-01-17 00:50:43,441] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,548] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:50:43,548] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:50:43,548] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:43,548] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:43,549] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:50:43,550] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:43,550] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:50:43,550] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:43,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_ebd8d090-3b40-4d86-8a25-eed5e1dfdbec are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:43,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_ebd8d090-3b40-4d86-8a25-eed5e1dfdbec are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:43,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:43,553] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_62649d9c-4a87-49fb-ae64-c819383dcd58 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:43,553] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_62649d9c-4a87-49fb-ae64-c819383dcd58 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:43,553] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:43,556] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,660] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,660] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-17 00:50:43,660] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-17 00:50:43,775] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,899] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:43,900] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:43,900] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 92 B in total
INFO  [2023-01-17 00:50:43,900] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000325361sINFO  [2023-01-17 00:50:43,933] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:50:43,933] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2d7b8b9e)
INFO  [2023-01-17 00:50:43,933] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:43,936] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:43,936] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:43,936] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:43,958] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:43 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:50:43,960] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:43,960] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:43,960] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:43,960] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:43,962] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:43,962] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:50:43,962] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:50:43,963] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:43,963] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table.1
INFO  [2023-01-17 00:50:43,963] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.table.table.0
INFO  [2023-01-17 00:50:44,068] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test QUERY INIT
INFO  [2023-01-17 00:50:44,085] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:44,086] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7e566957-2ecf-4117-afe1-e2e9f1b869fd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test))]]
INFO  [2023-01-17 00:50:44,090] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd
INFO  [2023-01-17 00:50:44,090] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd
INFO  [2023-01-17 00:50:44,092] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd] with 0 results within PT0.001498S
INFO  [2023-01-17 00:50:44,092] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd] with 1 results within PT0.001746S
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "POST /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test/queries HTTP/1.1" 201 1268 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:44,092] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_62649d9c-4a87-49fb-ae64-c819383dcd58, startTime=2023-01-17T00:50:44.090583, finishTime=2023-01-17T00:50:44.092081) of size 0
INFO  [2023-01-17 00:50:44,093] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test_ebd8d090-3b40-4d86-8a25-eed5e1dfdbec, startTime=2023-01-17T00:50:44.090517, finishTime=2023-01-17T00:50:44.092263) of size 1
INFO  [2023-01-17 00:50:44,094] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7e566957-2ecf-4117-afe1-e2e9f1b869fd ManagedQuery within PT0.008469S
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test/queries/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd HTTP/1.1" 200 1623 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:44,115] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test], queryId=7e566957-2ecf-4117-afe1-e2e9f1b869fd, label=concept	@§$, creationTime=2023-01-17T00:50:44.085820, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@68b0ecae[Count = 0], startTime=2023-01-17T00:50:44.086175, finishTime=2023-01-17T00:50:44.094644, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1f6da7d8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@599243ee, com.bakdata.conquery.models.query.ColumnDescriptor@67a55627]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:44,115] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test], queryId=7e566957-2ecf-4117-afe1-e2e9f1b869fd, label=concept	@§$, creationTime=2023-01-17T00:50:44.085820, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@68b0ecae[Count = 0], startTime=2023-01-17T00:50:44.086175, finishTime=2023-01-17T00:50:44.094644, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1f6da7d8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@599243ee, com.bakdata.conquery.models.query.ColumnDescriptor@67a55627]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER%20Test/result/BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test.7e566957-2ecf-4117-afe1-e2e9f1b869fd.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:50:44,133] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test on 2 rows
INFO  [2023-01-17 00:50:44,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:50:44,134] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:50:44,134] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:50:44,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_62649d9c-4a87-49fb-ae64-c819383dcd58
INFO  [2023-01-17 00:50:44,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_ebd8d090-3b40-4d86-8a25-eed5e1dfdbec
INFO  [2023-01-17 00:50:44,150] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:50:44,151] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_62649d9c-4a87-49fb-ae64-c819383dcd58
INFO  [2023-01-17 00:50:44,151] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test_ebd8d090-3b40-4d86-8a25-eed5e1dfdbec
INFO  [2023-01-17 00:50:44,163] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_SPEZIAL_CHARACTER$20Test
INFO  [2023-01-17 00:50:44,163] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:44,268] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:50:44,269] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:50:44,269] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:44,269] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:44,270] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:50:44,270] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:50:44,270] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:44,270] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:44,272] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_f92eb820-a0a3-4e33-9f70-378c079e855a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:44,272] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_f92eb820-a0a3-4e33-9f70-378c079e855a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:44,272] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:44,273] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_84a7a88f-681b-4ea9-81de-fa2cb76d4b41 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:44,273] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_84a7a88f-681b-4ea9-81de-fa2cb76d4b41 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:44,273] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:44,277] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:44,384] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:44,384] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-17 00:50:44,384] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-17 00:50:44,499] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:44,613] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:44,613] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:44,613] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 90 B in total
INFO  [2023-01-17 00:50:44,613] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000353454sINFO  [2023-01-17 00:50:44,649] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:50:44,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5b120d73)
INFO  [2023-01-17 00:50:44,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:44,652] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:44,653] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:44,653] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:44,674] com.bakdata.conquery.models.jobs.ImportJob: Importing table into BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:44,680] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:44,680] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:44,681] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:44,681] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:44,683] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:44,683] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:50:44,683] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:50:44,684] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:44,684] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.0
INFO  [2023-01-17 00:50:44,684] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.1
INFO  [2023-01-17 00:50:44,789] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test QUERY INIT
INFO  [2023-01-17 00:50:44,804] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:44,805] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7459e51b-ae9c-4c91-987f-7104554bb623] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test))]]
INFO  [2023-01-17 00:50:44,809] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623
INFO  [2023-01-17 00:50:44,809] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623
INFO  [2023-01-17 00:50:44,810] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623] with 0 results within PT0.000958S
INFO  [2023-01-17 00:50:44,810] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623] with 1 results within PT0.001325S
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "POST /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test/queries HTTP/1.1" 201 1273 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:50:44,811] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_84a7a88f-681b-4ea9-81de-fa2cb76d4b41, startTime=2023-01-17T00:50:44.809353, finishTime=2023-01-17T00:50:44.810311) of size 0
INFO  [2023-01-17 00:50:44,811] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623, workerId=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test_f92eb820-a0a3-4e33-9f70-378c079e855a, startTime=2023-01-17T00:50:44.809393, finishTime=2023-01-17T00:50:44.810718) of size 1
INFO  [2023-01-17 00:50:44,812] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7459e51b-ae9c-4c91-987f-7104554bb623 ManagedQuery within PT0.00763S
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test/queries/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623 HTTP/1.1" 200 1632 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:50:44,845] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test], queryId=7459e51b-ae9c-4c91-987f-7104554bb623, label=concept	@§$, creationTime=2023-01-17T00:50:44.804845, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7d1cabf[Count = 0], startTime=2023-01-17T00:50:44.805242, finishTime=2023-01-17T00:50:44.812872, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1654711b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@369c6bb5, com.bakdata.conquery.models.query.ColumnDescriptor@6ef38ea3]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:44,845] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test], queryId=7459e51b-ae9c-4c91-987f-7104554bb623, label=concept	@§$, creationTime=2023-01-17T00:50:44.804845, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7d1cabf[Count = 0], startTime=2023-01-17T00:50:44.805242, finishTime=2023-01-17T00:50:44.812872, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1654711b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@369c6bb5, com.bakdata.conquery.models.query.ColumnDescriptor@6ef38ea3]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
127.0.0.1 - - [17/Jan/2023:00:50:44 +0000] "GET /api/datasets/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2%20Test/result/BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test.7459e51b-ae9c-4c91-987f-7104554bb623.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:50:44,861] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test on 2 rows
INFO  [2023-01-17 00:50:44,862] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:50:44,862] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:50:44,862] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test, name=BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:50:44,862] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_84a7a88f-681b-4ea9-81de-fa2cb76d4b41
INFO  [2023-01-17 00:50:44,862] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_f92eb820-a0a3-4e33-9f70-378c079e855a
INFO  [2023-01-17 00:50:44,870] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:50:44,879] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_84a7a88f-681b-4ea9-81de-fa2cb76d4b41
INFO  [2023-01-17 00:50:44,879] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test_f92eb820-a0a3-4e33-9f70-378c079e855a
INFO  [2023-01-17 00:50:44,884] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_SPEZIAL_CHARACTER2$20Test
INFO  [2023-01-17 00:50:44,884] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:44,990] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:50:44,990] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT Test
INFO  [2023-01-17 00:50:44,991] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:44,991] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:44,992] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-17 00:50:44,992] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-17 00:50:44,992] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:44,992] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:44,994] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20Test.worker_COUNT$20Test_11570221-25ae-46a3-878b-05a95fe06d13 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:44,994] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20Test.worker_COUNT$20Test_11570221-25ae-46a3-878b-05a95fe06d13 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:44,994] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:44,994] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20Test.worker_COUNT$20Test_ff17ae51-b6a7-4c00-aabf-c85199274cdf are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:44,994] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20Test.worker_COUNT$20Test_ff17ae51-b6a7-4c00-aabf-c85199274cdf are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:44,994] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:44,999] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,102] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,102] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20Test.table
INFO  [2023-01-17 00:50:45,102] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20Test.table
INFO  [2023-01-17 00:50:45,223] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,344] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:45,344] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:45,344] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1 KiB in total
INFO  [2023-01-17 00:50:45,344] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000413175sINFO  [2023-01-17 00:50:45,387] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=36, min=1, average=7.200000, max=13}
INFO  [2023-01-17 00:50:45,387] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=36, nullLines=0), minParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@25205c9), maxParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@1fc570ab), dateReader=com.bakdata.conquery.util.DateReader@5fc3221a, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:50:45,387] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=36, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-17 00:50:45,392] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:45,392] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:45,392] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:45,416] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:45 +0000] "POST /admin/datasets/COUNT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:45,419] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,419] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:45,419] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:45,419] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:45,421] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
WARN  [2023-01-17 00:50:45,423] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:45,423] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20Test.table.table], containing 36 entries.
INFO  [2023-01-17 00:50:45,423] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20Test.table.table], containing 36 entries.
INFO  [2023-01-17 00:50:45,423] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20Test.table.table.1
INFO  [2023-01-17 00:50:45,423] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20Test.table.table.0
INFO  [2023-01-17 00:50:45,533] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT Test QUERY INIT
INFO  [2023-01-17 00:50:45,553] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:45,553] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[47f872d4-97fc-49fe-877c-81c6268743c3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT Test))]]
INFO  [2023-01-17 00:50:45,559] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3
INFO  [2023-01-17 00:50:45,559] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3
127.0.0.1 - - [17/Jan/2023:00:50:45 +0000] "POST /api/datasets/COUNT$20Test/queries HTTP/1.1" 201 1166 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:50:45,560] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3] with 1 results within PT0.0018S
INFO  [2023-01-17 00:50:45,561] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3] with 1 results within PT0.002026S
INFO  [2023-01-17 00:50:45,561] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3, workerId=COUNT$20Test.worker_COUNT$20Test_ff17ae51-b6a7-4c00-aabf-c85199274cdf, startTime=2023-01-17T00:50:45.559153, finishTime=2023-01-17T00:50:45.560953) of size 1
INFO  [2023-01-17 00:50:45,562] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3, workerId=COUNT$20Test.worker_COUNT$20Test_11570221-25ae-46a3-878b-05a95fe06d13, startTime=2023-01-17T00:50:45.559144, finishTime=2023-01-17T00:50:45.561170) of size 1
INFO  [2023-01-17 00:50:45,563] com.bakdata.conquery.models.execution.ManagedExecution: DONE 47f872d4-97fc-49fe-877c-81c6268743c3 ManagedQuery within PT0.009188S
127.0.0.1 - - [17/Jan/2023:00:50:45 +0000] "GET /api/datasets/COUNT$20Test/queries/COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3 HTTP/1.1" 200 1405 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:45,589] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT Test], queryId=47f872d4-97fc-49fe-877c-81c6268743c3, label=concept	@§$, creationTime=2023-01-17T00:50:45.553693, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1bed0485[Count = 0], startTime=2023-01-17T00:50:45.553955, finishTime=2023-01-17T00:50:45.563143, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@70c3c83e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2abc9d66, com.bakdata.conquery.models.query.ColumnDescriptor@8780f6c]) download on dataset Dataset[label=null, name=COUNT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:45,589] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT Test], queryId=47f872d4-97fc-49fe-877c-81c6268743c3, label=concept	@§$, creationTime=2023-01-17T00:50:45.553693, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1bed0485[Count = 0], startTime=2023-01-17T00:50:45.553955, finishTime=2023-01-17T00:50:45.563143, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@70c3c83e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2abc9d66, com.bakdata.conquery.models.query.ColumnDescriptor@8780f6c]) on dataset Dataset[label=null, name=COUNT Test]
127.0.0.1 - - [17/Jan/2023:00:50:45 +0000] "GET /api/datasets/COUNT%20Test/result/COUNT$20Test.47f872d4-97fc-49fe-877c-81c6268743c3.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:50:45,609] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT Test on 3 rows
INFO  [2023-01-17 00:50:45,609] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT Test
INFO  [2023-01-17 00:50:45,610] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-17 00:50:45,610] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT Test, name=COUNT Test]
INFO  [2023-01-17 00:50:45,610] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT Test_ff17ae51-b6a7-4c00-aabf-c85199274cdf
INFO  [2023-01-17 00:50:45,610] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT Test_11570221-25ae-46a3-878b-05a95fe06d13
INFO  [2023-01-17 00:50:45,707] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT Test_ff17ae51-b6a7-4c00-aabf-c85199274cdf
INFO  [2023-01-17 00:50:45,707] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT Test_11570221-25ae-46a3-878b-05a95fe06d13
INFO  [2023-01-17 00:50:45,707] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT Test
INFO  [2023-01-17 00:50:45,729] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT$20Test
INFO  [2023-01-17 00:50:45,729] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,838] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT Test
INFO  [2023-01-17 00:50:45,839] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT distinct Test
INFO  [2023-01-17 00:50:45,839] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:45,839] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:45,840] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-17 00:50:45,840] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-17 00:50:45,840] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:45,840] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:45,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_16879e8a-e302-4840-bacf-ca0803e90df1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:45,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_16879e8a-e302-4840-bacf-ca0803e90df1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:45,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:45,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_7c0da7fa-fce0-4743-a298-eecff95ef313 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:45,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_7c0da7fa-fce0-4743-a298-eecff95ef313 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:45,843] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:45,847] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,950] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:45,950] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20distinct$20Test.table
INFO  [2023-01-17 00:50:45,950] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20distinct$20Test.table
INFO  [2023-01-17 00:50:46,068] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:46,181] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:46,181] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:46,181] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1 KiB in total
INFO  [2023-01-17 00:50:46,182] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT distinct Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000565002sINFO  [2023-01-17 00:50:46,239] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=36, min=1, average=7.200000, max=13}
INFO  [2023-01-17 00:50:46,239] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=36, nullLines=0), minParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@1a9a8f9c), maxParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@118b182b), dateReader=com.bakdata.conquery.util.DateReader@65c43896, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:50:46,239] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=36, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-17 00:50:46,243] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:46,243] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:46,243] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:46,257] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT$20distinct$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:46 +0000] "POST /admin/datasets/COUNT%20distinct%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT+distinct+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:50:46,259] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:46,260] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:46,260] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:46,260] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:46,263] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:46,264] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20distinct$20Test.table.table], containing 36 entries.
INFO  [2023-01-17 00:50:46,264] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20distinct$20Test.table.table], containing 36 entries.
WARN  [2023-01-17 00:50:46,265] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:46,266] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20distinct$20Test.table.table.0
INFO  [2023-01-17 00:50:46,266] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20distinct$20Test.table.table.1
INFO  [2023-01-17 00:50:46,371] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT distinct Test QUERY INIT
INFO  [2023-01-17 00:50:46,390] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT$20distinct$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:46,390] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5bb9c420-d781-4d09-9f5d-d0b5ace0188c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT distinct Test))]]
INFO  [2023-01-17 00:50:46,394] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c
INFO  [2023-01-17 00:50:46,394] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c
INFO  [2023-01-17 00:50:46,396] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c] with 0 results within PT0.001455S
INFO  [2023-01-17 00:50:46,396] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c] with 2 results within PT0.001671S
127.0.0.1 - - [17/Jan/2023:00:50:46 +0000] "POST /api/datasets/COUNT$20distinct$20Test/queries HTTP/1.1" 201 1210 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:46,396] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c, workerId=COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_16879e8a-e302-4840-bacf-ca0803e90df1, startTime=2023-01-17T00:50:46.394762, finishTime=2023-01-17T00:50:46.396217) of size 0
INFO  [2023-01-17 00:50:46,397] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c, workerId=COUNT$20distinct$20Test.worker_COUNT$20distinct$20Test_7c0da7fa-fce0-4743-a298-eecff95ef313, startTime=2023-01-17T00:50:46.394775, finishTime=2023-01-17T00:50:46.396446) of size 2
INFO  [2023-01-17 00:50:46,398] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5bb9c420-d781-4d09-9f5d-d0b5ace0188c ManagedQuery within PT0.007405S
127.0.0.1 - - [17/Jan/2023:00:50:46 +0000] "GET /api/datasets/COUNT$20distinct$20Test/queries/COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c HTTP/1.1" 200 1493 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:46,422] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT distinct Test], queryId=5bb9c420-d781-4d09-9f5d-d0b5ace0188c, label=concept	@§$, creationTime=2023-01-17T00:50:46.390532, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1bdef01f[Count = 0], startTime=2023-01-17T00:50:46.390796, finishTime=2023-01-17T00:50:46.398201, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@278070cf), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@24636473, com.bakdata.conquery.models.query.ColumnDescriptor@1c016b33]) download on dataset Dataset[label=null, name=COUNT distinct Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:46,422] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT distinct Test], queryId=5bb9c420-d781-4d09-9f5d-d0b5ace0188c, label=concept	@§$, creationTime=2023-01-17T00:50:46.390532, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1bdef01f[Count = 0], startTime=2023-01-17T00:50:46.390796, finishTime=2023-01-17T00:50:46.398201, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@278070cf), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@24636473, com.bakdata.conquery.models.query.ColumnDescriptor@1c016b33]) on dataset Dataset[label=null, name=COUNT distinct Test]
127.0.0.1 - - [17/Jan/2023:00:50:46 +0000] "GET /api/datasets/COUNT%20distinct%20Test/result/COUNT$20distinct$20Test.5bb9c420-d781-4d09-9f5d-d0b5ace0188c.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:50:46,439] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT distinct Test on 3 rows
INFO  [2023-01-17 00:50:46,439] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT distinct Test
INFO  [2023-01-17 00:50:46,440] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-17 00:50:46,440] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT distinct Test, name=COUNT distinct Test]
INFO  [2023-01-17 00:50:46,440] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT distinct Test_16879e8a-e302-4840-bacf-ca0803e90df1
INFO  [2023-01-17 00:50:46,440] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT distinct Test_7c0da7fa-fce0-4743-a298-eecff95ef313
INFO  [2023-01-17 00:50:46,441] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT distinct Test
INFO  [2023-01-17 00:50:46,441] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT distinct Test_16879e8a-e302-4840-bacf-ca0803e90df1
INFO  [2023-01-17 00:50:46,442] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT distinct Test_7c0da7fa-fce0-4743-a298-eecff95ef313
INFO  [2023-01-17 00:50:46,466] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT$20distinct$20Test
INFO  [2023-01-17 00:50:46,466] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:46,572] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT distinct Test
INFO  [2023-01-17 00:50:46,573] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT multi distinct Test
INFO  [2023-01-17 00:50:46,573] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:46,573] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:46,604] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-17 00:50:46,604] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:46,605] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-17 00:50:46,605] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:46,659] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_648d786b-ddd1-4ef3-9571-fdc7e4b9f9dd are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:46,659] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_648d786b-ddd1-4ef3-9571-fdc7e4b9f9dd are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:46,659] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:46,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_bbbd784d-c1ac-4718-9659-d32ac662b38a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:46,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_bbbd784d-c1ac-4718-9659-d32ac662b38a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:46,685] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:46,686] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:46,793] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:46,794] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20multi$20distinct$20Test.table
INFO  [2023-01-17 00:50:46,794] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT$20multi$20distinct$20Test.table
INFO  [2023-01-17 00:50:46,904] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,018] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:47,019] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:47,019] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.2 KiB in total
INFO  [2023-01-17 00:50:47,019] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT multi distinct Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000445346sINFO  [2023-01-17 00:50:47,064] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=34, min=1, average=6.800000, max=13}
INFO  [2023-01-17 00:50:47,065] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[a] with StringParser(super=Parser(lines=34, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:47,065] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[b] with StringParser(super=Parser(lines=34, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:47,065] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=34, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-17 00:50:47,065] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=34, nullLines=0), minParser=DateParser(super=Parser(lines=34, nullLines=0), subType=IntegerParser(super=Parser(lines=34, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@3ec6a316), maxParser=DateParser(super=Parser(lines=34, nullLines=0), subType=IntegerParser(super=Parser(lines=34, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@574467dc), dateReader=com.bakdata.conquery.util.DateReader@5db07d8a, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:50:47,068] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT multi distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:47,068] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:47,068] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT multi distinct Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:47,085] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT$20multi$20distinct$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:47 +0000] "POST /admin/datasets/COUNT%20multi%20distinct%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT+multi+distinct+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:50:47,087] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,087] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:47,088] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:47,088] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:47,091] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:47,091] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20multi$20distinct$20Test.table.table], containing 34 entries.
INFO  [2023-01-17 00:50:47,091] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT$20multi$20distinct$20Test.table.table], containing 34 entries.
WARN  [2023-01-17 00:50:47,093] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:47,132] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20multi$20distinct$20Test.table.table.0
INFO  [2023-01-17 00:50:47,132] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT$20multi$20distinct$20Test.table.table.1
INFO  [2023-01-17 00:50:47,247] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT multi distinct Test QUERY INIT
INFO  [2023-01-17 00:50:47,265] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT$20multi$20distinct$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:47,265] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e8a83fba-8da1-4c46-82ad-ba3eefd66d2c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT multi distinct Test))]]
INFO  [2023-01-17 00:50:47,269] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c
INFO  [2023-01-17 00:50:47,269] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c
INFO  [2023-01-17 00:50:47,271] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c] with 0 results within PT0.001735S
INFO  [2023-01-17 00:50:47,271] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c] with 2 results within PT0.001877S
127.0.0.1 - - [17/Jan/2023:00:50:47 +0000] "POST /api/datasets/COUNT$20multi$20distinct$20Test/queries HTTP/1.1" 201 1242 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:50:47,272] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c, workerId=COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_648d786b-ddd1-4ef3-9571-fdc7e4b9f9dd, startTime=2023-01-17T00:50:47.269787, finishTime=2023-01-17T00:50:47.271522) of size 0
INFO  [2023-01-17 00:50:47,272] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c, workerId=COUNT$20multi$20distinct$20Test.worker_COUNT$20multi$20distinct$20Test_bbbd784d-c1ac-4718-9659-d32ac662b38a, startTime=2023-01-17T00:50:47.269668, finishTime=2023-01-17T00:50:47.271545) of size 2
INFO  [2023-01-17 00:50:47,273] com.bakdata.conquery.models.execution.ManagedExecution: DONE e8a83fba-8da1-4c46-82ad-ba3eefd66d2c ManagedQuery within PT0.008183S
127.0.0.1 - - [17/Jan/2023:00:50:47 +0000] "GET /api/datasets/COUNT$20multi$20distinct$20Test/queries/COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c HTTP/1.1" 200 1557 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:47,299] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT multi distinct Test], queryId=e8a83fba-8da1-4c46-82ad-ba3eefd66d2c, label=concept	@§$, creationTime=2023-01-17T00:50:47.265492, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@41809373[Count = 0], startTime=2023-01-17T00:50:47.265758, finishTime=2023-01-17T00:50:47.273941, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3ffe6d89), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT multi distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@179324ca, com.bakdata.conquery.models.query.ColumnDescriptor@1b4888bc]) download on dataset Dataset[label=null, name=COUNT multi distinct Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:47,300] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT multi distinct Test], queryId=e8a83fba-8da1-4c46-82ad-ba3eefd66d2c, label=concept	@§$, creationTime=2023-01-17T00:50:47.265492, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@41809373[Count = 0], startTime=2023-01-17T00:50:47.265758, finishTime=2023-01-17T00:50:47.273941, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3ffe6d89), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT multi distinct Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@179324ca, com.bakdata.conquery.models.query.ColumnDescriptor@1b4888bc]) on dataset Dataset[label=null, name=COUNT multi distinct Test]
127.0.0.1 - - [17/Jan/2023:00:50:47 +0000] "GET /api/datasets/COUNT%20multi%20distinct%20Test/result/COUNT$20multi$20distinct$20Test.e8a83fba-8da1-4c46-82ad-ba3eefd66d2c.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:47,317] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT multi distinct Test on 3 rows
INFO  [2023-01-17 00:50:47,317] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT multi distinct Test
INFO  [2023-01-17 00:50:47,317] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-17 00:50:47,317] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT multi distinct Test_648d786b-ddd1-4ef3-9571-fdc7e4b9f9dd
INFO  [2023-01-17 00:50:47,317] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT multi distinct Test, name=COUNT multi distinct Test]
INFO  [2023-01-17 00:50:47,318] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT multi distinct Test_bbbd784d-c1ac-4718-9659-d32ac662b38a
INFO  [2023-01-17 00:50:47,360] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT multi distinct Test_648d786b-ddd1-4ef3-9571-fdc7e4b9f9dd
INFO  [2023-01-17 00:50:47,394] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT multi distinct Test_bbbd784d-c1ac-4718-9659-d32ac662b38a
INFO  [2023-01-17 00:50:47,415] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT multi distinct Test
INFO  [2023-01-17 00:50:47,494] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT$20multi$20distinct$20Test
INFO  [2023-01-17 00:50:47,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,547] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT multi distinct Test
INFO  [2023-01-17 00:50:47,548] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_QUARTERS Test
INFO  [2023-01-17 00:50:47,548] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:47,548] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:47,549] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-17 00:50:47,549] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-17 00:50:47,549] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:47,550] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:47,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_2cb1b8dd-7732-484d-87f1-30426f2d4e75 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:47,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_2cb1b8dd-7732-484d-87f1-30426f2d4e75 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:47,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:47,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_6ab72a18-c426-4c74-8cbf-bbbb65c9a465 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:47,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_6ab72a18-c426-4c74-8cbf-bbbb65c9a465 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:47,552] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:47,556] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,659] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,659] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test.table
INFO  [2023-01-17 00:50:47,660] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test.table
INFO  [2023-01-17 00:50:47,782] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,898] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:47,898] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:47,898] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.4 KiB in total
INFO  [2023-01-17 00:50:47,898] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000390928sINFO  [2023-01-17 00:50:47,939] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=38, min=1, average=3.166667, max=6}
INFO  [2023-01-17 00:50:47,939] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=38, nullLines=0), minParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=16436), dateReader=com.bakdata.conquery.util.DateReader@7bc4aec7), maxParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=17166, maxValue=17166), dateReader=com.bakdata.conquery.util.DateReader@67ef90c4), dateReader=com.bakdata.conquery.util.DateReader@449b2fb2, onlyQuarters=false, maxValue=17166, minValue=16436, anyOpen=false)
INFO  [2023-01-17 00:50:47,939] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=17117), dateReader=com.bakdata.conquery.util.DateReader@19c819f1)
INFO  [2023-01-17 00:50:47,942] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:47,942] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:47,942] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:47,962] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_QUARTERS$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:47 +0000] "POST /admin/datasets/COUNT_QUARTERS%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT_QUARTERS+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:50:47,964] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:47,964] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:47,964] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:47,964] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:47,966] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:50:47,967] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test.table.table], containing 38 entries.
INFO  [2023-01-17 00:50:47,967] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test.table.table], containing 38 entries.
INFO  [2023-01-17 00:50:47,968] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.0
WARN  [2023-01-17 00:50:47,968] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:47,968] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.1
INFO  [2023-01-17 00:50:47,968] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.3
INFO  [2023-01-17 00:50:47,968] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test.table.table.2
INFO  [2023-01-17 00:50:48,073] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_QUARTERS Test QUERY INIT
INFO  [2023-01-17 00:50:48,091] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_QUARTERS$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:48,091] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[dabbaa58-0a86-4431-a2d3-ba347d3553d6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test))]]
INFO  [2023-01-17 00:50:48,095] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6
INFO  [2023-01-17 00:50:48,096] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "POST /api/datasets/COUNT_QUARTERS$20Test/queries HTTP/1.1" 201 1201 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:48,098] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6] with 2 results within PT0.002608S
INFO  [2023-01-17 00:50:48,098] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6] with 4 results within PT0.002475S
INFO  [2023-01-17 00:50:48,099] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6, workerId=COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_6ab72a18-c426-4c74-8cbf-bbbb65c9a465, startTime=2023-01-17T00:50:48.095934, finishTime=2023-01-17T00:50:48.098542) of size 2
INFO  [2023-01-17 00:50:48,099] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6, workerId=COUNT_QUARTERS$20Test.worker_COUNT_QUARTERS$20Test_2cb1b8dd-7732-484d-87f1-30426f2d4e75, startTime=2023-01-17T00:50:48.096304, finishTime=2023-01-17T00:50:48.098779) of size 4
INFO  [2023-01-17 00:50:48,100] com.bakdata.conquery.models.execution.ManagedExecution: DONE dabbaa58-0a86-4431-a2d3-ba347d3553d6 ManagedQuery within PT0.009152S
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "GET /api/datasets/COUNT_QUARTERS$20Test/queries/COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6 HTTP/1.1" 200 1476 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:48,142] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test], queryId=dabbaa58-0a86-4431-a2d3-ba347d3553d6, label=concept	@§$, creationTime=2023-01-17T00:50:48.091480, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@192cc3ec[Count = 0], startTime=2023-01-17T00:50:48.091781, finishTime=2023-01-17T00:50:48.100933, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@329d7137), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5ae1f5c7, com.bakdata.conquery.models.query.ColumnDescriptor@7ab54bb8]) download on dataset Dataset[label=null, name=COUNT_QUARTERS Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:48,142] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test], queryId=dabbaa58-0a86-4431-a2d3-ba347d3553d6, label=concept	@§$, creationTime=2023-01-17T00:50:48.091480, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@192cc3ec[Count = 0], startTime=2023-01-17T00:50:48.091781, finishTime=2023-01-17T00:50:48.100933, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@329d7137), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5ae1f5c7, com.bakdata.conquery.models.query.ColumnDescriptor@7ab54bb8]) on dataset Dataset[label=null, name=COUNT_QUARTERS Test]
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "GET /api/datasets/COUNT_QUARTERS%20Test/result/COUNT_QUARTERS$20Test.dabbaa58-0a86-4431-a2d3-ba347d3553d6.csv?pretty=false HTTP/1.1" 200 171 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:50:48,158] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT_QUARTERS Test on 7 rows
INFO  [2023-01-17 00:50:48,158] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_QUARTERS Test
INFO  [2023-01-17 00:50:48,159] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-17 00:50:48,159] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test, name=COUNT_QUARTERS Test]
INFO  [2023-01-17 00:50:48,159] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test_6ab72a18-c426-4c74-8cbf-bbbb65c9a465
INFO  [2023-01-17 00:50:48,159] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test_2cb1b8dd-7732-484d-87f1-30426f2d4e75
INFO  [2023-01-17 00:50:48,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test_2cb1b8dd-7732-484d-87f1-30426f2d4e75
INFO  [2023-01-17 00:50:48,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test_6ab72a18-c426-4c74-8cbf-bbbb65c9a465
INFO  [2023-01-17 00:50:48,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_QUARTERS Test
INFO  [2023-01-17 00:50:48,268] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_QUARTERS$20Test
INFO  [2023-01-17 00:50:48,268] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:48,379] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_QUARTERS Test
INFO  [2023-01-17 00:50:48,380] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_QUARTERS Test
INFO  [2023-01-17 00:50:48,380] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:48,380] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:48,381] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-17 00:50:48,381] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-17 00:50:48,381] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:48,381] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:48,383] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_60c60b5f-93f2-44bf-a4e9-7e529dcd87ef are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:48,383] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_60c60b5f-93f2-44bf-a4e9-7e529dcd87ef are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:48,383] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:48,383] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_c8ec79f1-3f81-41f3-871c-b687f6bc2980 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:48,384] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_c8ec79f1-3f81-41f3-871c-b687f6bc2980 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:48,384] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:48,388] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:48,491] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:48,491] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test[1].table
INFO  [2023-01-17 00:50:48,491] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS$20Test[1].table
INFO  [2023-01-17 00:50:48,605] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:48,718] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:48,718] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:48,718] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 205 B in total
INFO  [2023-01-17 00:50:48,719] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000409117sINFO  [2023-01-17 00:50:48,760] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:50:48,760] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=16436, maxValue=16436), dateReader=com.bakdata.conquery.util.DateReader@20258940)
INFO  [2023-01-17 00:50:48,760] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateRangeParser(super=Parser(lines=5, nullLines=0), minParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=16436, maxValue=16587), dateReader=com.bakdata.conquery.util.DateReader@112e1426), maxParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=16466, maxValue=16800), dateReader=com.bakdata.conquery.util.DateReader@66391f51), dateReader=com.bakdata.conquery.util.DateReader@32ff40fb, onlyQuarters=false, maxValue=16800, minValue=16436, anyOpen=false)
INFO  [2023-01-17 00:50:48,763] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:48,763] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:48,763] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:48,785] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_QUARTERS$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "POST /admin/datasets/COUNT_QUARTERS%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT_QUARTERS+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:50:48,787] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:48,787] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:48,787] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:48,787] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:48,789] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:48,789] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test[1].table.table], containing 5 entries.
INFO  [2023-01-17 00:50:48,789] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS$20Test[1].table.table], containing 5 entries.
WARN  [2023-01-17 00:50:48,790] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:48,790] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test[1].table.table.0
INFO  [2023-01-17 00:50:48,790] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS$20Test[1].table.table.1
INFO  [2023-01-17 00:50:48,896] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_QUARTERS Test QUERY INIT
INFO  [2023-01-17 00:50:48,915] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_QUARTERS$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:48,915] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[56b65df1-74f3-4139-a009-fd0042c39097] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test[1]))]]
INFO  [2023-01-17 00:50:48,920] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test[1].56b65df1-74f3-4139-a009-fd0042c39097
INFO  [2023-01-17 00:50:48,920] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS$20Test[1].56b65df1-74f3-4139-a009-fd0042c39097
INFO  [2023-01-17 00:50:48,921] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test[1].56b65df1-74f3-4139-a009-fd0042c39097] with 1 results within PT0.001444S
INFO  [2023-01-17 00:50:48,921] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS$20Test[1].56b65df1-74f3-4139-a009-fd0042c39097] with 1 results within PT0.001501S
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "POST /api/datasets/COUNT_QUARTERS$20Test%5B1%5D/queries HTTP/1.1" 201 1214 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:48,922] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test[1].56b65df1-74f3-4139-a009-fd0042c39097, workerId=COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_c8ec79f1-3f81-41f3-871c-b687f6bc2980, startTime=2023-01-17T00:50:48.920065, finishTime=2023-01-17T00:50:48.921566) of size 1
INFO  [2023-01-17 00:50:48,922] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS$20Test[1].56b65df1-74f3-4139-a009-fd0042c39097, workerId=COUNT_QUARTERS$20Test[1].worker_COUNT_QUARTERS$20Test[1]_60c60b5f-93f2-44bf-a4e9-7e529dcd87ef, startTime=2023-01-17T00:50:48.920079, finishTime=2023-01-17T00:50:48.921523) of size 1
INFO  [2023-01-17 00:50:48,923] com.bakdata.conquery.models.execution.ManagedExecution: DONE 56b65df1-74f3-4139-a009-fd0042c39097 ManagedQuery within PT0.007512S
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "GET /api/datasets/COUNT_QUARTERS$20Test%5B1%5D/queries/COUNT_QUARTERS$20Test%5B1%5D.56b65df1-74f3-4139-a009-fd0042c39097 HTTP/1.1" 200 1747 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:48,951] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test[1]], queryId=56b65df1-74f3-4139-a009-fd0042c39097, label=concept	@§$, creationTime=2023-01-17T00:50:48.915683, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6bb429e6[Count = 0], startTime=2023-01-17T00:50:48.915988, finishTime=2023-01-17T00:50:48.923500, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@33c33f7c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@26dda333, com.bakdata.conquery.models.query.ColumnDescriptor@6f6f3e38]) download on dataset Dataset[label=null, name=COUNT_QUARTERS Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:48,951] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS Test[1]], queryId=56b65df1-74f3-4139-a009-fd0042c39097, label=concept	@§$, creationTime=2023-01-17T00:50:48.915683, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6bb429e6[Count = 0], startTime=2023-01-17T00:50:48.915988, finishTime=2023-01-17T00:50:48.923500, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@33c33f7c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@26dda333, com.bakdata.conquery.models.query.ColumnDescriptor@6f6f3e38]) on dataset Dataset[label=null, name=COUNT_QUARTERS Test[1]]
127.0.0.1 - - [17/Jan/2023:00:50:48 +0000] "GET /api/datasets/COUNT_QUARTERS%20Test%5B1%5D/result/COUNT_QUARTERS$20Test%5B1%5D.56b65df1-74f3-4139-a009-fd0042c39097.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:50:48,968] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNT_QUARTERS Test on 3 rows
INFO  [2023-01-17 00:50:48,968] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_QUARTERS Test[1]
INFO  [2023-01-17 00:50:48,969] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-17 00:50:48,969] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS Test[1], name=COUNT_QUARTERS Test[1]]
INFO  [2023-01-17 00:50:48,969] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test[1]_60c60b5f-93f2-44bf-a4e9-7e529dcd87ef
INFO  [2023-01-17 00:50:48,969] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS Test[1]_c8ec79f1-3f81-41f3-871c-b687f6bc2980
INFO  [2023-01-17 00:50:48,981] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_QUARTERS Test[1]
INFO  [2023-01-17 00:50:48,982] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test[1]_60c60b5f-93f2-44bf-a4e9-7e529dcd87ef
INFO  [2023-01-17 00:50:48,982] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS Test[1]_c8ec79f1-3f81-41f3-871c-b687f6bc2980
INFO  [2023-01-17 00:50:48,990] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_QUARTERS$20Test[1]
INFO  [2023-01-17 00:50:48,990] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,096] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_QUARTERS Test
INFO  [2023-01-17 00:50:49,096] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNTfalse Test
INFO  [2023-01-17 00:50:49,097] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:49,097] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:49,098] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-17 00:50:49,098] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:49,098] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-17 00:50:49,098] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:49,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_f09aa376-6b31-44ca-ae0b-e96fb7f83a15 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:49,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_f09aa376-6b31-44ca-ae0b-e96fb7f83a15 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:49,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:49,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_a7956663-80f7-4863-a8d7-1d1c831acf28 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:49,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNTfalse$20Test.worker_COUNTfalse$20Test_a7956663-80f7-4863-a8d7-1d1c831acf28 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:49,100] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:49,104] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,212] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,212] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNTfalse$20Test.table
INFO  [2023-01-17 00:50:49,213] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNTfalse$20Test.table
INFO  [2023-01-17 00:50:49,327] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,441] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:49,441] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:49,441] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1 KiB in total
INFO  [2023-01-17 00:50:49,441] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNTfalse Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000570298sINFO  [2023-01-17 00:50:49,499] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=36, min=1, average=7.200000, max=13}
INFO  [2023-01-17 00:50:49,500] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=36, nullLines=0), minParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@169b4b1a), maxParser=DateParser(super=Parser(lines=36, nullLines=0), subType=IntegerParser(super=Parser(lines=36, nullLines=0), minValue=14640, maxValue=14640), dateReader=com.bakdata.conquery.util.DateReader@2ca00657), dateReader=com.bakdata.conquery.util.DateReader@3952a801, onlyQuarters=false, maxValue=14640, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:50:49,500] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=36, nullLines=0), minValue=1, maxValue=23)
INFO  [2023-01-17 00:50:49,503] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNTfalse Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:49,503] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:49,503] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNTfalse Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:49,532] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNTfalse$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:49 +0000] "POST /admin/datasets/COUNTfalse%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNTfalse+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:50:49,534] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,536] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:49,536] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:49,536] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:49,538] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:49,538] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNTfalse$20Test.table.table], containing 36 entries.
INFO  [2023-01-17 00:50:49,539] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNTfalse$20Test.table.table], containing 36 entries.
WARN  [2023-01-17 00:50:49,540] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:49,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNTfalse$20Test.table.table.0
INFO  [2023-01-17 00:50:49,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNTfalse$20Test.table.table.1
INFO  [2023-01-17 00:50:49,646] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNTfalse Test QUERY INIT
INFO  [2023-01-17 00:50:49,674] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNTfalse$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:49,674] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNTfalse Test))]]
INFO  [2023-01-17 00:50:49,677] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e
INFO  [2023-01-17 00:50:49,677] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e
INFO  [2023-01-17 00:50:49,678] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e] with 1 results within PT0.001255S
INFO  [2023-01-17 00:50:49,678] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e] with 1 results within PT0.001228S
127.0.0.1 - - [17/Jan/2023:00:50:49 +0000] "POST /api/datasets/COUNTfalse$20Test/queries HTTP/1.1" 201 1186 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:50:49,679] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e, workerId=COUNTfalse$20Test.worker_COUNTfalse$20Test_a7956663-80f7-4863-a8d7-1d1c831acf28, startTime=2023-01-17T00:50:49.677401, finishTime=2023-01-17T00:50:49.678656) of size 1
INFO  [2023-01-17 00:50:49,679] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e, workerId=COUNTfalse$20Test.worker_COUNTfalse$20Test_f09aa376-6b31-44ca-ae0b-e96fb7f83a15, startTime=2023-01-17T00:50:49.677496, finishTime=2023-01-17T00:50:49.678724) of size 1
INFO  [2023-01-17 00:50:49,680] com.bakdata.conquery.models.execution.ManagedExecution: DONE d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e ManagedQuery within PT0.006282S
127.0.0.1 - - [17/Jan/2023:00:50:49 +0000] "GET /api/datasets/COUNTfalse$20Test/queries/COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e HTTP/1.1" 200 1445 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:49,710] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNTfalse Test], queryId=d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e, label=concept	@§$, creationTime=2023-01-17T00:50:49.674326, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5424d54f[Count = 0], startTime=2023-01-17T00:50:49.674522, finishTime=2023-01-17T00:50:49.680804, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@27b339f6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNTfalse Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@55229db2, com.bakdata.conquery.models.query.ColumnDescriptor@54d62b98]) download on dataset Dataset[label=null, name=COUNTfalse Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:49,710] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNTfalse Test], queryId=d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e, label=concept	@§$, creationTime=2023-01-17T00:50:49.674326, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5424d54f[Count = 0], startTime=2023-01-17T00:50:49.674522, finishTime=2023-01-17T00:50:49.680804, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@27b339f6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNTfalse Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@55229db2, com.bakdata.conquery.models.query.ColumnDescriptor@54d62b98]) on dataset Dataset[label=null, name=COUNTfalse Test]
127.0.0.1 - - [17/Jan/2023:00:50:49 +0000] "GET /api/datasets/COUNTfalse%20Test/result/COUNTfalse$20Test.d0d3c8ff-ef3a-44bb-9a49-e8a93f69828e.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:50:49,732] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest COUNTfalse Test on 3 rows
INFO  [2023-01-17 00:50:49,732] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNTfalse Test
INFO  [2023-01-17 00:50:49,733] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-17 00:50:49,733] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNTfalse Test, name=COUNTfalse Test]
INFO  [2023-01-17 00:50:49,733] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNTfalse Test_a7956663-80f7-4863-a8d7-1d1c831acf28
INFO  [2023-01-17 00:50:49,733] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNTfalse Test_f09aa376-6b31-44ca-ae0b-e96fb7f83a15
INFO  [2023-01-17 00:50:49,798] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNTfalse Test
INFO  [2023-01-17 00:50:49,799] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNTfalse Test_f09aa376-6b31-44ca-ae0b-e96fb7f83a15
INFO  [2023-01-17 00:50:49,799] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNTfalse Test_a7956663-80f7-4863-a8d7-1d1c831acf28
INFO  [2023-01-17 00:50:49,840] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNTfalse$20Test
INFO  [2023-01-17 00:50:49,840] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,946] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNTfalse Test
INFO  [2023-01-17 00:50:49,946] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_REAL Test
INFO  [2023-01-17 00:50:49,947] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:49,947] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:49,949] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-17 00:50:49,949] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-17 00:50:49,949] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:49,949] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:49,952] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:49,953] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_39ae91b0-fd81-42b0-91bf-72cae6e7d5cc are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:49,953] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_39ae91b0-fd81-42b0-91bf-72cae6e7d5cc are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:49,953] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:49,953] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_040a311f-e45d-428e-be49-c14d953f9c21 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:49,953] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_040a311f-e45d-428e-be49-c14d953f9c21 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:49,953] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:50,062] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:50,063] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test.table
INFO  [2023-01-17 00:50:50,063] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test.table
INFO  [2023-01-17 00:50:50,186] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:50,304] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:50,304] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:50,304] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 147 B in total
INFO  [2023-01-17 00:50:50,305] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_REAL Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000447571sINFO  [2023-01-17 00:50:50,350] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:50:50,350] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[validity] with DateParser(super=Parser(lines=6, nullLines=1), subType=IntegerParser(super=Parser(lines=6, nullLines=1), minValue=10986, maxValue=10988), dateReader=com.bakdata.conquery.util.DateReader@741925db)
INFO  [2023-01-17 00:50:50,350] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=1), subType=IntegerParser(super=Parser(lines=6, nullLines=1), minValue=10977, maxValue=10997), dateReader=com.bakdata.conquery.util.DateReader@4fc396b6)
INFO  [2023-01-17 00:50:50,352] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_REAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:50,353] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:50,353] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_REAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:50,373] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_REAL$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:50 +0000] "POST /admin/datasets/NUMBER_REAL%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_REAL+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:50:50,376] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:50,376] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:50,376] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:50,377] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:50,378] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:50,378] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:50:50,378] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:50:50,379] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:50,379] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test.table.table.0
INFO  [2023-01-17 00:50:50,379] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test.table.table.1
INFO  [2023-01-17 00:50:50,486] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_REAL Test QUERY INIT
INFO  [2023-01-17 00:50:50,508] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_REAL$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:50,508] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[8a075009-ac05-4fbf-a2af-8c8142e61365] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test))]]
INFO  [2023-01-17 00:50:50,522] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365
INFO  [2023-01-17 00:50:50,522] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365
127.0.0.1 - - [17/Jan/2023:00:50:50 +0000] "POST /api/datasets/NUMBER_REAL$20Test/queries HTTP/1.1" 201 1293 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:50,524] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365] with 0 results within PT0.002042S
INFO  [2023-01-17 00:50:50,524] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365] with 2 results within PT0.002417S
INFO  [2023-01-17 00:50:50,524] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365, workerId=NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_040a311f-e45d-428e-be49-c14d953f9c21, startTime=2023-01-17T00:50:50.522248, finishTime=2023-01-17T00:50:50.524290) of size 0
INFO  [2023-01-17 00:50:50,525] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365, workerId=NUMBER_REAL$20Test.worker_NUMBER_REAL$20Test_39ae91b0-fd81-42b0-91bf-72cae6e7d5cc, startTime=2023-01-17T00:50:50.522164, finishTime=2023-01-17T00:50:50.524581) of size 2
INFO  [2023-01-17 00:50:50,526] com.bakdata.conquery.models.execution.ManagedExecution: DONE 8a075009-ac05-4fbf-a2af-8c8142e61365 ManagedQuery within PT0.017587S
127.0.0.1 - - [17/Jan/2023:00:50:50 +0000] "GET /api/datasets/NUMBER_REAL$20Test/queries/NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365 HTTP/1.1" 200 1557 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:50,557] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test], queryId=8a075009-ac05-4fbf-a2af-8c8142e61365, label=concept	@§$, creationTime=2023-01-17T00:50:50.508572, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@35c8ad99[Count = 0], startTime=2023-01-17T00:50:50.508889, finishTime=2023-01-17T00:50:50.526476, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@71d7b0ca), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5ac8359e, com.bakdata.conquery.models.query.ColumnDescriptor@3365845f]) download on dataset Dataset[label=null, name=NUMBER_REAL Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:50,557] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test], queryId=8a075009-ac05-4fbf-a2af-8c8142e61365, label=concept	@§$, creationTime=2023-01-17T00:50:50.508572, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@35c8ad99[Count = 0], startTime=2023-01-17T00:50:50.508889, finishTime=2023-01-17T00:50:50.526476, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@71d7b0ca), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5ac8359e, com.bakdata.conquery.models.query.ColumnDescriptor@3365845f]) on dataset Dataset[label=null, name=NUMBER_REAL Test]
127.0.0.1 - - [17/Jan/2023:00:50:50 +0000] "GET /api/datasets/NUMBER_REAL%20Test/result/NUMBER_REAL$20Test.8a075009-ac05-4fbf-a2af-8c8142e61365.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:50,574] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_REAL Test on 3 rows
INFO  [2023-01-17 00:50:50,575] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_REAL Test
INFO  [2023-01-17 00:50:50,575] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-17 00:50:50,575] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test, name=NUMBER_REAL Test]
INFO  [2023-01-17 00:50:50,575] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test_040a311f-e45d-428e-be49-c14d953f9c21
INFO  [2023-01-17 00:50:50,575] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test_39ae91b0-fd81-42b0-91bf-72cae6e7d5cc
INFO  [2023-01-17 00:50:50,648] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_REAL Test
INFO  [2023-01-17 00:50:50,650] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test_39ae91b0-fd81-42b0-91bf-72cae6e7d5cc
INFO  [2023-01-17 00:50:50,650] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test_040a311f-e45d-428e-be49-c14d953f9c21
INFO  [2023-01-17 00:50:50,679] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_REAL$20Test
INFO  [2023-01-17 00:50:50,679] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:50,785] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_REAL Test
INFO  [2023-01-17 00:50:50,786] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DIFFSUM_INTEGER Test
INFO  [2023-01-17 00:50:50,786] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:50,786] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:50,787] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-17 00:50:50,787] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-17 00:50:50,787] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:50,787] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:50,788] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:50,789] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_826d8ee8-9e77-48f4-a066-e2b0ad5629ad are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:50,789] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_826d8ee8-9e77-48f4-a066-e2b0ad5629ad are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:50,789] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:50,789] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_328fea63-b5d7-45e3-89fa-fcf61c31285a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:50,789] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_328fea63-b5d7-45e3-89fa-fcf61c31285a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:50,789] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:50,897] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:50,897] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DIFFSUM_INTEGER$20Test.table
INFO  [2023-01-17 00:50:50,897] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DIFFSUM_INTEGER$20Test.table
INFO  [2023-01-17 00:50:51,022] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:51,143] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:51,143] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:51,143] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 439 B in total
INFO  [2023-01-17 00:50:51,143] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DIFFSUM_INTEGER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000473903sINFO  [2023-01-17 00:50:51,192] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=21, min=1, average=1.750000, max=3}
INFO  [2023-01-17 00:50:51,192] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[minus] with IntegerParser(super=Parser(lines=21, nullLines=0), minValue=0, maxValue=200)
INFO  [2023-01-17 00:50:51,192] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14835, maxValue=17614), dateReader=com.bakdata.conquery.util.DateReader@6ba1d0c4)
INFO  [2023-01-17 00:50:51,192] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with IntegerParser(super=Parser(lines=21, nullLines=0), minValue=50, maxValue=250)
INFO  [2023-01-17 00:50:51,195] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DIFFSUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:51,195] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:51,195] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DIFFSUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:51,215] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DIFFSUM_INTEGER$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:51 +0000] "POST /admin/datasets/DIFFSUM_INTEGER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DIFFSUM_INTEGER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:50:51,216] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:51,216] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:51,216] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:51,216] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:51,218] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:50:51,219] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DIFFSUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-17 00:50:51,219] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DIFFSUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-17 00:50:51,219] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.0
WARN  [2023-01-17 00:50:51,219] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:51,219] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.1
INFO  [2023-01-17 00:50:51,220] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.2
INFO  [2023-01-17 00:50:51,220] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DIFFSUM_INTEGER$20Test.table.table.3
INFO  [2023-01-17 00:50:51,326] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DIFFSUM_INTEGER Test QUERY INIT
INFO  [2023-01-17 00:50:51,341] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DIFFSUM_INTEGER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:51,341] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[58e1fefe-e023-48e5-ba19-7aae490cc19e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DIFFSUM_INTEGER Test))]]
INFO  [2023-01-17 00:50:51,345] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e
INFO  [2023-01-17 00:50:51,345] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e
127.0.0.1 - - [17/Jan/2023:00:50:51 +0000] "POST /api/datasets/DIFFSUM_INTEGER$20Test/queries HTTP/1.1" 201 1210 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:51,347] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e] with 4 results within PT0.002244S
INFO  [2023-01-17 00:50:51,347] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e] with 3 results within PT0.002396S
INFO  [2023-01-17 00:50:51,348] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e, workerId=DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_826d8ee8-9e77-48f4-a066-e2b0ad5629ad, startTime=2023-01-17T00:50:51.345537, finishTime=2023-01-17T00:50:51.347781) of size 4
INFO  [2023-01-17 00:50:51,349] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e, workerId=DIFFSUM_INTEGER$20Test.worker_DIFFSUM_INTEGER$20Test_328fea63-b5d7-45e3-89fa-fcf61c31285a, startTime=2023-01-17T00:50:51.345560, finishTime=2023-01-17T00:50:51.347956) of size 3
INFO  [2023-01-17 00:50:51,350] com.bakdata.conquery.models.execution.ManagedExecution: DONE 58e1fefe-e023-48e5-ba19-7aae490cc19e ManagedQuery within PT0.008483S
127.0.0.1 - - [17/Jan/2023:00:50:51 +0000] "GET /api/datasets/DIFFSUM_INTEGER$20Test/queries/DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e HTTP/1.1" 200 1489 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:50:51,381] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DIFFSUM_INTEGER Test], queryId=58e1fefe-e023-48e5-ba19-7aae490cc19e, label=concept	@§$, creationTime=2023-01-17T00:50:51.341297, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2d9ea826[Count = 0], startTime=2023-01-17T00:50:51.341548, finishTime=2023-01-17T00:50:51.350031, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@c6d4663), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DIFFSUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@e72683f, com.bakdata.conquery.models.query.ColumnDescriptor@7a142bc4]) download on dataset Dataset[label=null, name=DIFFSUM_INTEGER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:51,381] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DIFFSUM_INTEGER Test], queryId=58e1fefe-e023-48e5-ba19-7aae490cc19e, label=concept	@§$, creationTime=2023-01-17T00:50:51.341297, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2d9ea826[Count = 0], startTime=2023-01-17T00:50:51.341548, finishTime=2023-01-17T00:50:51.350031, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@c6d4663), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DIFFSUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@e72683f, com.bakdata.conquery.models.query.ColumnDescriptor@7a142bc4]) on dataset Dataset[label=null, name=DIFFSUM_INTEGER Test]
127.0.0.1 - - [17/Jan/2023:00:50:51 +0000] "GET /api/datasets/DIFFSUM_INTEGER%20Test/result/DIFFSUM_INTEGER$20Test.58e1fefe-e023-48e5-ba19-7aae490cc19e.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:50:51,400] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest DIFFSUM_INTEGER Test on 8 rows
INFO  [2023-01-17 00:50:51,400] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DIFFSUM_INTEGER Test
INFO  [2023-01-17 00:50:51,400] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-17 00:50:51,400] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DIFFSUM_INTEGER Test, name=DIFFSUM_INTEGER Test]
INFO  [2023-01-17 00:50:51,401] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DIFFSUM_INTEGER Test_826d8ee8-9e77-48f4-a066-e2b0ad5629ad
INFO  [2023-01-17 00:50:51,401] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DIFFSUM_INTEGER Test_328fea63-b5d7-45e3-89fa-fcf61c31285a
INFO  [2023-01-17 00:50:51,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DIFFSUM_INTEGER Test
INFO  [2023-01-17 00:50:51,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DIFFSUM_INTEGER Test_328fea63-b5d7-45e3-89fa-fcf61c31285a
INFO  [2023-01-17 00:50:51,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DIFFSUM_INTEGER Test_826d8ee8-9e77-48f4-a066-e2b0ad5629ad
INFO  [2023-01-17 00:50:51,520] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DIFFSUM_INTEGER$20Test
INFO  [2023-01-17 00:50:51,520] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:51,627] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DIFFSUM_INTEGER Test
INFO  [2023-01-17 00:50:51,628] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_INTEGER Test
INFO  [2023-01-17 00:50:51,628] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:51,628] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:51,629] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-17 00:50:51,629] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-17 00:50:51,629] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:51,629] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:51,631] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_e924a0d5-d149-4c81-b5a4-4f5577d0f6d2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_e924a0d5-d149-4c81-b5a4-4f5577d0f6d2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_2e0341ef-5f88-441e-83fa-f988363e68d6 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_2e0341ef-5f88-441e-83fa-f988363e68d6 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:51,632] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:51,636] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:51,739] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:51,739] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test.table
INFO  [2023-01-17 00:50:51,739] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test.table
INFO  [2023-01-17 00:50:51,857] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:51,970] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:51,970] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:51,970] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 448 B in total
INFO  [2023-01-17 00:50:51,970] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_INTEGER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00025869sINFO  [2023-01-17 00:50:51,997] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=21, min=1, average=1.750000, max=3}
INFO  [2023-01-17 00:50:51,997] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[minus] with RealParser(super=Parser(lines=21, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.52587890625E-5)
INFO  [2023-01-17 00:50:51,997] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with RealParser(super=Parser(lines=21, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.52587890625E-5)
INFO  [2023-01-17 00:50:51,997] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14835, maxValue=17614), dateReader=com.bakdata.conquery.util.DateReader@6c21b18c)
INFO  [2023-01-17 00:50:52,005] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:52,005] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:52,005] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:52,025] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_INTEGER$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "POST /admin/datasets/SUM_INTEGER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SUM_INTEGER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:50:52,028] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:52,028] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:52,028] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:52,028] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:52,031] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:50:52,032] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-17 00:50:52,032] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test.table.table], containing 21 entries.
INFO  [2023-01-17 00:50:52,033] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.0
INFO  [2023-01-17 00:50:52,034] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.1
WARN  [2023-01-17 00:50:52,034] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:52,034] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.2
INFO  [2023-01-17 00:50:52,035] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test.table.table.3
INFO  [2023-01-17 00:50:52,141] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_INTEGER Test QUERY INIT
INFO  [2023-01-17 00:50:52,161] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_INTEGER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:52,161] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b4bcd4f2-0503-4aa6-acf8-c138fc8ee269] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test))]]
INFO  [2023-01-17 00:50:52,164] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "POST /api/datasets/SUM_INTEGER$20Test/queries HTTP/1.1" 201 1191 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:50:52,165] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269
INFO  [2023-01-17 00:50:52,166] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269] with 4 results within PT0.001337S
INFO  [2023-01-17 00:50:52,166] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269, workerId=SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_e924a0d5-d149-4c81-b5a4-4f5577d0f6d2, startTime=2023-01-17T00:50:52.164770, finishTime=2023-01-17T00:50:52.166107) of size 4
INFO  [2023-01-17 00:50:52,166] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269] with 3 results within PT0.001559S
INFO  [2023-01-17 00:50:52,167] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269, workerId=SUM_INTEGER$20Test.worker_SUM_INTEGER$20Test_2e0341ef-5f88-441e-83fa-f988363e68d6, startTime=2023-01-17T00:50:52.165331, finishTime=2023-01-17T00:50:52.166890) of size 3
INFO  [2023-01-17 00:50:52,171] com.bakdata.conquery.models.execution.ManagedExecution: DONE b4bcd4f2-0503-4aa6-acf8-c138fc8ee269 ManagedQuery within PT0.010085S
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "GET /api/datasets/SUM_INTEGER$20Test/queries/SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269 HTTP/1.1" 200 1454 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:52,193] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test], queryId=b4bcd4f2-0503-4aa6-acf8-c138fc8ee269, label=concept	@§$, creationTime=2023-01-17T00:50:52.161507, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22a1a82c[Count = 0], startTime=2023-01-17T00:50:52.161655, finishTime=2023-01-17T00:50:52.171740, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4406cc22), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@48adf5c, com.bakdata.conquery.models.query.ColumnDescriptor@4cd9dfd9]) download on dataset Dataset[label=null, name=SUM_INTEGER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:52,193] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test], queryId=b4bcd4f2-0503-4aa6-acf8-c138fc8ee269, label=concept	@§$, creationTime=2023-01-17T00:50:52.161507, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22a1a82c[Count = 0], startTime=2023-01-17T00:50:52.161655, finishTime=2023-01-17T00:50:52.171740, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4406cc22), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@48adf5c, com.bakdata.conquery.models.query.ColumnDescriptor@4cd9dfd9]) on dataset Dataset[label=null, name=SUM_INTEGER Test]
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "GET /api/datasets/SUM_INTEGER%20Test/result/SUM_INTEGER$20Test.b4bcd4f2-0503-4aa6-acf8-c138fc8ee269.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:52,211] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SUM_INTEGER Test on 8 rows
INFO  [2023-01-17 00:50:52,211] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_INTEGER Test
INFO  [2023-01-17 00:50:52,215] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-17 00:50:52,215] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test, name=SUM_INTEGER Test]
INFO  [2023-01-17 00:50:52,215] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test_e924a0d5-d149-4c81-b5a4-4f5577d0f6d2
INFO  [2023-01-17 00:50:52,215] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test_2e0341ef-5f88-441e-83fa-f988363e68d6
INFO  [2023-01-17 00:50:52,229] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_INTEGER Test
INFO  [2023-01-17 00:50:52,230] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test_e924a0d5-d149-4c81-b5a4-4f5577d0f6d2
INFO  [2023-01-17 00:50:52,231] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test_2e0341ef-5f88-441e-83fa-f988363e68d6
INFO  [2023-01-17 00:50:52,234] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_INTEGER$20Test
INFO  [2023-01-17 00:50:52,234] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:52,347] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_INTEGER Test
INFO  [2023-01-17 00:50:52,348] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM Test
INFO  [2023-01-17 00:50:52,348] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:52,348] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:52,349] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-17 00:50:52,349] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-17 00:50:52,349] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:52,349] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:52,351] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:52,352] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_6a0f7c91-4530-4402-937d-37cf46511123 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:52,352] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_6a0f7c91-4530-4402-937d-37cf46511123 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:52,352] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:52,352] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_43102c0c-1688-436b-a331-237c3b20efbc are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:52,352] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM$20Test.worker_DURATION_SUM$20Test_43102c0c-1688-436b-a331-237c3b20efbc are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:52,352] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:52,461] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:52,461] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM$20Test.table
INFO  [2023-01-17 00:50:52,461] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM$20Test.table
INFO  [2023-01-17 00:50:52,580] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:52,700] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:52,700] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:52,700] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 166 B in total
INFO  [2023-01-17 00:50:52,700] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000283258sINFO  [2023-01-17 00:50:52,729] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:50:52,729] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14610, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@257738c0), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14640, maxValue=15928), dateReader=com.bakdata.conquery.util.DateReader@6ed9c624), dateReader=com.bakdata.conquery.util.DateReader@5b26b690, onlyQuarters=false, maxValue=15928, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:50:52,731] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:52,731] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:52,731] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:52,748] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DURATION_SUM$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "POST /admin/datasets/DURATION_SUM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DURATION_SUM+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:52,751] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:52,751] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:52,751] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:52,751] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:52,755] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:52,755] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:50:52,755] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:50:52,757] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:52,757] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM$20Test.table.table.0
INFO  [2023-01-17 00:50:52,757] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM$20Test.table.table.1
INFO  [2023-01-17 00:50:52,862] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM Test QUERY INIT
INFO  [2023-01-17 00:50:52,879] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:52,880] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[13e7a628-819b-4f4f-a0a9-4e64a21ce8de] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM Test))]]
INFO  [2023-01-17 00:50:52,885] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de
INFO  [2023-01-17 00:50:52,886] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de
INFO  [2023-01-17 00:50:52,887] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de] with 1 results within PT0.001206S
INFO  [2023-01-17 00:50:52,887] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de] with 2 results within PT0.001813S
INFO  [2023-01-17 00:50:52,888] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de, workerId=DURATION_SUM$20Test.worker_DURATION_SUM$20Test_6a0f7c91-4530-4402-937d-37cf46511123, startTime=2023-01-17T00:50:52.886070, finishTime=2023-01-17T00:50:52.887276) of size 1
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "POST /api/datasets/DURATION_SUM$20Test/queries HTTP/1.1" 201 1197 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:52,888] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de, workerId=DURATION_SUM$20Test.worker_DURATION_SUM$20Test_43102c0c-1688-436b-a331-237c3b20efbc, startTime=2023-01-17T00:50:52.885971, finishTime=2023-01-17T00:50:52.887784) of size 2
INFO  [2023-01-17 00:50:52,888] com.bakdata.conquery.models.execution.ManagedExecution: DONE 13e7a628-819b-4f4f-a0a9-4e64a21ce8de ManagedQuery within PT0.008107S
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "GET /api/datasets/DURATION_SUM$20Test/queries/DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de HTTP/1.1" 200 1464 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:52,924] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM Test], queryId=13e7a628-819b-4f4f-a0a9-4e64a21ce8de, label=concept	@§$, creationTime=2023-01-17T00:50:52.880353, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@43a1926e[Count = 0], startTime=2023-01-17T00:50:52.880657, finishTime=2023-01-17T00:50:52.888764, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@29803244), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7aa57e08, com.bakdata.conquery.models.query.ColumnDescriptor@797467bc]) download on dataset Dataset[label=null, name=DURATION_SUM Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:52,924] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM Test], queryId=13e7a628-819b-4f4f-a0a9-4e64a21ce8de, label=concept	@§$, creationTime=2023-01-17T00:50:52.880353, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@43a1926e[Count = 0], startTime=2023-01-17T00:50:52.880657, finishTime=2023-01-17T00:50:52.888764, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@29803244), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7aa57e08, com.bakdata.conquery.models.query.ColumnDescriptor@797467bc]) on dataset Dataset[label=null, name=DURATION_SUM Test]
127.0.0.1 - - [17/Jan/2023:00:50:52 +0000] "GET /api/datasets/DURATION_SUM%20Test/result/DURATION_SUM$20Test.13e7a628-819b-4f4f-a0a9-4e64a21ce8de.csv?pretty=false HTTP/1.1" 200 91 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:50:52,945] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest DURATION_SUM Test on 4 rows
INFO  [2023-01-17 00:50:52,945] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM Test
INFO  [2023-01-17 00:50:52,945] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-17 00:50:52,945] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM Test, name=DURATION_SUM Test]
INFO  [2023-01-17 00:50:52,945] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM Test_6a0f7c91-4530-4402-937d-37cf46511123
INFO  [2023-01-17 00:50:52,945] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM Test_43102c0c-1688-436b-a331-237c3b20efbc
INFO  [2023-01-17 00:50:52,950] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM Test
INFO  [2023-01-17 00:50:52,951] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM Test_6a0f7c91-4530-4402-937d-37cf46511123
INFO  [2023-01-17 00:50:52,951] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM Test_43102c0c-1688-436b-a331-237c3b20efbc
INFO  [2023-01-17 00:50:52,957] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM$20Test
INFO  [2023-01-17 00:50:52,957] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,062] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM Test
INFO  [2023-01-17 00:50:53,062] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM_2 Test
INFO  [2023-01-17 00:50:53,063] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:53,063] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:53,064] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-17 00:50:53,064] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-17 00:50:53,064] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:53,064] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_3929568d-1a52-420c-a1ce-b6a831dd6c0b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_3929568d-1a52-420c-a1ce-b6a831dd6c0b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_7352b45d-a51f-456e-ba37-74a34574dee0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_7352b45d-a51f-456e-ba37-74a34574dee0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:53,066] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:53,172] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,173] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_2$20Test.table
INFO  [2023-01-17 00:50:53,173] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_2$20Test.table
INFO  [2023-01-17 00:50:53,287] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,397] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:53,398] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:53,398] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 170 B in total
INFO  [2023-01-17 00:50:53,398] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000213401sINFO  [2023-01-17 00:50:53,420] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-17 00:50:53,420] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=7, nullLines=1), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14610, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@33365aa4), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14640, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@4b35cb44), dateReader=com.bakdata.conquery.util.DateReader@6742a40e, onlyQuarters=false, maxValue=15927, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:50:53,422] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:53,423] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:53,423] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:53,451] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DURATION_SUM_2$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:53 +0000] "POST /admin/datasets/DURATION_SUM_2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DURATION_SUM_2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:50:53,453] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,454] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:53,454] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:53,454] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:53,456] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:53,456] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_2$20Test.table.table], containing 7 entries.
INFO  [2023-01-17 00:50:53,456] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_2$20Test.table.table], containing 7 entries.
WARN  [2023-01-17 00:50:53,458] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:53,458] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_2$20Test.table.table.0
INFO  [2023-01-17 00:50:53,458] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_2$20Test.table.table.1
INFO  [2023-01-17 00:50:53,573] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM_2 Test QUERY INIT
INFO  [2023-01-17 00:50:53,591] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM_2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:53,592] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1bc818ec-b24b-40ff-9481-409f9ce84467] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_2 Test))]]
INFO  [2023-01-17 00:50:53,596] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467
INFO  [2023-01-17 00:50:53,596] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467
INFO  [2023-01-17 00:50:53,597] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467] with 1 results within PT0.000916S
INFO  [2023-01-17 00:50:53,597] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467] with 3 results within PT0.001134S
127.0.0.1 - - [17/Jan/2023:00:50:53 +0000] "POST /api/datasets/DURATION_SUM_2$20Test/queries HTTP/1.1" 201 1204 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:53,597] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467, workerId=DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_7352b45d-a51f-456e-ba37-74a34574dee0, startTime=2023-01-17T00:50:53.596197, finishTime=2023-01-17T00:50:53.597113) of size 1
INFO  [2023-01-17 00:50:53,598] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467, workerId=DURATION_SUM_2$20Test.worker_DURATION_SUM_2$20Test_3929568d-1a52-420c-a1ce-b6a831dd6c0b, startTime=2023-01-17T00:50:53.596080, finishTime=2023-01-17T00:50:53.597214) of size 3
INFO  [2023-01-17 00:50:53,598] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1bc818ec-b24b-40ff-9481-409f9ce84467 ManagedQuery within PT0.005888S
127.0.0.1 - - [17/Jan/2023:00:50:53 +0000] "GET /api/datasets/DURATION_SUM_2$20Test/queries/DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467 HTTP/1.1" 200 1479 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:53,627] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_2 Test], queryId=1bc818ec-b24b-40ff-9481-409f9ce84467, label=concept	@§$, creationTime=2023-01-17T00:50:53.592040, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5996f26a[Count = 0], startTime=2023-01-17T00:50:53.592294, finishTime=2023-01-17T00:50:53.598182, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1201e925), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e86190c, com.bakdata.conquery.models.query.ColumnDescriptor@13d6c940]) download on dataset Dataset[label=null, name=DURATION_SUM_2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:53,628] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_2 Test], queryId=1bc818ec-b24b-40ff-9481-409f9ce84467, label=concept	@§$, creationTime=2023-01-17T00:50:53.592040, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5996f26a[Count = 0], startTime=2023-01-17T00:50:53.592294, finishTime=2023-01-17T00:50:53.598182, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1201e925), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e86190c, com.bakdata.conquery.models.query.ColumnDescriptor@13d6c940]) on dataset Dataset[label=null, name=DURATION_SUM_2 Test]
127.0.0.1 - - [17/Jan/2023:00:50:53 +0000] "GET /api/datasets/DURATION_SUM_2%20Test/result/DURATION_SUM_2$20Test.1bc818ec-b24b-40ff-9481-409f9ce84467.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:50:53,646] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest DURATION_SUM_2 Test on 5 rows
INFO  [2023-01-17 00:50:53,646] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM_2 Test
INFO  [2023-01-17 00:50:53,646] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-17 00:50:53,647] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_2 Test, name=DURATION_SUM_2 Test]
INFO  [2023-01-17 00:50:53,647] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_2 Test_3929568d-1a52-420c-a1ce-b6a831dd6c0b
INFO  [2023-01-17 00:50:53,647] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_2 Test_7352b45d-a51f-456e-ba37-74a34574dee0
INFO  [2023-01-17 00:50:53,671] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_2 Test_7352b45d-a51f-456e-ba37-74a34574dee0
INFO  [2023-01-17 00:50:53,671] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_2 Test_3929568d-1a52-420c-a1ce-b6a831dd6c0b
INFO  [2023-01-17 00:50:53,671] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM_2 Test
INFO  [2023-01-17 00:50:53,758] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM_2$20Test
INFO  [2023-01-17 00:50:53,758] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,864] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM_2 Test
INFO  [2023-01-17 00:50:53,865] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT Test
INFO  [2023-01-17 00:50:53,865] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:53,865] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:53,866] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-17 00:50:53,866] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-17 00:50:53,866] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:53,866] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_f365a023-a004-48f4-bb19-503e1d04bd4e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_f365a023-a004-48f4-bb19-503e1d04bd4e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_2ecf2e6e-a5e3-41e8-893a-e94ad79585d1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_2ecf2e6e-a5e3-41e8-893a-e94ad79585d1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:53,873] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:53,975] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:53,976] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT$20Test.table
INFO  [2023-01-17 00:50:53,976] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT$20Test.table
INFO  [2023-01-17 00:50:54,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,207] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:54,207] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:54,207] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 139 B in total
INFO  [2023-01-17 00:50:54,207] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000417501sINFO  [2023-01-17 00:50:54,249] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=8, min=1, average=1.600000, max=3}
INFO  [2023-01-17 00:50:54,250] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@282509ad)
INFO  [2023-01-17 00:50:54,250] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:54,253] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:54,253] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:54,253] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:54,274] com.bakdata.conquery.models.jobs.ImportJob: Importing table into MULTI_SELECT$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:54 +0000] "POST /admin/datasets/MULTI_SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:50:54,277] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,277] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:54,278] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:54,278] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:54,281] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:54,282] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT$20Test.table.table], containing 8 entries.
INFO  [2023-01-17 00:50:54,282] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT$20Test.table.table], containing 8 entries.
WARN  [2023-01-17 00:50:54,283] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:54,284] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT$20Test.table.table.0
INFO  [2023-01-17 00:50:54,284] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT$20Test.table.table.1
INFO  [2023-01-17 00:50:54,389] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT Test QUERY INIT
INFO  [2023-01-17 00:50:54,399] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:54,399] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[41994cdc-b0d5-443a-a931-947e970c9b45] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT Test))]]
INFO  [2023-01-17 00:50:54,403] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45
INFO  [2023-01-17 00:50:54,403] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45
127.0.0.1 - - [17/Jan/2023:00:50:54 +0000] "POST /api/datasets/MULTI_SELECT$20Test/queries HTTP/1.1" 201 1185 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:50:54,404] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45] with 1 results within PT0.001161S
INFO  [2023-01-17 00:50:54,404] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45] with 2 results within PT0.001277S
INFO  [2023-01-17 00:50:54,405] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45, workerId=MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_2ecf2e6e-a5e3-41e8-893a-e94ad79585d1, startTime=2023-01-17T00:50:54.403106, finishTime=2023-01-17T00:50:54.404267) of size 1
INFO  [2023-01-17 00:50:54,405] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45, workerId=MULTI_SELECT$20Test.worker_MULTI_SELECT$20Test_f365a023-a004-48f4-bb19-503e1d04bd4e, startTime=2023-01-17T00:50:54.403107, finishTime=2023-01-17T00:50:54.404384) of size 2
INFO  [2023-01-17 00:50:54,405] com.bakdata.conquery.models.execution.ManagedExecution: DONE 41994cdc-b0d5-443a-a931-947e970c9b45 ManagedQuery within PT0.00532S
127.0.0.1 - - [17/Jan/2023:00:50:54 +0000] "GET /api/datasets/MULTI_SELECT$20Test/queries/MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45 HTTP/1.1" 200 1452 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:54,427] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT Test], queryId=41994cdc-b0d5-443a-a931-947e970c9b45, label=concept	@§$, creationTime=2023-01-17T00:50:54.399714, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@52cba48d[Count = 0], startTime=2023-01-17T00:50:54.399937, finishTime=2023-01-17T00:50:54.405257, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@14f787f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6f7c97c6, com.bakdata.conquery.models.query.ColumnDescriptor@35037b3e]) download on dataset Dataset[label=null, name=MULTI_SELECT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:54,427] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT Test], queryId=41994cdc-b0d5-443a-a931-947e970c9b45, label=concept	@§$, creationTime=2023-01-17T00:50:54.399714, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@52cba48d[Count = 0], startTime=2023-01-17T00:50:54.399937, finishTime=2023-01-17T00:50:54.405257, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@14f787f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6f7c97c6, com.bakdata.conquery.models.query.ColumnDescriptor@35037b3e]) on dataset Dataset[label=null, name=MULTI_SELECT Test]
127.0.0.1 - - [17/Jan/2023:00:50:54 +0000] "GET /api/datasets/MULTI_SELECT%20Test/result/MULTI_SELECT$20Test.41994cdc-b0d5-443a-a931-947e970c9b45.csv?pretty=false HTTP/1.1" 200 91 "-" "Conquery (test client)" 16
INFO  [2023-01-17 00:50:54,442] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest MULTI_SELECT Test on 4 rows
INFO  [2023-01-17 00:50:54,443] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT Test
INFO  [2023-01-17 00:50:54,443] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-17 00:50:54,443] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT Test, name=MULTI_SELECT Test]
INFO  [2023-01-17 00:50:54,443] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT Test_f365a023-a004-48f4-bb19-503e1d04bd4e
INFO  [2023-01-17 00:50:54,443] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT Test_2ecf2e6e-a5e3-41e8-893a-e94ad79585d1
INFO  [2023-01-17 00:50:54,466] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT Test
INFO  [2023-01-17 00:50:54,467] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT Test_f365a023-a004-48f4-bb19-503e1d04bd4e
INFO  [2023-01-17 00:50:54,468] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT Test_2ecf2e6e-a5e3-41e8-893a-e94ad79585d1
INFO  [2023-01-17 00:50:54,484] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT$20Test
INFO  [2023-01-17 00:50:54,484] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,589] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT Test
INFO  [2023-01-17 00:50:54,589] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_DECIMAL Test
INFO  [2023-01-17 00:50:54,590] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:54,590] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:54,590] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-17 00:50:54,590] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-17 00:50:54,591] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:54,591] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:54,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_743e65f7-fd14-4c06-bd5c-6884aee63831 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:54,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_743e65f7-fd14-4c06-bd5c-6884aee63831 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:54,592] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:54,593] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_f3d43f0b-a0f9-4f00-830f-c8e27ff358be are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:54,593] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_f3d43f0b-a0f9-4f00-830f-c8e27ff358be are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:54,593] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:54,596] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,700] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,700] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_DECIMAL$20Test.table
INFO  [2023-01-17 00:50:54,700] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_DECIMAL$20Test.table
INFO  [2023-01-17 00:50:54,816] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,933] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:54,933] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:54,933] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 355 B in total
INFO  [2023-01-17 00:50:54,934] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_DECIMAL Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000308177sINFO  [2023-01-17 00:50:54,965] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=18, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:50:54,965] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=18, nullLines=0), subType=IntegerParser(super=Parser(lines=18, nullLines=0), minValue=14835, maxValue=17612), dateReader=com.bakdata.conquery.util.DateReader@5da0e191)
INFO  [2023-01-17 00:50:54,965] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with DecimalParser(super=Parser(lines=18, nullLines=3), maxScale=2, maxAbs=300)
INFO  [2023-01-17 00:50:54,971] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_DECIMAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:54,971] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:54,971] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_DECIMAL Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:54,989] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_DECIMAL$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:54 +0000] "POST /admin/datasets/NUMBER_DECIMAL%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_DECIMAL+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:54,990] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:54,991] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:54,991] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:54,991] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:54,992] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:50:54,992] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_DECIMAL$20Test.table.table], containing 18 entries.
INFO  [2023-01-17 00:50:54,993] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_DECIMAL$20Test.table.table], containing 18 entries.
WARN  [2023-01-17 00:50:54,993] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:54,993] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.0
INFO  [2023-01-17 00:50:54,993] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.1
INFO  [2023-01-17 00:50:54,993] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.3
INFO  [2023-01-17 00:50:54,993] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_DECIMAL$20Test.table.table.2
INFO  [2023-01-17 00:50:55,098] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_DECIMAL Test QUERY INIT
INFO  [2023-01-17 00:50:55,116] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_DECIMAL$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:55,116] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2c7912be-c269-4c33-8a68-4063b8770ec6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_DECIMAL Test))]]
INFO  [2023-01-17 00:50:55,121] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6
INFO  [2023-01-17 00:50:55,121] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6
INFO  [2023-01-17 00:50:55,123] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6] with 2 results within PT0.002129S
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "POST /api/datasets/NUMBER_DECIMAL$20Test/queries HTTP/1.1" 201 1203 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:50:55,123] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6] with 3 results within PT0.002284S
INFO  [2023-01-17 00:50:55,124] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6, workerId=NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_743e65f7-fd14-4c06-bd5c-6884aee63831, startTime=2023-01-17T00:50:55.121043, finishTime=2023-01-17T00:50:55.123172) of size 2
INFO  [2023-01-17 00:50:55,124] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6, workerId=NUMBER_DECIMAL$20Test.worker_NUMBER_DECIMAL$20Test_f3d43f0b-a0f9-4f00-830f-c8e27ff358be, startTime=2023-01-17T00:50:55.121075, finishTime=2023-01-17T00:50:55.123359) of size 3
INFO  [2023-01-17 00:50:55,124] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2c7912be-c269-4c33-8a68-4063b8770ec6 ManagedQuery within PT0.007456S
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "GET /api/datasets/NUMBER_DECIMAL$20Test/queries/NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6 HTTP/1.1" 200 1478 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:55,152] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_DECIMAL Test], queryId=2c7912be-c269-4c33-8a68-4063b8770ec6, label=concept	@§$, creationTime=2023-01-17T00:50:55.116695, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3c8958a7[Count = 0], startTime=2023-01-17T00:50:55.116951, finishTime=2023-01-17T00:50:55.124407, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ac270c4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_DECIMAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@156fb865, com.bakdata.conquery.models.query.ColumnDescriptor@7df6a9c2]) download on dataset Dataset[label=null, name=NUMBER_DECIMAL Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:55,152] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_DECIMAL Test], queryId=2c7912be-c269-4c33-8a68-4063b8770ec6, label=concept	@§$, creationTime=2023-01-17T00:50:55.116695, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3c8958a7[Count = 0], startTime=2023-01-17T00:50:55.116951, finishTime=2023-01-17T00:50:55.124407, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ac270c4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_DECIMAL Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@156fb865, com.bakdata.conquery.models.query.ColumnDescriptor@7df6a9c2]) on dataset Dataset[label=null, name=NUMBER_DECIMAL Test]
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "GET /api/datasets/NUMBER_DECIMAL%20Test/result/NUMBER_DECIMAL$20Test.2c7912be-c269-4c33-8a68-4063b8770ec6.csv?pretty=false HTTP/1.1" 200 144 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:50:55,174] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_DECIMAL Test on 6 rows
INFO  [2023-01-17 00:50:55,175] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_DECIMAL Test
INFO  [2023-01-17 00:50:55,175] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-17 00:50:55,175] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_DECIMAL Test, name=NUMBER_DECIMAL Test]
INFO  [2023-01-17 00:50:55,176] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_DECIMAL Test_f3d43f0b-a0f9-4f00-830f-c8e27ff358be
INFO  [2023-01-17 00:50:55,176] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_DECIMAL Test_743e65f7-fd14-4c06-bd5c-6884aee63831
INFO  [2023-01-17 00:50:55,191] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_DECIMAL Test
INFO  [2023-01-17 00:50:55,192] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_DECIMAL Test_f3d43f0b-a0f9-4f00-830f-c8e27ff358be
INFO  [2023-01-17 00:50:55,192] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_DECIMAL Test_743e65f7-fd14-4c06-bd5c-6884aee63831
INFO  [2023-01-17 00:50:55,193] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_DECIMAL$20Test
INFO  [2023-01-17 00:50:55,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:55,300] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_DECIMAL Test
INFO  [2023-01-17 00:50:55,301] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_INTEGER Test
INFO  [2023-01-17 00:50:55,301] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:55,301] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:55,302] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-17 00:50:55,302] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-17 00:50:55,302] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:55,302] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:55,304] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:55,304] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_73a4f4d7-81c3-44b9-bcf5-c4c5910c9c02 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:55,304] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_73a4f4d7-81c3-44b9-bcf5-c4c5910c9c02 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:55,304] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:55,305] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_7f6bbf25-06db-4e03-b192-cb3efef181b6 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:55,305] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_7f6bbf25-06db-4e03-b192-cb3efef181b6 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:55,305] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:55,412] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:55,412] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test.table
INFO  [2023-01-17 00:50:55,413] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test.table
INFO  [2023-01-17 00:50:55,526] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:55,639] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:55,640] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:55,640] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 255 B in total
INFO  [2023-01-17 00:50:55,640] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_INTEGER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000317795sINFO  [2023-01-17 00:50:55,672] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=9, sum=15, min=1, average=1.666667, max=2}
INFO  [2023-01-17 00:50:55,672] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=15, nullLines=0), subType=IntegerParser(super=Parser(lines=15, nullLines=0), minValue=14835, maxValue=17351), dateReader=com.bakdata.conquery.util.DateReader@7a8a59c3)
INFO  [2023-01-17 00:50:55,672] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=15, nullLines=3), minValue=50, maxValue=300)
INFO  [2023-01-17 00:50:55,675] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:55,675] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:55,675] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_INTEGER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:55,700] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_INTEGER$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "POST /admin/datasets/NUMBER_INTEGER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_INTEGER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:50:55,701] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:55,703] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:55,703] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:55,703] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:55,705] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:50:55,705] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test.table.table], containing 15 entries.
INFO  [2023-01-17 00:50:55,705] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test.table.table], containing 15 entries.
INFO  [2023-01-17 00:50:55,707] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test.table.table.0
WARN  [2023-01-17 00:50:55,707] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:55,707] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test.table.table.1
INFO  [2023-01-17 00:50:55,707] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test.table.table.2
INFO  [2023-01-17 00:50:55,812] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_INTEGER Test QUERY INIT
INFO  [2023-01-17 00:50:55,840] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_INTEGER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:55,841] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9fc78f14-68d6-4cf3-b8dc-32755c7db399] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test))]]
INFO  [2023-01-17 00:50:55,845] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399
INFO  [2023-01-17 00:50:55,845] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "POST /api/datasets/NUMBER_INTEGER$20Test/queries HTTP/1.1" 201 1206 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:55,846] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399] with 2 results within PT0.001468S
INFO  [2023-01-17 00:50:55,847] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399] with 2 results within PT0.002055S
INFO  [2023-01-17 00:50:55,847] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399, workerId=NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_73a4f4d7-81c3-44b9-bcf5-c4c5910c9c02, startTime=2023-01-17T00:50:55.845182, finishTime=2023-01-17T00:50:55.846650) of size 2
INFO  [2023-01-17 00:50:55,848] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399, workerId=NUMBER_INTEGER$20Test.worker_NUMBER_INTEGER$20Test_7f6bbf25-06db-4e03-b192-cb3efef181b6, startTime=2023-01-17T00:50:55.845191, finishTime=2023-01-17T00:50:55.847246) of size 2
INFO  [2023-01-17 00:50:55,848] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9fc78f14-68d6-4cf3-b8dc-32755c7db399 ManagedQuery within PT0.006954S
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "GET /api/datasets/NUMBER_INTEGER$20Test/queries/NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399 HTTP/1.1" 200 1481 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:55,879] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test], queryId=9fc78f14-68d6-4cf3-b8dc-32755c7db399, label=concept	@§$, creationTime=2023-01-17T00:50:55.841026, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@b0b7394[Count = 0], startTime=2023-01-17T00:50:55.841264, finishTime=2023-01-17T00:50:55.848218, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@426d7601), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2afde43d, com.bakdata.conquery.models.query.ColumnDescriptor@2471d1ed]) download on dataset Dataset[label=null, name=NUMBER_INTEGER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:55,879] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test], queryId=9fc78f14-68d6-4cf3-b8dc-32755c7db399, label=concept	@§$, creationTime=2023-01-17T00:50:55.841026, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@b0b7394[Count = 0], startTime=2023-01-17T00:50:55.841264, finishTime=2023-01-17T00:50:55.848218, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@426d7601), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2afde43d, com.bakdata.conquery.models.query.ColumnDescriptor@2471d1ed]) on dataset Dataset[label=null, name=NUMBER_INTEGER Test]
127.0.0.1 - - [17/Jan/2023:00:50:55 +0000] "GET /api/datasets/NUMBER_INTEGER%20Test/result/NUMBER_INTEGER$20Test.9fc78f14-68d6-4cf3-b8dc-32755c7db399.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:55,896] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_INTEGER Test on 5 rows
INFO  [2023-01-17 00:50:55,897] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_INTEGER Test
INFO  [2023-01-17 00:50:55,897] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-17 00:50:55,897] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test, name=NUMBER_INTEGER Test]
INFO  [2023-01-17 00:50:55,897] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test_73a4f4d7-81c3-44b9-bcf5-c4c5910c9c02
INFO  [2023-01-17 00:50:55,898] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test_7f6bbf25-06db-4e03-b192-cb3efef181b6
INFO  [2023-01-17 00:50:55,903] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_INTEGER Test
INFO  [2023-01-17 00:50:55,903] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test_7f6bbf25-06db-4e03-b192-cb3efef181b6
INFO  [2023-01-17 00:50:55,904] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test_73a4f4d7-81c3-44b9-bcf5-c4c5910c9c02
INFO  [2023-01-17 00:50:55,907] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_INTEGER$20Test
INFO  [2023-01-17 00:50:55,907] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,014] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_INTEGER Test
INFO  [2023-01-17 00:50:56,014] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_INTEGER Test
INFO  [2023-01-17 00:50:56,014] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:56,014] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:56,015] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-17 00:50:56,015] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-17 00:50:56,015] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:56,015] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:56,017] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_787eb2b3-eec7-4588-b500-5b2ed47b7e6c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:56,017] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_787eb2b3-eec7-4588-b500-5b2ed47b7e6c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:56,017] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:56,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_298747ae-763c-4c03-9185-2801c93ac3a6 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:56,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_298747ae-763c-4c03-9185-2801c93ac3a6 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:56,018] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:56,021] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,131] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,132] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test[1].table
INFO  [2023-01-17 00:50:56,132] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_INTEGER$20Test[1].table
INFO  [2023-01-17 00:50:56,243] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,354] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:56,354] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:56,354] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 41 B in total
INFO  [2023-01-17 00:50:56,354] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_INTEGER Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000436262sINFO  [2023-01-17 00:50:56,399] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=1, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:56,399] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=1, nullLines=0), minParser=DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=16511, maxValue=16511), dateReader=com.bakdata.conquery.util.DateReader@54bd90c0), maxParser=DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=16800, maxValue=16800), dateReader=com.bakdata.conquery.util.DateReader@7f9c3f38), dateReader=com.bakdata.conquery.util.DateReader@23a5276f, onlyQuarters=false, maxValue=16800, minValue=16511, anyOpen=false)
INFO  [2023-01-17 00:50:56,399] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with IntegerParser(super=Parser(lines=1, nullLines=0), minValue=50, maxValue=50)
INFO  [2023-01-17 00:50:56,401] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:56,401] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:56,401] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:56,416] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_INTEGER$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:50:56 +0000] "POST /admin/datasets/NUMBER_INTEGER%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_INTEGER+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:56,418] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,418] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:56,418] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:56,418] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:56,421] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:50:56,422] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test[1].table.table], containing 1 entries.
INFO  [2023-01-17 00:50:56,422] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_INTEGER$20Test[1].table.table], containing 1 entries.
WARN  [2023-01-17 00:50:56,423] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:56,423] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_INTEGER$20Test[1].table.table.0
INFO  [2023-01-17 00:50:56,529] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_INTEGER Test QUERY INIT
INFO  [2023-01-17 00:50:56,542] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_INTEGER$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:56,542] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[38246b28-8bf3-4e7b-a166-88c61a9d549d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test[1]))]]
INFO  [2023-01-17 00:50:56,546] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test[1].38246b28-8bf3-4e7b-a166-88c61a9d549d
INFO  [2023-01-17 00:50:56,546] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_INTEGER$20Test[1].38246b28-8bf3-4e7b-a166-88c61a9d549d
WARN  [2023-01-17 00:50:56,546] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:50:56,546] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test[1].38246b28-8bf3-4e7b-a166-88c61a9d549d] with 0 results within PT0.000195S
INFO  [2023-01-17 00:50:56,547] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_INTEGER$20Test[1].38246b28-8bf3-4e7b-a166-88c61a9d549d] with 1 results within PT0.000779S
INFO  [2023-01-17 00:50:56,547] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test[1].38246b28-8bf3-4e7b-a166-88c61a9d549d, workerId=NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_298747ae-763c-4c03-9185-2801c93ac3a6, startTime=2023-01-17T00:50:56.546519, finishTime=2023-01-17T00:50:56.546714) of size 0
127.0.0.1 - - [17/Jan/2023:00:50:56 +0000] "POST /api/datasets/NUMBER_INTEGER$20Test%5B1%5D/queries HTTP/1.1" 201 1216 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:56,548] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_INTEGER$20Test[1].38246b28-8bf3-4e7b-a166-88c61a9d549d, workerId=NUMBER_INTEGER$20Test[1].worker_NUMBER_INTEGER$20Test[1]_787eb2b3-eec7-4588-b500-5b2ed47b7e6c, startTime=2023-01-17T00:50:56.546427, finishTime=2023-01-17T00:50:56.547206) of size 1
INFO  [2023-01-17 00:50:56,548] com.bakdata.conquery.models.execution.ManagedExecution: DONE 38246b28-8bf3-4e7b-a166-88c61a9d549d ManagedQuery within PT0.005563S
127.0.0.1 - - [17/Jan/2023:00:50:56 +0000] "GET /api/datasets/NUMBER_INTEGER$20Test%5B1%5D/queries/NUMBER_INTEGER$20Test%5B1%5D.38246b28-8bf3-4e7b-a166-88c61a9d549d HTTP/1.1" 200 1750 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:56,586] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test[1]], queryId=38246b28-8bf3-4e7b-a166-88c61a9d549d, label=concept	@§$, creationTime=2023-01-17T00:50:56.542351, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2148692[Count = 0], startTime=2023-01-17T00:50:56.542617, finishTime=2023-01-17T00:50:56.548180, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b27bf5b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@24a9f637, com.bakdata.conquery.models.query.ColumnDescriptor@53b8b0fb]) download on dataset Dataset[label=null, name=NUMBER_INTEGER Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:56,586] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_INTEGER Test[1]], queryId=38246b28-8bf3-4e7b-a166-88c61a9d549d, label=concept	@§$, creationTime=2023-01-17T00:50:56.542351, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2148692[Count = 0], startTime=2023-01-17T00:50:56.542617, finishTime=2023-01-17T00:50:56.548180, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b27bf5b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@24a9f637, com.bakdata.conquery.models.query.ColumnDescriptor@53b8b0fb]) on dataset Dataset[label=null, name=NUMBER_INTEGER Test[1]]
127.0.0.1 - - [17/Jan/2023:00:50:56 +0000] "GET /api/datasets/NUMBER_INTEGER%20Test%5B1%5D/result/NUMBER_INTEGER$20Test%5B1%5D.38246b28-8bf3-4e7b-a166-88c61a9d549d.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 16
INFO  [2023-01-17 00:50:56,601] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_INTEGER Test on 2 rows
INFO  [2023-01-17 00:50:56,601] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_INTEGER Test[1]
INFO  [2023-01-17 00:50:56,602] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-17 00:50:56,602] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_INTEGER Test[1], name=NUMBER_INTEGER Test[1]]
INFO  [2023-01-17 00:50:56,602] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test[1]_298747ae-763c-4c03-9185-2801c93ac3a6
INFO  [2023-01-17 00:50:56,602] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_INTEGER Test[1]_787eb2b3-eec7-4588-b500-5b2ed47b7e6c
INFO  [2023-01-17 00:50:56,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_INTEGER Test[1]
INFO  [2023-01-17 00:50:56,617] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test[1]_298747ae-763c-4c03-9185-2801c93ac3a6
INFO  [2023-01-17 00:50:56,617] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_INTEGER Test[1]_787eb2b3-eec7-4588-b500-5b2ed47b7e6c
INFO  [2023-01-17 00:50:56,623] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_INTEGER$20Test[1]
INFO  [2023-01-17 00:50:56,623] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,729] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_INTEGER Test
INFO  [2023-01-17 00:50:56,729] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MONEY Test
INFO  [2023-01-17 00:50:56,729] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:56,730] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:56,731] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-17 00:50:56,731] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-17 00:50:56,731] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:56,731] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_eac2859b-1b42-4bc3-9521-e5e8c6d19565 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_eac2859b-1b42-4bc3-9521-e5e8c6d19565 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_11116cc2-8d97-403a-8e72-52b11e114552 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_11116cc2-8d97-403a-8e72-52b11e114552 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:56,732] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:56,840] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:56,846] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MONEY$20Test.table
INFO  [2023-01-17 00:50:56,846] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MONEY$20Test.table
INFO  [2023-01-17 00:50:56,959] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,073] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:57,073] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:57,073] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 355 B in total
INFO  [2023-01-17 00:50:57,073] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MONEY Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000350644sINFO  [2023-01-17 00:50:57,109] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=18, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:50:57,109] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=18, nullLines=0), subType=IntegerParser(super=Parser(lines=18, nullLines=0), minValue=14835, maxValue=17612), dateReader=com.bakdata.conquery.util.DateReader@3459f840)
INFO  [2023-01-17 00:50:57,109] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with MoneyParser(super=Parser(lines=18, nullLines=3), maxValue=30000, minValue=5000, moneyFactor=100)
INFO  [2023-01-17 00:50:57,115] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MONEY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:57,115] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:57,115] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MONEY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:57,134] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_MONEY$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:57 +0000] "POST /admin/datasets/NUMBER_MONEY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_MONEY+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:57,136] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,136] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:57,136] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:57,136] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:57,138] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:50:57,139] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MONEY$20Test.table.table], containing 18 entries.
INFO  [2023-01-17 00:50:57,139] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MONEY$20Test.table.table], containing 18 entries.
INFO  [2023-01-17 00:50:57,140] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.0
INFO  [2023-01-17 00:50:57,141] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.1
WARN  [2023-01-17 00:50:57,141] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:57,141] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.2
INFO  [2023-01-17 00:50:57,141] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MONEY$20Test.table.table.3
INFO  [2023-01-17 00:50:57,246] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MONEY Test QUERY INIT
INFO  [2023-01-17 00:50:57,263] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MONEY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:57,264] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[aef785a7-c94a-42f0-97b3-de21ac4c4dbd] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MONEY Test))]]
INFO  [2023-01-17 00:50:57,268] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd
INFO  [2023-01-17 00:50:57,268] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd
127.0.0.1 - - [17/Jan/2023:00:50:57 +0000] "POST /api/datasets/NUMBER_MONEY$20Test/queries HTTP/1.1" 201 1201 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:57,270] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd] with 2 results within PT0.002302S
INFO  [2023-01-17 00:50:57,270] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd] with 3 results within PT0.002549S
INFO  [2023-01-17 00:50:57,271] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd, workerId=NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_11116cc2-8d97-403a-8e72-52b11e114552, startTime=2023-01-17T00:50:57.268177, finishTime=2023-01-17T00:50:57.270479) of size 2
INFO  [2023-01-17 00:50:57,271] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd, workerId=NUMBER_MONEY$20Test.worker_NUMBER_MONEY$20Test_eac2859b-1b42-4bc3-9521-e5e8c6d19565, startTime=2023-01-17T00:50:57.268169, finishTime=2023-01-17T00:50:57.270718) of size 3
INFO  [2023-01-17 00:50:57,271] com.bakdata.conquery.models.execution.ManagedExecution: DONE aef785a7-c94a-42f0-97b3-de21ac4c4dbd ManagedQuery within PT0.007408S
127.0.0.1 - - [17/Jan/2023:00:50:57 +0000] "GET /api/datasets/NUMBER_MONEY$20Test/queries/NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd HTTP/1.1" 200 1468 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:57,297] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MONEY Test], queryId=aef785a7-c94a-42f0-97b3-de21ac4c4dbd, label=concept	@§$, creationTime=2023-01-17T00:50:57.264050, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@15a401b4[Count = 0], startTime=2023-01-17T00:50:57.264289, finishTime=2023-01-17T00:50:57.271697, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6e161256), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MONEY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6dd23e6b, com.bakdata.conquery.models.query.ColumnDescriptor@1c27a936]) download on dataset Dataset[label=null, name=NUMBER_MONEY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:57,297] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MONEY Test], queryId=aef785a7-c94a-42f0-97b3-de21ac4c4dbd, label=concept	@§$, creationTime=2023-01-17T00:50:57.264050, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@15a401b4[Count = 0], startTime=2023-01-17T00:50:57.264289, finishTime=2023-01-17T00:50:57.271697, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6e161256), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MONEY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6dd23e6b, com.bakdata.conquery.models.query.ColumnDescriptor@1c27a936]) on dataset Dataset[label=null, name=NUMBER_MONEY Test]
127.0.0.1 - - [17/Jan/2023:00:50:57 +0000] "GET /api/datasets/NUMBER_MONEY%20Test/result/NUMBER_MONEY$20Test.aef785a7-c94a-42f0-97b3-de21ac4c4dbd.csv?pretty=false HTTP/1.1" 200 144 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:50:57,319] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_MONEY Test on 6 rows
INFO  [2023-01-17 00:50:57,319] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MONEY Test
INFO  [2023-01-17 00:50:57,319] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-17 00:50:57,319] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MONEY Test, name=NUMBER_MONEY Test]
INFO  [2023-01-17 00:50:57,320] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MONEY Test_eac2859b-1b42-4bc3-9521-e5e8c6d19565
INFO  [2023-01-17 00:50:57,320] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MONEY Test_11116cc2-8d97-403a-8e72-52b11e114552
INFO  [2023-01-17 00:50:57,331] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MONEY Test
INFO  [2023-01-17 00:50:57,332] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MONEY Test_eac2859b-1b42-4bc3-9521-e5e8c6d19565
INFO  [2023-01-17 00:50:57,332] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MONEY Test_11116cc2-8d97-403a-8e72-52b11e114552
INFO  [2023-01-17 00:50:57,341] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MONEY$20Test
INFO  [2023-01-17 00:50:57,341] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,447] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MONEY Test
INFO  [2023-01-17 00:50:57,448] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_REAL Test
INFO  [2023-01-17 00:50:57,448] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:57,448] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:57,449] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-17 00:50:57,449] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-17 00:50:57,449] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:57,449] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:57,452] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_24568f0e-1b5c-483c-941f-5147e38fac94 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:57,452] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_24568f0e-1b5c-483c-941f-5147e38fac94 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:57,452] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:57,452] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_b2ec0af8-9b8b-484e-b28a-e66aadd1ed29 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:57,452] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_b2ec0af8-9b8b-484e-b28a-e66aadd1ed29 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:57,452] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:57,456] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,559] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,559] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test[1].table
INFO  [2023-01-17 00:50:57,559] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_REAL$20Test[1].table
INFO  [2023-01-17 00:50:57,672] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,784] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:57,785] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:57,785] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 355 B in total
INFO  [2023-01-17 00:50:57,785] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_REAL Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000325695sINFO  [2023-01-17 00:50:57,818] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=18, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:50:57,818] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=18, nullLines=0), subType=IntegerParser(super=Parser(lines=18, nullLines=0), minValue=14835, maxValue=17612), dateReader=com.bakdata.conquery.util.DateReader@370e33f)
INFO  [2023-01-17 00:50:57,818] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with RealParser(super=Parser(lines=18, nullLines=3), requiredPrecision=4.9E-324, floatULP=3.0517578125E-5)
INFO  [2023-01-17 00:50:57,820] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_REAL Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:57,820] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:57,820] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_REAL Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:57,840] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_REAL$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:50:57 +0000] "POST /admin/datasets/NUMBER_REAL%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_REAL+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:50:57,841] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:57,843] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:57,843] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:57,843] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:57,846] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:50:57,846] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test[1].table.table], containing 18 entries.
INFO  [2023-01-17 00:50:57,846] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_REAL$20Test[1].table.table], containing 18 entries.
INFO  [2023-01-17 00:50:57,848] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.0
WARN  [2023-01-17 00:50:57,848] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:57,848] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.1
INFO  [2023-01-17 00:50:57,848] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.2
INFO  [2023-01-17 00:50:57,848] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_REAL$20Test[1].table.table.3
INFO  [2023-01-17 00:50:57,954] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_REAL Test QUERY INIT
INFO  [2023-01-17 00:50:57,970] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_REAL$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:57,971] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[117214ee-f525-4313-a2aa-cbb10e748c1a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test[1]))]]
INFO  [2023-01-17 00:50:57,974] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test[1].117214ee-f525-4313-a2aa-cbb10e748c1a
INFO  [2023-01-17 00:50:57,974] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_REAL$20Test[1].117214ee-f525-4313-a2aa-cbb10e748c1a
127.0.0.1 - - [17/Jan/2023:00:50:57 +0000] "POST /api/datasets/NUMBER_REAL$20Test%5B1%5D/queries HTTP/1.1" 201 1203 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:50:57,987] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test[1].117214ee-f525-4313-a2aa-cbb10e748c1a] with 2 results within PT0.012911S
INFO  [2023-01-17 00:50:57,987] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_REAL$20Test[1].117214ee-f525-4313-a2aa-cbb10e748c1a] with 3 results within PT0.012794S
INFO  [2023-01-17 00:50:57,988] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test[1].117214ee-f525-4313-a2aa-cbb10e748c1a, workerId=NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_24568f0e-1b5c-483c-941f-5147e38fac94, startTime=2023-01-17T00:50:57.974620, finishTime=2023-01-17T00:50:57.987531) of size 2
INFO  [2023-01-17 00:50:57,988] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_REAL$20Test[1].117214ee-f525-4313-a2aa-cbb10e748c1a, workerId=NUMBER_REAL$20Test[1].worker_NUMBER_REAL$20Test[1]_b2ec0af8-9b8b-484e-b28a-e66aadd1ed29, startTime=2023-01-17T00:50:57.974792, finishTime=2023-01-17T00:50:57.987586) of size 3
INFO  [2023-01-17 00:50:57,988] com.bakdata.conquery.models.execution.ManagedExecution: DONE 117214ee-f525-4313-a2aa-cbb10e748c1a ManagedQuery within PT0.017575S
127.0.0.1 - - [17/Jan/2023:00:50:58 +0000] "GET /api/datasets/NUMBER_REAL$20Test%5B1%5D/queries/NUMBER_REAL$20Test%5B1%5D.117214ee-f525-4313-a2aa-cbb10e748c1a HTTP/1.1" 200 1715 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:58,012] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test[1]], queryId=117214ee-f525-4313-a2aa-cbb10e748c1a, label=concept	@§$, creationTime=2023-01-17T00:50:57.970921, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@41842cc3[Count = 0], startTime=2023-01-17T00:50:57.971112, finishTime=2023-01-17T00:50:57.988687, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@15b3998b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1e72489e, com.bakdata.conquery.models.query.ColumnDescriptor@553b85f5]) download on dataset Dataset[label=null, name=NUMBER_REAL Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:58,013] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_REAL Test[1]], queryId=117214ee-f525-4313-a2aa-cbb10e748c1a, label=concept	@§$, creationTime=2023-01-17T00:50:57.970921, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@41842cc3[Count = 0], startTime=2023-01-17T00:50:57.971112, finishTime=2023-01-17T00:50:57.988687, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@15b3998b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_REAL Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1e72489e, com.bakdata.conquery.models.query.ColumnDescriptor@553b85f5]) on dataset Dataset[label=null, name=NUMBER_REAL Test[1]]
127.0.0.1 - - [17/Jan/2023:00:50:58 +0000] "GET /api/datasets/NUMBER_REAL%20Test%5B1%5D/result/NUMBER_REAL$20Test%5B1%5D.117214ee-f525-4313-a2aa-cbb10e748c1a.csv?pretty=false HTTP/1.1" 200 144 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:50:58,032] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_REAL Test on 6 rows
INFO  [2023-01-17 00:50:58,032] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_REAL Test[1]
INFO  [2023-01-17 00:50:58,032] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-17 00:50:58,032] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_REAL Test[1], name=NUMBER_REAL Test[1]]
INFO  [2023-01-17 00:50:58,032] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test[1]_b2ec0af8-9b8b-484e-b28a-e66aadd1ed29
INFO  [2023-01-17 00:50:58,033] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_REAL Test[1]_24568f0e-1b5c-483c-941f-5147e38fac94
INFO  [2023-01-17 00:50:58,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_REAL Test[1]
INFO  [2023-01-17 00:50:58,050] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test[1]_24568f0e-1b5c-483c-941f-5147e38fac94
INFO  [2023-01-17 00:50:58,051] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_REAL Test[1]_b2ec0af8-9b8b-484e-b28a-e66aadd1ed29
INFO  [2023-01-17 00:50:58,148] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_REAL$20Test[1]
INFO  [2023-01-17 00:50:58,148] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:58,255] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_REAL Test
INFO  [2023-01-17 00:50:58,256] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING FILTER Test
INFO  [2023-01-17 00:50:58,256] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:58,256] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:58,257] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-17 00:50:58,257] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:58,257] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-17 00:50:58,257] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:58,259] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_c4f20ba5-ffcf-45ab-99be-934eba8f065e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:58,259] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_c4f20ba5-ffcf-45ab-99be-934eba8f065e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:58,259] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:58,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_b04470cd-6b76-4607-bda4-8c1b9d6ca5eb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:58,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_b04470cd-6b76-4607-bda4-8c1b9d6ca5eb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:58,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:58,263] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:58,367] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:58,367] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING$20FILTER$20Test.table
INFO  [2023-01-17 00:50:58,367] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING$20FILTER$20Test.table
INFO  [2023-01-17 00:50:58,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:58,597] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:58,597] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:58,597] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 67 B in total
INFO  [2023-01-17 00:50:58,598] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING FILTER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00041637sINFO  [2023-01-17 00:50:58,640] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:50:58,640] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=2, nullLines=0), minParser=DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=16511, maxValue=17292), dateReader=com.bakdata.conquery.util.DateReader@230e4544), maxParser=DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=16525, maxValue=17301), dateReader=com.bakdata.conquery.util.DateReader@34162bf2), dateReader=com.bakdata.conquery.util.DateReader@48f0441c, onlyQuarters=false, maxValue=17301, minValue=16511, anyOpen=false)
INFO  [2023-01-17 00:50:58,640] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[nr] with RealParser(super=Parser(lines=2, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-17 00:50:58,643] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING FILTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:58,643] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:58,643] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING FILTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:58,662] com.bakdata.conquery.models.jobs.ImportJob: Importing table into NUMBER_MISSING$20FILTER$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:58 +0000] "POST /admin/datasets/NUMBER_MISSING%20FILTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_MISSING+FILTER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:50:58,664] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:58,665] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:58,665] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:58,665] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:58,667] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING$20FILTER$20Test.table.table], containing 2 entries.
INFO  [2023-01-17 00:50:58,667] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:50:58,668] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING$20FILTER$20Test.table.table], containing 2 entries.
WARN  [2023-01-17 00:50:58,670] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:58,670] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING$20FILTER$20Test.table.table.0
INFO  [2023-01-17 00:50:58,775] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING FILTER Test QUERY INIT
INFO  [2023-01-17 00:50:58,787] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING$20FILTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:58,787] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ed4d611c-8e44-437b-bcbf-557599e15f31] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING FILTER Test))]]
INFO  [2023-01-17 00:50:58,791] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31
INFO  [2023-01-17 00:50:58,791] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31
WARN  [2023-01-17 00:50:58,791] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:50:58,791] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31] with 0 results within PT0.000155S
INFO  [2023-01-17 00:50:58,792] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31] with 1 results within PT0.000846S
INFO  [2023-01-17 00:50:58,792] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31, workerId=NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_b04470cd-6b76-4607-bda4-8c1b9d6ca5eb, startTime=2023-01-17T00:50:58.791766, finishTime=2023-01-17T00:50:58.791921) of size 0
127.0.0.1 - - [17/Jan/2023:00:50:58 +0000] "POST /api/datasets/NUMBER_MISSING$20FILTER$20Test/queries HTTP/1.1" 201 1237 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:50:58,793] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31, workerId=NUMBER_MISSING$20FILTER$20Test.worker_NUMBER_MISSING$20FILTER$20Test_c4f20ba5-ffcf-45ab-99be-934eba8f065e, startTime=2023-01-17T00:50:58.791732, finishTime=2023-01-17T00:50:58.792578) of size 1
INFO  [2023-01-17 00:50:58,793] com.bakdata.conquery.models.execution.ManagedExecution: DONE ed4d611c-8e44-437b-bcbf-557599e15f31 ManagedQuery within PT0.005871S
127.0.0.1 - - [17/Jan/2023:00:50:58 +0000] "GET /api/datasets/NUMBER_MISSING$20FILTER$20Test/queries/NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31 HTTP/1.1" 200 1548 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:50:58,837] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING FILTER Test], queryId=ed4d611c-8e44-437b-bcbf-557599e15f31, label=concept	@§$, creationTime=2023-01-17T00:50:58.787358, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@652860d1[Count = 0], startTime=2023-01-17T00:50:58.787587, finishTime=2023-01-17T00:50:58.793458, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@451d5f1d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING FILTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2fc9dc77, com.bakdata.conquery.models.query.ColumnDescriptor@5ff15c1]) download on dataset Dataset[label=null, name=NUMBER_MISSING FILTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:58,837] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING FILTER Test], queryId=ed4d611c-8e44-437b-bcbf-557599e15f31, label=concept	@§$, creationTime=2023-01-17T00:50:58.787358, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@652860d1[Count = 0], startTime=2023-01-17T00:50:58.787587, finishTime=2023-01-17T00:50:58.793458, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@451d5f1d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING FILTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2fc9dc77, com.bakdata.conquery.models.query.ColumnDescriptor@5ff15c1]) on dataset Dataset[label=null, name=NUMBER_MISSING FILTER Test]
127.0.0.1 - - [17/Jan/2023:00:50:58 +0000] "GET /api/datasets/NUMBER_MISSING%20FILTER%20Test/result/NUMBER_MISSING$20FILTER$20Test.ed4d611c-8e44-437b-bcbf-557599e15f31.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:50:58,860] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest NUMBER_MISSING FILTER Test on 2 rows
INFO  [2023-01-17 00:50:58,860] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING FILTER Test
INFO  [2023-01-17 00:50:58,860] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-17 00:50:58,860] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING FILTER Test, name=NUMBER_MISSING FILTER Test]
INFO  [2023-01-17 00:50:58,860] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING FILTER Test_b04470cd-6b76-4607-bda4-8c1b9d6ca5eb
INFO  [2023-01-17 00:50:58,860] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING FILTER Test_c4f20ba5-ffcf-45ab-99be-934eba8f065e
INFO  [2023-01-17 00:50:58,958] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING FILTER Test
INFO  [2023-01-17 00:50:58,958] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING FILTER Test_c4f20ba5-ffcf-45ab-99be-934eba8f065e
INFO  [2023-01-17 00:50:58,958] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING FILTER Test_b04470cd-6b76-4607-bda4-8c1b9d6ca5eb
INFO  [2023-01-17 00:50:58,970] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING$20FILTER$20Test
INFO  [2023-01-17 00:50:58,970] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:59,076] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING FILTER Test
INFO  [2023-01-17 00:50:59,076] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Prefix Test
INFO  [2023-01-17 00:50:59,076] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:59,076] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:59,078] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-17 00:50:59,078] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-17 00:50:59,078] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:59,078] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:59,080] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Prefix$20Test.worker_Prefix$20Test_01bc2043-dea3-4cdd-89e5-03c3cccb8707 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:59,080] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Prefix$20Test.worker_Prefix$20Test_01bc2043-dea3-4cdd-89e5-03c3cccb8707 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:59,080] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:59,080] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Prefix$20Test.worker_Prefix$20Test_a866c86e-90d0-4b88-bb52-8bd623e0da5f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:59,080] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Prefix$20Test.worker_Prefix$20Test_a866c86e-90d0-4b88-bb52-8bd623e0da5f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:59,080] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:59,084] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:59,187] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:59,188] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Prefix$20Test.table
INFO  [2023-01-17 00:50:59,188] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Prefix$20Test.table
INFO  [2023-01-17 00:50:59,304] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:59,430] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:50:59,431] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:50:59,431] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 109 B in total
INFO  [2023-01-17 00:50:59,431] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Prefix Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000398764sINFO  [2023-01-17 00:50:59,471] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=6, min=1, average=1.200000, max=2}
INFO  [2023-01-17 00:50:59,471] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1bb16f2a)
INFO  [2023-01-17 00:50:59,471] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:50:59,474] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Prefix Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:59,474] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:50:59,474] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Prefix Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:50:59,489] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Prefix$20Test.table
127.0.0.1 - - [17/Jan/2023:00:50:59 +0000] "POST /admin/datasets/Prefix%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Prefix+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:59,492] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:59,492] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:50:59,493] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:50:59,493] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:50:59,496] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:50:59,496] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Prefix$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:50:59,497] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Prefix$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:50:59,498] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:50:59,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Prefix$20Test.table.table.0
INFO  [2023-01-17 00:50:59,499] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Prefix$20Test.table.table.1
INFO  [2023-01-17 00:50:59,604] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Prefix Test QUERY INIT
INFO  [2023-01-17 00:50:59,638] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Prefix$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:50:59,638] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f9151cd7-8c73-4773-8e8a-dd4e16214a4b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Prefix Test))]]
INFO  [2023-01-17 00:50:59,642] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b
127.0.0.1 - - [17/Jan/2023:00:50:59 +0000] "POST /api/datasets/Prefix$20Test/queries HTTP/1.1" 201 1149 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:50:59,642] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b
INFO  [2023-01-17 00:50:59,643] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b] with 1 results within PT0.000947S
INFO  [2023-01-17 00:50:59,643] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b] with 2 results within PT0.000921S
INFO  [2023-01-17 00:50:59,644] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b, workerId=Prefix$20Test.worker_Prefix$20Test_01bc2043-dea3-4cdd-89e5-03c3cccb8707, startTime=2023-01-17T00:50:59.642222, finishTime=2023-01-17T00:50:59.643169) of size 1
INFO  [2023-01-17 00:50:59,644] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b, workerId=Prefix$20Test.worker_Prefix$20Test_a866c86e-90d0-4b88-bb52-8bd623e0da5f, startTime=2023-01-17T00:50:59.642514, finishTime=2023-01-17T00:50:59.643435) of size 2
INFO  [2023-01-17 00:50:59,644] com.bakdata.conquery.models.execution.ManagedExecution: DONE f9151cd7-8c73-4773-8e8a-dd4e16214a4b ManagedQuery within PT0.005605S
127.0.0.1 - - [17/Jan/2023:00:50:59 +0000] "GET /api/datasets/Prefix$20Test/queries/Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b HTTP/1.1" 200 1391 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:50:59,673] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Prefix Test], queryId=f9151cd7-8c73-4773-8e8a-dd4e16214a4b, label=concept	@§$, creationTime=2023-01-17T00:50:59.638362, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b41839f[Count = 0], startTime=2023-01-17T00:50:59.638685, finishTime=2023-01-17T00:50:59.644290, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@d0da6f9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Prefix Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2a97681b, com.bakdata.conquery.models.query.ColumnDescriptor@52991299]) download on dataset Dataset[label=null, name=Prefix Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:50:59,673] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Prefix Test], queryId=f9151cd7-8c73-4773-8e8a-dd4e16214a4b, label=concept	@§$, creationTime=2023-01-17T00:50:59.638362, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1b41839f[Count = 0], startTime=2023-01-17T00:50:59.638685, finishTime=2023-01-17T00:50:59.644290, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@d0da6f9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Prefix Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2a97681b, com.bakdata.conquery.models.query.ColumnDescriptor@52991299]) on dataset Dataset[label=null, name=Prefix Test]
127.0.0.1 - - [17/Jan/2023:00:50:59 +0000] "GET /api/datasets/Prefix%20Test/result/Prefix$20Test.f9151cd7-8c73-4773-8e8a-dd4e16214a4b.csv?pretty=false HTTP/1.1" 200 91 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:50:59,690] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest Prefix Test on 4 rows
INFO  [2023-01-17 00:50:59,691] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Prefix Test
INFO  [2023-01-17 00:50:59,691] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-17 00:50:59,691] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Prefix Test, name=Prefix Test]
INFO  [2023-01-17 00:50:59,691] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Prefix Test_01bc2043-dea3-4cdd-89e5-03c3cccb8707
INFO  [2023-01-17 00:50:59,691] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Prefix Test_a866c86e-90d0-4b88-bb52-8bd623e0da5f
INFO  [2023-01-17 00:50:59,788] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Prefix Test_a866c86e-90d0-4b88-bb52-8bd623e0da5f
INFO  [2023-01-17 00:50:59,788] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Prefix Test_01bc2043-dea3-4cdd-89e5-03c3cccb8707
INFO  [2023-01-17 00:50:59,789] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Prefix Test
INFO  [2023-01-17 00:50:59,798] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Prefix$20Test
INFO  [2023-01-17 00:50:59,799] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:50:59,904] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Prefix Test
INFO  [2023-01-17 00:50:59,905] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test QUARTERS_IN_YEAR Test
INFO  [2023-01-17 00:50:59,905] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:50:59,905] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:50:59,906] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-17 00:50:59,906] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-17 00:50:59,906] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:59,906] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:50:59,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_ee6ca3f4-b154-4187-893c-81ff55548f5e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:59,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_ee6ca3f4-b154-4187-893c-81ff55548f5e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:59,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:59,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_eb437c80-ede4-42c0-afba-410a9163f5e9 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:50:59,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_eb437c80-ede4-42c0-afba-410a9163f5e9 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:50:59,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:50:59,912] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,016] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,017] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTERS_IN_YEAR$20Test.table
INFO  [2023-01-17 00:51:00,017] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTERS_IN_YEAR$20Test.table
INFO  [2023-01-17 00:51:00,137] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,253] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:00,253] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:00,253] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.4 KiB in total
INFO  [2023-01-17 00:51:00,253] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_QUARTERS_IN_YEAR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000413669sINFO  [2023-01-17 00:51:00,296] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=12, sum=38, min=1, average=3.166667, max=6}
INFO  [2023-01-17 00:51:00,296] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=38, nullLines=0), minParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=16801), dateReader=com.bakdata.conquery.util.DateReader@18348b26), maxParser=DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16800, maxValue=17166), dateReader=com.bakdata.conquery.util.DateReader@23133fd2), dateReader=com.bakdata.conquery.util.DateReader@2efaf6ff, onlyQuarters=false, maxValue=17166, minValue=16436, anyOpen=false)
INFO  [2023-01-17 00:51:00,296] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateParser(super=Parser(lines=38, nullLines=0), subType=IntegerParser(super=Parser(lines=38, nullLines=0), minValue=16436, maxValue=17117), dateReader=com.bakdata.conquery.util.DateReader@1604d42b)
INFO  [2023-01-17 00:51:00,299] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_QUARTERS_IN_YEAR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:00,299] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:00,299] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_QUARTERS_IN_YEAR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:00,318] com.bakdata.conquery.models.jobs.ImportJob: Importing table into QUARTERS_IN_YEAR$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:00 +0000] "POST /admin/datasets/QUARTERS_IN_YEAR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_QUARTERS_IN_YEAR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:00,323] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,323] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:00,323] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:00,323] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:00,326] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:51:00,326] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTERS_IN_YEAR$20Test.table.table], containing 38 entries.
INFO  [2023-01-17 00:51:00,326] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTERS_IN_YEAR$20Test.table.table], containing 38 entries.
INFO  [2023-01-17 00:51:00,328] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.0
INFO  [2023-01-17 00:51:00,328] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.1
WARN  [2023-01-17 00:51:00,328] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:00,328] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.2
INFO  [2023-01-17 00:51:00,329] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTERS_IN_YEAR$20Test.table.table.3
INFO  [2023-01-17 00:51:00,433] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: QUARTERS_IN_YEAR Test QUERY INIT
INFO  [2023-01-17 00:51:00,444] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[QUARTERS_IN_YEAR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:00,445] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_QUARTERS_IN_YEAR Test))]]
INFO  [2023-01-17 00:51:00,449] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6
INFO  [2023-01-17 00:51:00,449] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6
127.0.0.1 - - [17/Jan/2023:00:51:00 +0000] "POST /api/datasets/QUARTERS_IN_YEAR$20Test/queries HTTP/1.1" 201 1210 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:00,452] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6] with 3 results within PT0.002949S
INFO  [2023-01-17 00:51:00,452] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6] with 4 results within PT0.002872S
INFO  [2023-01-17 00:51:00,452] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6, workerId=QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_eb437c80-ede4-42c0-afba-410a9163f5e9, startTime=2023-01-17T00:51:00.449047, finishTime=2023-01-17T00:51:00.451996) of size 3
INFO  [2023-01-17 00:51:00,452] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6, workerId=QUARTERS_IN_YEAR$20Test.worker_QUARTERS_IN_YEAR$20Test_ee6ca3f4-b154-4187-893c-81ff55548f5e, startTime=2023-01-17T00:51:00.449135, finishTime=2023-01-17T00:51:00.452007) of size 4
INFO  [2023-01-17 00:51:00,453] com.bakdata.conquery.models.execution.ManagedExecution: DONE 95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6 ManagedQuery within PT0.007878S
127.0.0.1 - - [17/Jan/2023:00:51:00 +0000] "GET /api/datasets/QUARTERS_IN_YEAR$20Test/queries/QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6 HTTP/1.1" 200 1492 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:51:00,490] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTERS_IN_YEAR Test], queryId=95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6, label=concept	@§$, creationTime=2023-01-17T00:51:00.444898, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7265bfd0[Count = 0], startTime=2023-01-17T00:51:00.445182, finishTime=2023-01-17T00:51:00.453060, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1d93ea65), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTERS_IN_YEAR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@43e6fdae, com.bakdata.conquery.models.query.ColumnDescriptor@542bce5a]) download on dataset Dataset[label=null, name=QUARTERS_IN_YEAR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:00,490] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTERS_IN_YEAR Test], queryId=95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6, label=concept	@§$, creationTime=2023-01-17T00:51:00.444898, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7265bfd0[Count = 0], startTime=2023-01-17T00:51:00.445182, finishTime=2023-01-17T00:51:00.453060, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1d93ea65), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTERS_IN_YEAR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@43e6fdae, com.bakdata.conquery.models.query.ColumnDescriptor@542bce5a]) on dataset Dataset[label=null, name=QUARTERS_IN_YEAR Test]
127.0.0.1 - - [17/Jan/2023:00:51:00 +0000] "GET /api/datasets/QUARTERS_IN_YEAR%20Test/result/QUARTERS_IN_YEAR$20Test.95bc37e8-c9e3-4cdc-8d0a-105c701ef9e6.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:00,510] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest QUARTERS_IN_YEAR Test on 8 rows
INFO  [2023-01-17 00:51:00,511] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast QUARTERS_IN_YEAR Test
INFO  [2023-01-17 00:51:00,511] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-17 00:51:00,511] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTERS_IN_YEAR Test, name=QUARTERS_IN_YEAR Test]
INFO  [2023-01-17 00:51:00,511] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTERS_IN_YEAR Test_eb437c80-ede4-42c0-afba-410a9163f5e9
INFO  [2023-01-17 00:51:00,512] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTERS_IN_YEAR Test_ee6ca3f4-b154-4187-893c-81ff55548f5e
INFO  [2023-01-17 00:51:00,608] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTERS_IN_YEAR Test_eb437c80-ede4-42c0-afba-410a9163f5e9
INFO  [2023-01-17 00:51:00,609] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTERS_IN_YEAR Test_ee6ca3f4-b154-4187-893c-81ff55548f5e
INFO  [2023-01-17 00:51:00,609] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow QUARTERS_IN_YEAR Test
INFO  [2023-01-17 00:51:00,629] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of QUARTERS_IN_YEAR$20Test
INFO  [2023-01-17 00:51:00,629] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,741] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test QUARTERS_IN_YEAR Test
INFO  [2023-01-17 00:51:00,741] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SELECT Test
INFO  [2023-01-17 00:51:00,741] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:00,741] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:00,742] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-17 00:51:00,743] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:00,743] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-17 00:51:00,743] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:00,745] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT$20Test.worker_SELECT$20Test_d144c8bc-4a98-486d-93a8-9be980eb7ba5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:00,745] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT$20Test.worker_SELECT$20Test_d144c8bc-4a98-486d-93a8-9be980eb7ba5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:00,745] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:00,745] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT$20Test.worker_SELECT$20Test_3e6b937c-d930-40ac-b1a6-c2135791e50c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:00,745] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT$20Test.worker_SELECT$20Test_3e6b937c-d930-40ac-b1a6-c2135791e50c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:00,745] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:00,749] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,852] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:00,852] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT$20Test.table
INFO  [2023-01-17 00:51:00,852] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT$20Test.table
INFO  [2023-01-17 00:51:00,976] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,093] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:01,093] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:01,093] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 76 B in total
INFO  [2023-01-17 00:51:01,093] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000326188sINFO  [2023-01-17 00:51:01,126] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:01,126] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@6316c3be)
INFO  [2023-01-17 00:51:01,126] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:01,129] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:01,129] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:01,129] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:01,145] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SELECT$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:01 +0000] "POST /admin/datasets/SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:01,146] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:01,147] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,147] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:01,147] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:01,148] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:01,149] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT$20Test.table.table], containing 4 entries.
INFO  [2023-01-17 00:51:01,149] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT$20Test.table.table], containing 4 entries.
WARN  [2023-01-17 00:51:01,150] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:01,150] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT$20Test.table.table.0
INFO  [2023-01-17 00:51:01,150] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT$20Test.table.table.1
INFO  [2023-01-17 00:51:01,255] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SELECT Test QUERY INIT
INFO  [2023-01-17 00:51:01,267] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SELECT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:01,268] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SELECT Test))]]
INFO  [2023-01-17 00:51:01,270] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4
127.0.0.1 - - [17/Jan/2023:00:51:01 +0000] "POST /api/datasets/SELECT$20Test/queries HTTP/1.1" 201 1149 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:51:01,270] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4
INFO  [2023-01-17 00:51:01,271] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4] with 2 results within PT0.001007S
INFO  [2023-01-17 00:51:01,271] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4] with 0 results within PT0.000431S
INFO  [2023-01-17 00:51:01,272] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4, workerId=SELECT$20Test.worker_SELECT$20Test_d144c8bc-4a98-486d-93a8-9be980eb7ba5, startTime=2023-01-17T00:51:01.270262, finishTime=2023-01-17T00:51:01.271269) of size 2
INFO  [2023-01-17 00:51:01,272] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4, workerId=SELECT$20Test.worker_SELECT$20Test_3e6b937c-d930-40ac-b1a6-c2135791e50c, startTime=2023-01-17T00:51:01.270995, finishTime=2023-01-17T00:51:01.271426) of size 0
INFO  [2023-01-17 00:51:01,272] com.bakdata.conquery.models.execution.ManagedExecution: DONE 19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4 ManagedQuery within PT0.004194S
127.0.0.1 - - [17/Jan/2023:00:51:01 +0000] "GET /api/datasets/SELECT$20Test/queries/SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4 HTTP/1.1" 200 1392 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:01,289] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT Test], queryId=19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4, label=concept	@§$, creationTime=2023-01-17T00:51:01.267927, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@ebf2417[Count = 0], startTime=2023-01-17T00:51:01.268065, finishTime=2023-01-17T00:51:01.272259, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@fe0a704), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@559b27db, com.bakdata.conquery.models.query.ColumnDescriptor@7ea2efae]) download on dataset Dataset[label=null, name=SELECT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:01,289] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT Test], queryId=19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4, label=concept	@§$, creationTime=2023-01-17T00:51:01.267927, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@ebf2417[Count = 0], startTime=2023-01-17T00:51:01.268065, finishTime=2023-01-17T00:51:01.272259, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@fe0a704), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@559b27db, com.bakdata.conquery.models.query.ColumnDescriptor@7ea2efae]) on dataset Dataset[label=null, name=SELECT Test]
127.0.0.1 - - [17/Jan/2023:00:51:01 +0000] "GET /api/datasets/SELECT%20Test/result/SELECT$20Test.19a6a5ac-a8f5-4c71-a091-8dca4fd5aef4.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:01,308] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SELECT Test on 3 rows
INFO  [2023-01-17 00:51:01,308] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SELECT Test
INFO  [2023-01-17 00:51:01,309] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-17 00:51:01,309] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT Test, name=SELECT Test]
INFO  [2023-01-17 00:51:01,309] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT Test_3e6b937c-d930-40ac-b1a6-c2135791e50c
INFO  [2023-01-17 00:51:01,309] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT Test_d144c8bc-4a98-486d-93a8-9be980eb7ba5
INFO  [2023-01-17 00:51:01,343] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SELECT Test
INFO  [2023-01-17 00:51:01,344] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT Test_d144c8bc-4a98-486d-93a8-9be980eb7ba5
INFO  [2023-01-17 00:51:01,344] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT Test_3e6b937c-d930-40ac-b1a6-c2135791e50c
INFO  [2023-01-17 00:51:01,350] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SELECT$20Test
INFO  [2023-01-17 00:51:01,350] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,455] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SELECT Test
INFO  [2023-01-17 00:51:01,456] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:51:01,456] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:01,456] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:01,457] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:51:01,457] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:01,457] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:51:01,457] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:01,458] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,459] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_ecb4dd52-7c64-4588-aff6-3075e9f01373 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:01,459] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_ecb4dd52-7c64-4588-aff6-3075e9f01373 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:01,459] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:01,459] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_3af1ef3d-635d-4e5e-839e-f9012921b398 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:01,459] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_3af1ef3d-635d-4e5e-839e-f9012921b398 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:01,459] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:01,566] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,567] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-17 00:51:01,567] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_EMPTY_VALUES$20Test.table
INFO  [2023-01-17 00:51:01,682] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,797] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:01,797] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:01,797] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 90 B in total
INFO  [2023-01-17 00:51:01,797] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000293802sINFO  [2023-01-17 00:51:01,827] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:51:01,827] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@14a593b4)
INFO  [2023-01-17 00:51:01,827] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:01,830] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:01,830] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:01,830] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_EMPTY_VALUES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:01,851] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_EMPTY_VALUES$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:01 +0000] "POST /admin/datasets/SINGLE_SELECT_EMPTY_VALUES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SINGLE_SELECT_EMPTY_VALUES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:01,852] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:01,853] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:01,854] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:01,854] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:01,856] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:01,857] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:51:01,857] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_EMPTY_VALUES$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:51:01,858] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:01,858] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_EMPTY_VALUES$20Test.table.table.0
INFO  [2023-01-17 00:51:01,859] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_EMPTY_VALUES$20Test.table.table.1
INFO  [2023-01-17 00:51:01,964] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_EMPTY_VALUES Test QUERY INIT
INFO  [2023-01-17 00:51:01,980] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_EMPTY_VALUES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:01,981] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2e583d42-a33c-4995-a023-c2650764ebc9] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_EMPTY_VALUES Test))]]
INFO  [2023-01-17 00:51:01,984] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9
INFO  [2023-01-17 00:51:01,984] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9
INFO  [2023-01-17 00:51:01,985] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9] with 0 results within PT0.000913S
INFO  [2023-01-17 00:51:01,986] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9] with 2 results within PT0.001226S
127.0.0.1 - - [17/Jan/2023:00:51:01 +0000] "POST /api/datasets/SINGLE_SELECT_EMPTY_VALUES$20Test/queries HTTP/1.1" 201 1229 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:01,986] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9, workerId=SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_ecb4dd52-7c64-4588-aff6-3075e9f01373, startTime=2023-01-17T00:51:01.984792, finishTime=2023-01-17T00:51:01.985705) of size 0
INFO  [2023-01-17 00:51:01,986] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9, workerId=SINGLE_SELECT_EMPTY_VALUES$20Test.worker_SINGLE_SELECT_EMPTY_VALUES$20Test_3af1ef3d-635d-4e5e-839e-f9012921b398, startTime=2023-01-17T00:51:01.984803, finishTime=2023-01-17T00:51:01.986029) of size 2
INFO  [2023-01-17 00:51:01,987] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2e583d42-a33c-4995-a023-c2650764ebc9 ManagedQuery within PT0.005809S
127.0.0.1 - - [17/Jan/2023:00:51:02 +0000] "GET /api/datasets/SINGLE_SELECT_EMPTY_VALUES$20Test/queries/SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9 HTTP/1.1" 200 1552 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:02,017] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test], queryId=2e583d42-a33c-4995-a023-c2650764ebc9, label=concept	@§$, creationTime=2023-01-17T00:51:01.980925, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@320fa31b[Count = 0], startTime=2023-01-17T00:51:01.981177, finishTime=2023-01-17T00:51:01.986986, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@72f0abcf), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e984588, com.bakdata.conquery.models.query.ColumnDescriptor@793e74c8]) download on dataset Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:02,018] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test], queryId=2e583d42-a33c-4995-a023-c2650764ebc9, label=concept	@§$, creationTime=2023-01-17T00:51:01.980925, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@320fa31b[Count = 0], startTime=2023-01-17T00:51:01.981177, finishTime=2023-01-17T00:51:01.986986, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@72f0abcf), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_EMPTY_VALUES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e984588, com.bakdata.conquery.models.query.ColumnDescriptor@793e74c8]) on dataset Dataset[label=null, name=SINGLE_SELECT_EMPTY_VALUES Test]
127.0.0.1 - - [17/Jan/2023:00:51:02 +0000] "GET /api/datasets/SINGLE_SELECT_EMPTY_VALUES%20Test/result/SINGLE_SELECT_EMPTY_VALUES$20Test.2e583d42-a33c-4995-a023-c2650764ebc9.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:02,038] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_EMPTY_VALUES Test on 3 rows
INFO  [2023-01-17 00:51:02,039] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:51:02,039] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:51:02,039] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_EMPTY_VALUES Test, name=SINGLE_SELECT_EMPTY_VALUES Test]
INFO  [2023-01-17 00:51:02,039] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_EMPTY_VALUES Test_ecb4dd52-7c64-4588-aff6-3075e9f01373
INFO  [2023-01-17 00:51:02,039] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_EMPTY_VALUES Test_3af1ef3d-635d-4e5e-839e-f9012921b398
INFO  [2023-01-17 00:51:02,057] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:51:02,058] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_EMPTY_VALUES Test_ecb4dd52-7c64-4588-aff6-3075e9f01373
INFO  [2023-01-17 00:51:02,058] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_EMPTY_VALUES Test_3af1ef3d-635d-4e5e-839e-f9012921b398
INFO  [2023-01-17 00:51:02,059] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_EMPTY_VALUES$20Test
INFO  [2023-01-17 00:51:02,059] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,164] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_EMPTY_VALUES Test
INFO  [2023-01-17 00:51:02,164] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:51:02,164] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:02,164] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:02,165] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:51:02,165] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:51:02,165] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:02,165] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:02,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_d0d5f775-cd55-42a9-baf7-a9a8d4649a3d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:02,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_d0d5f775-cd55-42a9-baf7-a9a8d4649a3d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:02,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:02,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_dc3496b9-5491-45e8-b76c-42505cc63373 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:02,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_dc3496b9-5491-45e8-b76c-42505cc63373 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:02,167] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:02,171] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,274] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,275] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-17 00:51:02,275] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table
INFO  [2023-01-17 00:51:02,390] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,503] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:02,503] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:02,503] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 92 B in total
INFO  [2023-01-17 00:51:02,504] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000335043sINFO  [2023-01-17 00:51:02,538] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:51:02,538] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@858ea4d)
INFO  [2023-01-17 00:51:02,538] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:02,541] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:02,541] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:02,541] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:02,566] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:02 +0000] "POST /admin/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SINGLE_SELECT_SPEZIAL_CHARACTER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:02,568] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,568] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:02,569] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:02,569] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:02,572] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:02,572] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:51:02,572] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:51:02,574] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:02,574] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table.0
INFO  [2023-01-17 00:51:02,574] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.table.table.1
INFO  [2023-01-17 00:51:02,679] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_SPEZIAL_CHARACTER Test QUERY INIT
INFO  [2023-01-17 00:51:02,694] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:02,694] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[eb359af9-3563-48b6-b010-14b0702bd7f1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER Test))]]
INFO  [2023-01-17 00:51:02,698] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1
INFO  [2023-01-17 00:51:02,698] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1
INFO  [2023-01-17 00:51:02,698] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1] with 0 results within PT0.000679S
INFO  [2023-01-17 00:51:02,699] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1] with 1 results within PT0.001161S
INFO  [2023-01-17 00:51:02,699] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_dc3496b9-5491-45e8-b76c-42505cc63373, startTime=2023-01-17T00:51:02.698182, finishTime=2023-01-17T00:51:02.698861) of size 0
127.0.0.1 - - [17/Jan/2023:00:51:02 +0000] "POST /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test/queries HTTP/1.1" 201 1249 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:02,700] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER$20Test_d0d5f775-cd55-42a9-baf7-a9a8d4649a3d, startTime=2023-01-17T00:51:02.698055, finishTime=2023-01-17T00:51:02.699216) of size 1
INFO  [2023-01-17 00:51:02,700] com.bakdata.conquery.models.execution.ManagedExecution: DONE eb359af9-3563-48b6-b010-14b0702bd7f1 ManagedQuery within PT0.005584S
127.0.0.1 - - [17/Jan/2023:00:51:02 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test/queries/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1 HTTP/1.1" 200 1592 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:02,736] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test], queryId=eb359af9-3563-48b6-b010-14b0702bd7f1, label=concept	@§$, creationTime=2023-01-17T00:51:02.694371, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2a73aedd[Count = 0], startTime=2023-01-17T00:51:02.694617, finishTime=2023-01-17T00:51:02.700201, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5e291fae), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@25b8cf2e, com.bakdata.conquery.models.query.ColumnDescriptor@48ad437c]) download on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:02,736] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test], queryId=eb359af9-3563-48b6-b010-14b0702bd7f1, label=concept	@§$, creationTime=2023-01-17T00:51:02.694371, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2a73aedd[Count = 0], startTime=2023-01-17T00:51:02.694617, finishTime=2023-01-17T00:51:02.700201, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5e291fae), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@25b8cf2e, com.bakdata.conquery.models.query.ColumnDescriptor@48ad437c]) on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:02 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER%20Test/result/SINGLE_SELECT_SPEZIAL_CHARACTER$20Test.eb359af9-3563-48b6-b010-14b0702bd7f1.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:51:02,757] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_SPEZIAL_CHARACTER Test on 2 rows
INFO  [2023-01-17 00:51:02,757] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:51:02,757] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:51:02,757] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER Test]
INFO  [2023-01-17 00:51:02,757] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_dc3496b9-5491-45e8-b76c-42505cc63373
INFO  [2023-01-17 00:51:02,758] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_d0d5f775-cd55-42a9-baf7-a9a8d4649a3d
INFO  [2023-01-17 00:51:02,766] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:51:02,766] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_d0d5f775-cd55-42a9-baf7-a9a8d4649a3d
INFO  [2023-01-17 00:51:02,767] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER Test_dc3496b9-5491-45e8-b76c-42505cc63373
INFO  [2023-01-17 00:51:02,774] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_SPEZIAL_CHARACTER$20Test
INFO  [2023-01-17 00:51:02,774] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,880] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_SPEZIAL_CHARACTER Test
INFO  [2023-01-17 00:51:02,880] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:51:02,880] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:02,880] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:02,882] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:51:02,882] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:51:02,882] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:02,882] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:02,884] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_f76b648a-79b9-4ac2-819c-469145124833 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:02,884] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_f76b648a-79b9-4ac2-819c-469145124833 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:02,884] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:02,884] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_b15712e9-61e7-424b-8bd6-314a5f2f495e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:02,884] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_b15712e9-61e7-424b-8bd6-314a5f2f495e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:02,884] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:02,888] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,990] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:02,990] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-17 00:51:02,990] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table
INFO  [2023-01-17 00:51:03,105] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:03,219] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:03,219] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:03,220] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 105 B in total
INFO  [2023-01-17 00:51:03,220] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00033376sINFO  [2023-01-17 00:51:03,254] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=6, min=1, average=1.200000, max=2}
INFO  [2023-01-17 00:51:03,254] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2911efd2)
INFO  [2023-01-17 00:51:03,254] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:03,257] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:03,257] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:03,257] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:03,278] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:03 +0000] "POST /admin/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SINGLE_SELECT_SPEZIAL_CHARACTER2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:03,279] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:03,280] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:03,280] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:03,280] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:03,281] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:03,281] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:03,281] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:51:03,282] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:03,282] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.0
INFO  [2023-01-17 00:51:03,282] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.table.table.1
INFO  [2023-01-17 00:51:03,388] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_SPEZIAL_CHARACTER2 Test QUERY INIT
INFO  [2023-01-17 00:51:03,405] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:03,405] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ba74d4f3-fc50-4ba9-8392-8824e09012f4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test))]]
INFO  [2023-01-17 00:51:03,409] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4
INFO  [2023-01-17 00:51:03,409] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4
INFO  [2023-01-17 00:51:03,410] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4] with 1 results within PT0.001102S
INFO  [2023-01-17 00:51:03,410] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4] with 0 results within PT0.000882S
127.0.0.1 - - [17/Jan/2023:00:51:03 +0000] "POST /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test/queries HTTP/1.1" 201 1252 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:03,411] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_f76b648a-79b9-4ac2-819c-469145124833, startTime=2023-01-17T00:51:03.409466, finishTime=2023-01-17T00:51:03.410568) of size 1
INFO  [2023-01-17 00:51:03,411] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test_b15712e9-61e7-424b-8bd6-314a5f2f495e, startTime=2023-01-17T00:51:03.409749, finishTime=2023-01-17T00:51:03.410631) of size 0
INFO  [2023-01-17 00:51:03,411] com.bakdata.conquery.models.execution.ManagedExecution: DONE ba74d4f3-fc50-4ba9-8392-8824e09012f4 ManagedQuery within PT0.005827S
127.0.0.1 - - [17/Jan/2023:00:51:03 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test/queries/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4 HTTP/1.1" 200 1599 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:03,451] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test], queryId=ba74d4f3-fc50-4ba9-8392-8824e09012f4, label=concept	@§$, creationTime=2023-01-17T00:51:03.405649, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7b5b22e1[Count = 0], startTime=2023-01-17T00:51:03.405890, finishTime=2023-01-17T00:51:03.411717, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@84016a1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e040852, com.bakdata.conquery.models.query.ColumnDescriptor@74c8ebb7]) download on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:03,451] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test], queryId=ba74d4f3-fc50-4ba9-8392-8824e09012f4, label=concept	@§$, creationTime=2023-01-17T00:51:03.405649, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7b5b22e1[Count = 0], startTime=2023-01-17T00:51:03.405890, finishTime=2023-01-17T00:51:03.411717, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@84016a1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e040852, com.bakdata.conquery.models.query.ColumnDescriptor@74c8ebb7]) on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
127.0.0.1 - - [17/Jan/2023:00:51:03 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER2%20Test/result/SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test.ba74d4f3-fc50-4ba9-8392-8824e09012f4.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:03,471] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_SPEZIAL_CHARACTER2 Test on 2 rows
INFO  [2023-01-17 00:51:03,472] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:51:03,472] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:51:03,472] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER2 Test]
INFO  [2023-01-17 00:51:03,472] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_b15712e9-61e7-424b-8bd6-314a5f2f495e
INFO  [2023-01-17 00:51:03,472] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_f76b648a-79b9-4ac2-819c-469145124833
INFO  [2023-01-17 00:51:03,483] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:51:03,483] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_f76b648a-79b9-4ac2-819c-469145124833
INFO  [2023-01-17 00:51:03,483] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER2 Test_b15712e9-61e7-424b-8bd6-314a5f2f495e
INFO  [2023-01-17 00:51:03,583] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_SPEZIAL_CHARACTER2$20Test
INFO  [2023-01-17 00:51:03,583] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:03,691] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_SPEZIAL_CHARACTER2 Test
INFO  [2023-01-17 00:51:03,692] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-17 00:51:03,692] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:03,692] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:03,693] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-17 00:51:03,693] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-17 00:51:03,693] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:03,693] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:03,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_8397dd0f-f9e2-4b1e-9f81-5f98c44b6988 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:03,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_8397dd0f-f9e2-4b1e-9f81-5f98c44b6988 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:03,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:03,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_310185be-286d-4bed-aa1a-47aaeb6f8ac4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:03,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_310185be-286d-4bed-aa1a-47aaeb6f8ac4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:03,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:03,699] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:03,803] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:03,803] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table
INFO  [2023-01-17 00:51:03,804] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table
INFO  [2023-01-17 00:51:03,920] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,033] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:04,033] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:04,033] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 105 B in total
INFO  [2023-01-17 00:51:04,033] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00036384sINFO  [2023-01-17 00:51:04,070] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=6, min=1, average=1.200000, max=2}
INFO  [2023-01-17 00:51:04,070] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@fa29c79)
INFO  [2023-01-17 00:51:04,070] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:04,073] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:04,073] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:04,073] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:04,090] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "POST /admin/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SINGLE_SELECT_SPEZIAL_CHARACTER3+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:04,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,092] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:04,093] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:04,093] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:04,096] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:04,096] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:04,096] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:51:04,098] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:04,098] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table.0
INFO  [2023-01-17 00:51:04,098] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.table.table.1
INFO  [2023-01-17 00:51:04,204] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_SPEZIAL_CHARACTER3 Test QUERY INIT
INFO  [2023-01-17 00:51:04,221] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:04,222] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a15bc7c3-6701-48f4-8271-12642d250c0d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test))]]
INFO  [2023-01-17 00:51:04,225] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d
INFO  [2023-01-17 00:51:04,225] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d
INFO  [2023-01-17 00:51:04,226] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d] with 1 results within PT0.001294S
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "POST /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test/queries HTTP/1.1" 201 1253 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:04,227] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d] with 0 results within PT0.001356S
INFO  [2023-01-17 00:51:04,227] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_8397dd0f-f9e2-4b1e-9f81-5f98c44b6988, startTime=2023-01-17T00:51:04.225672, finishTime=2023-01-17T00:51:04.227028) of size 0
INFO  [2023-01-17 00:51:04,227] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d, workerId=SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.worker_SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test_310185be-286d-4bed-aa1a-47aaeb6f8ac4, startTime=2023-01-17T00:51:04.225675, finishTime=2023-01-17T00:51:04.226969) of size 1
INFO  [2023-01-17 00:51:04,227] com.bakdata.conquery.models.execution.ManagedExecution: DONE a15bc7c3-6701-48f4-8271-12642d250c0d ManagedQuery within PT0.005645S
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test/queries/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d HTTP/1.1" 200 1600 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:04,264] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test], queryId=a15bc7c3-6701-48f4-8271-12642d250c0d, label=concept	@§$, creationTime=2023-01-17T00:51:04.222011, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@105a8b75[Count = 0], startTime=2023-01-17T00:51:04.222246, finishTime=2023-01-17T00:51:04.227891, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@daa8fd2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6ddb9269, com.bakdata.conquery.models.query.ColumnDescriptor@36a6cba7]) download on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:04,264] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test], queryId=a15bc7c3-6701-48f4-8271-12642d250c0d, label=concept	@§$, creationTime=2023-01-17T00:51:04.222011, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@105a8b75[Count = 0], startTime=2023-01-17T00:51:04.222246, finishTime=2023-01-17T00:51:04.227891, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@daa8fd2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6ddb9269, com.bakdata.conquery.models.query.ColumnDescriptor@36a6cba7]) on dataset Dataset[label=null, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "GET /api/datasets/SINGLE_SELECT_SPEZIAL_CHARACTER3%20Test/result/SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test.a15bc7c3-6701-48f4-8271-12642d250c0d.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:51:04,283] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SINGLE_SELECT_SPEZIAL_CHARACTER3 Test on 2 rows
INFO  [2023-01-17 00:51:04,283] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-17 00:51:04,284] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-17 00:51:04,284] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test, name=SINGLE_SELECT_SPEZIAL_CHARACTER3 Test]
INFO  [2023-01-17 00:51:04,284] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_310185be-286d-4bed-aa1a-47aaeb6f8ac4
INFO  [2023-01-17 00:51:04,284] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_8397dd0f-f9e2-4b1e-9f81-5f98c44b6988
INFO  [2023-01-17 00:51:04,293] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-17 00:51:04,294] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_8397dd0f-f9e2-4b1e-9f81-5f98c44b6988
INFO  [2023-01-17 00:51:04,295] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_SPEZIAL_CHARACTER3 Test_310185be-286d-4bed-aa1a-47aaeb6f8ac4
INFO  [2023-01-17 00:51:04,299] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_SPEZIAL_CHARACTER3$20Test
INFO  [2023-01-17 00:51:04,299] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,404] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_SPEZIAL_CHARACTER3 Test
INFO  [2023-01-17 00:51:04,404] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_DECIMAL
INFO  [2023-01-17 00:51:04,405] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:04,405] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:04,406] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-17 00:51:04,406] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-17 00:51:04,406] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:04,406] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:04,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DECIMAL.worker_SUM_DECIMAL_1f9bd81e-3244-4893-aab9-bb76ac29536d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:04,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DECIMAL.worker_SUM_DECIMAL_1f9bd81e-3244-4893-aab9-bb76ac29536d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:04,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:04,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DECIMAL.worker_SUM_DECIMAL_ebee1567-d773-45a6-bc5c-7363f9612f37 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:04,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DECIMAL.worker_SUM_DECIMAL_ebee1567-d773-45a6-bc5c-7363f9612f37 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:04,408] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:04,411] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,515] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,516] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DECIMAL.table
INFO  [2023-01-17 00:51:04,516] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DECIMAL.table
INFO  [2023-01-17 00:51:04,631] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,751] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:04,752] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:04,752] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 185 B in total
INFO  [2023-01-17 00:51:04,752] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_DECIMAL/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00025365sINFO  [2023-01-17 00:51:04,778] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=4}
INFO  [2023-01-17 00:51:04,778] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=16511, maxValue=16514), dateReader=com.bakdata.conquery.util.DateReader@79a8f533)
INFO  [2023-01-17 00:51:04,778] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with DecimalParser(super=Parser(lines=10, nullLines=0), maxScale=0, maxAbs=250)
INFO  [2023-01-17 00:51:04,781] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_DECIMAL/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:04,781] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:04,781] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_DECIMAL/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:04,795] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_DECIMAL.table
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "POST /admin/datasets/SUM_DECIMAL/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SUM_DECIMAL%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:04,796] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:04,796] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:04,796] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:04,796] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:04,798] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:04,798] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DECIMAL.table.table], containing 10 entries.
INFO  [2023-01-17 00:51:04,798] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DECIMAL.table.table], containing 10 entries.
WARN  [2023-01-17 00:51:04,799] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:04,799] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DECIMAL.table.table.0
INFO  [2023-01-17 00:51:04,800] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DECIMAL.table.table.1
INFO  [2023-01-17 00:51:04,905] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_DECIMAL QUERY INIT
INFO  [2023-01-17 00:51:04,922] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_DECIMAL] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:04,922] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[392ae84f-1bbc-4c6a-83bd-0c07ba7446a3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DECIMAL))]]
INFO  [2023-01-17 00:51:04,927] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3
INFO  [2023-01-17 00:51:04,927] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "POST /api/datasets/SUM_DECIMAL/queries HTTP/1.1" 201 1163 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:04,928] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3] with 3 results within PT0.001435S
INFO  [2023-01-17 00:51:04,928] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3] with 1 results within PT0.001312S
INFO  [2023-01-17 00:51:04,929] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3, workerId=SUM_DECIMAL.worker_SUM_DECIMAL_ebee1567-d773-45a6-bc5c-7363f9612f37, startTime=2023-01-17T00:51:04.927185, finishTime=2023-01-17T00:51:04.928620) of size 3
INFO  [2023-01-17 00:51:04,929] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3, workerId=SUM_DECIMAL.worker_SUM_DECIMAL_1f9bd81e-3244-4893-aab9-bb76ac29536d, startTime=2023-01-17T00:51:04.927318, finishTime=2023-01-17T00:51:04.928630) of size 1
INFO  [2023-01-17 00:51:04,929] com.bakdata.conquery.models.execution.ManagedExecution: DONE 392ae84f-1bbc-4c6a-83bd-0c07ba7446a3 ManagedQuery within PT0.006588S
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "GET /api/datasets/SUM_DECIMAL/queries/SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3 HTTP/1.1" 200 1398 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:04,957] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DECIMAL], queryId=392ae84f-1bbc-4c6a-83bd-0c07ba7446a3, label=concept	@§$, creationTime=2023-01-17T00:51:04.922694, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2ed8c0c3[Count = 0], startTime=2023-01-17T00:51:04.922924, finishTime=2023-01-17T00:51:04.929512, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7226b978), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DECIMAL)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1cd1d445, com.bakdata.conquery.models.query.ColumnDescriptor@505a598f]) download on dataset Dataset[label=null, name=SUM_DECIMAL] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:04,957] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DECIMAL], queryId=392ae84f-1bbc-4c6a-83bd-0c07ba7446a3, label=concept	@§$, creationTime=2023-01-17T00:51:04.922694, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2ed8c0c3[Count = 0], startTime=2023-01-17T00:51:04.922924, finishTime=2023-01-17T00:51:04.929512, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7226b978), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DECIMAL)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1cd1d445, com.bakdata.conquery.models.query.ColumnDescriptor@505a598f]) on dataset Dataset[label=null, name=SUM_DECIMAL]
127.0.0.1 - - [17/Jan/2023:00:51:04 +0000] "GET /api/datasets/SUM_DECIMAL/result/SUM_DECIMAL.392ae84f-1bbc-4c6a-83bd-0c07ba7446a3.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:04,978] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SUM_DECIMAL on 5 rows
INFO  [2023-01-17 00:51:04,978] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_DECIMAL
INFO  [2023-01-17 00:51:04,979] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-17 00:51:04,979] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DECIMAL, name=SUM_DECIMAL]
INFO  [2023-01-17 00:51:04,979] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DECIMAL_ebee1567-d773-45a6-bc5c-7363f9612f37
INFO  [2023-01-17 00:51:04,979] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DECIMAL_1f9bd81e-3244-4893-aab9-bb76ac29536d
INFO  [2023-01-17 00:51:05,006] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_DECIMAL
INFO  [2023-01-17 00:51:05,007] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DECIMAL_1f9bd81e-3244-4893-aab9-bb76ac29536d
INFO  [2023-01-17 00:51:05,007] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DECIMAL_ebee1567-d773-45a6-bc5c-7363f9612f37
INFO  [2023-01-17 00:51:05,099] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_DECIMAL
INFO  [2023-01-17 00:51:05,099] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:05,206] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_DECIMAL
INFO  [2023-01-17 00:51:05,206] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_INTEGER Test
INFO  [2023-01-17 00:51:05,206] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:05,206] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:05,207] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-17 00:51:05,207] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-17 00:51:05,207] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:05,207] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:05,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_d889e273-6411-4cba-bef2-344ac613b45a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:05,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_d889e273-6411-4cba-bef2-344ac613b45a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:05,209] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:05,210] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_3be10b26-30e8-4faf-b9bc-fa7ae439af44 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:05,210] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_3be10b26-30e8-4faf-b9bc-fa7ae439af44 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:05,210] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:05,214] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:05,316] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:05,317] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test[1].table
INFO  [2023-01-17 00:51:05,317] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_INTEGER$20Test[1].table
INFO  [2023-01-17 00:51:05,434] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:05,555] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:05,555] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:05,555] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 185 B in total
INFO  [2023-01-17 00:51:05,555] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_INTEGER Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000419246sINFO  [2023-01-17 00:51:05,598] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=4}
INFO  [2023-01-17 00:51:05,598] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=16511, maxValue=16514), dateReader=com.bakdata.conquery.util.DateReader@6a5d1b5)
INFO  [2023-01-17 00:51:05,598] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[plus] with IntegerParser(super=Parser(lines=10, nullLines=0), minValue=-50, maxValue=250)
INFO  [2023-01-17 00:51:05,600] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:05,600] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:05,600] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_INTEGER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:05,619] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_INTEGER$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:51:05 +0000] "POST /admin/datasets/SUM_INTEGER%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SUM_INTEGER+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:51:05,621] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:05,621] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:05,621] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:05,621] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:05,623] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:05,623] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test[1].table.table], containing 10 entries.
INFO  [2023-01-17 00:51:05,624] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_INTEGER$20Test[1].table.table], containing 10 entries.
WARN  [2023-01-17 00:51:05,625] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:05,625] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test[1].table.table.0
INFO  [2023-01-17 00:51:05,625] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_INTEGER$20Test[1].table.table.1
INFO  [2023-01-17 00:51:05,730] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_INTEGER Test QUERY INIT
INFO  [2023-01-17 00:51:05,741] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_INTEGER$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:05,742] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[dfeb79f7-fdda-48f9-b4f5-29e934d69590] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test[1]))]]
INFO  [2023-01-17 00:51:05,747] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test[1].dfeb79f7-fdda-48f9-b4f5-29e934d69590
INFO  [2023-01-17 00:51:05,747] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_INTEGER$20Test[1].dfeb79f7-fdda-48f9-b4f5-29e934d69590
INFO  [2023-01-17 00:51:05,748] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test[1].dfeb79f7-fdda-48f9-b4f5-29e934d69590] with 1 results within PT0.001157S
INFO  [2023-01-17 00:51:05,748] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_INTEGER$20Test[1].dfeb79f7-fdda-48f9-b4f5-29e934d69590] with 3 results within PT0.00139S
127.0.0.1 - - [17/Jan/2023:00:51:05 +0000] "POST /api/datasets/SUM_INTEGER$20Test%5B1%5D/queries HTTP/1.1" 201 1206 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:05,749] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test[1].dfeb79f7-fdda-48f9-b4f5-29e934d69590, workerId=SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_3be10b26-30e8-4faf-b9bc-fa7ae439af44, startTime=2023-01-17T00:51:05.747213, finishTime=2023-01-17T00:51:05.748370) of size 1
INFO  [2023-01-17 00:51:05,749] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_INTEGER$20Test[1].dfeb79f7-fdda-48f9-b4f5-29e934d69590, workerId=SUM_INTEGER$20Test[1].worker_SUM_INTEGER$20Test[1]_d889e273-6411-4cba-bef2-344ac613b45a, startTime=2023-01-17T00:51:05.747228, finishTime=2023-01-17T00:51:05.748618) of size 3
INFO  [2023-01-17 00:51:05,749] com.bakdata.conquery.models.execution.ManagedExecution: DONE dfeb79f7-fdda-48f9-b4f5-29e934d69590 ManagedQuery within PT0.00693S
127.0.0.1 - - [17/Jan/2023:00:51:05 +0000] "GET /api/datasets/SUM_INTEGER$20Test%5B1%5D/queries/SUM_INTEGER$20Test%5B1%5D.dfeb79f7-fdda-48f9-b4f5-29e934d69590 HTTP/1.1" 200 1717 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:05,782] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test[1]], queryId=dfeb79f7-fdda-48f9-b4f5-29e934d69590, label=concept	@§$, creationTime=2023-01-17T00:51:05.742257, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7781bc12[Count = 0], startTime=2023-01-17T00:51:05.742506, finishTime=2023-01-17T00:51:05.749436, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@71fac32), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@654fbd08, com.bakdata.conquery.models.query.ColumnDescriptor@fd0d0c9]) download on dataset Dataset[label=null, name=SUM_INTEGER Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:05,782] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_INTEGER Test[1]], queryId=dfeb79f7-fdda-48f9-b4f5-29e934d69590, label=concept	@§$, creationTime=2023-01-17T00:51:05.742257, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7781bc12[Count = 0], startTime=2023-01-17T00:51:05.742506, finishTime=2023-01-17T00:51:05.749436, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@71fac32), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_INTEGER Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@654fbd08, com.bakdata.conquery.models.query.ColumnDescriptor@fd0d0c9]) on dataset Dataset[label=null, name=SUM_INTEGER Test[1]]
127.0.0.1 - - [17/Jan/2023:00:51:05 +0000] "GET /api/datasets/SUM_INTEGER%20Test%5B1%5D/result/SUM_INTEGER$20Test%5B1%5D.dfeb79f7-fdda-48f9-b4f5-29e934d69590.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 27
INFO  [2023-01-17 00:51:05,807] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL FilterTest SUM_INTEGER Test on 5 rows
INFO  [2023-01-17 00:51:05,807] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_INTEGER Test[1]
INFO  [2023-01-17 00:51:05,808] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-17 00:51:05,808] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_INTEGER Test[1], name=SUM_INTEGER Test[1]]
INFO  [2023-01-17 00:51:05,808] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test[1]_3be10b26-30e8-4faf-b9bc-fa7ae439af44
INFO  [2023-01-17 00:51:05,808] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_INTEGER Test[1]_d889e273-6411-4cba-bef2-344ac613b45a
INFO  [2023-01-17 00:51:05,808] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test[1]_d889e273-6411-4cba-bef2-344ac613b45a
INFO  [2023-01-17 00:51:05,808] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_INTEGER Test[1]_3be10b26-30e8-4faf-b9bc-fa7ae439af44
INFO  [2023-01-17 00:51:05,908] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_INTEGER Test[1]
INFO  [2023-01-17 00:51:05,925] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_INTEGER$20Test[1]
INFO  [2023-01-17 00:51:05,925] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:05,930] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_INTEGER Test
INFO  [2023-01-17 00:51:05,931] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:05,931] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:05,931] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:05,932] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:05,932] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:05,933] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:05,933] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:05,934] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_28398f31-1bbe-4447-876d-ce23b67bec1f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:05,934] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_28398f31-1bbe-4447-876d-ce23b67bec1f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:05,934] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:05,935] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_8e3eb87e-1ee1-487a-936d-ba04aba09df0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:05,935] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_8e3eb87e-1ee1-487a-936d-ba04aba09df0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:05,935] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:05,939] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,075] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ABS-EXPORT-FORM$20SECONDARY_ID.secondary]
INFO  [2023-01-17 00:51:06,076] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ABS-EXPORT-FORM$20SECONDARY_ID.ignored]
INFO  [2023-01-17 00:51:06,076] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,077] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-17 00:51:06,077] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-17 00:51:06,077] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.ignored
INFO  [2023-01-17 00:51:06,078] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ABS-EXPORT-FORM$20SECONDARY_ID.ignored
INFO  [2023-01-17 00:51:06,185] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-17 00:51:06,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-17 00:51:06,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:06,185] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:06,291] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID IMPORT TABLES
INFO  [2023-01-17 00:51:06,320] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,474] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID IMPORT CONCEPTS
INFO  [2023-01-17 00:51:06,475] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:06,475] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:06,475] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:06,475] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:06,475] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.005335539sINFO  [2023-01-17 00:51:06,520] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:06,520] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@728c6590)
INFO  [2023-01-17 00:51:06,520] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@70d74b8e)
INFO  [2023-01-17 00:51:06,520] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@79b0bd73), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@7d8f369d), dateReader=com.bakdata.conquery.util.DateReader@388db748, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:06,520] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@5b633605)
INFO  [2023-01-17 00:51:06,520] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:06,523] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:06,523] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000668414sINFO  [2023-01-17 00:51:06,543] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-17 00:51:06,543] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@2325944f)
INFO  [2023-01-17 00:51:06,543] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:06,543] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:06,546] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:06,546] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:06,546] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:06,546] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:06,562] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:06 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ABS-EXPORT-FORM+SECONDARY_ID%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:06,564] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:06,565] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:06,565] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:06,568] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:06,568] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:06,568] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:06,570] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:06,578] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:06,578] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:06,578] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:06,579] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:06,579] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:06,580] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:06,580] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:06,580] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:06,585] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ABS-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:06,585] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:51:06 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ABS-EXPORT-FORM+SECONDARY_ID%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:06,585] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,585] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:06,585] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:06,586] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:06,586] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:06,586] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
WARN  [2023-01-17 00:51:06,586] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:06,587] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20SECONDARY_ID.table.table.0
INFO  [2023-01-17 00:51:06,691] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,697] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:06,706] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,711] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:06,743] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:06,743] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:06,743] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:06,857] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e01b7a08-dc5b-45c3-81c5-9498a5d42cb6] in Datasets[[]]
INFO  [2023-01-17 00:51:06,874] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20SECONDARY_ID.e01b7a08-dc5b-45c3-81c5-9498a5d42cb6
INFO  [2023-01-17 00:51:06,876] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20SECONDARY_ID.e01b7a08-dc5b-45c3-81c5-9498a5d42cb6
INFO  [2023-01-17 00:51:06,881] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20SECONDARY_ID.e01b7a08-dc5b-45c3-81c5-9498a5d42cb6] with 0 results within PT0.004627S
INFO  [2023-01-17 00:51:06,884] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20SECONDARY_ID.e01b7a08-dc5b-45c3-81c5-9498a5d42cb6] with 1 results within PT0.00925S
INFO  [2023-01-17 00:51:06,885] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20SECONDARY_ID.e01b7a08-dc5b-45c3-81c5-9498a5d42cb6, workerId=ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_28398f31-1bbe-4447-876d-ce23b67bec1f, startTime=2023-01-17T00:51:06.876506, finishTime=2023-01-17T00:51:06.881133) of size 0
INFO  [2023-01-17 00:51:06,886] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20SECONDARY_ID.e01b7a08-dc5b-45c3-81c5-9498a5d42cb6, workerId=ABS-EXPORT-FORM$20SECONDARY_ID.worker_ABS-EXPORT-FORM$20SECONDARY_ID_8e3eb87e-1ee1-487a-936d-ba04aba09df0, startTime=2023-01-17T00:51:06.874767, finishTime=2023-01-17T00:51:06.884017) of size 1
INFO  [2023-01-17 00:51:06,886] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7104d5ee-7559-428a-9d5e-b5d24e3b8092 ManagedQuery within PT0.028748S
INFO  [2023-01-17 00:51:06,888] com.bakdata.conquery.models.execution.ManagedExecution: DONE e01b7a08-dc5b-45c3-81c5-9498a5d42cb6 ManagedInternalForm within PT0.030022S
INFO  [2023-01-17 00:51:06,888] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID QUERIES EXECUTED
INFO  [2023-01-17 00:51:06,908] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM SECONDARY_ID CSV TESTING: results
INFO  [2023-01-17 00:51:06,910] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:06,911] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:06,911] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM SECONDARY_ID, name=ABS-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:06,911] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM SECONDARY_ID_8e3eb87e-1ee1-487a-936d-ba04aba09df0
INFO  [2023-01-17 00:51:06,911] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM SECONDARY_ID_28398f31-1bbe-4447-876d-ce23b67bec1f
INFO  [2023-01-17 00:51:06,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM SECONDARY_ID_8e3eb87e-1ee1-487a-936d-ba04aba09df0
INFO  [2023-01-17 00:51:06,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:06,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM SECONDARY_ID_28398f31-1bbe-4447-876d-ce23b67bec1f
INFO  [2023-01-17 00:51:06,989] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS-EXPORT-FORM$20SECONDARY_ID
INFO  [2023-01-17 00:51:06,989] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,050] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:07,051] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:07,051] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:07,051] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:07,052] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:07,052] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:07,052] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:07,052] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:07,054] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_d9fc1d35-b4e9-4f13-b5b9-e3abcbb1b3f2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:07,054] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_d9fc1d35-b4e9-4f13-b5b9-e3abcbb1b3f2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:07,054] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:07,054] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_82f2c94c-134e-4547-a2cb-fdf9fd802a32 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:07,054] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_82f2c94c-134e-4547-a2cb-fdf9fd802a32 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:07,054] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:07,058] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,158] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,165] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,165] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-17 00:51:07,165] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-17 00:51:07,270] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-17 00:51:07,287] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,398] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:07,399] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:07,399] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:07,400] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-17 00:51:07,400] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000252631sINFO  [2023-01-17 00:51:07,426] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:07,426] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@cd925b1)
INFO  [2023-01-17 00:51:07,426] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:07,426] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@75581d81)
INFO  [2023-01-17 00:51:07,426] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@acf76b2), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2422332d), dateReader=com.bakdata.conquery.util.DateReader@6c1247ca, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:07,426] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@9ff6d7c)
INFO  [2023-01-17 00:51:07,434] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:07,434] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:07,434] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:07,456] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:07 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20ADD%20DEFAULT%20SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ABS-EXPORT-FORM+ADD+DEFAULT+SELECT+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:07,457] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,458] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:07,459] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:07,459] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:07,462] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:07,462] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:07,462] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:07,463] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:07,464] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:07,464] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:07,464] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.4
WARN  [2023-01-17 00:51:07,464] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:07,464] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:07,465] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:07,465] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:07,466] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:07,571] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,576] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:07,622] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,627] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,632] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:07,638] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:07,638] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:07,638] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:07,751] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ce1d4068-d369-4876-96ec-5d5c0951297b] in Datasets[[]]
INFO  [2023-01-17 00:51:07,774] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.ce1d4068-d369-4876-96ec-5d5c0951297b
INFO  [2023-01-17 00:51:07,775] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.ce1d4068-d369-4876-96ec-5d5c0951297b
INFO  [2023-01-17 00:51:07,776] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.ce1d4068-d369-4876-96ec-5d5c0951297b] with 0 results within PT0.002254S
INFO  [2023-01-17 00:51:07,777] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.ce1d4068-d369-4876-96ec-5d5c0951297b, workerId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_82f2c94c-134e-4547-a2cb-fdf9fd802a32, startTime=2023-01-17T00:51:07.774579, finishTime=2023-01-17T00:51:07.776833) of size 0
INFO  [2023-01-17 00:51:07,778] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.ce1d4068-d369-4876-96ec-5d5c0951297b] with 1 results within PT0.002696S
INFO  [2023-01-17 00:51:07,780] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.ce1d4068-d369-4876-96ec-5d5c0951297b, workerId=ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test.worker_ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test_d9fc1d35-b4e9-4f13-b5b9-e3abcbb1b3f2, startTime=2023-01-17T00:51:07.775705, finishTime=2023-01-17T00:51:07.778401) of size 1
INFO  [2023-01-17 00:51:07,780] com.bakdata.conquery.models.execution.ManagedExecution: DONE 822a0379-e15e-451c-87b0-00a06feaad56 ManagedQuery within PT0.028058S
INFO  [2023-01-17 00:51:07,781] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:07,781] com.bakdata.conquery.models.execution.ManagedExecution: DONE ce1d4068-d369-4876-96ec-5d5c0951297b ManagedInternalForm within PT0.029209S
INFO  [2023-01-17 00:51:07,798] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-17 00:51:07,800] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:07,801] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:07,801] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM ADD DEFAULT SELECT Test, name=ABS-EXPORT-FORM ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:07,802] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_82f2c94c-134e-4547-a2cb-fdf9fd802a32
INFO  [2023-01-17 00:51:07,802] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_d9fc1d35-b4e9-4f13-b5b9-e3abcbb1b3f2
INFO  [2023-01-17 00:51:07,852] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:07,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_d9fc1d35-b4e9-4f13-b5b9-e3abcbb1b3f2
INFO  [2023-01-17 00:51:07,853] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM ADD DEFAULT SELECT Test_82f2c94c-134e-4547-a2cb-fdf9fd802a32
INFO  [2023-01-17 00:51:07,865] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS-EXPORT-FORM$20ADD$20DEFAULT$20SELECT$20Test
INFO  [2023-01-17 00:51:07,865] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,044] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS-EXPORT-FORM ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:08,044] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-17 00:51:08,045] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:08,045] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:08,046] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-17 00:51:08,046] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-17 00:51:08,046] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:08,046] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:08,048] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_68824406-16d8-425e-b378-a60b284a5fb1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:08,048] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_68824406-16d8-425e-b378-a60b284a5fb1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:08,048] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:08,048] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_7b1b68b8-52fd-46f4-a3e0-5916df5de352 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:08,048] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_7b1b68b8-52fd-46f4-a3e0-5916df5de352 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:08,048] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:08,053] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,152] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,159] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,160] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-17 00:51:08,160] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-17 00:51:08,265] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test IMPORT TABLES
INFO  [2023-01-17 00:51:08,284] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,398] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:08,399] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:08,399] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:08,399] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-17 00:51:08,399] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000409468sINFO  [2023-01-17 00:51:08,441] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:08,441] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@16f6062)
INFO  [2023-01-17 00:51:08,441] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@9509d7c)
INFO  [2023-01-17 00:51:08,441] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@78db0002), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4dd5ff6f), dateReader=com.bakdata.conquery.util.DateReader@3307cd08, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:08,441] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:08,442] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@78d18404)
INFO  [2023-01-17 00:51:08,445] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:08,445] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:08,445] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS-EXPORT-FORM WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:08,468] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-17 00:51:08,469] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:08 +0000] "POST /admin/datasets/ABS-EXPORT-FORM%20WITH%20SELECT%20SET%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ABS-EXPORT-FORM+WITH+SELECT+SET+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-17 00:51:08,470] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:08,471] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:08,471] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:08,475] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:08,475] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:08,475] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:08,477] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:08,478] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:08,479] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:08,480] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:08,481] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.4
WARN  [2023-01-17 00:51:08,481] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:08,482] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:08,482] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:08,482] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:08,587] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,592] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:08,627] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,633] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,638] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:08,643] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:08,644] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:08,644] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:08,752] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c1275048-5868-41f3-be8c-5a915b3d1b66] in Datasets[[]]
INFO  [2023-01-17 00:51:08,757] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.c1275048-5868-41f3-be8c-5a915b3d1b66
INFO  [2023-01-17 00:51:08,758] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.c1275048-5868-41f3-be8c-5a915b3d1b66
INFO  [2023-01-17 00:51:08,760] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.c1275048-5868-41f3-be8c-5a915b3d1b66] with 0 results within PT0.002774S
INFO  [2023-01-17 00:51:08,761] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.c1275048-5868-41f3-be8c-5a915b3d1b66] with 1 results within PT0.003301S
INFO  [2023-01-17 00:51:08,761] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.c1275048-5868-41f3-be8c-5a915b3d1b66, workerId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_7b1b68b8-52fd-46f4-a3e0-5916df5de352, startTime=2023-01-17T00:51:08.758015, finishTime=2023-01-17T00:51:08.760789) of size 0
INFO  [2023-01-17 00:51:08,762] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.c1275048-5868-41f3-be8c-5a915b3d1b66, workerId=ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test.worker_ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test_68824406-16d8-425e-b378-a60b284a5fb1, startTime=2023-01-17T00:51:08.758118, finishTime=2023-01-17T00:51:08.761419) of size 1
INFO  [2023-01-17 00:51:08,762] com.bakdata.conquery.models.execution.ManagedExecution: DONE 8f89d767-cedd-46fb-b906-d62c81a65fb8 ManagedQuery within PT0.010024S
INFO  [2023-01-17 00:51:08,763] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:08,763] com.bakdata.conquery.models.execution.ManagedExecution: DONE c1275048-5868-41f3-be8c-5a915b3d1b66 ManagedInternalForm within PT0.01084S
INFO  [2023-01-17 00:51:08,778] com.bakdata.conquery.integration.json.FormTest: ABS-EXPORT-FORM WITH SELECT SET Test CSV TESTING: results
INFO  [2023-01-17 00:51:08,780] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-17 00:51:08,780] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-17 00:51:08,780] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS-EXPORT-FORM WITH SELECT SET Test, name=ABS-EXPORT-FORM WITH SELECT SET Test]
INFO  [2023-01-17 00:51:08,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM WITH SELECT SET Test_7b1b68b8-52fd-46f4-a3e0-5916df5de352
INFO  [2023-01-17 00:51:08,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS-EXPORT-FORM WITH SELECT SET Test_68824406-16d8-425e-b378-a60b284a5fb1
INFO  [2023-01-17 00:51:08,846] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-17 00:51:08,848] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM WITH SELECT SET Test_68824406-16d8-425e-b378-a60b284a5fb1
INFO  [2023-01-17 00:51:08,848] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS-EXPORT-FORM WITH SELECT SET Test_7b1b68b8-52fd-46f4-a3e0-5916df5de352
INFO  [2023-01-17 00:51:08,882] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS-EXPORT-FORM$20WITH$20SELECT$20SET$20Test
INFO  [2023-01-17 00:51:08,882] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,051] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS-EXPORT-FORM WITH SELECT SET Test
INFO  [2023-01-17 00:51:09,051] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:09,052] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:09,052] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:09,066] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:09,066] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:09,067] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:09,067] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:09,109] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_58482713-79b4-40b7-b1d3-3c4ea200eac7 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:09,109] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_58482713-79b4-40b7-b1d3-3c4ea200eac7 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:09,109] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:09,125] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_e514d318-a4a4-46f9-9d0d-4216db1f5195 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:09,125] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_e514d318-a4a4-46f9-9d0d-4216db1f5195 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:09,125] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:09,128] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,230] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ADD$20DEFAULT$20SELECT$20Test.secondary]
INFO  [2023-01-17 00:51:09,230] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,231] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test.secondary
INFO  [2023-01-17 00:51:09,231] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test.secondary
INFO  [2023-01-17 00:51:09,336] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,337] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-17 00:51:09,337] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.vers_stamm
INFO  [2023-01-17 00:51:09,337] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.table
INFO  [2023-01-17 00:51:09,337] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test.table
INFO  [2023-01-17 00:51:09,442] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-17 00:51:09,459] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,569] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:09,570] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:09,570] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:09,570] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:09,570] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:09,570] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.005013093sINFO  [2023-01-17 00:51:09,612] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:09,612] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@55a215e7)
INFO  [2023-01-17 00:51:09,612] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:09,612] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@dc09900)
INFO  [2023-01-17 00:51:09,612] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@2682590c)
INFO  [2023-01-17 00:51:09,612] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3af782f7), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@34307bd9), dateReader=com.bakdata.conquery.util.DateReader@300fe987, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:09,616] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:09,616] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000646172sINFO  [2023-01-17 00:51:09,636] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-17 00:51:09,636] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@23b2574e)
INFO  [2023-01-17 00:51:09,636] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:09,636] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:09,639] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:09,639] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:09,639] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:09,639] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:09,661] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ADD$20DEFAULT$20SELECT$20Test.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:09 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ADD+DEFAULT+SELECT+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:09,662] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:09,663] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:09,663] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:09,665] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:09,665] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:09,665] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:09,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:09,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:09,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:09,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.1
WARN  [2023-01-17 00:51:09,666] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:09,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:09,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:09,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:09,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:09,689] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ADD$20DEFAULT$20SELECT$20Test.table
INFO  [2023-01-17 00:51:09,690] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:09,690] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:09 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ADD+DEFAULT+SELECT+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:51:09,690] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:09,690] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:09,691] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:51:09,692] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:09,692] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:09,692] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:09,692] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test.table.table.0
INFO  [2023-01-17 00:51:09,802] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,807] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:09,815] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,827] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:09,844] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:09,849] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:09,851] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:09,968] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2daa4611-c236-45e2-bed0-3fdf906339b2] in Datasets[[]]
INFO  [2023-01-17 00:51:09,977] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test.2daa4611-c236-45e2-bed0-3fdf906339b2
INFO  [2023-01-17 00:51:09,977] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test.2daa4611-c236-45e2-bed0-3fdf906339b2
INFO  [2023-01-17 00:51:09,982] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test.2daa4611-c236-45e2-bed0-3fdf906339b2] with 0 results within PT0.004232S
INFO  [2023-01-17 00:51:09,982] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test.2daa4611-c236-45e2-bed0-3fdf906339b2] with 1 results within PT0.00542S
INFO  [2023-01-17 00:51:09,982] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test.2daa4611-c236-45e2-bed0-3fdf906339b2, workerId=ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_e514d318-a4a4-46f9-9d0d-4216db1f5195, startTime=2023-01-17T00:51:09.977776, finishTime=2023-01-17T00:51:09.982008) of size 0
INFO  [2023-01-17 00:51:09,983] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test.2daa4611-c236-45e2-bed0-3fdf906339b2, workerId=ADD$20DEFAULT$20SELECT$20Test.worker_ADD$20DEFAULT$20SELECT$20Test_58482713-79b4-40b7-b1d3-3c4ea200eac7, startTime=2023-01-17T00:51:09.977280, finishTime=2023-01-17T00:51:09.982700) of size 1
INFO  [2023-01-17 00:51:09,983] com.bakdata.conquery.models.execution.ManagedExecution: DONE d7e548f2-7d69-4c9f-9320-b8fa278b486e ManagedQuery within PT0.015303S
INFO  [2023-01-17 00:51:09,985] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:09,985] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2daa4611-c236-45e2-bed0-3fdf906339b2 ManagedInternalForm within PT0.016576S
INFO  [2023-01-17 00:51:10,002] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-17 00:51:10,003] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:10,003] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:10,003] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test, name=ADD DEFAULT SELECT Test]
INFO  [2023-01-17 00:51:10,004] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test_58482713-79b4-40b7-b1d3-3c4ea200eac7
INFO  [2023-01-17 00:51:10,004] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test_e514d318-a4a4-46f9-9d0d-4216db1f5195
INFO  [2023-01-17 00:51:10,022] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test_58482713-79b4-40b7-b1d3-3c4ea200eac7
INFO  [2023-01-17 00:51:10,026] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test_e514d318-a4a4-46f9-9d0d-4216db1f5195
INFO  [2023-01-17 00:51:10,067] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:10,101] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ADD$20DEFAULT$20SELECT$20Test
INFO  [2023-01-17 00:51:10,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,160] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:10,161] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:10,161] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:10,161] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:10,162] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-17 00:51:10,162] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-17 00:51:10,162] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:10,162] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_09c7bd3f-fe1a-479e-93fb-4fa8322356db are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_09c7bd3f-fe1a-479e-93fb-4fa8322356db are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_3e63a65e-fa04-4714-b2e5-3216c8efb45a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_3e63a65e-fa04-4714-b2e5-3216c8efb45a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:10,268] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ADD$20DEFAULT$20SELECT$20Test[1].secondary]
INFO  [2023-01-17 00:51:10,269] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,270] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test[1].secondary
INFO  [2023-01-17 00:51:10,270] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ADD$20DEFAULT$20SELECT$20Test[1].secondary
INFO  [2023-01-17 00:51:10,377] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,378] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm
INFO  [2023-01-17 00:51:10,378] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm
INFO  [2023-01-17 00:51:10,378] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].table
INFO  [2023-01-17 00:51:10,378] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[1].table
INFO  [2023-01-17 00:51:10,483] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-17 00:51:10,519] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,633] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:10,634] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:10,634] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:10,634] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:10,635] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:10,635] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[1]/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.005309097sINFO  [2023-01-17 00:51:10,679] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:10,679] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@3252af16)
INFO  [2023-01-17 00:51:10,679] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:10,679] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@2845b3ac)
INFO  [2023-01-17 00:51:10,679] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@750d71e2)
INFO  [2023-01-17 00:51:10,679] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@2c7a327c), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@6978b997), dateReader=com.bakdata.conquery.util.DateReader@5d86c78d, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:10,683] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:10,683] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00066777sINFO  [2023-01-17 00:51:10,702] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-17 00:51:10,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@35d8a97c)
INFO  [2023-01-17 00:51:10,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:10,702] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:10,705] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:10,705] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:10,705] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:10,705] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:10,731] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:10 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ADD+DEFAULT+SELECT+Test%5B1%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:51:10,733] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:10,734] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:10,734] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:10,744] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:10,745] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:10,745] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:10,747] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:10,747] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:10,747] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:10,747] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:10,749] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:10,749] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:10,750] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:10,750] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:10,750] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:10,755] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ADD$20DEFAULT$20SELECT$20Test[1].table
INFO  [2023-01-17 00:51:10,755] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:51:10 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ADD+DEFAULT+SELECT+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:10,755] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:10,755] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:10,755] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,756] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:51:10,756] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:10,756] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].table.table], containing 6 entries.
INFO  [2023-01-17 00:51:10,756] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[1].table.table], containing 6 entries.
INFO  [2023-01-17 00:51:10,757] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[1].table.table.0
INFO  [2023-01-17 00:51:10,862] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,867] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:10,887] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,892] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:10,901] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:10,902] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:10,902] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:11,008] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0664300d-5330-4e46-a299-eeb0e60e11b0] in Datasets[[]]
INFO  [2023-01-17 00:51:11,014] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[1].0664300d-5330-4e46-a299-eeb0e60e11b0
INFO  [2023-01-17 00:51:11,014] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[1].0664300d-5330-4e46-a299-eeb0e60e11b0
INFO  [2023-01-17 00:51:11,018] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[1].0664300d-5330-4e46-a299-eeb0e60e11b0] with 0 results within PT0.003427S
INFO  [2023-01-17 00:51:11,018] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[1].0664300d-5330-4e46-a299-eeb0e60e11b0] with 1 results within PT0.003802S
INFO  [2023-01-17 00:51:11,020] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[1].0664300d-5330-4e46-a299-eeb0e60e11b0, workerId=ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_3e63a65e-fa04-4714-b2e5-3216c8efb45a, startTime=2023-01-17T00:51:11.014951, finishTime=2023-01-17T00:51:11.018378) of size 0
INFO  [2023-01-17 00:51:11,020] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[1].0664300d-5330-4e46-a299-eeb0e60e11b0, workerId=ADD$20DEFAULT$20SELECT$20Test[1].worker_ADD$20DEFAULT$20SELECT$20Test[1]_09c7bd3f-fe1a-479e-93fb-4fa8322356db, startTime=2023-01-17T00:51:11.014840, finishTime=2023-01-17T00:51:11.018642) of size 1
INFO  [2023-01-17 00:51:11,020] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1b536822-8b5b-4bb1-8ac9-f2c2cb2a9ad6 ManagedQuery within PT0.011683S
INFO  [2023-01-17 00:51:11,021] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0664300d-5330-4e46-a299-eeb0e60e11b0 ManagedInternalForm within PT0.01268S
INFO  [2023-01-17 00:51:11,021] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:11,038] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-17 00:51:11,040] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ADD DEFAULT SELECT Test[1]
INFO  [2023-01-17 00:51:11,040] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-17 00:51:11,040] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[1], name=ADD DEFAULT SELECT Test[1]]
INFO  [2023-01-17 00:51:11,041] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[1]_09c7bd3f-fe1a-479e-93fb-4fa8322356db
INFO  [2023-01-17 00:51:11,041] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[1]_3e63a65e-fa04-4714-b2e5-3216c8efb45a
INFO  [2023-01-17 00:51:11,063] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ADD DEFAULT SELECT Test[1]
INFO  [2023-01-17 00:51:11,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[1]_09c7bd3f-fe1a-479e-93fb-4fa8322356db
INFO  [2023-01-17 00:51:11,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[1]_3e63a65e-fa04-4714-b2e5-3216c8efb45a
INFO  [2023-01-17 00:51:11,157] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ADD$20DEFAULT$20SELECT$20Test[1]
INFO  [2023-01-17 00:51:11,157] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,207] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:11,208] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:11,208] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:11,208] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:11,209] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:11,209] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:11,209] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:11,209] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:11,210] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_bd5109c1-772c-4f97-847e-01b087bbc962 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:11,210] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_bd5109c1-772c-4f97-847e-01b087bbc962 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:11,210] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:11,211] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_1bb8a032-8088-40b5-bb4a-a6677616e344 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:11,211] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_1bb8a032-8088-40b5-bb4a-a6677616e344 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:11,211] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:11,215] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,314] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.secondary]
INFO  [2023-01-17 00:51:11,315] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,316] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-17 00:51:11,316] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-17 00:51:11,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-17 00:51:11,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-17 00:51:11,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:11,423] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:11,528] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID IMPORT TABLES
INFO  [2023-01-17 00:51:11,545] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,664] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID IMPORT CONCEPTS
INFO  [2023-01-17 00:51:11,665] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:11,665] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:11,665] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:11,665] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:11,666] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.004956765sINFO  [2023-01-17 00:51:11,707] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:11,707] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5725945)
INFO  [2023-01-17 00:51:11,707] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:11,707] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@48c4b2b9)
INFO  [2023-01-17 00:51:11,707] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@585bd742)
INFO  [2023-01-17 00:51:11,707] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@736bb58e), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@44b2e689), dateReader=com.bakdata.conquery.util.DateReader@b3e7c49, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:11,710] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:11,710] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000633728sINFO  [2023-01-17 00:51:11,730] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-17 00:51:11,730] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@7fabb385)
INFO  [2023-01-17 00:51:11,730] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:11,730] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:11,733] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:11,733] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:11,733] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:11,733] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ENTITY-DATE-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:11,748] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:11 +0000] "POST /admin/datasets/ENTITY-DATE-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ENTITY-DATE-EXPORT-FORM+SECONDARY_ID%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:11,750] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:11,751] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:11,751] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:11,754] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:11,755] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:11,755] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:11,757] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:11,757] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:11,758] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:11,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.3
WARN  [2023-01-17 00:51:11,763] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:11,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:11,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:11,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:11,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:11,794] com.bakdata.conquery.models.jobs.ImportJob: Importing table into ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table
127.0.0.1 - - [17/Jan/2023:00:51:11 +0000] "POST /admin/datasets/ENTITY-DATE-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ENTITY-DATE-EXPORT-FORM+SECONDARY_ID%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 40
INFO  [2023-01-17 00:51:11,796] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:11,796] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,796] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:11,796] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:11,797] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:51:11,797] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:11,798] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:11,798] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:11,798] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.table.table.0
INFO  [2023-01-17 00:51:11,917] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,923] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:11,933] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,939] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:11,948] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:11,949] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:11,949] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:12,057] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f810d4b5-3744-472f-93ac-71f7b77c2018] in Datasets[[]]
INFO  [2023-01-17 00:51:12,065] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.f810d4b5-3744-472f-93ac-71f7b77c2018
INFO  [2023-01-17 00:51:12,065] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.f810d4b5-3744-472f-93ac-71f7b77c2018
INFO  [2023-01-17 00:51:12,069] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.f810d4b5-3744-472f-93ac-71f7b77c2018] with 1 results within PT0.004028S
INFO  [2023-01-17 00:51:12,069] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.f810d4b5-3744-472f-93ac-71f7b77c2018] with 0 results within PT0.003687S
INFO  [2023-01-17 00:51:12,070] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.f810d4b5-3744-472f-93ac-71f7b77c2018, workerId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_bd5109c1-772c-4f97-847e-01b087bbc962, startTime=2023-01-17T00:51:12.065365, finishTime=2023-01-17T00:51:12.069393) of size 1
INFO  [2023-01-17 00:51:12,070] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.f810d4b5-3744-472f-93ac-71f7b77c2018, workerId=ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID.worker_ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID_1bb8a032-8088-40b5-bb4a-a6677616e344, startTime=2023-01-17T00:51:12.065848, finishTime=2023-01-17T00:51:12.069535) of size 0
INFO  [2023-01-17 00:51:12,070] com.bakdata.conquery.models.execution.ManagedExecution: DONE 802c6204-41ba-414f-a2e2-353f29e69f77 ManagedQuery within PT0.013189S
INFO  [2023-01-17 00:51:12,071] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID QUERIES EXECUTED
INFO  [2023-01-17 00:51:12,071] com.bakdata.conquery.models.execution.ManagedExecution: DONE f810d4b5-3744-472f-93ac-71f7b77c2018 ManagedInternalForm within PT0.014281S
INFO  [2023-01-17 00:51:12,099] com.bakdata.conquery.integration.json.FormTest: ENTITY-DATE-EXPORT-FORM SECONDARY_ID CSV TESTING: results
INFO  [2023-01-17 00:51:12,100] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:12,101] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:12,101] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ENTITY-DATE-EXPORT-FORM SECONDARY_ID, name=ENTITY-DATE-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:12,101] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_1bb8a032-8088-40b5-bb4a-a6677616e344
INFO  [2023-01-17 00:51:12,101] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_bd5109c1-772c-4f97-847e-01b087bbc962
INFO  [2023-01-17 00:51:12,114] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_bd5109c1-772c-4f97-847e-01b087bbc962
INFO  [2023-01-17 00:51:12,114] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:12,115] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ENTITY-DATE-EXPORT-FORM SECONDARY_ID_1bb8a032-8088-40b5-bb4a-a6677616e344
INFO  [2023-01-17 00:51:12,199] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ENTITY-DATE-EXPORT-FORM$20SECONDARY_ID
INFO  [2023-01-17 00:51:12,199] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,256] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ENTITY-DATE-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:12,256] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:12,256] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:12,257] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:12,258] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-17 00:51:12,258] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-17 00:51:12,258] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:12,258] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_8ddc3220-c716-4c30-ba1d-1a18993c7dd0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_8ddc3220-c716-4c30-ba1d-1a18993c7dd0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_b8f18300-d51d-4fa7-8f57-6bd3ea2a62ca are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_b8f18300-d51d-4fa7-8f57-6bd3ea2a62ca are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:12,260] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,364] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,372] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm
INFO  [2023-01-17 00:51:12,372] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm
INFO  [2023-01-17 00:51:12,477] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLES
INFO  [2023-01-17 00:51:12,493] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,610] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:12,611] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:12,611] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:12,611] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-17 00:51:12,611] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[2]/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000302804sINFO  [2023-01-17 00:51:12,642] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:12,642] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@17096a7b)
INFO  [2023-01-17 00:51:12,642] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:12,642] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@2050b14)
INFO  [2023-01-17 00:51:12,642] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@15228ead)
INFO  [2023-01-17 00:51:12,642] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3e311d69), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@1ecad1bd), dateReader=com.bakdata.conquery.util.DateReader@7c067b8c, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:12,645] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:12,645] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:12,645] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ADD DEFAULT SELECT Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:12,659] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:12 +0000] "POST /admin/datasets/ADD%20DEFAULT%20SELECT%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ADD+DEFAULT+SELECT+Test%5B2%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:12,659] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,661] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:12,662] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:12,662] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:12,664] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:12,665] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:12,665] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:12,666] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:12,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:12,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:12,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:12,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:12,667] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:12,667] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:12,668] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:12,668] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ADD$20DEFAULT$20SELECT$20Test[2].vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:12,782] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,787] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:12,832] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,837] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,842] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:12,848] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:12,848] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:12,848] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:12,956] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b35cc83b-b519-4e9f-a304-7965d8ebed1b] in Datasets[[]]
INFO  [2023-01-17 00:51:12,961] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[2].b35cc83b-b519-4e9f-a304-7965d8ebed1b
INFO  [2023-01-17 00:51:12,962] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form ADD$20DEFAULT$20SELECT$20Test[2].b35cc83b-b519-4e9f-a304-7965d8ebed1b
INFO  [2023-01-17 00:51:12,964] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[2].b35cc83b-b519-4e9f-a304-7965d8ebed1b] with 0 results within PT0.001345S
INFO  [2023-01-17 00:51:12,964] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ADD$20DEFAULT$20SELECT$20Test[2].b35cc83b-b519-4e9f-a304-7965d8ebed1b] with 1 results within PT0.002645S
INFO  [2023-01-17 00:51:12,964] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[2].b35cc83b-b519-4e9f-a304-7965d8ebed1b, workerId=ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_8ddc3220-c716-4c30-ba1d-1a18993c7dd0, startTime=2023-01-17T00:51:12.962801, finishTime=2023-01-17T00:51:12.964146) of size 0
INFO  [2023-01-17 00:51:12,964] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ADD$20DEFAULT$20SELECT$20Test[2].b35cc83b-b519-4e9f-a304-7965d8ebed1b, workerId=ADD$20DEFAULT$20SELECT$20Test[2].worker_ADD$20DEFAULT$20SELECT$20Test[2]_b8f18300-d51d-4fa7-8f57-6bd3ea2a62ca, startTime=2023-01-17T00:51:12.961617, finishTime=2023-01-17T00:51:12.964262) of size 1
INFO  [2023-01-17 00:51:12,965] com.bakdata.conquery.models.execution.ManagedExecution: DONE 018d5712-c387-4a2b-a7ac-01942670c4e9 ManagedQuery within PT0.008883S
INFO  [2023-01-17 00:51:12,966] com.bakdata.conquery.models.execution.ManagedExecution: DONE b35cc83b-b519-4e9f-a304-7965d8ebed1b ManagedInternalForm within PT0.00986S
INFO  [2023-01-17 00:51:12,966] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:12,978] com.bakdata.conquery.integration.json.FormTest: ADD DEFAULT SELECT Test CSV TESTING: results
INFO  [2023-01-17 00:51:12,979] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ADD DEFAULT SELECT Test[2]
INFO  [2023-01-17 00:51:12,980] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-17 00:51:12,980] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ADD DEFAULT SELECT Test[2], name=ADD DEFAULT SELECT Test[2]]
INFO  [2023-01-17 00:51:12,980] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[2]_8ddc3220-c716-4c30-ba1d-1a18993c7dd0
INFO  [2023-01-17 00:51:12,980] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ADD DEFAULT SELECT Test[2]_b8f18300-d51d-4fa7-8f57-6bd3ea2a62ca
INFO  [2023-01-17 00:51:13,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ADD DEFAULT SELECT Test[2]
INFO  [2023-01-17 00:51:13,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[2]_8ddc3220-c716-4c30-ba1d-1a18993c7dd0
INFO  [2023-01-17 00:51:13,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ADD DEFAULT SELECT Test[2]_b8f18300-d51d-4fa7-8f57-6bd3ea2a62ca
INFO  [2023-01-17 00:51:13,078] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ADD$20DEFAULT$20SELECT$20Test[2]
INFO  [2023-01-17 00:51:13,078] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,255] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ADD DEFAULT SELECT Test
INFO  [2023-01-17 00:51:13,255] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test WITH SELECT SET Test
INFO  [2023-01-17 00:51:13,255] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:13,255] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:13,256] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-17 00:51:13,256] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-17 00:51:13,257] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:13,257] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_bc043c21-e62f-4c5a-9566-167d3897a1ef are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_bc043c21-e62f-4c5a-9566-167d3897a1ef are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_cfbe60d0-1bc5-4e82-98d0-29c35aba6b4f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_cfbe60d0-1bc5-4e82-98d0-29c35aba6b4f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:13,263] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,367] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,374] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,375] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-17 00:51:13,375] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table WITH$20SELECT$20SET$20Test.vers_stamm
INFO  [2023-01-17 00:51:13,479] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test IMPORT TABLES
INFO  [2023-01-17 00:51:13,495] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,606] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:13,607] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:13,608] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:13,608] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-17 00:51:13,608] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000397866sINFO  [2023-01-17 00:51:13,648] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:13,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@3b80c4c7)
INFO  [2023-01-17 00:51:13,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@71e48689)
INFO  [2023-01-17 00:51:13,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:13,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@4014c64f)
INFO  [2023-01-17 00:51:13,649] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3bcb8e42), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@689ea11c), dateReader=com.bakdata.conquery.util.DateReader@103a5472, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:13,653] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:13,653] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:13,653] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_WITH SELECT SET Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:13,680] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into WITH$20SELECT$20SET$20Test.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:13 +0000] "POST /admin/datasets/WITH%20SELECT%20SET%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_WITH+SELECT+SET+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:51:13,681] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,682] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:13,682] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:13,683] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:13,685] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:13,686] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:13,686] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:13,688] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:13,688] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:13,688] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:13,688] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:13,689] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:13,691] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:13,691] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:13,691] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:13,691] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received WITH$20SELECT$20SET$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:13,796] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,802] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:13,840] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,845] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,850] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:13,857] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:13,858] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:13,858] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:13,965] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d77e046d-4a1c-49c0-9668-158e30da6b3c] in Datasets[[]]
INFO  [2023-01-17 00:51:13,971] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form WITH$20SELECT$20SET$20Test.d77e046d-4a1c-49c0-9668-158e30da6b3c
INFO  [2023-01-17 00:51:13,971] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form WITH$20SELECT$20SET$20Test.d77e046d-4a1c-49c0-9668-158e30da6b3c
INFO  [2023-01-17 00:51:13,973] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[WITH$20SELECT$20SET$20Test.d77e046d-4a1c-49c0-9668-158e30da6b3c] with 0 results within PT0.002226S
INFO  [2023-01-17 00:51:13,974] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=WITH$20SELECT$20SET$20Test.d77e046d-4a1c-49c0-9668-158e30da6b3c, workerId=WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_cfbe60d0-1bc5-4e82-98d0-29c35aba6b4f, startTime=2023-01-17T00:51:13.971266, finishTime=2023-01-17T00:51:13.973492) of size 0
INFO  [2023-01-17 00:51:13,974] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[WITH$20SELECT$20SET$20Test.d77e046d-4a1c-49c0-9668-158e30da6b3c] with 1 results within PT0.003087S
INFO  [2023-01-17 00:51:13,976] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=WITH$20SELECT$20SET$20Test.d77e046d-4a1c-49c0-9668-158e30da6b3c, workerId=WITH$20SELECT$20SET$20Test.worker_WITH$20SELECT$20SET$20Test_bc043c21-e62f-4c5a-9566-167d3897a1ef, startTime=2023-01-17T00:51:13.971659, finishTime=2023-01-17T00:51:13.974746) of size 1
INFO  [2023-01-17 00:51:13,976] com.bakdata.conquery.models.execution.ManagedExecution: DONE c27fc855-5ad8-46fe-bd31-c352f7f9e355 ManagedQuery within PT0.010608S
INFO  [2023-01-17 00:51:13,977] com.bakdata.conquery.models.execution.ManagedExecution: DONE d77e046d-4a1c-49c0-9668-158e30da6b3c ManagedInternalForm within PT0.011539S
INFO  [2023-01-17 00:51:13,977] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:13,987] com.bakdata.conquery.integration.json.FormTest: WITH SELECT SET Test CSV TESTING: results
INFO  [2023-01-17 00:51:13,989] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast WITH SELECT SET Test
INFO  [2023-01-17 00:51:13,989] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-17 00:51:13,990] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=WITH SELECT SET Test, name=WITH SELECT SET Test]
INFO  [2023-01-17 00:51:13,990] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_WITH SELECT SET Test_bc043c21-e62f-4c5a-9566-167d3897a1ef
INFO  [2023-01-17 00:51:13,990] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_WITH SELECT SET Test_cfbe60d0-1bc5-4e82-98d0-29c35aba6b4f
INFO  [2023-01-17 00:51:14,057] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow WITH SELECT SET Test
INFO  [2023-01-17 00:51:14,062] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_WITH SELECT SET Test_bc043c21-e62f-4c5a-9566-167d3897a1ef
INFO  [2023-01-17 00:51:14,062] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_WITH SELECT SET Test_cfbe60d0-1bc5-4e82-98d0-29c35aba6b4f
INFO  [2023-01-17 00:51:14,091] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of WITH$20SELECT$20SET$20Test
INFO  [2023-01-17 00:51:14,091] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,264] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test WITH SELECT SET Test
INFO  [2023-01-17 00:51:14,265] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:14,265] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:14,265] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:14,266] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-17 00:51:14,266] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-17 00:51:14,266] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:14,266] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:14,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_668f00f8-beca-4f45-a4a0-6c9bc4903961 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:14,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_668f00f8-beca-4f45-a4a0-6c9bc4903961 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:14,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:14,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_e4d38b55-9fc6-4853-82e7-96be5e510589 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:14,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_e4d38b55-9fc6-4853-82e7-96be5e510589 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:14,269] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:14,273] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,379] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,379] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_stamm
INFO  [2023-01-17 00:51:14,379] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_stamm
INFO  [2023-01-17 00:51:14,420] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_tage_range
INFO  [2023-01-17 00:51:14,420] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test.vers_tage_range
INFO  [2023-01-17 00:51:14,524] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-17 00:51:14,551] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,672] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:14,672] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:14,673] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:14,673] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:14,673] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-17 00:51:14,673] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.030894855sINFO  [2023-01-17 00:51:14,714] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:14,714] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@590112b)
INFO  [2023-01-17 00:51:14,714] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:14,714] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@254ff7e7)
INFO  [2023-01-17 00:51:14,714] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@116cf73e)
INFO  [2023-01-17 00:51:14,714] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@47880789), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@f8761fc), dateReader=com.bakdata.conquery.util.DateReader@4092b19b, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:14,717] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:14,717] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000620887sINFO  [2023-01-17 00:51:14,736] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:14,736] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@67ae23bc)
INFO  [2023-01-17 00:51:14,736] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@1517a7ce)
INFO  [2023-01-17 00:51:14,736] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@34918bed), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@59adb6bc), dateReader=com.bakdata.conquery.util.DateReader@73da5b36, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-17 00:51:14,738] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:14,738] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:14,738] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:14,738] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:14,754] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:14 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:14,756] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:14,757] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:14,757] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:14,760] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:14,761] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:14,761] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:14,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:14,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:14,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:14,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:14,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:14,765] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:14,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:14,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:14,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:14,791] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test.vers_tage_range
INFO  [2023-01-17 00:51:14,792] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:14,792] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:14,792] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:14,792] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:14 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:14,793] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:14,793] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:14,793] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:14,793] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.0
INFO  [2023-01-17 00:51:14,794] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.1
INFO  [2023-01-17 00:51:14,794] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.2
INFO  [2023-01-17 00:51:14,794] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.3
WARN  [2023-01-17 00:51:14,794] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:14,836] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.5
INFO  [2023-01-17 00:51:14,836] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.4
INFO  [2023-01-17 00:51:14,836] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.7
INFO  [2023-01-17 00:51:14,836] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test.vers_tage_range.vers_tage_range.6
INFO  [2023-01-17 00:51:14,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,947] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:14,992] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:14,998] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,003] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:15,010] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,011] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:15,011] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:15,124] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[849e4eb4-1ec2-4e8c-a4d5-e1230297efc0] in Datasets[[]]
INFO  [2023-01-17 00:51:15,146] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test.849e4eb4-1ec2-4e8c-a4d5-e1230297efc0
INFO  [2023-01-17 00:51:15,146] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test.849e4eb4-1ec2-4e8c-a4d5-e1230297efc0
INFO  [2023-01-17 00:51:15,149] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test.849e4eb4-1ec2-4e8c-a4d5-e1230297efc0] with 1 results within PT0.00356S
INFO  [2023-01-17 00:51:15,149] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test.849e4eb4-1ec2-4e8c-a4d5-e1230297efc0] with 1 results within PT0.003672S
INFO  [2023-01-17 00:51:15,150] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test.849e4eb4-1ec2-4e8c-a4d5-e1230297efc0, workerId=REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_668f00f8-beca-4f45-a4a0-6c9bc4903961, startTime=2023-01-17T00:51:15.146076, finishTime=2023-01-17T00:51:15.149748) of size 1
INFO  [2023-01-17 00:51:15,150] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test.849e4eb4-1ec2-4e8c-a4d5-e1230297efc0, workerId=REL-EXPORT-FORM$20Test.worker_REL-EXPORT-FORM$20Test_e4d38b55-9fc6-4853-82e7-96be5e510589, startTime=2023-01-17T00:51:15.146084, finishTime=2023-01-17T00:51:15.149644) of size 1
INFO  [2023-01-17 00:51:15,151] com.bakdata.conquery.models.execution.ManagedExecution: DONE ddf4545d-3c43-4e6c-bd04-c76fe2eea54e ManagedQuery within PT0.026331S
INFO  [2023-01-17 00:51:15,152] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:15,152] com.bakdata.conquery.models.execution.ManagedExecution: DONE 849e4eb4-1ec2-4e8c-a4d5-e1230297efc0 ManagedInternalForm within PT0.02738S
INFO  [2023-01-17 00:51:15,171] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-17 00:51:15,174] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:15,174] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-17 00:51:15,174] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test, name=REL-EXPORT-FORM Test]
INFO  [2023-01-17 00:51:15,174] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test_e4d38b55-9fc6-4853-82e7-96be5e510589
INFO  [2023-01-17 00:51:15,174] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test_668f00f8-beca-4f45-a4a0-6c9bc4903961
INFO  [2023-01-17 00:51:15,270] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test_e4d38b55-9fc6-4853-82e7-96be5e510589
INFO  [2023-01-17 00:51:15,270] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test_668f00f8-beca-4f45-a4a0-6c9bc4903961
INFO  [2023-01-17 00:51:15,270] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:15,295] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test
INFO  [2023-01-17 00:51:15,295] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,418] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:15,418] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:15,418] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:15,418] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:15,420] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-17 00:51:15,420] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-17 00:51:15,420] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:15,420] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_5c22f010-c4b3-4e1a-b55f-8cf361589497 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_5c22f010-c4b3-4e1a-b55f-8cf361589497 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_0391009a-4e84-4721-ba2c-0b51258f7a98 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_0391009a-4e84-4721-ba2c-0b51258f7a98 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:15,422] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:15,526] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,539] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,540] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_stamm
INFO  [2023-01-17 00:51:15,540] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_stamm
INFO  [2023-01-17 00:51:15,540] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_tage_range
INFO  [2023-01-17 00:51:15,540] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[1].vers_tage_range
INFO  [2023-01-17 00:51:15,645] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-17 00:51:15,668] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:15,781] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:15,782] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:15,782] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:15,782] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:15,782] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-17 00:51:15,782] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[1]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.031397126sINFO  [2023-01-17 00:51:15,824] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:15,824] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@49dbd8fb)
INFO  [2023-01-17 00:51:15,824] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:15,824] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@76b2c2cf)
INFO  [2023-01-17 00:51:15,824] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@12245eaa)
INFO  [2023-01-17 00:51:15,824] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@95746d8), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@620642d0), dateReader=com.bakdata.conquery.util.DateReader@64e16815, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:15,828] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:15,828] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[1]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000638232sINFO  [2023-01-17 00:51:15,847] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:15,847] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@773a13e7)
INFO  [2023-01-17 00:51:15,847] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@58a24807)
INFO  [2023-01-17 00:51:15,847] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@3e4b5ba1), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@d19bb36), dateReader=com.bakdata.conquery.util.DateReader@5d596887, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-17 00:51:15,849] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[1]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:15,849] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:15,849] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[1]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:15,849] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[1]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:15,866] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[1].vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:15 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B1%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:15,867] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:15,868] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:15,868] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:15,870] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:15,870] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:15,870] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:15,871] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.0
WARN  [2023-01-17 00:51:15,872] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:15,879] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:15,879] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:15,880] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:15,880] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:15,880] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:15,880] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:15,881] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:15,899] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[1].vers_tage_range
INFO  [2023-01-17 00:51:15,900] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:15,900] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:15,900] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:15,900] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:15 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B1%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:15,900] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:15,901] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:15,901] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:15,901] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.0
INFO  [2023-01-17 00:51:15,901] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.2
INFO  [2023-01-17 00:51:15,901] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.1
WARN  [2023-01-17 00:51:15,901] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:15,901] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.3
INFO  [2023-01-17 00:51:15,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.4
INFO  [2023-01-17 00:51:15,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.5
INFO  [2023-01-17 00:51:15,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.6
INFO  [2023-01-17 00:51:15,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[1].vers_tage_range.vers_tage_range.7
INFO  [2023-01-17 00:51:16,049] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,054] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:16,098] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,103] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,108] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:16,114] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,114] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:16,115] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:16,223] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b0a3b9e7-a0e4-442f-8c1e-9e7232387b65] in Datasets[[]]
INFO  [2023-01-17 00:51:16,228] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[1].b0a3b9e7-a0e4-442f-8c1e-9e7232387b65
INFO  [2023-01-17 00:51:16,228] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[1].b0a3b9e7-a0e4-442f-8c1e-9e7232387b65
INFO  [2023-01-17 00:51:16,231] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[1].b0a3b9e7-a0e4-442f-8c1e-9e7232387b65] with 1 results within PT0.003397S
INFO  [2023-01-17 00:51:16,231] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[1].b0a3b9e7-a0e4-442f-8c1e-9e7232387b65] with 1 results within PT0.003536S
INFO  [2023-01-17 00:51:16,232] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[1].b0a3b9e7-a0e4-442f-8c1e-9e7232387b65, workerId=REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_5c22f010-c4b3-4e1a-b55f-8cf361589497, startTime=2023-01-17T00:51:16.228247, finishTime=2023-01-17T00:51:16.231644) of size 1
INFO  [2023-01-17 00:51:16,232] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[1].b0a3b9e7-a0e4-442f-8c1e-9e7232387b65, workerId=REL-EXPORT-FORM$20Test[1].worker_REL-EXPORT-FORM$20Test[1]_0391009a-4e84-4721-ba2c-0b51258f7a98, startTime=2023-01-17T00:51:16.228253, finishTime=2023-01-17T00:51:16.231789) of size 1
INFO  [2023-01-17 00:51:16,232] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1aa4c521-9672-4be3-8b91-f9a4e0af3a11 ManagedQuery within PT0.008541S
INFO  [2023-01-17 00:51:16,233] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:16,233] com.bakdata.conquery.models.execution.ManagedExecution: DONE b0a3b9e7-a0e4-442f-8c1e-9e7232387b65 ManagedInternalForm within PT0.009399S
INFO  [2023-01-17 00:51:16,248] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-17 00:51:16,250] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[1]
INFO  [2023-01-17 00:51:16,250] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-17 00:51:16,250] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[1], name=REL-EXPORT-FORM Test[1]]
INFO  [2023-01-17 00:51:16,251] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[1]_0391009a-4e84-4721-ba2c-0b51258f7a98
INFO  [2023-01-17 00:51:16,251] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[1]_5c22f010-c4b3-4e1a-b55f-8cf361589497
INFO  [2023-01-17 00:51:16,320] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[1]
INFO  [2023-01-17 00:51:16,321] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[1]_5c22f010-c4b3-4e1a-b55f-8cf361589497
INFO  [2023-01-17 00:51:16,321] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[1]_0391009a-4e84-4721-ba2c-0b51258f7a98
INFO  [2023-01-17 00:51:16,402] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[1]
INFO  [2023-01-17 00:51:16,402] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,521] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:16,521] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:16,521] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:16,521] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:16,523] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-17 00:51:16,523] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-17 00:51:16,523] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:16,523] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:16,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_82522bae-eda9-4c3c-814a-dc0f4112ff64 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:16,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_82522bae-eda9-4c3c-814a-dc0f4112ff64 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:16,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:16,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_7dafed74-c31a-4957-b2bf-c18511bbd4b4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:16,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_7dafed74-c31a-4957-b2bf-c18511bbd4b4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:16,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:16,529] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,629] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,635] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,635] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_stamm
INFO  [2023-01-17 00:51:16,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_stamm
INFO  [2023-01-17 00:51:16,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_tage_range
INFO  [2023-01-17 00:51:16,636] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[2].vers_tage_range
INFO  [2023-01-17 00:51:16,741] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-17 00:51:16,768] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:16,881] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:16,882] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:16,882] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:16,882] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:16,882] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-17 00:51:16,882] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[2]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.033241262sINFO  [2023-01-17 00:51:16,927] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:16,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2869438a)
INFO  [2023-01-17 00:51:16,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:16,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@159addbc)
INFO  [2023-01-17 00:51:16,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@6da3f287)
INFO  [2023-01-17 00:51:16,927] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@6b390b44), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@29e24b95), dateReader=com.bakdata.conquery.util.DateReader@5fc18c90, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:16,930] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:16,930] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[2]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000666945sINFO  [2023-01-17 00:51:16,950] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:16,950] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@f2b1bdb)
INFO  [2023-01-17 00:51:16,950] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@3f05991d)
INFO  [2023-01-17 00:51:16,950] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@329702f9), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@5665c159), dateReader=com.bakdata.conquery.util.DateReader@166500a8, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-17 00:51:16,952] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[2]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:16,952] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:16,952] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[2]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:16,952] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[2]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:16,986] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[2].vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:16 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B2%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:16,988] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:16,988] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:16,988] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:16,991] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:16,991] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:16,991] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:16,992] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:16,992] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:16,993] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:16,993] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:16,994] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:16,994] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:16,998] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:16,998] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:16,998] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:17,006] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[2].vers_tage_range
INFO  [2023-01-17 00:51:17,007] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:17,007] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:17,007] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:17,007] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:17,008] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:17,008] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:17,008] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.2
WARN  [2023-01-17 00:51:17,011] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:17,014] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:17 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B2%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:17,023] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.3
INFO  [2023-01-17 00:51:17,024] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.5
INFO  [2023-01-17 00:51:17,024] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.7
INFO  [2023-01-17 00:51:17,026] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.0
INFO  [2023-01-17 00:51:17,026] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.1
INFO  [2023-01-17 00:51:17,026] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.4
INFO  [2023-01-17 00:51:17,026] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[2].vers_tage_range.vers_tage_range.6
INFO  [2023-01-17 00:51:17,131] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,136] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:17,176] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,182] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,249] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:17,254] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,254] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:17,254] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:17,363] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4c2fcbb5-1b70-430d-b931-5e3e8d71fea6] in Datasets[[]]
INFO  [2023-01-17 00:51:17,374] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[2].4c2fcbb5-1b70-430d-b931-5e3e8d71fea6
INFO  [2023-01-17 00:51:17,374] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[2].4c2fcbb5-1b70-430d-b931-5e3e8d71fea6
INFO  [2023-01-17 00:51:17,378] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[2].4c2fcbb5-1b70-430d-b931-5e3e8d71fea6] with 1 results within PT0.003541S
INFO  [2023-01-17 00:51:17,378] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[2].4c2fcbb5-1b70-430d-b931-5e3e8d71fea6] with 1 results within PT0.003796S
INFO  [2023-01-17 00:51:17,379] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[2].4c2fcbb5-1b70-430d-b931-5e3e8d71fea6, workerId=REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_82522bae-eda9-4c3c-814a-dc0f4112ff64, startTime=2023-01-17T00:51:17.374555, finishTime=2023-01-17T00:51:17.378096) of size 1
INFO  [2023-01-17 00:51:17,379] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[2].4c2fcbb5-1b70-430d-b931-5e3e8d71fea6, workerId=REL-EXPORT-FORM$20Test[2].worker_REL-EXPORT-FORM$20Test[2]_7dafed74-c31a-4957-b2bf-c18511bbd4b4, startTime=2023-01-17T00:51:17.374741, finishTime=2023-01-17T00:51:17.378537) of size 1
INFO  [2023-01-17 00:51:17,379] com.bakdata.conquery.models.execution.ManagedExecution: DONE f95e2b68-8416-4475-b5ee-ae9a8d9d4234 ManagedQuery within PT0.015899S
INFO  [2023-01-17 00:51:17,380] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4c2fcbb5-1b70-430d-b931-5e3e8d71fea6 ManagedInternalForm within PT0.017051S
INFO  [2023-01-17 00:51:17,380] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:17,405] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-17 00:51:17,409] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[2]
INFO  [2023-01-17 00:51:17,410] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-17 00:51:17,410] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[2], name=REL-EXPORT-FORM Test[2]]
INFO  [2023-01-17 00:51:17,410] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[2]_82522bae-eda9-4c3c-814a-dc0f4112ff64
INFO  [2023-01-17 00:51:17,410] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[2]_7dafed74-c31a-4957-b2bf-c18511bbd4b4
INFO  [2023-01-17 00:51:17,444] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[2]
INFO  [2023-01-17 00:51:17,444] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[2]_82522bae-eda9-4c3c-814a-dc0f4112ff64
INFO  [2023-01-17 00:51:17,444] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[2]_7dafed74-c31a-4957-b2bf-c18511bbd4b4
INFO  [2023-01-17 00:51:17,544] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[2]
INFO  [2023-01-17 00:51:17,544] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,563] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:17,563] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:17,563] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:17,563] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:17,564] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-17 00:51:17,564] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-17 00:51:17,564] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:17,564] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:17,566] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_2799efb7-6c52-49dd-bf61-059f5264b82e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:17,566] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_2799efb7-6c52-49dd-bf61-059f5264b82e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:17,566] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:17,566] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_f2429ffb-08bf-4d9c-b460-49d39f145107 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:17,566] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_f2429ffb-08bf-4d9c-b460-49d39f145107 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:17,566] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:17,570] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,670] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,677] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,678] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_stamm
INFO  [2023-01-17 00:51:17,678] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_stamm
INFO  [2023-01-17 00:51:17,678] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_tage_range
INFO  [2023-01-17 00:51:17,678] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[3].vers_tage_range
INFO  [2023-01-17 00:51:17,783] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-17 00:51:17,807] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:17,928] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:17,929] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:17,930] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:17,930] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:17,930] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-17 00:51:17,930] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[3]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.029371144sINFO  [2023-01-17 00:51:17,969] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:17,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@5ef4ff0)
INFO  [2023-01-17 00:51:17,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:17,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@1f45e6c2)
INFO  [2023-01-17 00:51:17,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@2d6ce201), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@19c0b946), dateReader=com.bakdata.conquery.util.DateReader@38e252a3, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:17,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@ee5fb3)
INFO  [2023-01-17 00:51:17,972] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[3]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:17,972] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[3]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000608608sINFO  [2023-01-17 00:51:17,992] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:17,992] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@484180a7)
INFO  [2023-01-17 00:51:17,992] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@1c45d692), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@2740886c), dateReader=com.bakdata.conquery.util.DateReader@74b63518, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-17 00:51:17,992] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@ae4159d)
INFO  [2023-01-17 00:51:17,994] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[3]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:17,994] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:17,994] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[3]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:17,994] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[3]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:18,015] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[3].vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:18 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B3%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B3%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:18,016] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:18,017] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:18,017] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:18,019] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:18,020] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:18,020] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:18,021] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:18,021] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.2
WARN  [2023-01-17 00:51:18,021] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:18,021] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:18,022] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:18,022] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:18,023] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:18,023] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:18,023] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:18,043] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[3].vers_tage_range
INFO  [2023-01-17 00:51:18,043] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:18,043] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:18,043] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [17/Jan/2023:00:51:18 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B3%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B3%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:18,044] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,044] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:18,044] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:18,044] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:18,045] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.0
INFO  [2023-01-17 00:51:18,045] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.2
INFO  [2023-01-17 00:51:18,045] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.1
INFO  [2023-01-17 00:51:18,045] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.3
WARN  [2023-01-17 00:51:18,045] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:18,088] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.5
INFO  [2023-01-17 00:51:18,088] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.4
INFO  [2023-01-17 00:51:18,088] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.7
INFO  [2023-01-17 00:51:18,088] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[3].vers_tage_range.vers_tage_range.6
INFO  [2023-01-17 00:51:18,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,199] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:18,247] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,252] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,258] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:18,264] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,264] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:18,264] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:18,373] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1f7e706d-aff3-485c-ba71-84dcf8a43458] in Datasets[[]]
INFO  [2023-01-17 00:51:18,381] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[3].1f7e706d-aff3-485c-ba71-84dcf8a43458
INFO  [2023-01-17 00:51:18,381] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[3].1f7e706d-aff3-485c-ba71-84dcf8a43458
INFO  [2023-01-17 00:51:18,383] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[3].1f7e706d-aff3-485c-ba71-84dcf8a43458] with 1 results within PT0.002015S
INFO  [2023-01-17 00:51:18,383] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[3].1f7e706d-aff3-485c-ba71-84dcf8a43458] with 1 results within PT0.002187S
INFO  [2023-01-17 00:51:18,383] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[3].1f7e706d-aff3-485c-ba71-84dcf8a43458, workerId=REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_2799efb7-6c52-49dd-bf61-059f5264b82e, startTime=2023-01-17T00:51:18.381051, finishTime=2023-01-17T00:51:18.383066) of size 1
INFO  [2023-01-17 00:51:18,384] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[3].1f7e706d-aff3-485c-ba71-84dcf8a43458, workerId=REL-EXPORT-FORM$20Test[3].worker_REL-EXPORT-FORM$20Test[3]_f2429ffb-08bf-4d9c-b460-49d39f145107, startTime=2023-01-17T00:51:18.381387, finishTime=2023-01-17T00:51:18.383574) of size 1
INFO  [2023-01-17 00:51:18,384] com.bakdata.conquery.models.execution.ManagedExecution: DONE 92bb8e45-b61e-4191-bfc6-4a97b3819078 ManagedQuery within PT0.010411S
INFO  [2023-01-17 00:51:18,385] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1f7e706d-aff3-485c-ba71-84dcf8a43458 ManagedInternalForm within PT0.011294S
INFO  [2023-01-17 00:51:18,385] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:18,397] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-17 00:51:18,398] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[3]
INFO  [2023-01-17 00:51:18,399] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-17 00:51:18,399] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[3], name=REL-EXPORT-FORM Test[3]]
INFO  [2023-01-17 00:51:18,400] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[3]_f2429ffb-08bf-4d9c-b460-49d39f145107
INFO  [2023-01-17 00:51:18,400] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[3]_2799efb7-6c52-49dd-bf61-059f5264b82e
INFO  [2023-01-17 00:51:18,464] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[3]
INFO  [2023-01-17 00:51:18,466] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[3]_f2429ffb-08bf-4d9c-b460-49d39f145107
INFO  [2023-01-17 00:51:18,466] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[3]_2799efb7-6c52-49dd-bf61-059f5264b82e
INFO  [2023-01-17 00:51:18,548] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[3]
INFO  [2023-01-17 00:51:18,548] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,671] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:18,672] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:18,672] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:18,672] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:18,673] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:18,673] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:18,673] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:18,673] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:18,675] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_3697eb5d-e86d-4784-a272-18f650ae7a33 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:18,675] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_3697eb5d-e86d-4784-a272-18f650ae7a33 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:18,675] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:18,675] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_21d7a8a9-ea7c-4c00-aaaa-f856ed0666f6 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:18,675] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_21d7a8a9-ea7c-4c00-aaaa-f856ed0666f6 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:18,675] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:18,679] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,784] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[REL-EXPORT-FORM$20SECONDARY_ID.secondary]
INFO  [2023-01-17 00:51:18,785] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,785] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId REL-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-17 00:51:18,785] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId REL-EXPORT-FORM$20SECONDARY_ID.secondary
INFO  [2023-01-17 00:51:18,892] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:18,892] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-17 00:51:18,892] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm
INFO  [2023-01-17 00:51:18,892] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:18,892] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:18,997] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID IMPORT TABLES
INFO  [2023-01-17 00:51:19,019] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,130] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID IMPORT CONCEPTS
INFO  [2023-01-17 00:51:19,131] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:19,131] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:19,131] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:19,131] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:19,131] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.006734691sINFO  [2023-01-17 00:51:19,188] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:19,188] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@79f9fccd)
INFO  [2023-01-17 00:51:19,188] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:19,188] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@16c474b3)
INFO  [2023-01-17 00:51:19,188] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@72f36291)
INFO  [2023-01-17 00:51:19,188] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@7bd61f3e), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@316086af), dateReader=com.bakdata.conquery.util.DateReader@2d76134c, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:19,191] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:19,191] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000787951sINFO  [2023-01-17 00:51:19,211] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-17 00:51:19,211] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@58c2259a)
INFO  [2023-01-17 00:51:19,211] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:19,211] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:19,214] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:19,214] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:19,214] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM SECONDARY_ID/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:19,214] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM SECONDARY_ID/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:19,243] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:19 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+SECONDARY_ID%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:19,245] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:19,246] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:19,246] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:19,248] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:19,248] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:19,248] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:19,250] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:19,250] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:19,250] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:19,251] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:19,251] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.3
WARN  [2023-01-17 00:51:19,251] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:19,251] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:19,252] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:19,253] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:19,261] com.bakdata.conquery.models.jobs.ImportJob: Importing table into REL-EXPORT-FORM$20SECONDARY_ID.table
INFO  [2023-01-17 00:51:19,261] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:19,262] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:19,262] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [17/Jan/2023:00:51:19 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20SECONDARY_ID/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+SECONDARY_ID%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:19,262] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,262] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:19,262] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:19,262] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20SECONDARY_ID.table.table], containing 6 entries.
WARN  [2023-01-17 00:51:19,262] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:19,263] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20SECONDARY_ID.table.table.0
INFO  [2023-01-17 00:51:19,376] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,381] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:19,388] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,393] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:19,401] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,401] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:19,401] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:19,508] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[e3499eca-8197-4449-8aca-e1270975f6ec] in Datasets[[]]
INFO  [2023-01-17 00:51:19,517] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20SECONDARY_ID.e3499eca-8197-4449-8aca-e1270975f6ec
INFO  [2023-01-17 00:51:19,517] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20SECONDARY_ID.e3499eca-8197-4449-8aca-e1270975f6ec
INFO  [2023-01-17 00:51:19,521] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20SECONDARY_ID.e3499eca-8197-4449-8aca-e1270975f6ec] with 0 results within PT0.003592S
INFO  [2023-01-17 00:51:19,521] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20SECONDARY_ID.e3499eca-8197-4449-8aca-e1270975f6ec] with 1 results within PT0.004248S
INFO  [2023-01-17 00:51:19,522] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20SECONDARY_ID.e3499eca-8197-4449-8aca-e1270975f6ec, workerId=REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_21d7a8a9-ea7c-4c00-aaaa-f856ed0666f6, startTime=2023-01-17T00:51:19.517717, finishTime=2023-01-17T00:51:19.521309) of size 0
INFO  [2023-01-17 00:51:19,522] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20SECONDARY_ID.e3499eca-8197-4449-8aca-e1270975f6ec, workerId=REL-EXPORT-FORM$20SECONDARY_ID.worker_REL-EXPORT-FORM$20SECONDARY_ID_3697eb5d-e86d-4784-a272-18f650ae7a33, startTime=2023-01-17T00:51:19.517716, finishTime=2023-01-17T00:51:19.521964) of size 1
INFO  [2023-01-17 00:51:19,522] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7a8b6540-5f20-4613-af9b-07137b1df561 ManagedQuery within PT0.013677S
INFO  [2023-01-17 00:51:19,523] com.bakdata.conquery.models.execution.ManagedExecution: DONE e3499eca-8197-4449-8aca-e1270975f6ec ManagedInternalForm within PT0.014803S
INFO  [2023-01-17 00:51:19,523] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID QUERIES EXECUTED
INFO  [2023-01-17 00:51:19,541] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM SECONDARY_ID CSV TESTING: results
INFO  [2023-01-17 00:51:19,544] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:19,545] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:19,545] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM SECONDARY_ID, name=REL-EXPORT-FORM SECONDARY_ID]
INFO  [2023-01-17 00:51:19,545] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM SECONDARY_ID_3697eb5d-e86d-4784-a272-18f650ae7a33
INFO  [2023-01-17 00:51:19,545] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM SECONDARY_ID_21d7a8a9-ea7c-4c00-aaaa-f856ed0666f6
INFO  [2023-01-17 00:51:19,580] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM SECONDARY_ID_21d7a8a9-ea7c-4c00-aaaa-f856ed0666f6
INFO  [2023-01-17 00:51:19,580] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM SECONDARY_ID_3697eb5d-e86d-4784-a272-18f650ae7a33
INFO  [2023-01-17 00:51:19,580] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:19,670] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20SECONDARY_ID
INFO  [2023-01-17 00:51:19,670] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,707] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM SECONDARY_ID
INFO  [2023-01-17 00:51:19,708] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:19,708] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:19,708] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:19,709] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-17 00:51:19,709] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:19,709] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-17 00:51:19,709] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_f4211af8-09b4-4400-9f10-9501c17837e5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_f4211af8-09b4-4400-9f10-9501c17837e5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_887f312a-2568-4f9a-8ba6-4fc0c7b7ae80 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_887f312a-2568-4f9a-8ba6-4fc0c7b7ae80 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:19,711] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:19,815] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,823] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:19,823] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_stamm
INFO  [2023-01-17 00:51:19,823] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_stamm
INFO  [2023-01-17 00:51:19,823] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_tage_range
INFO  [2023-01-17 00:51:19,823] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL-EXPORT-FORM$20Test[4].vers_tage_range
INFO  [2023-01-17 00:51:19,928] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLES
INFO  [2023-01-17 00:51:19,954] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,068] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT CONCEPTS
INFO  [2023-01-17 00:51:20,069] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:20,069] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:20,069] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:20,069] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 2.3 KiB in total
INFO  [2023-01-17 00:51:20,069] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[4]/vers_stamm.import.json, tag=Optional.empty)
████████████████████████████▌                     ▌  57%	est. time remaining: 0.036169767sINFO  [2023-01-17 00:51:20,117] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:20,117] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@15b97cdd)
INFO  [2023-01-17 00:51:20,117] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:20,117] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@676897e2)
INFO  [2023-01-17 00:51:20,117] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@72b3b0b)
INFO  [2023-01-17 00:51:20,117] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@5cecf41c), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2ea902cc), dateReader=com.bakdata.conquery.util.DateReader@1eea822d, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:20,120] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[4]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:20,120] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[4]/vers_tage_range.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000700404sINFO  [2023-01-17 00:51:20,140] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:20,140] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@18ce961)
INFO  [2023-01-17 00:51:20,140] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@4c2d5c27)
INFO  [2023-01-17 00:51:20,140] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=21, nullLines=0), minParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=14975, maxValue=15522), dateReader=com.bakdata.conquery.util.DateReader@2b3e0caf), maxParser=DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=15614, maxValue=15705), dateReader=com.bakdata.conquery.util.DateReader@104941c0), dateReader=com.bakdata.conquery.util.DateReader@76908e67, onlyQuarters=false, maxValue=15705, minValue=14975, anyOpen=false)
INFO  [2023-01-17 00:51:20,143] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[4]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:20,143] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:20,143] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[4]/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:20,143] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL-EXPORT-FORM Test[4]/vers_tage_range.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:20,163] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into REL-EXPORT-FORM$20Test[4].vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:20 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B4%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B4%5D%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:20,165] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:20,166] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:20,166] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:20,170] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:20,170] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:20,170] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:20,172] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:20,174] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:20,174] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.4
WARN  [2023-01-17 00:51:20,175] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:20,175] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:20,175] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:20,175] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.5
INFO  [2023-01-17 00:51:20,176] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:20,185] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_tage_range into REL-EXPORT-FORM$20Test[4].vers_tage_range
INFO  [2023-01-17 00:51:20,186] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:20,186] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:20,186] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [17/Jan/2023:00:51:20 +0000] "POST /admin/datasets/REL-EXPORT-FORM%20Test%5B4%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL-EXPORT-FORM+Test%5B4%5D%2Fvers_tage_range.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:20,193] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,194] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:20,194] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:20,194] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range], containing 21 entries.
INFO  [2023-01-17 00:51:20,194] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.0
INFO  [2023-01-17 00:51:20,195] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.1
INFO  [2023-01-17 00:51:20,195] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.2
INFO  [2023-01-17 00:51:20,195] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.3
WARN  [2023-01-17 00:51:20,195] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:20,236] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.5
INFO  [2023-01-17 00:51:20,236] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.4
INFO  [2023-01-17 00:51:20,236] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.7
INFO  [2023-01-17 00:51:20,236] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL-EXPORT-FORM$20Test[4].vers_tage_range.vers_tage_range.6
INFO  [2023-01-17 00:51:20,341] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,347] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:20,393] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,399] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,404] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:20,409] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,409] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:20,409] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 3 Concepts
INFO  [2023-01-17 00:51:20,517] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2828173a-77fb-4220-bcf9-4b16122d409a] in Datasets[[]]
INFO  [2023-01-17 00:51:20,526] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[4].2828173a-77fb-4220-bcf9-4b16122d409a
INFO  [2023-01-17 00:51:20,527] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form REL-EXPORT-FORM$20Test[4].2828173a-77fb-4220-bcf9-4b16122d409a
INFO  [2023-01-17 00:51:20,529] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[4].2828173a-77fb-4220-bcf9-4b16122d409a] with 1 results within PT0.002149S
INFO  [2023-01-17 00:51:20,529] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[4].2828173a-77fb-4220-bcf9-4b16122d409a, workerId=REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_887f312a-2568-4f9a-8ba6-4fc0c7b7ae80, startTime=2023-01-17T00:51:20.526957, finishTime=2023-01-17T00:51:20.529106) of size 1
INFO  [2023-01-17 00:51:20,530] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL-EXPORT-FORM$20Test[4].2828173a-77fb-4220-bcf9-4b16122d409a] with 1 results within PT0.002852S
INFO  [2023-01-17 00:51:20,531] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL-EXPORT-FORM$20Test[4].2828173a-77fb-4220-bcf9-4b16122d409a, workerId=REL-EXPORT-FORM$20Test[4].worker_REL-EXPORT-FORM$20Test[4]_f4211af8-09b4-4400-9f10-9501c17837e5, startTime=2023-01-17T00:51:20.527759, finishTime=2023-01-17T00:51:20.530611) of size 1
INFO  [2023-01-17 00:51:20,531] com.bakdata.conquery.models.execution.ManagedExecution: DONE ada7ceca-6086-4137-a3f2-819499b5e435 ManagedQuery within PT0.013395S
INFO  [2023-01-17 00:51:20,532] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test QUERIES EXECUTED
INFO  [2023-01-17 00:51:20,532] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2828173a-77fb-4220-bcf9-4b16122d409a ManagedInternalForm within PT0.014344S
INFO  [2023-01-17 00:51:20,548] com.bakdata.conquery.integration.json.FormTest: REL-EXPORT-FORM Test CSV TESTING: results
INFO  [2023-01-17 00:51:20,550] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL-EXPORT-FORM Test[4]
INFO  [2023-01-17 00:51:20,551] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-17 00:51:20,551] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL-EXPORT-FORM Test[4], name=REL-EXPORT-FORM Test[4]]
INFO  [2023-01-17 00:51:20,551] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[4]_f4211af8-09b4-4400-9f10-9501c17837e5
INFO  [2023-01-17 00:51:20,551] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL-EXPORT-FORM Test[4]_887f312a-2568-4f9a-8ba6-4fc0c7b7ae80
INFO  [2023-01-17 00:51:20,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[4]_887f312a-2568-4f9a-8ba6-4fc0c7b7ae80
INFO  [2023-01-17 00:51:20,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL-EXPORT-FORM Test[4]_f4211af8-09b4-4400-9f10-9501c17837e5
INFO  [2023-01-17 00:51:20,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL-EXPORT-FORM Test[4]
INFO  [2023-01-17 00:51:20,708] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL-EXPORT-FORM$20Test[4]
INFO  [2023-01-17 00:51:20,708] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,815] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL-EXPORT-FORM Test
INFO  [2023-01-17 00:51:20,816] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FULL_EXPORT_FORM
INFO  [2023-01-17 00:51:20,816] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:20,816] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:20,817] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-17 00:51:20,817] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-17 00:51:20,817] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:20,817] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:20,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_a26c102f-0bbe-4f13-8d10-b563dfaf4d54 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:20,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_a26c102f-0bbe-4f13-8d10-b563dfaf4d54 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:20,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:20,820] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_d487928a-e561-4d3d-9ab5-a2deed8fa5fb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:20,820] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_d487928a-e561-4d3d-9ab5-a2deed8fa5fb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:20,820] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:20,824] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,923] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[FULL_EXPORT_FORM.secondary]
INFO  [2023-01-17 00:51:20,924] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:20,924] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId FULL_EXPORT_FORM.secondary
INFO  [2023-01-17 00:51:20,925] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId FULL_EXPORT_FORM.secondary
INFO  [2023-01-17 00:51:21,032] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,032] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.vers_stamm
INFO  [2023-01-17 00:51:21,032] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.vers_stamm
INFO  [2023-01-17 00:51:21,032] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.table
INFO  [2023-01-17 00:51:21,032] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FULL_EXPORT_FORM.table
INFO  [2023-01-17 00:51:21,137] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM IMPORT TABLES
INFO  [2023-01-17 00:51:21,149] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,261] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM IMPORT CONCEPTS
INFO  [2023-01-17 00:51:21,262] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:21,262] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:21,262] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:21,262] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:21,263] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FULL_EXPORT_FORM/vers_stamm.import.json, tag=Optional.empty)
█████████████████████████████████████████████     ▌  90%	est. time remaining: 0.00504742sINFO  [2023-01-17 00:51:21,305] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:21,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_end] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4d51b329)
INFO  [2023-01-17 00:51:21,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date_start] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@3e4c81d6)
INFO  [2023-01-17 00:51:21,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:21,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=-6759, maxValue=9131), dateReader=com.bakdata.conquery.util.DateReader@26184593)
INFO  [2023-01-17 00:51:21,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[date] with DateRangeParser(super=Parser(lines=22, nullLines=0), minParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15340, maxValue=15706), dateReader=com.bakdata.conquery.util.DateReader@55ab3cdb), maxParser=DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=15614, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@760bcdea), dateReader=com.bakdata.conquery.util.DateReader@335028c6, onlyQuarters=false, maxValue=16070, minValue=15340, anyOpen=false)
INFO  [2023-01-17 00:51:21,308] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FULL_EXPORT_FORM/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:21,308] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FULL_EXPORT_FORM/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00063588sINFO  [2023-01-17 00:51:21,327] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=6, min=6, average=6.000000, max=6}
INFO  [2023-01-17 00:51:21,327] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15400, maxValue=15405), dateReader=com.bakdata.conquery.util.DateReader@30b5fb72)
INFO  [2023-01-17 00:51:21,327] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:21,327] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[second_id] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:21,330] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FULL_EXPORT_FORM/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:21,330] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:21,330] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FULL_EXPORT_FORM/vers_stamm.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:21,330] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FULL_EXPORT_FORM/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:21,352] com.bakdata.conquery.models.jobs.ImportJob: Importing vers_stamm into FULL_EXPORT_FORM.vers_stamm
127.0.0.1 - - [17/Jan/2023:00:51:21 +0000] "POST /admin/datasets/FULL_EXPORT_FORM/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_FULL_EXPORT_FORM%2Fvers_stamm.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:21,354] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:21,355] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:21,355] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:21,358] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:21,366] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:21,366] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.vers_stamm.vers_stamm], containing 22 entries.
INFO  [2023-01-17 00:51:21,368] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.0
INFO  [2023-01-17 00:51:21,368] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.1
INFO  [2023-01-17 00:51:21,368] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.2
INFO  [2023-01-17 00:51:21,368] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.3
INFO  [2023-01-17 00:51:21,368] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.5
WARN  [2023-01-17 00:51:21,368] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:21,368] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.4
INFO  [2023-01-17 00:51:21,369] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.6
INFO  [2023-01-17 00:51:21,369] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.vers_stamm.vers_stamm.7
INFO  [2023-01-17 00:51:21,380] com.bakdata.conquery.models.jobs.ImportJob: Importing table into FULL_EXPORT_FORM.table
INFO  [2023-01-17 00:51:21,381] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:21,381] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:21,381] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [17/Jan/2023:00:51:21 +0000] "POST /admin/datasets/FULL_EXPORT_FORM/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_FULL_EXPORT_FORM%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:21,381] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,381] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:51:21,381] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:21,382] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:21,382] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FULL_EXPORT_FORM.table.table], containing 6 entries.
INFO  [2023-01-17 00:51:21,382] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FULL_EXPORT_FORM.table.table.0
INFO  [2023-01-17 00:51:21,487] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,504] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM IMPORT TABLE CONTENTS
INFO  [2023-01-17 00:51:21,511] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,517] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM PARSE JSON FORM DESCRIPTION
INFO  [2023-01-17 00:51:21,528] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,528] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:21,528] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:21,640] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7fba8f8b-3495-4e1d-b517-53911b97b8d6] in Datasets[[]]
INFO  [2023-01-17 00:51:21,653] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form FULL_EXPORT_FORM.7fba8f8b-3495-4e1d-b517-53911b97b8d6
INFO  [2023-01-17 00:51:21,653] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteForm: Started Form FULL_EXPORT_FORM.7fba8f8b-3495-4e1d-b517-53911b97b8d6
INFO  [2023-01-17 00:51:21,696] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FULL_EXPORT_FORM.7fba8f8b-3495-4e1d-b517-53911b97b8d6] with 0 results within PT0.043264S
INFO  [2023-01-17 00:51:21,697] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FULL_EXPORT_FORM.7fba8f8b-3495-4e1d-b517-53911b97b8d6] with 1 results within PT0.04382S
INFO  [2023-01-17 00:51:21,697] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FULL_EXPORT_FORM.7fba8f8b-3495-4e1d-b517-53911b97b8d6, workerId=FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_a26c102f-0bbe-4f13-8d10-b563dfaf4d54, startTime=2023-01-17T00:51:21.653213, finishTime=2023-01-17T00:51:21.696477) of size 0
INFO  [2023-01-17 00:51:21,697] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FULL_EXPORT_FORM.7fba8f8b-3495-4e1d-b517-53911b97b8d6, workerId=FULL_EXPORT_FORM.worker_FULL_EXPORT_FORM_d487928a-e561-4d3d-9ab5-a2deed8fa5fb, startTime=2023-01-17T00:51:21.653203, finishTime=2023-01-17T00:51:21.697023) of size 1
INFO  [2023-01-17 00:51:21,697] com.bakdata.conquery.models.execution.ManagedExecution: DONE 551dcdac-ad9c-491d-9e14-534cd22c9b6c ManagedQuery within PT0.057559S
INFO  [2023-01-17 00:51:21,698] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7fba8f8b-3495-4e1d-b517-53911b97b8d6 ManagedInternalForm within PT0.058449S
INFO  [2023-01-17 00:51:21,698] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM QUERIES EXECUTED
INFO  [2023-01-17 00:51:21,718] com.bakdata.conquery.integration.json.FormTest: FULL_EXPORT_FORM CSV TESTING: results
INFO  [2023-01-17 00:51:21,720] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FULL_EXPORT_FORM
INFO  [2023-01-17 00:51:21,720] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-17 00:51:21,720] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FULL_EXPORT_FORM, name=FULL_EXPORT_FORM]
INFO  [2023-01-17 00:51:21,720] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FULL_EXPORT_FORM_a26c102f-0bbe-4f13-8d10-b563dfaf4d54
INFO  [2023-01-17 00:51:21,720] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FULL_EXPORT_FORM_d487928a-e561-4d3d-9ab5-a2deed8fa5fb
INFO  [2023-01-17 00:51:21,818] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FULL_EXPORT_FORM
INFO  [2023-01-17 00:51:21,819] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FULL_EXPORT_FORM_d487928a-e561-4d3d-9ab5-a2deed8fa5fb
INFO  [2023-01-17 00:51:21,819] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FULL_EXPORT_FORM_a26c102f-0bbe-4f13-8d10-b563dfaf4d54
INFO  [2023-01-17 00:51:21,882] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FULL_EXPORT_FORM
INFO  [2023-01-17 00:51:21,882] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:21,937] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FULL_EXPORT_FORM
INFO  [2023-01-17 00:51:21,938] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ABS_EXPORT Test
INFO  [2023-01-17 00:51:21,938] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:21,938] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:21,939] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-17 00:51:21,939] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:21,939] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-17 00:51:21,939] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:21,941] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_deab9ee7-5a6b-421e-afcc-6f0c48232f82 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:21,941] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_deab9ee7-5a6b-421e-afcc-6f0c48232f82 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:21,941] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:21,941] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_a38754a7-59cb-4649-8336-ff81ea78f74e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:21,941] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_a38754a7-59cb-4649-8336-ff81ea78f74e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:21,941] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:21,945] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,045] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,052] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,052] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS_EXPORT$20Test.test_table
INFO  [2023-01-17 00:51:22,052] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ABS_EXPORT$20Test.test_table
INFO  [2023-01-17 00:51:22,170] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,280] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:22,280] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:22,280] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 183 B in total
INFO  [2023-01-17 00:51:22,280] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS_EXPORT Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000339005sINFO  [2023-01-17 00:51:22,315] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=9, min=1, average=4.500000, max=8}
INFO  [2023-01-17 00:51:22,315] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=A, suffix=)
INFO  [2023-01-17 00:51:22,315] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=11109, maxValue=12935), dateReader=com.bakdata.conquery.util.DateReader@10247ec9)
INFO  [2023-01-17 00:51:22,321] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS_EXPORT Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:22,321] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:22,321] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ABS_EXPORT Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:22,343] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into ABS_EXPORT$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:51:22 +0000] "POST /admin/datasets/ABS_EXPORT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ABS_EXPORT+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:22,343] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,344] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:22,345] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:22,345] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:22,348] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:22,349] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS_EXPORT$20Test.test_table.test_table], containing 9 entries.
INFO  [2023-01-17 00:51:22,350] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ABS_EXPORT$20Test.test_table.test_table], containing 9 entries.
WARN  [2023-01-17 00:51:22,350] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:22,350] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ABS_EXPORT$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:22,455] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,460] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,483] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,483] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:22,590] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: ABS_EXPORT Test QUERY INIT
INFO  [2023-01-17 00:51:22,611] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ABS_EXPORT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:22,612] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0160dd01-b247-4ad3-809a-4dc0d4466410] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ABS_EXPORT Test))]]
INFO  [2023-01-17 00:51:22,617] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started AbsoluteFormQuery ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410
INFO  [2023-01-17 00:51:22,618] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started AbsoluteFormQuery ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410
WARN  [2023-01-17 00:51:22,618] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:51:22,618] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410] with 0 results within PT0.000724S
INFO  [2023-01-17 00:51:22,619] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410, workerId=ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_deab9ee7-5a6b-421e-afcc-6f0c48232f82, startTime=2023-01-17T00:51:22.618157, finishTime=2023-01-17T00:51:22.618881) of size 0
INFO  [2023-01-17 00:51:22,620] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410] with 1 results within PT0.00205S
INFO  [2023-01-17 00:51:22,620] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410, workerId=ABS_EXPORT$20Test.worker_ABS_EXPORT$20Test_a38754a7-59cb-4649-8336-ff81ea78f74e, startTime=2023-01-17T00:51:22.617993, finishTime=2023-01-17T00:51:22.620043) of size 1
INFO  [2023-01-17 00:51:22,621] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0160dd01-b247-4ad3-809a-4dc0d4466410 ManagedQuery within PT0.008376S
127.0.0.1 - - [17/Jan/2023:00:51:22 +0000] "POST /api/datasets/ABS_EXPORT$20Test/queries HTTP/1.1" 201 2252 "-" "Conquery (test client)" 15
127.0.0.1 - - [17/Jan/2023:00:51:22 +0000] "GET /api/datasets/ABS_EXPORT$20Test/queries/ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410 HTTP/1.1" 200 2511 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:22,650] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ABS_EXPORT Test], queryId=0160dd01-b247-4ad3-809a-4dc0d4466410, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:22.612352, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6607a6db[Count = 0], startTime=2023-01-17T00:51:22.612645, finishTime=2023-01-17T00:51:22.621021, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@380c8481), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ABS_EXPORT Test)), query=com.bakdata.conquery.models.forms.managed.AbsoluteFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7eb804be, com.bakdata.conquery.models.query.ColumnDescriptor@7eb10562, com.bakdata.conquery.models.query.ColumnDescriptor@255b5511, com.bakdata.conquery.models.query.ColumnDescriptor@79aff5b7, com.bakdata.conquery.models.query.ColumnDescriptor@470c83c8]) download on dataset Dataset[label=null, name=ABS_EXPORT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:22,651] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ABS_EXPORT Test], queryId=0160dd01-b247-4ad3-809a-4dc0d4466410, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:22.612352, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6607a6db[Count = 0], startTime=2023-01-17T00:51:22.612645, finishTime=2023-01-17T00:51:22.621021, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@380c8481), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ABS_EXPORT Test)), query=com.bakdata.conquery.models.forms.managed.AbsoluteFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7eb804be, com.bakdata.conquery.models.query.ColumnDescriptor@7eb10562, com.bakdata.conquery.models.query.ColumnDescriptor@255b5511, com.bakdata.conquery.models.query.ColumnDescriptor@79aff5b7, com.bakdata.conquery.models.query.ColumnDescriptor@470c83c8]) on dataset Dataset[label=null, name=ABS_EXPORT Test]
127.0.0.1 - - [17/Jan/2023:00:51:22 +0000] "GET /api/datasets/ABS_EXPORT%20Test/result/ABS_EXPORT$20Test.0160dd01-b247-4ad3-809a-4dc0d4466410.csv?pretty=false HTTP/1.1" 200 199 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:22,670] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest ABS_EXPORT Test on 5 rows
INFO  [2023-01-17 00:51:22,670] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ABS_EXPORT Test
INFO  [2023-01-17 00:51:22,671] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-17 00:51:22,671] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ABS_EXPORT Test, name=ABS_EXPORT Test]
INFO  [2023-01-17 00:51:22,671] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS_EXPORT Test_deab9ee7-5a6b-421e-afcc-6f0c48232f82
INFO  [2023-01-17 00:51:22,671] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ABS_EXPORT Test_a38754a7-59cb-4649-8336-ff81ea78f74e
INFO  [2023-01-17 00:51:22,739] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ABS_EXPORT Test
INFO  [2023-01-17 00:51:22,740] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS_EXPORT Test_deab9ee7-5a6b-421e-afcc-6f0c48232f82
INFO  [2023-01-17 00:51:22,740] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ABS_EXPORT Test_a38754a7-59cb-4649-8336-ff81ea78f74e
INFO  [2023-01-17 00:51:22,757] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ABS_EXPORT$20Test
INFO  [2023-01-17 00:51:22,757] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,889] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ABS_EXPORT Test
INFO  [2023-01-17 00:51:22,889] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:22,889] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:22,889] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:22,891] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:22,891] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:22,891] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:22,891] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:22,893] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_bb6c747b-9ae1-4c4c-af62-8c514ecea210 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:22,893] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_bb6c747b-9ae1-4c4c-af62-8c514ecea210 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:22,893] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:22,893] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_29fe85aa-0e9e-4b18-bc88-ee6d45765199 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:22,893] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_29fe85aa-0e9e-4b18-bc88-ee6d45765199 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:22,893] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:22,897] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:22,997] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,004] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,004] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ARRAY_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:23,004] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table ARRAY_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:23,138] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,253] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:23,253] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:23,253] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 137 B in total
INFO  [2023-01-17 00:51:23,253] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ARRAY_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00060871sINFO  [2023-01-17 00:51:23,315] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=7, min=1, average=1.750000, max=4}
INFO  [2023-01-17 00:51:23,315] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:23,315] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1ce5c5f2)
INFO  [2023-01-17 00:51:23,319] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ARRAY_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:23,319] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:23,319] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_ARRAY_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:23,341] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into ARRAY_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:23,342] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:23 +0000] "POST /admin/datasets/ARRAY_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_ARRAY_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:23,343] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:23,344] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:23,344] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:23,347] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:23,347] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ARRAY_CONCEPT_QUERY$20Test.table1.table1], containing 7 entries.
INFO  [2023-01-17 00:51:23,347] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[ARRAY_CONCEPT_QUERY$20Test.table1.table1], containing 7 entries.
WARN  [2023-01-17 00:51:23,348] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:23,349] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ARRAY_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:51:23,349] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received ARRAY_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:51:23,454] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,459] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,474] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,474] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:23,474] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:23,580] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: ARRAY_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:23,597] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[ARRAY_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:23,597] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[df0068d8-4425-49d7-a907-d58b5501e5b1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_ARRAY_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:51:23,602] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1
INFO  [2023-01-17 00:51:23,602] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1
127.0.0.1 - - [17/Jan/2023:00:51:23 +0000] "POST /api/datasets/ARRAY_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 2161 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:23,604] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1] with 1 results within PT0.001754S
INFO  [2023-01-17 00:51:23,604] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1] with 3 results within PT0.002074S
INFO  [2023-01-17 00:51:23,605] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1, workerId=ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_bb6c747b-9ae1-4c4c-af62-8c514ecea210, startTime=2023-01-17T00:51:23.602565, finishTime=2023-01-17T00:51:23.604319) of size 1
INFO  [2023-01-17 00:51:23,605] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1, workerId=ARRAY_CONCEPT_QUERY$20Test.worker_ARRAY_CONCEPT_QUERY$20Test_29fe85aa-0e9e-4b18-bc88-ee6d45765199, startTime=2023-01-17T00:51:23.602529, finishTime=2023-01-17T00:51:23.604603) of size 3
INFO  [2023-01-17 00:51:23,605] com.bakdata.conquery.models.execution.ManagedExecution: DONE df0068d8-4425-49d7-a907-d58b5501e5b1 ManagedQuery within PT0.00745S
127.0.0.1 - - [17/Jan/2023:00:51:23 +0000] "GET /api/datasets/ARRAY_CONCEPT_QUERY$20Test/queries/ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1 HTTP/1.1" 200 2456 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:23,654] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test], queryId=df0068d8-4425-49d7-a907-d58b5501e5b1, label=select	@§$, creationTime=2023-01-17T00:51:23.597694, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4e3f1802[Count = 0], startTime=2023-01-17T00:51:23.597945, finishTime=2023-01-17T00:51:23.605395, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6bb32513), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ARRAY_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@629e997a, com.bakdata.conquery.models.query.ColumnDescriptor@3d1e8191, com.bakdata.conquery.models.query.ColumnDescriptor@b577138, com.bakdata.conquery.models.query.ColumnDescriptor@793e46fc]) download on dataset Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:23,655] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test], queryId=df0068d8-4425-49d7-a907-d58b5501e5b1, label=select	@§$, creationTime=2023-01-17T00:51:23.597694, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4e3f1802[Count = 0], startTime=2023-01-17T00:51:23.597945, finishTime=2023-01-17T00:51:23.605395, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6bb32513), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_ARRAY_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@629e997a, com.bakdata.conquery.models.query.ColumnDescriptor@3d1e8191, com.bakdata.conquery.models.query.ColumnDescriptor@b577138, com.bakdata.conquery.models.query.ColumnDescriptor@793e46fc]) on dataset Dataset[label=null, name=ARRAY_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:51:23 +0000] "GET /api/datasets/ARRAY_CONCEPT_QUERY%20Test/result/ARRAY_CONCEPT_QUERY$20Test.df0068d8-4425-49d7-a907-d58b5501e5b1.csv?pretty=false HTTP/1.1" 200 197 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:23,657] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest ARRAY_CONCEPT_QUERY Test on 5 rows
INFO  [2023-01-17 00:51:23,657] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:23,657] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:23,657] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=ARRAY_CONCEPT_QUERY Test, name=ARRAY_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:23,658] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ARRAY_CONCEPT_QUERY Test_bb6c747b-9ae1-4c4c-af62-8c514ecea210
INFO  [2023-01-17 00:51:23,658] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_ARRAY_CONCEPT_QUERY Test_29fe85aa-0e9e-4b18-bc88-ee6d45765199
INFO  [2023-01-17 00:51:23,691] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:23,692] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ARRAY_CONCEPT_QUERY Test_bb6c747b-9ae1-4c4c-af62-8c514ecea210
INFO  [2023-01-17 00:51:23,692] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_ARRAY_CONCEPT_QUERY Test_29fe85aa-0e9e-4b18-bc88-ee6d45765199
INFO  [2023-01-17 00:51:23,753] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of ARRAY_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:51:23,753] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,881] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test ARRAY_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:23,881] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:23,881] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:23,881] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:23,882] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:23,883] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:23,882] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:23,883] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:23,885] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e9c502bf-2bec-4a90-9ce4-467732391c60 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:23,885] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e9c502bf-2bec-4a90-9ce4-467732391c60 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:23,885] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:23,885] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_29591b2e-2890-449e-b2b3-09df3e60ec97 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:23,885] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_29591b2e-2890-449e-b2b3-09df3e60ec97 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:23,885] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:23,897] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,988] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,995] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:23,996] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:23,996] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:24,122] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,244] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:24,244] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:24,244] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:51:24,244] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000268109sINFO  [2023-01-17 00:51:24,271] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:24,271] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:24,271] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@34a93f19)
INFO  [2023-01-17 00:51:24,274] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:24,274] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:24,274] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:24,293] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:24,294] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:24 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:24,295] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:24,295] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:24,295] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:24,297] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:51:24,297] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:51:24,297] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:51:24,298] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:24,298] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:51:24,298] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:51:24,299] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:51:24,403] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,408] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,421] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,422] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:24,422] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:24,528] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:24,548] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:24,549] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[dda43f80-876d-4486-a9b0-06ea5944a114] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:51:24,555] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114
INFO  [2023-01-17 00:51:24,555] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114
127.0.0.1 - - [17/Jan/2023:00:51:24 +0000] "POST /api/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1579 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:24,556] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114] with 0 results within PT0.001636S
INFO  [2023-01-17 00:51:24,556] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114] with 1 results within PT0.001736S
INFO  [2023-01-17 00:51:24,557] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114, workerId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_29591b2e-2890-449e-b2b3-09df3e60ec97, startTime=2023-01-17T00:51:24.555189, finishTime=2023-01-17T00:51:24.556825) of size 0
INFO  [2023-01-17 00:51:24,557] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114, workerId=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_e9c502bf-2bec-4a90-9ce4-467732391c60, startTime=2023-01-17T00:51:24.555194, finishTime=2023-01-17T00:51:24.556930) of size 1
INFO  [2023-01-17 00:51:24,557] com.bakdata.conquery.models.execution.ManagedExecution: DONE dda43f80-876d-4486-a9b0-06ea5944a114 ManagedQuery within PT0.008436S
127.0.0.1 - - [17/Jan/2023:00:51:24 +0000] "GET /api/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114 HTTP/1.1" 200 1998 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:24,605] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=dda43f80-876d-4486-a9b0-06ea5944a114, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:51:24.549054, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@28ebdafa[Count = 0], startTime=2023-01-17T00:51:24.549296, finishTime=2023-01-17T00:51:24.557732, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2fbc869c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2b8b0d96, com.bakdata.conquery.models.query.ColumnDescriptor@5ff30a93]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:24,605] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=dda43f80-876d-4486-a9b0-06ea5944a114, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:51:24.549054, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@28ebdafa[Count = 0], startTime=2023-01-17T00:51:24.549296, finishTime=2023-01-17T00:51:24.557732, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2fbc869c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2b8b0d96, com.bakdata.conquery.models.query.ColumnDescriptor@5ff30a93]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:51:24 +0000] "GET /api/datasets/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.dda43f80-876d-4486-a9b0-06ea5944a114.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:51:24,608] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 2 rows
INFO  [2023-01-17 00:51:24,608] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:24,609] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:24,609] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:24,609] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_29591b2e-2890-449e-b2b3-09df3e60ec97
INFO  [2023-01-17 00:51:24,609] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_e9c502bf-2bec-4a90-9ce4-467732391c60
INFO  [2023-01-17 00:51:24,683] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:24,684] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_e9c502bf-2bec-4a90-9ce4-467732391c60
INFO  [2023-01-17 00:51:24,684] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_29591b2e-2890-449e-b2b3-09df3e60ec97
INFO  [2023-01-17 00:51:24,699] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:51:24,699] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,827] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:24,828] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:24,828] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:24,828] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:24,829] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:24,829] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:24,829] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:24,829] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:24,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9b22b5a6-4574-48cd-8dab-454a32aba78a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:24,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9b22b5a6-4574-48cd-8dab-454a32aba78a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:24,831] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:24,832] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_58a23fca-039b-4b2c-9d21-192e0a2c3cb4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:24,832] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_58a23fca-039b-4b2c-9d21-192e0a2c3cb4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:24,832] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:24,838] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,935] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,942] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:24,942] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:24,942] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:25,061] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,178] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:25,179] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:25,179] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:51:25,179] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000302329sINFO  [2023-01-17 00:51:25,209] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:25,210] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:25,210] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@60d3752)
INFO  [2023-01-17 00:51:25,212] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:25,212] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:25,212] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:25,232] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:25,233] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:25 +0000] "POST /admin/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:25,234] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:25,237] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:25,237] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:25,240] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:51:25,240] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:51:25,240] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:51:25,242] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:25,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:51:25,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:51:25,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:51:25,347] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,353] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,373] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:25,373] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:25,479] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:25,493] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:25,494] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1c151911-f820-4960-925b-3f568ea2311d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
127.0.0.1 - - [17/Jan/2023:00:51:25 +0000] "POST /api/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1655 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:25,498] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d
INFO  [2023-01-17 00:51:25,498] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d
INFO  [2023-01-17 00:51:25,499] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d] with 3 results within PT0.00161S
INFO  [2023-01-17 00:51:25,500] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d] with 4 results within PT0.001999S
INFO  [2023-01-17 00:51:25,500] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d, workerId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_58a23fca-039b-4b2c-9d21-192e0a2c3cb4, startTime=2023-01-17T00:51:25.498203, finishTime=2023-01-17T00:51:25.499813) of size 3
INFO  [2023-01-17 00:51:25,501] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d, workerId=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9b22b5a6-4574-48cd-8dab-454a32aba78a, startTime=2023-01-17T00:51:25.498257, finishTime=2023-01-17T00:51:25.500256) of size 4
INFO  [2023-01-17 00:51:25,501] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1c151911-f820-4960-925b-3f568ea2311d ManagedQuery within PT0.006773S
127.0.0.1 - - [17/Jan/2023:00:51:25 +0000] "GET /api/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d HTTP/1.1" 200 2110 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:25,527] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=1c151911-f820-4960-925b-3f568ea2311d, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:51:25.493905, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@73a5796b[Count = 0], startTime=2023-01-17T00:51:25.494330, finishTime=2023-01-17T00:51:25.501103, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@117e6541), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@14b43a72, com.bakdata.conquery.models.query.ColumnDescriptor@49d65834]) download on dataset Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:25,527] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=1c151911-f820-4960-925b-3f568ea2311d, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:51:25.493905, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@73a5796b[Count = 0], startTime=2023-01-17T00:51:25.494330, finishTime=2023-01-17T00:51:25.501103, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@117e6541), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@14b43a72, com.bakdata.conquery.models.query.ColumnDescriptor@49d65834]) on dataset Dataset[label=null, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:51:25 +0000] "GET /api/datasets/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1c151911-f820-4960-925b-3f568ea2311d.csv?pretty=false HTTP/1.1" 200 48 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:51:25,551] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 8 rows
INFO  [2023-01-17 00:51:25,551] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:25,551] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:25,552] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:25,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_9b22b5a6-4574-48cd-8dab-454a32aba78a
INFO  [2023-01-17 00:51:25,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_58a23fca-039b-4b2c-9d21-192e0a2c3cb4
INFO  [2023-01-17 00:51:25,649] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:25,649] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_9b22b5a6-4574-48cd-8dab-454a32aba78a
INFO  [2023-01-17 00:51:25,649] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_58a23fca-039b-4b2c-9d21-192e0a2c3cb4
INFO  [2023-01-17 00:51:25,749] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:51:25,749] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,778] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test BIG_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:25,778] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-17 00:51:25,778] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:25,778] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:25,779] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-17 00:51:25,779] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-17 00:51:25,780] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:25,780] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_68634fb3-34ce-4c4f-93d9-004f515020e7 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_68634fb3-34ce-4c4f-93d9-004f515020e7 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_e36d08d0-7791-4335-a105-e91d31c18b03 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_e36d08d0-7791-4335-a105-e91d31c18b03 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:25,782] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:25,786] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,886] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,893] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:25,893] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose
INFO  [2023-01-17 00:51:25,893] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose
INFO  [2023-01-17 00:51:26,012] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,133] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:26,134] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:26,134] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 136 B in total
INFO  [2023-01-17 00:51:26,134] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COMMON_CONCEPT_ICD_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000308425sINFO  [2023-01-17 00:51:26,165] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=3, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:26,165] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17256, maxValue=17347), dateReader=com.bakdata.conquery.util.DateReader@2fcb8734)
INFO  [2023-01-17 00:51:26,165] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=3, nullLines=0), encoding=null, prefix=F2, suffix=)
INFO  [2023-01-17 00:51:26,165] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[aufnahmedatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17167, maxValue=17257), dateReader=com.bakdata.conquery.util.DateReader@3c218172)
INFO  [2023-01-17 00:51:26,168] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COMMON_CONCEPT_ICD_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:26,168] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:26,168] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COMMON_CONCEPT_ICD_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:26,187] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose
127.0.0.1 - - [17/Jan/2023:00:51:26 +0000] "POST /admin/datasets/COMMON_CONCEPT_ICD_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COMMON_CONCEPT_ICD_QUERY+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:26,188] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:26,188] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,189] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:26,189] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:26,190] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:26,191] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
WARN  [2023-01-17 00:51:26,191] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:26,191] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
INFO  [2023-01-17 00:51:26,191] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMMON_CONCEPT_ICD_QUERY$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-17 00:51:26,299] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,304] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,321] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,321] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:26,427] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COMMON_CONCEPT_ICD_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:26,448] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COMMON_CONCEPT_ICD_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:26,449] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[cca35c6e-553e-4de6-b599-b23f9e88556d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COMMON_CONCEPT_ICD_QUERY Test))]]
INFO  [2023-01-17 00:51:26,453] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d
INFO  [2023-01-17 00:51:26,453] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d
WARN  [2023-01-17 00:51:26,453] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:51:26,453] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d] with 0 results within PT0.000153S
INFO  [2023-01-17 00:51:26,454] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d] with 2 results within PT0.000875S
INFO  [2023-01-17 00:51:26,454] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d, workerId=COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_68634fb3-34ce-4c4f-93d9-004f515020e7, startTime=2023-01-17T00:51:26.453460, finishTime=2023-01-17T00:51:26.453613) of size 0
INFO  [2023-01-17 00:51:26,454] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d, workerId=COMMON_CONCEPT_ICD_QUERY$20Test.worker_COMMON_CONCEPT_ICD_QUERY$20Test_e36d08d0-7791-4335-a105-e91d31c18b03, startTime=2023-01-17T00:51:26.453199, finishTime=2023-01-17T00:51:26.454074) of size 2
127.0.0.1 - - [17/Jan/2023:00:51:26 +0000] "POST /api/datasets/COMMON_CONCEPT_ICD_QUERY$20Test/queries HTTP/1.1" 201 1306 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:26,454] com.bakdata.conquery.models.execution.ManagedExecution: DONE cca35c6e-553e-4de6-b599-b23f9e88556d ManagedQuery within PT0.005574S
127.0.0.1 - - [17/Jan/2023:00:51:26 +0000] "GET /api/datasets/COMMON_CONCEPT_ICD_QUERY$20Test/queries/COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d HTTP/1.1" 200 1621 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:26,485] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test], queryId=cca35c6e-553e-4de6-b599-b23f9e88556d, label=F20	@§$, creationTime=2023-01-17T00:51:26.448958, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6fb3fcd8[Count = 0], startTime=2023-01-17T00:51:26.449243, finishTime=2023-01-17T00:51:26.454817, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@35a99c0c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMMON_CONCEPT_ICD_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7c1236e4, com.bakdata.conquery.models.query.ColumnDescriptor@2b547e72]) download on dataset Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:26,485] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test], queryId=cca35c6e-553e-4de6-b599-b23f9e88556d, label=F20	@§$, creationTime=2023-01-17T00:51:26.448958, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6fb3fcd8[Count = 0], startTime=2023-01-17T00:51:26.449243, finishTime=2023-01-17T00:51:26.454817, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@35a99c0c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMMON_CONCEPT_ICD_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7c1236e4, com.bakdata.conquery.models.query.ColumnDescriptor@2b547e72]) on dataset Dataset[label=null, name=COMMON_CONCEPT_ICD_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:51:26 +0000] "GET /api/datasets/COMMON_CONCEPT_ICD_QUERY%20Test/result/COMMON_CONCEPT_ICD_QUERY$20Test.cca35c6e-553e-4de6-b599-b23f9e88556d.csv?pretty=false HTTP/1.1" 200 66 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:26,506] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COMMON_CONCEPT_ICD_QUERY Test on 3 rows
INFO  [2023-01-17 00:51:26,506] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-17 00:51:26,506] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-17 00:51:26,506] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMMON_CONCEPT_ICD_QUERY Test, name=COMMON_CONCEPT_ICD_QUERY Test]
INFO  [2023-01-17 00:51:26,506] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMMON_CONCEPT_ICD_QUERY Test_68634fb3-34ce-4c4f-93d9-004f515020e7
INFO  [2023-01-17 00:51:26,507] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMMON_CONCEPT_ICD_QUERY Test_e36d08d0-7791-4335-a105-e91d31c18b03
INFO  [2023-01-17 00:51:26,580] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-17 00:51:26,581] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMMON_CONCEPT_ICD_QUERY Test_68634fb3-34ce-4c4f-93d9-004f515020e7
INFO  [2023-01-17 00:51:26,581] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMMON_CONCEPT_ICD_QUERY Test_e36d08d0-7791-4335-a105-e91d31c18b03
INFO  [2023-01-17 00:51:26,604] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COMMON_CONCEPT_ICD_QUERY$20Test
INFO  [2023-01-17 00:51:26,604] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,750] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COMMON_CONCEPT_ICD_QUERY Test
INFO  [2023-01-17 00:51:26,751] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COMPOUND_DATERANGE Test
INFO  [2023-01-17 00:51:26,751] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:26,751] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:26,765] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-17 00:51:26,765] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-17 00:51:26,765] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:26,765] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:26,799] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_beb21288-6175-4af7-ba27-eae39759cf33 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:26,799] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_beb21288-6175-4af7-ba27-eae39759cf33 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:26,799] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:26,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_af47a3bf-fc2c-49e9-9ab8-0065c75d2d4a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:26,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_af47a3bf-fc2c-49e9-9ab8-0065c75d2d4a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:26,811] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:26,815] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,915] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,923] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:26,924] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMPOUND_DATERANGE$20Test.test_table
INFO  [2023-01-17 00:51:26,924] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COMPOUND_DATERANGE$20Test.test_table
INFO  [2023-01-17 00:51:27,042] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,154] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:27,155] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:27,155] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 213 B in total
INFO  [2023-01-17 00:51:27,155] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COMPOUND_DATERANGE Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000345541sINFO  [2023-01-17 00:51:27,190] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=9, sum=9, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:27,190] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlung_ende] with DateParser(super=Parser(lines=9, nullLines=2), subType=IntegerParser(super=Parser(lines=9, nullLines=2), minValue=14958, maxValue=16139), dateReader=com.bakdata.conquery.util.DateReader@774fbceb)
INFO  [2023-01-17 00:51:27,190] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlung_start] with DateParser(super=Parser(lines=9, nullLines=2), subType=IntegerParser(super=Parser(lines=9, nullLines=2), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@356cc36c)
INFO  [2023-01-17 00:51:27,190] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlung] with CompoundDateRangeParser(super=Parser(lines=9, nullLines=0), startColumn=behandlung_start, endColumn=behandlung_ende)
INFO  [2023-01-17 00:51:27,201] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COMPOUND_DATERANGE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:27,201] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:27,201] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COMPOUND_DATERANGE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:27,218] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into COMPOUND_DATERANGE$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:51:27 +0000] "POST /admin/datasets/COMPOUND_DATERANGE%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COMPOUND_DATERANGE+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:27,219] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,219] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:27,219] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:27,219] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:27,221] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:51:27,221] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMPOUND_DATERANGE$20Test.test_table.test_table], containing 9 entries.
WARN  [2023-01-17 00:51:27,222] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:27,222] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMPOUND_DATERANGE$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:27,222] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMPOUND_DATERANGE$20Test.test_table.test_table.2
INFO  [2023-01-17 00:51:27,223] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COMPOUND_DATERANGE$20Test.test_table.test_table], containing 9 entries.
INFO  [2023-01-17 00:51:27,223] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COMPOUND_DATERANGE$20Test.test_table.test_table.1
INFO  [2023-01-17 00:51:27,329] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,334] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,357] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,357] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:27,357] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:27,464] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COMPOUND_DATERANGE Test QUERY INIT
INFO  [2023-01-17 00:51:27,480] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COMPOUND_DATERANGE$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:27,481] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COMPOUND_DATERANGE Test))]]
INFO  [2023-01-17 00:51:27,484] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d
INFO  [2023-01-17 00:51:27,484] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d
127.0.0.1 - - [17/Jan/2023:00:51:27 +0000] "POST /api/datasets/COMPOUND_DATERANGE$20Test/queries HTTP/1.1" 201 1114 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:27,486] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d] with 3 results within PT0.001541S
INFO  [2023-01-17 00:51:27,486] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d] with 4 results within PT0.002165S
INFO  [2023-01-17 00:51:27,487] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d, workerId=COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_beb21288-6175-4af7-ba27-eae39759cf33, startTime=2023-01-17T00:51:27.484588, finishTime=2023-01-17T00:51:27.486129) of size 3
INFO  [2023-01-17 00:51:27,487] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d, workerId=COMPOUND_DATERANGE$20Test.worker_COMPOUND_DATERANGE$20Test_af47a3bf-fc2c-49e9-9ab8-0065c75d2d4a, startTime=2023-01-17T00:51:27.484576, finishTime=2023-01-17T00:51:27.486741) of size 4
INFO  [2023-01-17 00:51:27,487] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d ManagedQuery within PT0.006065S
127.0.0.1 - - [17/Jan/2023:00:51:27 +0000] "GET /api/datasets/COMPOUND_DATERANGE$20Test/queries/COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d HTTP/1.1" 200 1405 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:27,514] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMPOUND_DATERANGE Test], queryId=0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d, label=test_tree	@§$, creationTime=2023-01-17T00:51:27.481154, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1aeca66c[Count = 0], startTime=2023-01-17T00:51:27.481446, finishTime=2023-01-17T00:51:27.487511, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@771bf8c3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMPOUND_DATERANGE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2d7ac79e, com.bakdata.conquery.models.query.ColumnDescriptor@a21d3a6]) download on dataset Dataset[label=null, name=COMPOUND_DATERANGE Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:27,514] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COMPOUND_DATERANGE Test], queryId=0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d, label=test_tree	@§$, creationTime=2023-01-17T00:51:27.481154, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1aeca66c[Count = 0], startTime=2023-01-17T00:51:27.481446, finishTime=2023-01-17T00:51:27.487511, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@771bf8c3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COMPOUND_DATERANGE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2d7ac79e, com.bakdata.conquery.models.query.ColumnDescriptor@a21d3a6]) on dataset Dataset[label=null, name=COMPOUND_DATERANGE Test]
127.0.0.1 - - [17/Jan/2023:00:51:27 +0000] "GET /api/datasets/COMPOUND_DATERANGE%20Test/result/COMPOUND_DATERANGE$20Test.0a9d26c5-a70d-4eb3-8255-3dbbc715bb6d.csv?pretty=false HTTP/1.1" 200 183 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:51:27,533] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COMPOUND_DATERANGE Test on 8 rows
INFO  [2023-01-17 00:51:27,533] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COMPOUND_DATERANGE Test
INFO  [2023-01-17 00:51:27,533] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-17 00:51:27,533] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COMPOUND_DATERANGE Test, name=COMPOUND_DATERANGE Test]
INFO  [2023-01-17 00:51:27,534] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMPOUND_DATERANGE Test_beb21288-6175-4af7-ba27-eae39759cf33
INFO  [2023-01-17 00:51:27,534] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COMPOUND_DATERANGE Test_af47a3bf-fc2c-49e9-9ab8-0065c75d2d4a
INFO  [2023-01-17 00:51:27,566] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COMPOUND_DATERANGE Test
INFO  [2023-01-17 00:51:27,580] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMPOUND_DATERANGE Test_beb21288-6175-4af7-ba27-eae39759cf33
INFO  [2023-01-17 00:51:27,611] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COMPOUND_DATERANGE Test_af47a3bf-fc2c-49e9-9ab8-0065c75d2d4a
INFO  [2023-01-17 00:51:27,638] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COMPOUND_DATERANGE$20Test
INFO  [2023-01-17 00:51:27,638] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,763] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COMPOUND_DATERANGE Test
INFO  [2023-01-17 00:51:27,764] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:27,764] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:27,764] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:27,766] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:27,766] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:27,766] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:27,766] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_2f7f235a-fe8b-49c6-85b0-b22a8ef587af are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_2f7f235a-fe8b-49c6-85b0-b22a8ef587af are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_0dcd1f49-6af8-405e-ac60-71eed746ef73 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_0dcd1f49-6af8-405e-ac60-71eed746ef73 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:27,770] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:27,872] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,882] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:27,882] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-17 00:51:27,882] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-17 00:51:28,002] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,120] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:28,120] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:28,120] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 44 B in total
INFO  [2023-01-17 00:51:28,121] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000312767sINFO  [2023-01-17 00:51:28,152] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:28,152] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:28,155] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:28,155] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:28,155] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:28,177] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-17 00:51:28,178] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:28 +0000] "POST /admin/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:28,180] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:28,180] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:28,181] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:28,183] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:28,183] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:51:28,183] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
WARN  [2023-01-17 00:51:28,185] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:28,185] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:28,185] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.1
INFO  [2023-01-17 00:51:28,290] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,295] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,309] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,310] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:28,310] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:28,416] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test QUERY INIT
INFO  [2023-01-17 00:51:28,432] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:28,433] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9d62b010-13d5-43a6-94c3-f9c0683e24a6] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test))]]
127.0.0.1 - - [17/Jan/2023:00:51:28 +0000] "POST /api/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test/queries HTTP/1.1" 201 1647 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:28,444] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6
INFO  [2023-01-17 00:51:28,448] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6
INFO  [2023-01-17 00:51:28,449] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6] with 0 results within PT0.005598S
INFO  [2023-01-17 00:51:28,450] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6] with 2 results within PT0.001915S
INFO  [2023-01-17 00:51:28,450] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6, workerId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_2f7f235a-fe8b-49c6-85b0-b22a8ef587af, startTime=2023-01-17T00:51:28.444131, finishTime=2023-01-17T00:51:28.449729) of size 0
INFO  [2023-01-17 00:51:28,450] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6, workerId=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test_0dcd1f49-6af8-405e-ac60-71eed746ef73, startTime=2023-01-17T00:51:28.448160, finishTime=2023-01-17T00:51:28.450075) of size 2
INFO  [2023-01-17 00:51:28,450] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9d62b010-13d5-43a6-94c3-f9c0683e24a6 ManagedQuery within PT0.017525S
127.0.0.1 - - [17/Jan/2023:00:51:28 +0000] "GET /api/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test/queries/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6 HTTP/1.1" 200 2046 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:28,486] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test], queryId=9d62b010-13d5-43a6-94c3-f9c0683e24a6, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:28.432889, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1e2f4709[Count = 0], startTime=2023-01-17T00:51:28.433165, finishTime=2023-01-17T00:51:28.450690, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@372d9b25), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@461657fa, com.bakdata.conquery.models.query.ColumnDescriptor@55a8c9a5, com.bakdata.conquery.models.query.ColumnDescriptor@315bcb16]) download on dataset Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:28,486] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test], queryId=9d62b010-13d5-43a6-94c3-f9c0683e24a6, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:28.432889, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1e2f4709[Count = 0], startTime=2023-01-17T00:51:28.433165, finishTime=2023-01-17T00:51:28.450690, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@372d9b25), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@461657fa, com.bakdata.conquery.models.query.ColumnDescriptor@55a8c9a5, com.bakdata.conquery.models.query.ColumnDescriptor@315bcb16]) on dataset Dataset[label=null, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
127.0.0.1 - - [17/Jan/2023:00:51:28 +0000] "GET /api/datasets/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE%20Test/result/CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test.9d62b010-13d5-43a6-94c3-f9c0683e24a6.csv?pretty=false HTTP/1.1" 200 106 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:28,489] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test on 3 rows
INFO  [2023-01-17 00:51:28,489] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:28,489] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:28,489] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test, name=CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:28,489] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_2f7f235a-fe8b-49c6-85b0-b22a8ef587af
INFO  [2023-01-17 00:51:28,489] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_0dcd1f49-6af8-405e-ac60-71eed746ef73
INFO  [2023-01-17 00:51:28,576] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_2f7f235a-fe8b-49c6-85b0-b22a8ef587af
INFO  [2023-01-17 00:51:28,576] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:28,576] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test_0dcd1f49-6af8-405e-ac60-71eed746ef73
INFO  [2023-01-17 00:51:28,585] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE$20Test
INFO  [2023-01-17 00:51:28,586] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,716] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CONCEPT_DATE_RESTRICTION_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:28,716] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CONCEPT_RESTRICTION Test
INFO  [2023-01-17 00:51:28,717] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:28,717] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:28,718] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-17 00:51:28,718] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-17 00:51:28,718] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:28,718] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:28,719] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_aa09ce34-feae-4956-9af0-12b6ba9ef579 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:28,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_aa09ce34-feae-4956-9af0-12b6ba9ef579 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:28,719] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:28,720] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_e117f74c-7934-41db-a9c4-84c96916de1b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:28,720] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_e117f74c-7934-41db-a9c4-84c96916de1b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:28,720] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:28,824] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,831] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:28,831] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_RESTRICTION$20Test.kh_diagnose
INFO  [2023-01-17 00:51:28,831] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_RESTRICTION$20Test.kh_diagnose
INFO  [2023-01-17 00:51:28,947] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,058] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:29,058] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:29,058] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 137 B in total
INFO  [2023-01-17 00:51:29,058] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_RESTRICTION Test/kh_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000305087sINFO  [2023-01-17 00:51:29,089] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=3, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:29,089] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17256, maxValue=17347), dateReader=com.bakdata.conquery.util.DateReader@7de67f7d)
INFO  [2023-01-17 00:51:29,089] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[aufnahmedatum] with DateParser(super=Parser(lines=3, nullLines=0), subType=IntegerParser(super=Parser(lines=3, nullLines=0), minValue=17167, maxValue=17257), dateReader=com.bakdata.conquery.util.DateReader@6e6f6679)
INFO  [2023-01-17 00:51:29,090] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=3, nullLines=0), encoding=null, prefix=F2, suffix=)
INFO  [2023-01-17 00:51:29,092] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_RESTRICTION Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:29,092] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:29,092] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_RESTRICTION Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:29,112] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into CONCEPT_RESTRICTION$20Test.kh_diagnose
INFO  [2023-01-17 00:51:29,113] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:29 +0000] "POST /admin/datasets/CONCEPT_RESTRICTION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_CONCEPT_RESTRICTION+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:29,114] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:29,115] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:29,115] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:29,118] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:29,118] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_RESTRICTION$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
INFO  [2023-01-17 00:51:29,118] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_RESTRICTION$20Test.kh_diagnose.kh_diagnose], containing 3 entries.
WARN  [2023-01-17 00:51:29,120] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:29,120] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_RESTRICTION$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-17 00:51:29,240] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,246] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,260] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,261] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:29,367] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CONCEPT_RESTRICTION Test QUERY INIT
INFO  [2023-01-17 00:51:29,384] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CONCEPT_RESTRICTION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:29,385] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[2cfcf4f0-75d7-4792-87cd-53616451e48d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_RESTRICTION Test))]]
INFO  [2023-01-17 00:51:29,389] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d
INFO  [2023-01-17 00:51:29,389] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d
WARN  [2023-01-17 00:51:29,389] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:51:29,389] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d] with 0 results within PT0.000176S
INFO  [2023-01-17 00:51:29,390] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d, workerId=CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_e117f74c-7934-41db-a9c4-84c96916de1b, startTime=2023-01-17T00:51:29.389702, finishTime=2023-01-17T00:51:29.389878) of size 0
INFO  [2023-01-17 00:51:29,390] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d] with 1 results within PT0.000928S
127.0.0.1 - - [17/Jan/2023:00:51:29 +0000] "POST /api/datasets/CONCEPT_RESTRICTION$20Test/queries HTTP/1.1" 201 1291 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:29,391] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d, workerId=CONCEPT_RESTRICTION$20Test.worker_CONCEPT_RESTRICTION$20Test_aa09ce34-feae-4956-9af0-12b6ba9ef579, startTime=2023-01-17T00:51:29.389673, finishTime=2023-01-17T00:51:29.390601) of size 1
INFO  [2023-01-17 00:51:29,391] com.bakdata.conquery.models.execution.ManagedExecution: DONE 2cfcf4f0-75d7-4792-87cd-53616451e48d ManagedQuery within PT0.006169S
127.0.0.1 - - [17/Jan/2023:00:51:29 +0000] "GET /api/datasets/CONCEPT_RESTRICTION$20Test/queries/CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d HTTP/1.1" 200 1586 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:29,419] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_RESTRICTION Test], queryId=2cfcf4f0-75d7-4792-87cd-53616451e48d, label=F20	@§$, creationTime=2023-01-17T00:51:29.385046, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6bd9a28f[Count = 0], startTime=2023-01-17T00:51:29.385292, finishTime=2023-01-17T00:51:29.391461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5a2a094e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@758cc7f9, com.bakdata.conquery.models.query.ColumnDescriptor@7e3bc7e3]) download on dataset Dataset[label=null, name=CONCEPT_RESTRICTION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:29,420] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_RESTRICTION Test], queryId=2cfcf4f0-75d7-4792-87cd-53616451e48d, label=F20	@§$, creationTime=2023-01-17T00:51:29.385046, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6bd9a28f[Count = 0], startTime=2023-01-17T00:51:29.385292, finishTime=2023-01-17T00:51:29.391461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5a2a094e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@758cc7f9, com.bakdata.conquery.models.query.ColumnDescriptor@7e3bc7e3]) on dataset Dataset[label=null, name=CONCEPT_RESTRICTION Test]
127.0.0.1 - - [17/Jan/2023:00:51:29 +0000] "GET /api/datasets/CONCEPT_RESTRICTION%20Test/result/CONCEPT_RESTRICTION$20Test.2cfcf4f0-75d7-4792-87cd-53616451e48d.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:51:29,437] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CONCEPT_RESTRICTION Test on 2 rows
INFO  [2023-01-17 00:51:29,437] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CONCEPT_RESTRICTION Test
INFO  [2023-01-17 00:51:29,438] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-17 00:51:29,438] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_RESTRICTION Test, name=CONCEPT_RESTRICTION Test]
INFO  [2023-01-17 00:51:29,438] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_RESTRICTION Test_e117f74c-7934-41db-a9c4-84c96916de1b
INFO  [2023-01-17 00:51:29,438] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_RESTRICTION Test_aa09ce34-feae-4956-9af0-12b6ba9ef579
INFO  [2023-01-17 00:51:29,518] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CONCEPT_RESTRICTION Test
INFO  [2023-01-17 00:51:29,519] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_RESTRICTION Test_aa09ce34-feae-4956-9af0-12b6ba9ef579
INFO  [2023-01-17 00:51:29,519] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_RESTRICTION Test_e117f74c-7934-41db-a9c4-84c96916de1b
INFO  [2023-01-17 00:51:29,520] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CONCEPT_RESTRICTION$20Test
INFO  [2023-01-17 00:51:29,520] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,670] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CONCEPT_RESTRICTION Test
INFO  [2023-01-17 00:51:29,671] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:29,671] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:29,671] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:29,672] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:29,672] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:29,672] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:29,672] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:29,674] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_b29e929e-b361-4558-b276-56221a1ee542 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:29,674] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_b29e929e-b361-4558-b276-56221a1ee542 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:29,674] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:29,674] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_b8083463-346e-4b78-8b02-f19f0268fef1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:29,674] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_b8083463-346e-4b78-8b02-f19f0268fef1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:29,674] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:29,678] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,777] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,784] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:29,785] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-17 00:51:29,785] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table
INFO  [2023-01-17 00:51:29,910] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,022] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:30,022] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:30,022] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 44 B in total
INFO  [2023-01-17 00:51:30,022] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000308518sINFO  [2023-01-17 00:51:30,053] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:30,053] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:30,056] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:30,056] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:30,056] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CONCEPT_WITHOUT_VALIDITYDATE Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:30,068] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:51:30 +0000] "POST /admin/datasets/CONCEPT_WITHOUT_VALIDITYDATE%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_CONCEPT_WITHOUT_VALIDITYDATE+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:30,068] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,069] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:30,070] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:30,070] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:30,072] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:30,073] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:51:30,073] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table], containing 4 entries.
WARN  [2023-01-17 00:51:30,074] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:30,074] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:30,074] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CONCEPT_WITHOUT_VALIDITYDATE$20Test.test_table.test_table.1
INFO  [2023-01-17 00:51:30,179] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,184] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,198] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,198] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:30,198] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:30,304] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CONCEPT_WITHOUT_VALIDITYDATE Test QUERY INIT
INFO  [2023-01-17 00:51:30,321] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CONCEPT_WITHOUT_VALIDITYDATE$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:30,321] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9ed49e5d-2ebf-40c0-ab94-7989323f4a45] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_WITHOUT_VALIDITYDATE Test))]]
INFO  [2023-01-17 00:51:30,324] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45
INFO  [2023-01-17 00:51:30,324] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45
INFO  [2023-01-17 00:51:30,325] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45] with 0 results within PT0.000658S
INFO  [2023-01-17 00:51:30,325] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45] with 2 results within PT0.001105S
INFO  [2023-01-17 00:51:30,325] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45, workerId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_b8083463-346e-4b78-8b02-f19f0268fef1, startTime=2023-01-17T00:51:30.324688, finishTime=2023-01-17T00:51:30.325346) of size 0
127.0.0.1 - - [17/Jan/2023:00:51:30 +0000] "POST /api/datasets/CONCEPT_WITHOUT_VALIDITYDATE$20Test/queries HTTP/1.1" 201 1460 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:30,326] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45, workerId=CONCEPT_WITHOUT_VALIDITYDATE$20Test.worker_CONCEPT_WITHOUT_VALIDITYDATE$20Test_b29e929e-b361-4558-b276-56221a1ee542, startTime=2023-01-17T00:51:30.324694, finishTime=2023-01-17T00:51:30.325799) of size 2
INFO  [2023-01-17 00:51:30,326] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9ed49e5d-2ebf-40c0-ab94-7989323f4a45 ManagedQuery within PT0.004853S
127.0.0.1 - - [17/Jan/2023:00:51:30 +0000] "GET /api/datasets/CONCEPT_WITHOUT_VALIDITYDATE$20Test/queries/CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45 HTTP/1.1" 200 1791 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:30,367] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test], queryId=9ed49e5d-2ebf-40c0-ab94-7989323f4a45, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:30.321521, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1d4a9541[Count = 0], startTime=2023-01-17T00:51:30.321760, finishTime=2023-01-17T00:51:30.326613, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@181dc115), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@cb343b2, com.bakdata.conquery.models.query.ColumnDescriptor@5bea74ce, com.bakdata.conquery.models.query.ColumnDescriptor@12c3126f]) download on dataset Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:30,368] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test], queryId=9ed49e5d-2ebf-40c0-ab94-7989323f4a45, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:30.321521, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1d4a9541[Count = 0], startTime=2023-01-17T00:51:30.321760, finishTime=2023-01-17T00:51:30.326613, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@181dc115), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CONCEPT_WITHOUT_VALIDITYDATE Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@cb343b2, com.bakdata.conquery.models.query.ColumnDescriptor@5bea74ce, com.bakdata.conquery.models.query.ColumnDescriptor@12c3126f]) on dataset Dataset[label=null, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
127.0.0.1 - - [17/Jan/2023:00:51:30 +0000] "GET /api/datasets/CONCEPT_WITHOUT_VALIDITYDATE%20Test/result/CONCEPT_WITHOUT_VALIDITYDATE$20Test.9ed49e5d-2ebf-40c0-ab94-7989323f4a45.csv?pretty=false HTTP/1.1" 200 82 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:30,370] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CONCEPT_WITHOUT_VALIDITYDATE Test on 3 rows
INFO  [2023-01-17 00:51:30,370] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:30,370] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:30,370] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CONCEPT_WITHOUT_VALIDITYDATE Test, name=CONCEPT_WITHOUT_VALIDITYDATE Test]
INFO  [2023-01-17 00:51:30,370] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_WITHOUT_VALIDITYDATE Test_b8083463-346e-4b78-8b02-f19f0268fef1
INFO  [2023-01-17 00:51:30,371] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CONCEPT_WITHOUT_VALIDITYDATE Test_b29e929e-b361-4558-b276-56221a1ee542
INFO  [2023-01-17 00:51:30,372] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:30,373] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_WITHOUT_VALIDITYDATE Test_b29e929e-b361-4558-b276-56221a1ee542
INFO  [2023-01-17 00:51:30,373] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CONCEPT_WITHOUT_VALIDITYDATE Test_b8083463-346e-4b78-8b02-f19f0268fef1
INFO  [2023-01-17 00:51:30,374] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CONCEPT_WITHOUT_VALIDITYDATE$20Test
INFO  [2023-01-17 00:51:30,375] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,504] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CONCEPT_WITHOUT_VALIDITYDATE Test
INFO  [2023-01-17 00:51:30,505] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:51:30,505] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:30,505] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:30,506] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:30,506] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:30,507] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:30,507] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:30,510] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_02970d2d-ba7d-4d24-a634-5bfd0c67bf98 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:30,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_02970d2d-ba7d-4d24-a634-5bfd0c67bf98 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:30,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:30,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_2490c211-e401-4de6-a932-11e64c7fddd4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:30,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_2490c211-e401-4de6-a932-11e64c7fddd4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:30,511] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:30,615] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,623] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,624] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test.table
INFO  [2023-01-17 00:51:30,624] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test.table
INFO  [2023-01-17 00:51:30,743] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:30,856] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:30,856] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:30,856] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 175 B in total
INFO  [2023-01-17 00:51:30,857] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000350415sINFO  [2023-01-17 00:51:30,892] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=7, min=1, average=1.166667, max=2}
INFO  [2023-01-17 00:51:30,892] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:30,892] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[include] with StringParser(super=Parser(lines=7, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:30,892] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@73d336a2)
INFO  [2023-01-17 00:51:30,895] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:30,895] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:30,895] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:30,916] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SIMPLE_TREECONCEPT_QUERY$20Test.table
INFO  [2023-01-17 00:51:30,916] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:30 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_TREECONCEPT_QUERY+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:30,917] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:30,919] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:30,919] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:30,922] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:30,922] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test.table.table], containing 7 entries.
INFO  [2023-01-17 00:51:30,922] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test.table.table], containing 7 entries.
WARN  [2023-01-17 00:51:30,924] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:30,924] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test.table.table.0
INFO  [2023-01-17 00:51:30,924] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test.table.table.1
INFO  [2023-01-17 00:51:31,029] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,035] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,047] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,047] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:31,047] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:31,153] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:31,168] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:31,169] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0dd36ab2-ad9d-4e74-9157-1065daecde3a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:51:31,172] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a
INFO  [2023-01-17 00:51:31,172] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a
INFO  [2023-01-17 00:51:31,173] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a] with 2 results within PT0.001227S
INFO  [2023-01-17 00:51:31,173] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a] with 0 results within PT0.001272S
127.0.0.1 - - [17/Jan/2023:00:51:31 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1120 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:31,174] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a, workerId=SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_2490c211-e401-4de6-a932-11e64c7fddd4, startTime=2023-01-17T00:51:31.172433, finishTime=2023-01-17T00:51:31.173705) of size 0
INFO  [2023-01-17 00:51:31,174] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a, workerId=SIMPLE_TREECONCEPT_QUERY$20Test.worker_SIMPLE_TREECONCEPT_QUERY$20Test_02970d2d-ba7d-4d24-a634-5bfd0c67bf98, startTime=2023-01-17T00:51:31.172303, finishTime=2023-01-17T00:51:31.173530) of size 2
INFO  [2023-01-17 00:51:31,174] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0dd36ab2-ad9d-4e74-9157-1065daecde3a ManagedQuery within PT0.005312S
127.0.0.1 - - [17/Jan/2023:00:51:31 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test/queries/SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a HTTP/1.1" 200 1435 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:31,201] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test], queryId=0dd36ab2-ad9d-4e74-9157-1065daecde3a, label=tree-a1	@§$, creationTime=2023-01-17T00:51:31.169004, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@568ebd08[Count = 0], startTime=2023-01-17T00:51:31.169290, finishTime=2023-01-17T00:51:31.174602, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@366b7d9b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@247bf5a, com.bakdata.conquery.models.query.ColumnDescriptor@679d3a0c]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:31,202] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test], queryId=0dd36ab2-ad9d-4e74-9157-1065daecde3a, label=tree-a1	@§$, creationTime=2023-01-17T00:51:31.169004, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@568ebd08[Count = 0], startTime=2023-01-17T00:51:31.169290, finishTime=2023-01-17T00:51:31.174602, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@366b7d9b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@247bf5a, com.bakdata.conquery.models.query.ColumnDescriptor@679d3a0c]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:51:31 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY%20Test/result/SIMPLE_TREECONCEPT_QUERY$20Test.0dd36ab2-ad9d-4e74-9157-1065daecde3a.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 26
INFO  [2023-01-17 00:51:31,227] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:51:31,227] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:51:31,227] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:31,227] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test, name=SIMPLE_TREECONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:31,227] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test_2490c211-e401-4de6-a932-11e64c7fddd4
INFO  [2023-01-17 00:51:31,227] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test_02970d2d-ba7d-4d24-a634-5bfd0c67bf98
INFO  [2023-01-17 00:51:31,327] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:51:31,327] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test_02970d2d-ba7d-4d24-a634-5bfd0c67bf98
INFO  [2023-01-17 00:51:31,327] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test_2490c211-e401-4de6-a932-11e64c7fddd4
INFO  [2023-01-17 00:51:31,427] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY$20Test
INFO  [2023-01-17 00:51:31,427] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,453] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:51:31,453] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test CQExternal Extra Data Test
INFO  [2023-01-17 00:51:31,453] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:31,453] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:31,454] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-17 00:51:31,454] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-17 00:51:31,454] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:31,454] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:31,456] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_f9e57ee3-596b-48ed-b5d4-7dff2679ec25 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:31,456] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_f9e57ee3-596b-48ed-b5d4-7dff2679ec25 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:31,456] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:31,456] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_05469858-a2fd-43b5-8ea1-10c3a3efba4b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:31,456] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_05469858-a2fd-43b5-8ea1-10c3a3efba4b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:31,456] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:31,460] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,560] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,567] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,568] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CQExternal$20Extra$20Data$20Test.test_table
INFO  [2023-01-17 00:51:31,568] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table CQExternal$20Extra$20Data$20Test.test_table
INFO  [2023-01-17 00:51:31,682] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,799] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:31,799] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:31,799] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 88 B in total
INFO  [2023-01-17 00:51:31,799] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CQExternal Extra Data Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000275354sINFO  [2023-01-17 00:51:31,827] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=6, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:31,827] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@48ef6e5d)
INFO  [2023-01-17 00:51:31,829] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CQExternal Extra Data Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:31,829] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:31,829] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_CQExternal Extra Data Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:31,845] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into CQExternal$20Extra$20Data$20Test.test_table
INFO  [2023-01-17 00:51:31,846] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:31 +0000] "POST /admin/datasets/CQExternal%20Extra%20Data%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_CQExternal+Extra+Data+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:31,847] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:31,847] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:31,847] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:31,849] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:31,849] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CQExternal$20Extra$20Data$20Test.test_table.test_table], containing 6 entries.
INFO  [2023-01-17 00:51:31,849] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[CQExternal$20Extra$20Data$20Test.test_table.test_table], containing 6 entries.
WARN  [2023-01-17 00:51:31,850] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:31,850] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CQExternal$20Extra$20Data$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:31,850] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received CQExternal$20Extra$20Data$20Test.test_table.test_table.1
INFO  [2023-01-17 00:51:31,967] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,972] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,984] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:31,985] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:31,985] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:32,091] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: CQExternal Extra Data Test QUERY INIT
INFO  [2023-01-17 00:51:32,107] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[CQExternal$20Extra$20Data$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:32,108] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[bb368cf9-3bbc-41a6-b320-937d657fb45b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_CQExternal Extra Data Test))]]
INFO  [2023-01-17 00:51:32,118] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b
INFO  [2023-01-17 00:51:32,118] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b
127.0.0.1 - - [17/Jan/2023:00:51:32 +0000] "POST /api/datasets/CQExternal$20Extra$20Data$20Test/queries HTTP/1.1" 201 1186 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:32,119] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b] with 1 results within PT0.001089S
INFO  [2023-01-17 00:51:32,119] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b] with 2 results within PT0.001205S
INFO  [2023-01-17 00:51:32,120] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b, workerId=CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_05469858-a2fd-43b5-8ea1-10c3a3efba4b, startTime=2023-01-17T00:51:32.118302, finishTime=2023-01-17T00:51:32.119391) of size 1
INFO  [2023-01-17 00:51:32,120] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b, workerId=CQExternal$20Extra$20Data$20Test.worker_CQExternal$20Extra$20Data$20Test_f9e57ee3-596b-48ed-b5d4-7dff2679ec25, startTime=2023-01-17T00:51:32.118346, finishTime=2023-01-17T00:51:32.119551) of size 2
INFO  [2023-01-17 00:51:32,120] com.bakdata.conquery.models.execution.ManagedExecution: DONE bb368cf9-3bbc-41a6-b320-937d657fb45b ManagedQuery within PT0.011872S
127.0.0.1 - - [17/Jan/2023:00:51:32 +0000] "GET /api/datasets/CQExternal$20Extra$20Data$20Test/queries/CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b HTTP/1.1" 200 1506 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:32,148] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CQExternal Extra Data Test], queryId=bb368cf9-3bbc-41a6-b320-937d657fb45b, label=Uploaded-List	@§$, creationTime=2023-01-17T00:51:32.107434, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3d6811fa[Count = 0], startTime=2023-01-17T00:51:32.108524, finishTime=2023-01-17T00:51:32.120396, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2c6b2a3d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CQExternal Extra Data Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e55c6e1, com.bakdata.conquery.models.query.ColumnDescriptor@f872e9b, com.bakdata.conquery.models.query.ColumnDescriptor@721d1950]) download on dataset Dataset[label=null, name=CQExternal Extra Data Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:32,148] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=CQExternal Extra Data Test], queryId=bb368cf9-3bbc-41a6-b320-937d657fb45b, label=Uploaded-List	@§$, creationTime=2023-01-17T00:51:32.107434, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3d6811fa[Count = 0], startTime=2023-01-17T00:51:32.108524, finishTime=2023-01-17T00:51:32.120396, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2c6b2a3d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_CQExternal Extra Data Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e55c6e1, com.bakdata.conquery.models.query.ColumnDescriptor@f872e9b, com.bakdata.conquery.models.query.ColumnDescriptor@721d1950]) on dataset Dataset[label=null, name=CQExternal Extra Data Test]
127.0.0.1 - - [17/Jan/2023:00:51:32 +0000] "GET /api/datasets/CQExternal%20Extra%20Data%20Test/result/CQExternal$20Extra$20Data$20Test.bb368cf9-3bbc-41a6-b320-937d657fb45b.csv?pretty=false HTTP/1.1" 200 132 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:51:32,171] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest CQExternal Extra Data Test on 4 rows
INFO  [2023-01-17 00:51:32,171] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast CQExternal Extra Data Test
INFO  [2023-01-17 00:51:32,172] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-17 00:51:32,172] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=CQExternal Extra Data Test, name=CQExternal Extra Data Test]
INFO  [2023-01-17 00:51:32,172] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CQExternal Extra Data Test_f9e57ee3-596b-48ed-b5d4-7dff2679ec25
INFO  [2023-01-17 00:51:32,172] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_CQExternal Extra Data Test_05469858-a2fd-43b5-8ea1-10c3a3efba4b
INFO  [2023-01-17 00:51:32,269] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow CQExternal Extra Data Test
INFO  [2023-01-17 00:51:32,269] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CQExternal Extra Data Test_f9e57ee3-596b-48ed-b5d4-7dff2679ec25
INFO  [2023-01-17 00:51:32,269] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_CQExternal Extra Data Test_05469858-a2fd-43b5-8ea1-10c3a3efba4b
INFO  [2023-01-17 00:51:32,369] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of CQExternal$20Extra$20Data$20Test
INFO  [2023-01-17 00:51:32,369] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,390] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test CQExternal Extra Data Test
INFO  [2023-01-17 00:51:32,391] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_ERSTER Test
INFO  [2023-01-17 00:51:32,391] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:32,391] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:32,392] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-17 00:51:32,392] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-17 00:51:32,392] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:32,392] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:32,394] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_3d5d6919-8dff-423d-9ad5-493aca41f1f8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:32,394] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_3d5d6919-8dff-423d-9ad5-493aca41f1f8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:32,394] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:32,394] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_d2535c07-9744-4a71-8b62-63bb8aafee65 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:32,394] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_d2535c07-9744-4a71-8b62-63bb8aafee65 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:32,394] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:32,398] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,497] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,504] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_ERSTER$20Test.table1
INFO  [2023-01-17 00:51:32,505] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,505] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_ERSTER$20Test.table1
INFO  [2023-01-17 00:51:32,620] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,729] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:32,729] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:32,729] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:32,729] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_ERSTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000449728sINFO  [2023-01-17 00:51:32,775] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:32,775] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@33631084)
INFO  [2023-01-17 00:51:32,775] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@6721b92d)
INFO  [2023-01-17 00:51:32,775] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@5b52d6c1)
INFO  [2023-01-17 00:51:32,775] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@7945a66), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@7ab8fc7a), dateReader=com.bakdata.conquery.util.DateReader@15e0e3d8, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:32,777] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:32,778] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:32,778] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:32,794] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_ERSTER$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:51:32 +0000] "POST /admin/datasets/DATE_DISTANCE_ERSTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_ERSTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:32,795] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,796] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:32,796] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:32,796] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:32,798] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:32,799] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:32,799] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:32,800] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.0
INFO  [2023-01-17 00:51:32,800] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.1
INFO  [2023-01-17 00:51:32,800] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.2
INFO  [2023-01-17 00:51:32,800] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.4
INFO  [2023-01-17 00:51:32,800] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.3
WARN  [2023-01-17 00:51:32,801] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:32,801] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.5
INFO  [2023-01-17 00:51:32,801] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.6
INFO  [2023-01-17 00:51:32,801] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_ERSTER$20Test.table1.table1.7
INFO  [2023-01-17 00:51:32,907] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,912] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,924] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:32,924] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:32,924] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:33,030] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_ERSTER Test QUERY INIT
INFO  [2023-01-17 00:51:33,047] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_ERSTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:33,047] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9ba5a207-1068-4c65-bd19-46296d2008e3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_ERSTER Test))]]
INFO  [2023-01-17 00:51:33,050] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3
127.0.0.1 - - [17/Jan/2023:00:51:33 +0000] "POST /api/datasets/DATE_DISTANCE_ERSTER$20Test/queries HTTP/1.1" 201 1393 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:33,050] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3
INFO  [2023-01-17 00:51:33,053] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3] with 2 results within PT0.003144S
INFO  [2023-01-17 00:51:33,053] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3] with 0 results within PT0.002801S
INFO  [2023-01-17 00:51:33,054] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3, workerId=DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_d2535c07-9744-4a71-8b62-63bb8aafee65, startTime=2023-01-17T00:51:33.050341, finishTime=2023-01-17T00:51:33.053485) of size 2
INFO  [2023-01-17 00:51:33,054] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3, workerId=DATE_DISTANCE_ERSTER$20Test.worker_DATE_DISTANCE_ERSTER$20Test_3d5d6919-8dff-423d-9ad5-493aca41f1f8, startTime=2023-01-17T00:51:33.050871, finishTime=2023-01-17T00:51:33.053672) of size 0
INFO  [2023-01-17 00:51:33,054] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9ba5a207-1068-4c65-bd19-46296d2008e3 ManagedQuery within PT0.006742S
127.0.0.1 - - [17/Jan/2023:00:51:33 +0000] "GET /api/datasets/DATE_DISTANCE_ERSTER$20Test/queries/DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3 HTTP/1.1" 200 1692 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:33,077] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_ERSTER Test], queryId=9ba5a207-1068-4c65-bd19-46296d2008e3, label=Alter	@§$, creationTime=2023-01-17T00:51:33.047454, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@47a65bb5[Count = 0], startTime=2023-01-17T00:51:33.047722, finishTime=2023-01-17T00:51:33.054464, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1493574f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3b9ecb03, com.bakdata.conquery.models.query.ColumnDescriptor@3cce4073]) download on dataset Dataset[label=null, name=DATE_DISTANCE_ERSTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:33,077] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_ERSTER Test], queryId=9ba5a207-1068-4c65-bd19-46296d2008e3, label=Alter	@§$, creationTime=2023-01-17T00:51:33.047454, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@47a65bb5[Count = 0], startTime=2023-01-17T00:51:33.047722, finishTime=2023-01-17T00:51:33.054464, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1493574f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3b9ecb03, com.bakdata.conquery.models.query.ColumnDescriptor@3cce4073]) on dataset Dataset[label=null, name=DATE_DISTANCE_ERSTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:33 +0000] "GET /api/datasets/DATE_DISTANCE_ERSTER%20Test/result/DATE_DISTANCE_ERSTER$20Test.9ba5a207-1068-4c65-bd19-46296d2008e3.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:33,096] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_ERSTER Test on 3 rows
INFO  [2023-01-17 00:51:33,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_ERSTER Test
INFO  [2023-01-17 00:51:33,097] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-17 00:51:33,097] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_ERSTER Test, name=DATE_DISTANCE_ERSTER Test]
INFO  [2023-01-17 00:51:33,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_ERSTER Test_3d5d6919-8dff-423d-9ad5-493aca41f1f8
INFO  [2023-01-17 00:51:33,097] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_ERSTER Test_d2535c07-9744-4a71-8b62-63bb8aafee65
INFO  [2023-01-17 00:51:33,195] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_ERSTER Test_d2535c07-9744-4a71-8b62-63bb8aafee65
INFO  [2023-01-17 00:51:33,195] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_ERSTER Test
INFO  [2023-01-17 00:51:33,195] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_ERSTER Test_3d5d6919-8dff-423d-9ad5-493aca41f1f8
INFO  [2023-01-17 00:51:33,201] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_ERSTER$20Test
INFO  [2023-01-17 00:51:33,201] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,330] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_ERSTER Test
INFO  [2023-01-17 00:51:33,330] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_LETZTER Test
INFO  [2023-01-17 00:51:33,330] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:33,331] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:33,332] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-17 00:51:33,332] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-17 00:51:33,332] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:33,332] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:33,333] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,333] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_1efdc2fe-2868-4697-881e-1aee0ac47aa0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:33,333] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_1efdc2fe-2868-4697-881e-1aee0ac47aa0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:33,333] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:33,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_7c4e180b-2f17-4b5b-a448-abb062cbcf17 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:33,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_7c4e180b-2f17-4b5b-a448-abb062cbcf17 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:33,334] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:33,437] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,444] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,445] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:33,445] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:33,562] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,680] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:33,680] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:33,680] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:33,680] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_LETZTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000294465sINFO  [2023-01-17 00:51:33,710] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:33,710] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@2efcaf11)
INFO  [2023-01-17 00:51:33,710] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6b33020f)
INFO  [2023-01-17 00:51:33,710] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@46c425ab), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@6326107f), dateReader=com.bakdata.conquery.util.DateReader@5ec06b4d, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:33,711] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@4cacad6e)
INFO  [2023-01-17 00:51:33,714] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:33,715] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:33,715] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:33,739] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:33,739] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:33 +0000] "POST /admin/datasets/DATE_DISTANCE_LETZTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_LETZTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:33,740] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:33,740] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:33,740] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:33,742] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:33,743] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:33,743] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:33,744] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.0
INFO  [2023-01-17 00:51:33,744] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.2
INFO  [2023-01-17 00:51:33,744] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.1
INFO  [2023-01-17 00:51:33,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.3
INFO  [2023-01-17 00:51:33,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.5
INFO  [2023-01-17 00:51:33,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.4
WARN  [2023-01-17 00:51:33,745] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:33,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.6
INFO  [2023-01-17 00:51:33,745] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_LETZTER$20Test.table1.table1.7
INFO  [2023-01-17 00:51:33,850] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,855] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,871] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:33,872] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:33,872] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:33,978] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_LETZTER Test QUERY INIT
INFO  [2023-01-17 00:51:33,994] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_LETZTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:33,995] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[cdc3802f-5c07-49df-8ca1-eefdf215cb4b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_LETZTER Test))]]
INFO  [2023-01-17 00:51:34,000] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b
INFO  [2023-01-17 00:51:34,000] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "POST /api/datasets/DATE_DISTANCE_LETZTER$20Test/queries HTTP/1.1" 201 1397 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:34,003] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b] with 0 results within PT0.003173S
INFO  [2023-01-17 00:51:34,003] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b] with 2 results within PT0.003665S
INFO  [2023-01-17 00:51:34,004] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b, workerId=DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_1efdc2fe-2868-4697-881e-1aee0ac47aa0, startTime=2023-01-17T00:51:34.000460, finishTime=2023-01-17T00:51:34.003633) of size 0
INFO  [2023-01-17 00:51:34,004] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b, workerId=DATE_DISTANCE_LETZTER$20Test.worker_DATE_DISTANCE_LETZTER$20Test_7c4e180b-2f17-4b5b-a448-abb062cbcf17, startTime=2023-01-17T00:51:34.000303, finishTime=2023-01-17T00:51:34.003968) of size 2
INFO  [2023-01-17 00:51:34,004] com.bakdata.conquery.models.execution.ManagedExecution: DONE cdc3802f-5c07-49df-8ca1-eefdf215cb4b ManagedQuery within PT0.009549S
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "GET /api/datasets/DATE_DISTANCE_LETZTER$20Test/queries/DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b HTTP/1.1" 200 1700 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:34,030] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_LETZTER Test], queryId=cdc3802f-5c07-49df-8ca1-eefdf215cb4b, label=Alter	@§$, creationTime=2023-01-17T00:51:33.995084, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2271c940[Count = 0], startTime=2023-01-17T00:51:33.995307, finishTime=2023-01-17T00:51:34.004856, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@59d10dcd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2319048c, com.bakdata.conquery.models.query.ColumnDescriptor@492cbd51]) download on dataset Dataset[label=null, name=DATE_DISTANCE_LETZTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:34,030] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_LETZTER Test], queryId=cdc3802f-5c07-49df-8ca1-eefdf215cb4b, label=Alter	@§$, creationTime=2023-01-17T00:51:33.995084, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2271c940[Count = 0], startTime=2023-01-17T00:51:33.995307, finishTime=2023-01-17T00:51:34.004856, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@59d10dcd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2319048c, com.bakdata.conquery.models.query.ColumnDescriptor@492cbd51]) on dataset Dataset[label=null, name=DATE_DISTANCE_LETZTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "GET /api/datasets/DATE_DISTANCE_LETZTER%20Test/result/DATE_DISTANCE_LETZTER$20Test.cdc3802f-5c07-49df-8ca1-eefdf215cb4b.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:51:34,048] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_LETZTER Test on 3 rows
INFO  [2023-01-17 00:51:34,048] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_LETZTER Test
INFO  [2023-01-17 00:51:34,049] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-17 00:51:34,049] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_LETZTER Test, name=DATE_DISTANCE_LETZTER Test]
INFO  [2023-01-17 00:51:34,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_LETZTER Test_1efdc2fe-2868-4697-881e-1aee0ac47aa0
INFO  [2023-01-17 00:51:34,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_LETZTER Test_7c4e180b-2f17-4b5b-a448-abb062cbcf17
INFO  [2023-01-17 00:51:34,146] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_LETZTER Test
INFO  [2023-01-17 00:51:34,146] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_LETZTER Test_1efdc2fe-2868-4697-881e-1aee0ac47aa0
INFO  [2023-01-17 00:51:34,146] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_LETZTER Test_7c4e180b-2f17-4b5b-a448-abb062cbcf17
INFO  [2023-01-17 00:51:34,246] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_LETZTER$20Test
INFO  [2023-01-17 00:51:34,246] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,281] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_LETZTER Test
INFO  [2023-01-17 00:51:34,281] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:34,281] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:34,281] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:34,282] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:34,282] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:34,282] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:34,282] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:34,284] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_e89fc47d-0493-42de-98e4-664d797f9f90 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:34,284] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_e89fc47d-0493-42de-98e4-664d797f9f90 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:34,284] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:34,284] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_b8b31947-8a65-49ac-a39d-74268edcbd8b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:34,284] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_b8b31947-8a65-49ac-a39d-74268edcbd8b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:34,284] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:34,289] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,388] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,395] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,395] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:34,395] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:34,512] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,622] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:34,622] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:34,623] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:34,623] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000244853sINFO  [2023-01-17 00:51:34,648] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:34,648] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@6f675321)
INFO  [2023-01-17 00:51:34,648] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@2ba2302d)
INFO  [2023-01-17 00:51:34,648] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@14ae337f)
INFO  [2023-01-17 00:51:34,648] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@3dbca348), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4f2df12e), dateReader=com.bakdata.conquery.util.DateReader@7fb7c3a8, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:34,650] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:34,650] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:34,650] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:34,666] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "POST /admin/datasets/DATE_DISTANCE_VERSICHERTENZEIT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_VERSICHERTENZEIT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:34,667] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,667] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:34,667] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:34,667] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:34,669] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:34,669] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:34,670] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:34,671] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.0
INFO  [2023-01-17 00:51:34,671] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.1
INFO  [2023-01-17 00:51:34,671] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.2
WARN  [2023-01-17 00:51:34,671] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:34,672] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.4
INFO  [2023-01-17 00:51:34,672] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.3
INFO  [2023-01-17 00:51:34,672] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.6
INFO  [2023-01-17 00:51:34,672] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.5
INFO  [2023-01-17 00:51:34,672] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_VERSICHERTENZEIT$20Test.table1.table1.7
INFO  [2023-01-17 00:51:34,777] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,794] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,809] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:34,809] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:34,809] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:34,916] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_VERSICHERTENZEIT Test QUERY INIT
INFO  [2023-01-17 00:51:34,932] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_VERSICHERTENZEIT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:34,933] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1abefb79-3197-461d-848e-72024cd96b10] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_VERSICHERTENZEIT Test))]]
INFO  [2023-01-17 00:51:34,938] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10
INFO  [2023-01-17 00:51:34,938] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "POST /api/datasets/DATE_DISTANCE_VERSICHERTENZEIT$20Test/queries HTTP/1.1" 201 1433 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:34,941] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10] with 4 results within PT0.003415S
INFO  [2023-01-17 00:51:34,941] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10] with 0 results within PT0.003061S
INFO  [2023-01-17 00:51:34,942] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10, workerId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_b8b31947-8a65-49ac-a39d-74268edcbd8b, startTime=2023-01-17T00:51:34.938150, finishTime=2023-01-17T00:51:34.941565) of size 4
INFO  [2023-01-17 00:51:34,942] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10, workerId=DATE_DISTANCE_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_VERSICHERTENZEIT$20Test_e89fc47d-0493-42de-98e4-664d797f9f90, startTime=2023-01-17T00:51:34.938707, finishTime=2023-01-17T00:51:34.941768) of size 0
INFO  [2023-01-17 00:51:34,942] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1abefb79-3197-461d-848e-72024cd96b10 ManagedQuery within PT0.009232S
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "GET /api/datasets/DATE_DISTANCE_VERSICHERTENZEIT$20Test/queries/DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10 HTTP/1.1" 200 1772 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:51:34,963] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test], queryId=1abefb79-3197-461d-848e-72024cd96b10, label=Alter	@§$, creationTime=2023-01-17T00:51:34.933091, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6233d7df[Count = 0], startTime=2023-01-17T00:51:34.933346, finishTime=2023-01-17T00:51:34.942578, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2c5c38e0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7fea318a, com.bakdata.conquery.models.query.ColumnDescriptor@fb57798]) download on dataset Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:34,963] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test], queryId=1abefb79-3197-461d-848e-72024cd96b10, label=Alter	@§$, creationTime=2023-01-17T00:51:34.933091, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6233d7df[Count = 0], startTime=2023-01-17T00:51:34.933346, finishTime=2023-01-17T00:51:34.942578, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2c5c38e0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7fea318a, com.bakdata.conquery.models.query.ColumnDescriptor@fb57798]) on dataset Dataset[label=null, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
127.0.0.1 - - [17/Jan/2023:00:51:34 +0000] "GET /api/datasets/DATE_DISTANCE_VERSICHERTENZEIT%20Test/result/DATE_DISTANCE_VERSICHERTENZEIT$20Test.1abefb79-3197-461d-848e-72024cd96b10.csv?pretty=false HTTP/1.1" 200 117 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:34,984] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_VERSICHERTENZEIT Test on 5 rows
INFO  [2023-01-17 00:51:34,984] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:34,984] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:34,984] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_VERSICHERTENZEIT Test, name=DATE_DISTANCE_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:34,985] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_VERSICHERTENZEIT Test_e89fc47d-0493-42de-98e4-664d797f9f90
INFO  [2023-01-17 00:51:34,985] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_VERSICHERTENZEIT Test_b8b31947-8a65-49ac-a39d-74268edcbd8b
INFO  [2023-01-17 00:51:34,994] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_VERSICHERTENZEIT Test_e89fc47d-0493-42de-98e4-664d797f9f90
INFO  [2023-01-17 00:51:34,994] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_VERSICHERTENZEIT Test_b8b31947-8a65-49ac-a39d-74268edcbd8b
INFO  [2023-01-17 00:51:34,995] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:35,082] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_VERSICHERTENZEIT$20Test
INFO  [2023-01-17 00:51:35,082] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,115] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:35,115] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-17 00:51:35,116] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:35,116] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:35,117] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-17 00:51:35,117] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:35,117] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-17 00:51:35,117] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_fb33eda0-9ea5-417e-9a34-ba57715750a5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_fb33eda0-9ea5-417e-9a34-ba57715750a5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_7016fba9-e54e-4324-bd57-fe1396647683 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_7016fba9-e54e-4324-bd57-fe1396647683 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:35,118] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:35,227] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,233] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,234] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1
INFO  [2023-01-17 00:51:35,234] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1
INFO  [2023-01-17 00:51:35,351] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,462] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:35,462] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:35,462] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:35,462] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_ERSTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00033328sINFO  [2023-01-17 00:51:35,496] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:35,496] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@215919cc)
INFO  [2023-01-17 00:51:35,496] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@15b5e5a2), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@4dcba2bf), dateReader=com.bakdata.conquery.util.DateReader@6d4f9ea9, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:35,496] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@7d202c91)
INFO  [2023-01-17 00:51:35,496] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@785468ce)
INFO  [2023-01-17 00:51:35,499] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:35,499] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:35,499] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:35,517] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:51:35 +0000] "POST /admin/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_AGE_SPAN_ERSTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:51:35,518] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,519] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:35,519] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:35,519] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:35,528] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:35,528] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:35,528] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:35,530] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.0
INFO  [2023-01-17 00:51:35,530] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.1
INFO  [2023-01-17 00:51:35,530] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.2
INFO  [2023-01-17 00:51:35,530] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.3
INFO  [2023-01-17 00:51:35,530] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.4
INFO  [2023-01-17 00:51:35,530] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.5
WARN  [2023-01-17 00:51:35,531] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:35,531] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.7
INFO  [2023-01-17 00:51:35,572] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.table1.table1.6
INFO  [2023-01-17 00:51:35,681] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,686] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,701] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:35,701] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:35,701] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:35,807] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGE_SPAN_ERSTER Test QUERY INIT
INFO  [2023-01-17 00:51:35,825] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:35,826] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_ERSTER Test))]]
INFO  [2023-01-17 00:51:35,831] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5
INFO  [2023-01-17 00:51:35,831] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5
127.0.0.1 - - [17/Jan/2023:00:51:35 +0000] "POST /api/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test/queries HTTP/1.1" 201 1429 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:35,835] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5] with 1 results within PT0.003339S
INFO  [2023-01-17 00:51:35,835] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5] with 3 results within PT0.003698S
INFO  [2023-01-17 00:51:35,835] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5, workerId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_fb33eda0-9ea5-417e-9a34-ba57715750a5, startTime=2023-01-17T00:51:35.831669, finishTime=2023-01-17T00:51:35.835008) of size 1
INFO  [2023-01-17 00:51:35,835] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5, workerId=DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_ERSTER$20Test_7016fba9-e54e-4324-bd57-fe1396647683, startTime=2023-01-17T00:51:35.831418, finishTime=2023-01-17T00:51:35.835116) of size 3
INFO  [2023-01-17 00:51:35,836] com.bakdata.conquery.models.execution.ManagedExecution: DONE d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5 ManagedQuery within PT0.009763S
127.0.0.1 - - [17/Jan/2023:00:51:35 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test/queries/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5 HTTP/1.1" 200 1764 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:35,857] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test], queryId=d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5, label=Alter	@§$, creationTime=2023-01-17T00:51:35.826022, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@613ea8e2[Count = 0], startTime=2023-01-17T00:51:35.826248, finishTime=2023-01-17T00:51:35.836011, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3fcb58e3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3b1fb2eb, com.bakdata.conquery.models.query.ColumnDescriptor@29c86366]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:35,857] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test], queryId=d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5, label=Alter	@§$, creationTime=2023-01-17T00:51:35.826022, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@613ea8e2[Count = 0], startTime=2023-01-17T00:51:35.826248, finishTime=2023-01-17T00:51:35.836011, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3fcb58e3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3b1fb2eb, com.bakdata.conquery.models.query.ColumnDescriptor@29c86366]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:35 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_ERSTER%20Test/result/DATE_DISTANCE_AGE_SPAN_ERSTER$20Test.d7ebec29-f1ff-4b2c-a9ee-7cb01ddd60f5.csv?pretty=false HTTP/1.1" 200 119 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:51:35,875] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGE_SPAN_ERSTER Test on 5 rows
INFO  [2023-01-17 00:51:35,875] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-17 00:51:35,875] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-17 00:51:35,875] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_ERSTER Test, name=DATE_DISTANCE_AGE_SPAN_ERSTER Test]
INFO  [2023-01-17 00:51:35,876] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_fb33eda0-9ea5-417e-9a34-ba57715750a5
INFO  [2023-01-17 00:51:35,876] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_7016fba9-e54e-4324-bd57-fe1396647683
INFO  [2023-01-17 00:51:35,917] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-17 00:51:35,918] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_7016fba9-e54e-4324-bd57-fe1396647683
INFO  [2023-01-17 00:51:35,918] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_ERSTER Test_fb33eda0-9ea5-417e-9a34-ba57715750a5
INFO  [2023-01-17 00:51:35,931] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGE_SPAN_ERSTER$20Test
INFO  [2023-01-17 00:51:35,931] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,107] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGE_SPAN_ERSTER Test
INFO  [2023-01-17 00:51:36,107] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-17 00:51:36,107] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:36,107] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:36,108] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-17 00:51:36,108] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-17 00:51:36,108] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:36,108] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_de3d2df2-5ed4-422e-b8a5-220b4a4da415 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_de3d2df2-5ed4-422e-b8a5-220b4a4da415 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_cda869e8-aae4-4869-895e-819821dfeab4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_cda869e8-aae4-4869-895e-819821dfeab4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:36,110] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:36,214] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,221] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,221] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:36,222] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:36,338] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,455] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:36,455] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:36,455] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:36,455] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_LETZTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00017785sINFO  [2023-01-17 00:51:36,474] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:36,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@896d079)
INFO  [2023-01-17 00:51:36,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@2513ee8a)
INFO  [2023-01-17 00:51:36,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@5bca9b76)
INFO  [2023-01-17 00:51:36,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@2ca4aa2b), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@1686d2e8), dateReader=com.bakdata.conquery.util.DateReader@6c854dd, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:36,476] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:36,476] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:36,476] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:36,494] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:36,495] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:36 +0000] "POST /admin/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_AGE_SPAN_LETZTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:36,496] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:36,496] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:36,496] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:36,498] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:36,498] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:36,498] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:36,500] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.0
INFO  [2023-01-17 00:51:36,500] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.1
INFO  [2023-01-17 00:51:36,500] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.2
INFO  [2023-01-17 00:51:36,500] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.3
INFO  [2023-01-17 00:51:36,500] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.4
WARN  [2023-01-17 00:51:36,501] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:36,501] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.5
INFO  [2023-01-17 00:51:36,501] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.6
INFO  [2023-01-17 00:51:36,501] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.table1.table1.7
INFO  [2023-01-17 00:51:36,606] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,611] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,628] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,628] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:36,628] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:36,733] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGE_SPAN_LETZTER Test QUERY INIT
INFO  [2023-01-17 00:51:36,750] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:36,750] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a14bdc2c-43d5-4d64-9c12-c6c75fa29d40] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_LETZTER Test))]]
INFO  [2023-01-17 00:51:36,753] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40
INFO  [2023-01-17 00:51:36,754] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40
127.0.0.1 - - [17/Jan/2023:00:51:36 +0000] "POST /api/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test/queries HTTP/1.1" 201 1433 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:36,758] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40] with 0 results within PT0.003895S
INFO  [2023-01-17 00:51:36,758] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40] with 4 results within PT0.004506S
INFO  [2023-01-17 00:51:36,758] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40, workerId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_de3d2df2-5ed4-422e-b8a5-220b4a4da415, startTime=2023-01-17T00:51:36.754113, finishTime=2023-01-17T00:51:36.758008) of size 0
INFO  [2023-01-17 00:51:36,759] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40, workerId=DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.worker_DATE_DISTANCE_AGE_SPAN_LETZTER$20Test_cda869e8-aae4-4869-895e-819821dfeab4, startTime=2023-01-17T00:51:36.753998, finishTime=2023-01-17T00:51:36.758504) of size 4
INFO  [2023-01-17 00:51:36,759] com.bakdata.conquery.models.execution.ManagedExecution: DONE a14bdc2c-43d5-4d64-9c12-c6c75fa29d40 ManagedQuery within PT0.008351S
127.0.0.1 - - [17/Jan/2023:00:51:36 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test/queries/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40 HTTP/1.1" 200 1772 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:36,777] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test], queryId=a14bdc2c-43d5-4d64-9c12-c6c75fa29d40, label=Alter	@§$, creationTime=2023-01-17T00:51:36.750571, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@70867e69[Count = 0], startTime=2023-01-17T00:51:36.750766, finishTime=2023-01-17T00:51:36.759117, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@17239606), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@25649cb1, com.bakdata.conquery.models.query.ColumnDescriptor@3d797ec9]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:36,777] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test], queryId=a14bdc2c-43d5-4d64-9c12-c6c75fa29d40, label=Alter	@§$, creationTime=2023-01-17T00:51:36.750571, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@70867e69[Count = 0], startTime=2023-01-17T00:51:36.750766, finishTime=2023-01-17T00:51:36.759117, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@17239606), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@25649cb1, com.bakdata.conquery.models.query.ColumnDescriptor@3d797ec9]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:36 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_LETZTER%20Test/result/DATE_DISTANCE_AGE_SPAN_LETZTER$20Test.a14bdc2c-43d5-4d64-9c12-c6c75fa29d40.csv?pretty=false HTTP/1.1" 200 119 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:51:36,794] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGE_SPAN_LETZTER Test on 5 rows
INFO  [2023-01-17 00:51:36,794] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-17 00:51:36,794] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-17 00:51:36,794] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_LETZTER Test, name=DATE_DISTANCE_AGE_SPAN_LETZTER Test]
INFO  [2023-01-17 00:51:36,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_cda869e8-aae4-4869-895e-819821dfeab4
INFO  [2023-01-17 00:51:36,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_de3d2df2-5ed4-422e-b8a5-220b4a4da415
INFO  [2023-01-17 00:51:36,809] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-17 00:51:36,809] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_de3d2df2-5ed4-422e-b8a5-220b4a4da415
INFO  [2023-01-17 00:51:36,810] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_LETZTER Test_cda869e8-aae4-4869-895e-819821dfeab4
INFO  [2023-01-17 00:51:36,901] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGE_SPAN_LETZTER$20Test
INFO  [2023-01-17 00:51:36,901] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:36,933] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGE_SPAN_LETZTER Test
INFO  [2023-01-17 00:51:36,933] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:36,933] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:36,933] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:36,934] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:36,934] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:36,934] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:36,934] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:36,935] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_545eb76b-9e31-4603-93bf-356022343c09 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:36,935] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_545eb76b-9e31-4603-93bf-356022343c09 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:36,935] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:36,936] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_a52830fe-0964-46a3-a739-50e982f92ad8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:36,936] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_a52830fe-0964-46a3-a739-50e982f92ad8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:36,936] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:36,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,047] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,053] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,053] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:37,053] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:37,171] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,283] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:37,283] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:37,283] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:37,283] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000209157sINFO  [2023-01-17 00:51:37,305] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:37,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@54877819)
INFO  [2023-01-17 00:51:37,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@17e3287b)
INFO  [2023-01-17 00:51:37,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@cfdf09b)
INFO  [2023-01-17 00:51:37,305] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1d2f3e8), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@64021d66), dateReader=com.bakdata.conquery.util.DateReader@18baf1d3, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:37,308] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:37,308] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:37,308] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:37,329] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:37,330] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:37 +0000] "POST /admin/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:51:37,331] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:37,331] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:37,331] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:37,332] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:37,332] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:37,332] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:37,333] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.0
INFO  [2023-01-17 00:51:37,333] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.1
INFO  [2023-01-17 00:51:37,333] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.4
WARN  [2023-01-17 00:51:37,333] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:37,333] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.2
INFO  [2023-01-17 00:51:37,333] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.3
INFO  [2023-01-17 00:51:37,334] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.5
INFO  [2023-01-17 00:51:37,334] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.7
INFO  [2023-01-17 00:51:37,380] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.table1.table1.6
INFO  [2023-01-17 00:51:37,485] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,490] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,506] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:37,506] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:37,612] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test QUERY INIT
INFO  [2023-01-17 00:51:37,629] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:37,629] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c450ce49-6414-4afe-baaa-ca607c0943e0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test))]]
INFO  [2023-01-17 00:51:37,633] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0
INFO  [2023-01-17 00:51:37,633] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0
127.0.0.1 - - [17/Jan/2023:00:51:37 +0000] "POST /api/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test/queries HTTP/1.1" 201 1469 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:37,635] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0] with 2 results within PT0.002139S
INFO  [2023-01-17 00:51:37,635] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0] with 6 results within PT0.002668S
INFO  [2023-01-17 00:51:37,636] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0, workerId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_a52830fe-0964-46a3-a739-50e982f92ad8, startTime=2023-01-17T00:51:37.633048, finishTime=2023-01-17T00:51:37.635187) of size 2
INFO  [2023-01-17 00:51:37,636] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0, workerId=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test_545eb76b-9e31-4603-93bf-356022343c09, startTime=2023-01-17T00:51:37.633214, finishTime=2023-01-17T00:51:37.635882) of size 6
INFO  [2023-01-17 00:51:37,637] com.bakdata.conquery.models.execution.ManagedExecution: DONE c450ce49-6414-4afe-baaa-ca607c0943e0 ManagedQuery within PT0.007109S
127.0.0.1 - - [17/Jan/2023:00:51:37 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test/queries/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0 HTTP/1.1" 200 1844 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:37,661] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test], queryId=c450ce49-6414-4afe-baaa-ca607c0943e0, label=Alter	@§$, creationTime=2023-01-17T00:51:37.629713, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@18f3453c[Count = 0], startTime=2023-01-17T00:51:37.629879, finishTime=2023-01-17T00:51:37.636988, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@19c20bc5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=8, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2470682d, com.bakdata.conquery.models.query.ColumnDescriptor@30d169bf]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:37,661] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test], queryId=c450ce49-6414-4afe-baaa-ca607c0943e0, label=Alter	@§$, creationTime=2023-01-17T00:51:37.629713, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@18f3453c[Count = 0], startTime=2023-01-17T00:51:37.629879, finishTime=2023-01-17T00:51:37.636988, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@19c20bc5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=8, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2470682d, com.bakdata.conquery.models.query.ColumnDescriptor@30d169bf]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
127.0.0.1 - - [17/Jan/2023:00:51:37 +0000] "GET /api/datasets/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT%20Test/result/DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test.c450ce49-6414-4afe-baaa-ca607c0943e0.csv?pretty=false HTTP/1.1" 200 225 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:37,682] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test on 9 rows
INFO  [2023-01-17 00:51:37,682] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:37,682] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:37,682] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test, name=DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:37,683] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_a52830fe-0964-46a3-a739-50e982f92ad8
INFO  [2023-01-17 00:51:37,683] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_545eb76b-9e31-4603-93bf-356022343c09
INFO  [2023-01-17 00:51:37,735] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:37,735] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_545eb76b-9e31-4603-93bf-356022343c09
INFO  [2023-01-17 00:51:37,743] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test_a52830fe-0964-46a3-a739-50e982f92ad8
INFO  [2023-01-17 00:51:37,835] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT$20Test
INFO  [2023-01-17 00:51:37,835] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:37,914] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGE_SPAN_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:37,920] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-17 00:51:37,920] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:37,920] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:37,928] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-17 00:51:37,928] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-17 00:51:37,928] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:37,928] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:37,938] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_28e9fde5-221d-4373-9292-f70c98fe087c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:37,938] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_28e9fde5-221d-4373-9292-f70c98fe087c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:37,938] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:37,938] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_de57bc3b-ed93-4cc5-8a09-c719add83220 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:37,938] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_de57bc3b-ed93-4cc5-8a09-c719add83220 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:37,938] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:37,941] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,041] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,047] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,047] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_ERSTER$20Test.table1
INFO  [2023-01-17 00:51:38,048] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_ERSTER$20Test.table1
INFO  [2023-01-17 00:51:38,165] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,275] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:38,275] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:38,275] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:38,276] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_ERSTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000342251sINFO  [2023-01-17 00:51:38,310] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:38,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@9f74285)
INFO  [2023-01-17 00:51:38,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@36648a71)
INFO  [2023-01-17 00:51:38,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@1e9f2d5e)
INFO  [2023-01-17 00:51:38,310] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1724d899), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@17f2426), dateReader=com.bakdata.conquery.util.DateReader@75e2e042, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:38,320] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:38,320] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:38,320] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_ERSTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:38,335] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_NEGATION_ERSTER$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:51:38 +0000] "POST /admin/datasets/DATE_DISTANCE_NEGATION_ERSTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_NEGATION_ERSTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:38,336] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,337] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:38,337] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:38,337] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:38,339] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:38,339] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:38,339] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:38,340] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.0
INFO  [2023-01-17 00:51:38,340] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.1
INFO  [2023-01-17 00:51:38,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.2
INFO  [2023-01-17 00:51:38,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.3
INFO  [2023-01-17 00:51:38,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.4
INFO  [2023-01-17 00:51:38,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.5
WARN  [2023-01-17 00:51:38,341] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:38,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.6
INFO  [2023-01-17 00:51:38,341] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_ERSTER$20Test.table1.table1.7
INFO  [2023-01-17 00:51:38,446] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,465] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,481] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,481] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:38,481] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:38,587] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_NEGATION_ERSTER Test QUERY INIT
INFO  [2023-01-17 00:51:38,604] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_NEGATION_ERSTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:38,604] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[318a0d30-028b-45ef-bff7-c98996b89a43] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_ERSTER Test))]]
INFO  [2023-01-17 00:51:38,610] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43
INFO  [2023-01-17 00:51:38,610] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43
127.0.0.1 - - [17/Jan/2023:00:51:38 +0000] "POST /api/datasets/DATE_DISTANCE_NEGATION_ERSTER$20Test/queries HTTP/1.1" 201 1469 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:38,613] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43] with 10 results within PT0.003354S
INFO  [2023-01-17 00:51:38,614] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43, workerId=DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_de57bc3b-ed93-4cc5-8a09-c719add83220, startTime=2023-01-17T00:51:38.610381, finishTime=2023-01-17T00:51:38.613735) of size 10
INFO  [2023-01-17 00:51:38,614] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43] with 10 results within PT0.004451S
INFO  [2023-01-17 00:51:38,615] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43, workerId=DATE_DISTANCE_NEGATION_ERSTER$20Test.worker_DATE_DISTANCE_NEGATION_ERSTER$20Test_28e9fde5-221d-4373-9292-f70c98fe087c, startTime=2023-01-17T00:51:38.610347, finishTime=2023-01-17T00:51:38.614798) of size 10
INFO  [2023-01-17 00:51:38,615] com.bakdata.conquery.models.execution.ManagedExecution: DONE 318a0d30-028b-45ef-bff7-c98996b89a43 ManagedQuery within PT0.010873S
127.0.0.1 - - [17/Jan/2023:00:51:38 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_ERSTER$20Test/queries/DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43 HTTP/1.1" 200 1806 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:38,633] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test], queryId=318a0d30-028b-45ef-bff7-c98996b89a43, label=Alter	@§$, creationTime=2023-01-17T00:51:38.604630, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6bf81e45[Count = 0], startTime=2023-01-17T00:51:38.604865, finishTime=2023-01-17T00:51:38.615738, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7b452149), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e694c03, com.bakdata.conquery.models.query.ColumnDescriptor@6572856a]) download on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:38,634] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test], queryId=318a0d30-028b-45ef-bff7-c98996b89a43, label=Alter	@§$, creationTime=2023-01-17T00:51:38.604630, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6bf81e45[Count = 0], startTime=2023-01-17T00:51:38.604865, finishTime=2023-01-17T00:51:38.615738, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7b452149), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_ERSTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e694c03, com.bakdata.conquery.models.query.ColumnDescriptor@6572856a]) on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_ERSTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:38 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_ERSTER%20Test/result/DATE_DISTANCE_NEGATION_ERSTER$20Test.318a0d30-028b-45ef-bff7-c98996b89a43.csv?pretty=false HTTP/1.1" 200 130 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:51:38,656] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_NEGATION_ERSTER Test on 21 rows
INFO  [2023-01-17 00:51:38,656] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-17 00:51:38,656] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-17 00:51:38,656] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_ERSTER Test, name=DATE_DISTANCE_NEGATION_ERSTER Test]
INFO  [2023-01-17 00:51:38,657] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_ERSTER Test_de57bc3b-ed93-4cc5-8a09-c719add83220
INFO  [2023-01-17 00:51:38,657] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_ERSTER Test_28e9fde5-221d-4373-9292-f70c98fe087c
INFO  [2023-01-17 00:51:38,730] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-17 00:51:38,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_ERSTER Test_de57bc3b-ed93-4cc5-8a09-c719add83220
INFO  [2023-01-17 00:51:38,754] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_NEGATION_ERSTER$20Test
INFO  [2023-01-17 00:51:38,754] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,754] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_ERSTER Test_28e9fde5-221d-4373-9292-f70c98fe087c
INFO  [2023-01-17 00:51:38,886] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_NEGATION_ERSTER Test
INFO  [2023-01-17 00:51:38,887] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-17 00:51:38,887] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:38,887] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:38,888] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-17 00:51:38,888] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-17 00:51:38,888] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:38,888] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_ec1c9fce-189f-42f7-8215-ff342e0fadcc are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_ec1c9fce-189f-42f7-8215-ff342e0fadcc are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_a3bcb0bf-495f-465d-b3d7-5cc4448d5a69 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_a3bcb0bf-495f-465d-b3d7-5cc4448d5a69 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:38,890] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:38,995] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,001] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,002] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:39,002] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:39,119] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,236] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:39,237] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:39,237] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:39,237] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_LETZTER Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000179666sINFO  [2023-01-17 00:51:39,255] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:39,255] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@538e1783)
INFO  [2023-01-17 00:51:39,255] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@3a99b344)
INFO  [2023-01-17 00:51:39,255] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@6d2d966b)
INFO  [2023-01-17 00:51:39,255] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@3eaebdf7), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@7a047843), dateReader=com.bakdata.conquery.util.DateReader@74aa2788, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:39,258] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:39,258] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:39,258] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_LETZTER Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:39,277] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_NEGATION_LETZTER$20Test.table1
INFO  [2023-01-17 00:51:39,277] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:39 +0000] "POST /admin/datasets/DATE_DISTANCE_NEGATION_LETZTER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_NEGATION_LETZTER+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:39,278] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:39,278] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:39,278] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:39,279] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:39,279] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:39,279] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:39,280] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.0
INFO  [2023-01-17 00:51:39,280] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.2
WARN  [2023-01-17 00:51:39,280] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:39,280] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.1
INFO  [2023-01-17 00:51:39,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.3
INFO  [2023-01-17 00:51:39,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.4
INFO  [2023-01-17 00:51:39,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.5
INFO  [2023-01-17 00:51:39,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.6
INFO  [2023-01-17 00:51:39,281] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_LETZTER$20Test.table1.table1.7
INFO  [2023-01-17 00:51:39,386] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,391] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,407] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,407] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:39,407] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:39,514] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_NEGATION_LETZTER Test QUERY INIT
INFO  [2023-01-17 00:51:39,531] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_NEGATION_LETZTER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:39,531] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7832d84b-8b80-4322-b77a-d2c6607dc14a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_LETZTER Test))]]
INFO  [2023-01-17 00:51:39,537] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a
INFO  [2023-01-17 00:51:39,537] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a
127.0.0.1 - - [17/Jan/2023:00:51:39 +0000] "POST /api/datasets/DATE_DISTANCE_NEGATION_LETZTER$20Test/queries HTTP/1.1" 201 1474 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:39,539] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a] with 10 results within PT0.002789S
INFO  [2023-01-17 00:51:39,540] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a] with 10 results within PT0.003208S
INFO  [2023-01-17 00:51:39,540] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a, workerId=DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_a3bcb0bf-495f-465d-b3d7-5cc4448d5a69, startTime=2023-01-17T00:51:39.537149, finishTime=2023-01-17T00:51:39.539938) of size 10
INFO  [2023-01-17 00:51:39,540] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a, workerId=DATE_DISTANCE_NEGATION_LETZTER$20Test.worker_DATE_DISTANCE_NEGATION_LETZTER$20Test_ec1c9fce-189f-42f7-8215-ff342e0fadcc, startTime=2023-01-17T00:51:39.537057, finishTime=2023-01-17T00:51:39.540265) of size 10
INFO  [2023-01-17 00:51:39,541] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7832d84b-8b80-4322-b77a-d2c6607dc14a ManagedQuery within PT0.009048S
127.0.0.1 - - [17/Jan/2023:00:51:39 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_LETZTER$20Test/queries/DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a HTTP/1.1" 200 1814 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:39,566] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test], queryId=7832d84b-8b80-4322-b77a-d2c6607dc14a, label=Alter	@§$, creationTime=2023-01-17T00:51:39.531797, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@446fc93e[Count = 0], startTime=2023-01-17T00:51:39.532035, finishTime=2023-01-17T00:51:39.541083, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@b9d43b0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7284d2d7, com.bakdata.conquery.models.query.ColumnDescriptor@1eead3c2]) download on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:39,566] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test], queryId=7832d84b-8b80-4322-b77a-d2c6607dc14a, label=Alter	@§$, creationTime=2023-01-17T00:51:39.531797, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@446fc93e[Count = 0], startTime=2023-01-17T00:51:39.532035, finishTime=2023-01-17T00:51:39.541083, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@b9d43b0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_LETZTER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=20, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7284d2d7, com.bakdata.conquery.models.query.ColumnDescriptor@1eead3c2]) on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_LETZTER Test]
127.0.0.1 - - [17/Jan/2023:00:51:39 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_LETZTER%20Test/result/DATE_DISTANCE_NEGATION_LETZTER$20Test.7832d84b-8b80-4322-b77a-d2c6607dc14a.csv?pretty=false HTTP/1.1" 200 130 "-" "Conquery (test client)" 25
INFO  [2023-01-17 00:51:39,590] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_NEGATION_LETZTER Test on 21 rows
INFO  [2023-01-17 00:51:39,590] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-17 00:51:39,591] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-17 00:51:39,591] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_LETZTER Test, name=DATE_DISTANCE_NEGATION_LETZTER Test]
INFO  [2023-01-17 00:51:39,591] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_LETZTER Test_a3bcb0bf-495f-465d-b3d7-5cc4448d5a69
INFO  [2023-01-17 00:51:39,591] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_LETZTER Test_ec1c9fce-189f-42f7-8215-ff342e0fadcc
INFO  [2023-01-17 00:51:39,688] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-17 00:51:39,689] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_LETZTER Test_ec1c9fce-189f-42f7-8215-ff342e0fadcc
INFO  [2023-01-17 00:51:39,690] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_LETZTER Test_a3bcb0bf-495f-465d-b3d7-5cc4448d5a69
INFO  [2023-01-17 00:51:39,788] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_NEGATION_LETZTER$20Test
INFO  [2023-01-17 00:51:39,788] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,814] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_NEGATION_LETZTER Test
INFO  [2023-01-17 00:51:39,814] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:39,814] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:39,814] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:39,815] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:39,815] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:39,815] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:39,815] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:39,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_0a165be7-c896-448d-8b1c-ec8aa5a5a706 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:39,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_0a165be7-c896-448d-8b1c-ec8aa5a5a706 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:39,817] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:39,818] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_044cea52-1864-44c9-8a4a-f09bb21d3a32 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:39,818] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_044cea52-1864-44c9-8a4a-f09bb21d3a32 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:39,818] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:39,822] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,921] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,928] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:39,929] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:39,929] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1
INFO  [2023-01-17 00:51:40,046] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,163] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:40,163] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:40,163] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.5 KiB in total
INFO  [2023-01-17 00:51:40,163] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000303219sINFO  [2023-01-17 00:51:40,194] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=26, min=1, average=1.181818, max=2}
INFO  [2023-01-17 00:51:40,194] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[letzter] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@155d809e)
INFO  [2023-01-17 00:51:40,194] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[versichertenzeit] with DateRangeParser(super=Parser(lines=26, nullLines=2), minParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@b48449e), maxParser=DateParser(super=Parser(lines=24, nullLines=0), subType=IntegerParser(super=Parser(lines=24, nullLines=0), minValue=14974, maxValue=16070), dateReader=com.bakdata.conquery.util.DateReader@73ee0fc5), dateReader=com.bakdata.conquery.util.DateReader@116c010a, onlyQuarters=false, maxValue=16070, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:51:40,194] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=26, nullLines=7), subType=IntegerParser(super=Parser(lines=26, nullLines=7), minValue=10592, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@57f052fa)
INFO  [2023-01-17 00:51:40,194] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[erster] with DateParser(super=Parser(lines=26, nullLines=2), subType=IntegerParser(super=Parser(lines=26, nullLines=2), minValue=14610, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@1338019e)
INFO  [2023-01-17 00:51:40,197] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:40,197] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:40,197] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:40,210] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:51:40 +0000] "POST /admin/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:40,210] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,213] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:40,213] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:40,213] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:40,215] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:51:40,215] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
INFO  [2023-01-17 00:51:40,215] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1], containing 26 entries.
WARN  [2023-01-17 00:51:40,216] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:40,216] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.0
INFO  [2023-01-17 00:51:40,216] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.2
INFO  [2023-01-17 00:51:40,216] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.1
INFO  [2023-01-17 00:51:40,216] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.3
INFO  [2023-01-17 00:51:40,216] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.4
INFO  [2023-01-17 00:51:40,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.6
INFO  [2023-01-17 00:51:40,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.5
INFO  [2023-01-17 00:51:40,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.table1.table1.7
INFO  [2023-01-17 00:51:40,321] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,327] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,342] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,343] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:40,343] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:40,449] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test QUERY INIT
INFO  [2023-01-17 00:51:40,467] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:40,467] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test))]]
INFO  [2023-01-17 00:51:40,473] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d
INFO  [2023-01-17 00:51:40,473] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d
127.0.0.1 - - [17/Jan/2023:00:51:40 +0000] "POST /api/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test/queries HTTP/1.1" 201 1510 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:40,476] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d] with 10 results within PT0.002859S
INFO  [2023-01-17 00:51:40,477] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d] with 8 results within PT0.003163S
INFO  [2023-01-17 00:51:40,477] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d, workerId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_044cea52-1864-44c9-8a4a-f09bb21d3a32, startTime=2023-01-17T00:51:40.473611, finishTime=2023-01-17T00:51:40.476470) of size 10
INFO  [2023-01-17 00:51:40,477] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d, workerId=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test_0a165be7-c896-448d-8b1c-ec8aa5a5a706, startTime=2023-01-17T00:51:40.473950, finishTime=2023-01-17T00:51:40.477113) of size 8
INFO  [2023-01-17 00:51:40,477] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d ManagedQuery within PT0.01027S
127.0.0.1 - - [17/Jan/2023:00:51:40 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test/queries/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d HTTP/1.1" 200 1887 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:40,520] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test], queryId=5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d, label=Alter	@§$, creationTime=2023-01-17T00:51:40.467334, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@73e92704[Count = 0], startTime=2023-01-17T00:51:40.467565, finishTime=2023-01-17T00:51:40.477835, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4c5a1b3a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=18, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5410c043, com.bakdata.conquery.models.query.ColumnDescriptor@152e6a1e]) download on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:40,520] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test], queryId=5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d, label=Alter	@§$, creationTime=2023-01-17T00:51:40.467334, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@73e92704[Count = 0], startTime=2023-01-17T00:51:40.467565, finishTime=2023-01-17T00:51:40.477835, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4c5a1b3a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=18, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5410c043, com.bakdata.conquery.models.query.ColumnDescriptor@152e6a1e]) on dataset Dataset[label=null, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
127.0.0.1 - - [17/Jan/2023:00:51:40 +0000] "GET /api/datasets/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT%20Test/result/DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test.5e7bf85c-3bc8-46d5-a3a1-1a53a2fdaf6d.csv?pretty=false HTTP/1.1" 200 120 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:40,522] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test on 19 rows
INFO  [2023-01-17 00:51:40,522] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:40,523] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:40,523] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test, name=DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test]
INFO  [2023-01-17 00:51:40,523] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_0a165be7-c896-448d-8b1c-ec8aa5a5a706
INFO  [2023-01-17 00:51:40,523] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_044cea52-1864-44c9-8a4a-f09bb21d3a32
INFO  [2023-01-17 00:51:40,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:40,616] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_NEGATION_VERSICHERTENZEIT$20Test
INFO  [2023-01-17 00:51:40,616] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,617] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_0a165be7-c896-448d-8b1c-ec8aa5a5a706
INFO  [2023-01-17 00:51:40,617] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test_044cea52-1864-44c9-8a4a-f09bb21d3a32
INFO  [2023-01-17 00:51:40,749] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_NEGATION_VERSICHERTENZEIT Test
INFO  [2023-01-17 00:51:40,749] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DELETE_IMPORT_TESTS Test
INFO  [2023-01-17 00:51:40,749] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:40,749] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:40,751] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-17 00:51:40,751] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-17 00:51:40,751] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:40,751] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:40,753] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_f7d56e42-beda-4d31-9f33-61606b18f23e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:40,753] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_f7d56e42-beda-4d31-9f33-61606b18f23e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:40,753] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:40,753] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_ccd1a09b-0354-483b-b4a5-39eec2f0ed79 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:40,753] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_ccd1a09b-0354-483b-b4a5-39eec2f0ed79 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:40,753] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:40,757] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,856] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,863] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:40,864] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table
INFO  [2023-01-17 00:51:40,864] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table
INFO  [2023-01-17 00:51:40,864] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table2
INFO  [2023-01-17 00:51:40,864] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DELETE_IMPORT_TESTS$20Test.test_table2
INFO  [2023-01-17 00:51:40,979] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,089] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:41,089] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:41,089] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:41,090] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 115 B in total
INFO  [2023-01-17 00:51:41,090] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DELETE_IMPORT_TESTS Test/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.036240124sINFO  [2023-01-17 00:51:41,126] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:41,126] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:41,126] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=15655, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@58b2470b)
INFO  [2023-01-17 00:51:41,130] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DELETE_IMPORT_TESTS Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:41,130] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DELETE_IMPORT_TESTS Test/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000682419sINFO  [2023-01-17 00:51:41,159] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=2, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:41,159] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=2, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:41,159] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=2, nullLines=0), subType=IntegerParser(super=Parser(lines=2, nullLines=0), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@12dfa3e)
INFO  [2023-01-17 00:51:41,161] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DELETE_IMPORT_TESTS Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:41,161] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:41,161] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DELETE_IMPORT_TESTS Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:41,161] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DELETE_IMPORT_TESTS Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:41,175] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into DELETE_IMPORT_TESTS$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:51:41 +0000] "POST /admin/datasets/DELETE_IMPORT_TESTS%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DELETE_IMPORT_TESTS+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:41,176] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:41,177] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:41,177] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:41,180] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:41,180] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table.test_table], containing 2 entries.
INFO  [2023-01-17 00:51:41,180] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table.test_table], containing 2 entries.
WARN  [2023-01-17 00:51:41,181] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:41,181] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DELETE_IMPORT_TESTS$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:41,197] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table2 into DELETE_IMPORT_TESTS$20Test.test_table2
INFO  [2023-01-17 00:51:41,198] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:51:41 +0000] "POST /admin/datasets/DELETE_IMPORT_TESTS%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DELETE_IMPORT_TESTS+Test%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:41,198] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,198] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:41,198] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:41,198] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:41,199] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table2.test_table2], containing 2 entries.
INFO  [2023-01-17 00:51:41,199] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DELETE_IMPORT_TESTS$20Test.test_table2.test_table2], containing 2 entries.
WARN  [2023-01-17 00:51:41,199] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:41,199] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DELETE_IMPORT_TESTS$20Test.test_table2.test_table2.0
INFO  [2023-01-17 00:51:41,200] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DELETE_IMPORT_TESTS$20Test.test_table2.test_table2.1
INFO  [2023-01-17 00:51:41,305] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,311] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,324] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:41,324] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:41,431] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DELETE_IMPORT_TESTS Test QUERY INIT
INFO  [2023-01-17 00:51:41,447] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DELETE_IMPORT_TESTS$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:41,447] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[71ef647b-f586-4c9c-add6-096f889c807e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DELETE_IMPORT_TESTS Test))]]
INFO  [2023-01-17 00:51:41,450] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e
INFO  [2023-01-17 00:51:41,450] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e
INFO  [2023-01-17 00:51:41,451] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e] with 0 results within PT0.001088S
127.0.0.1 - - [17/Jan/2023:00:51:41 +0000] "POST /api/datasets/DELETE_IMPORT_TESTS$20Test/queries HTTP/1.1" 201 1243 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:41,452] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e] with 2 results within PT0.00141S
INFO  [2023-01-17 00:51:41,452] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e, workerId=DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_f7d56e42-beda-4d31-9f33-61606b18f23e, startTime=2023-01-17T00:51:41.450619, finishTime=2023-01-17T00:51:41.451707) of size 0
INFO  [2023-01-17 00:51:41,452] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e, workerId=DELETE_IMPORT_TESTS$20Test.worker_DELETE_IMPORT_TESTS$20Test_ccd1a09b-0354-483b-b4a5-39eec2f0ed79, startTime=2023-01-17T00:51:41.450608, finishTime=2023-01-17T00:51:41.452018) of size 2
INFO  [2023-01-17 00:51:41,452] com.bakdata.conquery.models.execution.ManagedExecution: DONE 71ef647b-f586-4c9c-add6-096f889c807e ManagedQuery within PT0.00517S
127.0.0.1 - - [17/Jan/2023:00:51:41 +0000] "GET /api/datasets/DELETE_IMPORT_TESTS$20Test/queries/DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e HTTP/1.1" 200 1537 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:41,499] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DELETE_IMPORT_TESTS Test], queryId=71ef647b-f586-4c9c-add6-096f889c807e, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:41.447389, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@34f7c523[Count = 0], startTime=2023-01-17T00:51:41.447640, finishTime=2023-01-17T00:51:41.452810, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@61428717), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DELETE_IMPORT_TESTS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4c34d918, com.bakdata.conquery.models.query.ColumnDescriptor@fe59358]) download on dataset Dataset[label=null, name=DELETE_IMPORT_TESTS Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:41,499] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DELETE_IMPORT_TESTS Test], queryId=71ef647b-f586-4c9c-add6-096f889c807e, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:41.447389, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@34f7c523[Count = 0], startTime=2023-01-17T00:51:41.447640, finishTime=2023-01-17T00:51:41.452810, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@61428717), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DELETE_IMPORT_TESTS Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4c34d918, com.bakdata.conquery.models.query.ColumnDescriptor@fe59358]) on dataset Dataset[label=null, name=DELETE_IMPORT_TESTS Test]
127.0.0.1 - - [17/Jan/2023:00:51:41 +0000] "GET /api/datasets/DELETE_IMPORT_TESTS%20Test/result/DELETE_IMPORT_TESTS$20Test.71ef647b-f586-4c9c-add6-096f889c807e.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:41,502] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DELETE_IMPORT_TESTS Test on 3 rows
INFO  [2023-01-17 00:51:41,502] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DELETE_IMPORT_TESTS Test
INFO  [2023-01-17 00:51:41,502] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-17 00:51:41,502] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DELETE_IMPORT_TESTS Test, name=DELETE_IMPORT_TESTS Test]
INFO  [2023-01-17 00:51:41,502] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DELETE_IMPORT_TESTS Test_f7d56e42-beda-4d31-9f33-61606b18f23e
INFO  [2023-01-17 00:51:41,502] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DELETE_IMPORT_TESTS Test_ccd1a09b-0354-483b-b4a5-39eec2f0ed79
INFO  [2023-01-17 00:51:41,551] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DELETE_IMPORT_TESTS Test
INFO  [2023-01-17 00:51:41,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DELETE_IMPORT_TESTS Test_ccd1a09b-0354-483b-b4a5-39eec2f0ed79
INFO  [2023-01-17 00:51:41,552] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DELETE_IMPORT_TESTS Test_f7d56e42-beda-4d31-9f33-61606b18f23e
INFO  [2023-01-17 00:51:41,599] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DELETE_IMPORT_TESTS$20Test
INFO  [2023-01-17 00:51:41,599] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,730] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DELETE_IMPORT_TESTS Test
INFO  [2023-01-17 00:51:41,730] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:41,730] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:41,730] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:41,732] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:41,732] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:41,732] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:41,732] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:41,734] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_39ef1578-f74a-4fe9-8e77-0615da78f09c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:41,734] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_39ef1578-f74a-4fe9-8e77-0615da78f09c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:41,734] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:41,734] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_05789169-1219-48bd-91f6-7326c2f419a8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:41,734] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_05789169-1219-48bd-91f6-7326c2f419a8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:41,734] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:41,738] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,838] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,845] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:41,846] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:41,846] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:41,963] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,075] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:42,075] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:42,075] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 221 B in total
INFO  [2023-01-17 00:51:42,075] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000700763sINFO  [2023-01-17 00:51:42,146] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=5, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:42,146] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[ende] with DateParser(super=Parser(lines=5, nullLines=1), subType=IntegerParser(super=Parser(lines=5, nullLines=1), minValue=14608, maxValue=14644), dateReader=com.bakdata.conquery.util.DateReader@3aefdbe6)
INFO  [2023-01-17 00:51:42,146] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[anfang] with DateParser(super=Parser(lines=5, nullLines=1), subType=IntegerParser(super=Parser(lines=5, nullLines=1), minValue=14608, maxValue=14642), dateReader=com.bakdata.conquery.util.DateReader@249e1bf4)
INFO  [2023-01-17 00:51:42,146] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=5, nullLines=1), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14608, maxValue=14642), dateReader=com.bakdata.conquery.util.DateReader@346a09b7), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14608, maxValue=14644), dateReader=com.bakdata.conquery.util.DateReader@26cf2a49), dateReader=com.bakdata.conquery.util.DateReader@27d07e35, onlyQuarters=false, maxValue=14644, minValue=14608, anyOpen=false)
INFO  [2023-01-17 00:51:42,149] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:42,149] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:42,149] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:42,165] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:42,165] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:42 +0000] "POST /admin/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:42,166] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:42,167] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:42,167] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:42,169] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:42,169] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 5 entries.
INFO  [2023-01-17 00:51:42,169] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 5 entries.
WARN  [2023-01-17 00:51:42,171] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:42,171] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:51:42,171] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:51:42,276] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,281] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,296] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,296] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:42,296] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:42,402] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:42,418] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:42,418] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a4280abc-6151-4f67-95ec-77bcbf52df96] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:51:42,423] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96
INFO  [2023-01-17 00:51:42,423] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96
127.0.0.1 - - [17/Jan/2023:00:51:42 +0000] "POST /api/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1483 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:42,424] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96] with 0 results within PT0.001229S
INFO  [2023-01-17 00:51:42,424] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96] with 2 results within PT0.001576S
INFO  [2023-01-17 00:51:42,425] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96, workerId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_05789169-1219-48bd-91f6-7326c2f419a8, startTime=2023-01-17T00:51:42.423276, finishTime=2023-01-17T00:51:42.424505) of size 0
INFO  [2023-01-17 00:51:42,425] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96, workerId=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_39ef1578-f74a-4fe9-8e77-0615da78f09c, startTime=2023-01-17T00:51:42.423067, finishTime=2023-01-17T00:51:42.424643) of size 2
INFO  [2023-01-17 00:51:42,425] com.bakdata.conquery.models.execution.ManagedExecution: DONE a4280abc-6151-4f67-95ec-77bcbf52df96 ManagedQuery within PT0.006633S
127.0.0.1 - - [17/Jan/2023:00:51:42 +0000] "GET /api/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96 HTTP/1.1" 200 1850 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:42,452] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=a4280abc-6151-4f67-95ec-77bcbf52df96, label=KG-Tage-DURATION_SUM	@§$, creationTime=2023-01-17T00:51:42.418528, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22e3c2a[Count = 0], startTime=2023-01-17T00:51:42.418732, finishTime=2023-01-17T00:51:42.425365, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b7c0a4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@482f8542, com.bakdata.conquery.models.query.ColumnDescriptor@eae26b]) download on dataset Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:42,452] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=a4280abc-6151-4f67-95ec-77bcbf52df96, label=KG-Tage-DURATION_SUM	@§$, creationTime=2023-01-17T00:51:42.418528, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@22e3c2a[Count = 0], startTime=2023-01-17T00:51:42.418732, finishTime=2023-01-17T00:51:42.425365, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b7c0a4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@482f8542, com.bakdata.conquery.models.query.ColumnDescriptor@eae26b]) on dataset Dataset[label=null, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:51:42 +0000] "GET /api/datasets/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/result/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.a4280abc-6151-4f67-95ec-77bcbf52df96.csv?pretty=false HTTP/1.1" 200 66 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:51:42,474] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:51:42,474] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:42,475] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:42,475] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:42,475] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_05789169-1219-48bd-91f6-7326c2f419a8
INFO  [2023-01-17 00:51:42,475] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_39ef1578-f74a-4fe9-8e77-0615da78f09c
INFO  [2023-01-17 00:51:42,532] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:42,533] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_39ef1578-f74a-4fe9-8e77-0615da78f09c
INFO  [2023-01-17 00:51:42,533] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test_05789169-1219-48bd-91f6-7326c2f419a8
INFO  [2023-01-17 00:51:42,572] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:51:42,572] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,706] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:42,706] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND Test
INFO  [2023-01-17 00:51:42,706] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:42,706] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:42,707] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-17 00:51:42,707] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-17 00:51:42,707] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:42,707] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:42,709] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test.worker_AND$20Test_14da613f-f539-4708-b42b-1901103dce7c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:42,709] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test.worker_AND$20Test_14da613f-f539-4708-b42b-1901103dce7c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:42,709] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:42,709] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test.worker_AND$20Test_80f5432f-1605-4c27-b834-3f3d9e499992 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:42,709] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test.worker_AND$20Test_80f5432f-1605-4c27-b834-3f3d9e499992 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:42,710] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:42,714] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,813] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,820] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:42,820] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test.table
INFO  [2023-01-17 00:51:42,820] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test.table
INFO  [2023-01-17 00:51:42,943] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,053] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:43,054] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:43,054] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 152 B in total
INFO  [2023-01-17 00:51:43,054] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00033028sINFO  [2023-01-17 00:51:43,089] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=1, average=2.000000, max=3}
INFO  [2023-01-17 00:51:43,089] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:43,089] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@72628fb7)
INFO  [2023-01-17 00:51:43,091] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:43,091] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:43,091] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:43,110] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:43 +0000] "POST /admin/datasets/AND%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:51:43,110] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,110] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:43,111] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:43,111] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:43,112] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:43,113] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test.table.table], containing 8 entries.
INFO  [2023-01-17 00:51:43,113] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test.table.table], containing 8 entries.
WARN  [2023-01-17 00:51:43,114] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:43,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test.table.table.0
INFO  [2023-01-17 00:51:43,114] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test.table.table.1
INFO  [2023-01-17 00:51:43,218] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,224] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,235] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,236] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:43,236] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:43,342] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND Test QUERY INIT
INFO  [2023-01-17 00:51:43,359] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:43,359] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7d386212-f4ea-4537-b081-1582ea28886a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND Test))]]
INFO  [2023-01-17 00:51:43,363] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a
INFO  [2023-01-17 00:51:43,363] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a
INFO  [2023-01-17 00:51:43,364] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a] with 1 results within PT0.001069S
INFO  [2023-01-17 00:51:43,364] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a] with 1 results within PT0.001252S
INFO  [2023-01-17 00:51:43,364] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a, workerId=AND$20Test.worker_AND$20Test_80f5432f-1605-4c27-b834-3f3d9e499992, startTime=2023-01-17T00:51:43.363240, finishTime=2023-01-17T00:51:43.364309) of size 1
INFO  [2023-01-17 00:51:43,365] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a, workerId=AND$20Test.worker_AND$20Test_14da613f-f539-4708-b42b-1901103dce7c, startTime=2023-01-17T00:51:43.363200, finishTime=2023-01-17T00:51:43.364452) of size 1
INFO  [2023-01-17 00:51:43,365] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7d386212-f4ea-4537-b081-1582ea28886a ManagedQuery within PT0.005587S
127.0.0.1 - - [17/Jan/2023:00:51:43 +0000] "POST /api/datasets/AND$20Test/queries HTTP/1.1" 201 1706 "-" "Conquery (test client)" 10
127.0.0.1 - - [17/Jan/2023:00:51:43 +0000] "GET /api/datasets/AND$20Test/queries/AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a HTTP/1.1" 200 1721 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:43,400] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test], queryId=7d386212-f4ea-4537-b081-1582ea28886a, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:43.359305, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@74bb16ef[Count = 0], startTime=2023-01-17T00:51:43.359639, finishTime=2023-01-17T00:51:43.365226, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@64360ae2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5ba5568a, com.bakdata.conquery.models.query.ColumnDescriptor@5185a231, com.bakdata.conquery.models.query.ColumnDescriptor@7da5da80]) download on dataset Dataset[label=null, name=AND Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:43,400] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test], queryId=7d386212-f4ea-4537-b081-1582ea28886a, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:43.359305, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@74bb16ef[Count = 0], startTime=2023-01-17T00:51:43.359639, finishTime=2023-01-17T00:51:43.365226, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@64360ae2), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5ba5568a, com.bakdata.conquery.models.query.ColumnDescriptor@5185a231, com.bakdata.conquery.models.query.ColumnDescriptor@7da5da80]) on dataset Dataset[label=null, name=AND Test]
127.0.0.1 - - [17/Jan/2023:00:51:43 +0000] "GET /api/datasets/AND%20Test/result/AND$20Test.7d386212-f4ea-4537-b081-1582ea28886a.csv?pretty=false HTTP/1.1" 200 87 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:51:43,422] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND Test on 3 rows
INFO  [2023-01-17 00:51:43,422] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND Test
INFO  [2023-01-17 00:51:43,423] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-17 00:51:43,423] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test, name=AND Test]
INFO  [2023-01-17 00:51:43,423] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test_14da613f-f539-4708-b42b-1901103dce7c
INFO  [2023-01-17 00:51:43,423] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test_80f5432f-1605-4c27-b834-3f3d9e499992
INFO  [2023-01-17 00:51:43,520] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test_80f5432f-1605-4c27-b834-3f3d9e499992
INFO  [2023-01-17 00:51:43,520] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND Test
INFO  [2023-01-17 00:51:43,520] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test_14da613f-f539-4708-b42b-1901103dce7c
INFO  [2023-01-17 00:51:43,620] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20Test
INFO  [2023-01-17 00:51:43,620] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,641] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND Test
INFO  [2023-01-17 00:51:43,642] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DATE LOGIC Test
INFO  [2023-01-17 00:51:43,642] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:43,642] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:43,643] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-17 00:51:43,643] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-17 00:51:43,643] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:43,644] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:43,645] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_53d95eb3-990d-41dc-9fc4-054502186192 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:43,645] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_53d95eb3-990d-41dc-9fc4-054502186192 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:43,645] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:43,646] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_84740b2a-1044-4ca8-9322-92d291042af6 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:43,646] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_84740b2a-1044-4ca8-9322-92d291042af6 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:43,646] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:43,650] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,749] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,756] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,756] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:43,756] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:43,872] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:43,989] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:43,989] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:43,989] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 240 B in total
INFO  [2023-01-17 00:51:43,989] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DATE LOGIC Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000315519sINFO  [2023-01-17 00:51:44,021] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=13, min=2, average=2.600000, max=4}
INFO  [2023-01-17 00:51:44,021] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=13, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:44,021] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=0), subType=IntegerParser(super=Parser(lines=13, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@16a117fd)
INFO  [2023-01-17 00:51:44,024] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:44,024] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:44,024] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:44,042] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:44,044] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:44 +0000] "POST /admin/datasets/AND%20DATE%20LOGIC%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DATE+LOGIC+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:44,045] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:44,046] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:44,047] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:44,048] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:44,049] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
INFO  [2023-01-17 00:51:44,049] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
WARN  [2023-01-17 00:51:44,049] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:44,049] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DATE$20LOGIC$20Test.table.table.0
INFO  [2023-01-17 00:51:44,049] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DATE$20LOGIC$20Test.table.table.1
INFO  [2023-01-17 00:51:44,155] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,160] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,174] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,175] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:44,175] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:44,281] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DATE LOGIC Test QUERY INIT
INFO  [2023-01-17 00:51:44,298] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DATE$20LOGIC$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:44,299] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[830e0579-4134-4930-bd3b-9647edc22e10] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DATE LOGIC Test))]]
INFO  [2023-01-17 00:51:44,302] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10
INFO  [2023-01-17 00:51:44,302] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10
INFO  [2023-01-17 00:51:44,303] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10] with 2 results within PT0.001101S
INFO  [2023-01-17 00:51:44,303] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10] with 2 results within PT0.001205S
127.0.0.1 - - [17/Jan/2023:00:51:44 +0000] "POST /api/datasets/AND$20DATE$20LOGIC$20Test/queries HTTP/1.1" 201 1567 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:44,304] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10, workerId=AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_84740b2a-1044-4ca8-9322-92d291042af6, startTime=2023-01-17T00:51:44.302732, finishTime=2023-01-17T00:51:44.303937) of size 2
INFO  [2023-01-17 00:51:44,304] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10, workerId=AND$20DATE$20LOGIC$20Test.worker_AND$20DATE$20LOGIC$20Test_53d95eb3-990d-41dc-9fc4-054502186192, startTime=2023-01-17T00:51:44.302831, finishTime=2023-01-17T00:51:44.303932) of size 2
INFO  [2023-01-17 00:51:44,304] com.bakdata.conquery.models.execution.ManagedExecution: DONE 830e0579-4134-4930-bd3b-9647edc22e10 ManagedQuery within PT0.005759S
127.0.0.1 - - [17/Jan/2023:00:51:44 +0000] "GET /api/datasets/AND$20DATE$20LOGIC$20Test/queries/AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10 HTTP/1.1" 200 1858 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:44,331] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DATE LOGIC Test], queryId=830e0579-4134-4930-bd3b-9647edc22e10, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:44.298732, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1eba6589[Count = 0], startTime=2023-01-17T00:51:44.299058, finishTime=2023-01-17T00:51:44.304817, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6ffe4562), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4a992080, com.bakdata.conquery.models.query.ColumnDescriptor@2cc5eae6, com.bakdata.conquery.models.query.ColumnDescriptor@619e54da]) download on dataset Dataset[label=null, name=AND DATE LOGIC Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:44,332] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DATE LOGIC Test], queryId=830e0579-4134-4930-bd3b-9647edc22e10, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:44.298732, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1eba6589[Count = 0], startTime=2023-01-17T00:51:44.299058, finishTime=2023-01-17T00:51:44.304817, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6ffe4562), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4a992080, com.bakdata.conquery.models.query.ColumnDescriptor@2cc5eae6, com.bakdata.conquery.models.query.ColumnDescriptor@619e54da]) on dataset Dataset[label=null, name=AND DATE LOGIC Test]
127.0.0.1 - - [17/Jan/2023:00:51:44 +0000] "GET /api/datasets/AND%20DATE%20LOGIC%20Test/result/AND$20DATE$20LOGIC$20Test.830e0579-4134-4930-bd3b-9647edc22e10.csv?pretty=false HTTP/1.1" 200 122 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:51:44,348] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DATE LOGIC Test on 5 rows
INFO  [2023-01-17 00:51:44,348] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DATE LOGIC Test
INFO  [2023-01-17 00:51:44,349] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-17 00:51:44,349] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DATE LOGIC Test, name=AND DATE LOGIC Test]
INFO  [2023-01-17 00:51:44,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DATE LOGIC Test_53d95eb3-990d-41dc-9fc4-054502186192
INFO  [2023-01-17 00:51:44,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DATE LOGIC Test_84740b2a-1044-4ca8-9322-92d291042af6
INFO  [2023-01-17 00:51:44,446] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DATE LOGIC Test_53d95eb3-990d-41dc-9fc4-054502186192
INFO  [2023-01-17 00:51:44,446] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DATE LOGIC Test_84740b2a-1044-4ca8-9322-92d291042af6
INFO  [2023-01-17 00:51:44,446] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DATE LOGIC Test
INFO  [2023-01-17 00:51:44,449] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DATE$20LOGIC$20Test
INFO  [2023-01-17 00:51:44,449] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,582] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DATE LOGIC Test
INFO  [2023-01-17 00:51:44,582] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:44,582] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:44,582] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:44,583] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-17 00:51:44,583] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-17 00:51:44,584] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:44,584] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:44,586] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_aac76ce4-903a-4606-80f7-7d5e4e6f8bb3 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:44,586] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_aac76ce4-903a-4606-80f7-7d5e4e6f8bb3 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:44,586] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:44,586] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_f5428a4a-e461-4c42-b21e-a42f657824c2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:44,586] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_f5428a4a-e461-4c42-b21e-a42f657824c2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:44,586] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:44,590] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,689] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,696] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,696] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test.table
INFO  [2023-01-17 00:51:44,696] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test.table
INFO  [2023-01-17 00:51:44,824] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:44,937] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:44,937] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:44,937] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 307 B in total
INFO  [2023-01-17 00:51:44,937] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000305937sINFO  [2023-01-17 00:51:44,968] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-17 00:51:44,968] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:44,968] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@4963fa02)
INFO  [2023-01-17 00:51:44,970] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:44,971] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:44,971] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:44,986] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test.table
INFO  [2023-01-17 00:51:44,986] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:44 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DURATION+SUM+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:44,987] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:44,987] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:44,987] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:44,989] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:44,989] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test.table.table], containing 17 entries.
INFO  [2023-01-17 00:51:44,990] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test.table.table], containing 17 entries.
WARN  [2023-01-17 00:51:44,990] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:44,990] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test.table.table.0
INFO  [2023-01-17 00:51:44,990] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test.table.table.1
INFO  [2023-01-17 00:51:45,095] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,101] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,115] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,115] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:45,115] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:45,221] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-17 00:51:45,237] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:45,237] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[69e239c2-f341-478a-877b-d35bd0814d3f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test))]]
INFO  [2023-01-17 00:51:45,241] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f
INFO  [2023-01-17 00:51:45,241] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f
127.0.0.1 - - [17/Jan/2023:00:51:45 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test/queries HTTP/1.1" 201 1829 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:45,243] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f] with 2 results within PT0.002068S
INFO  [2023-01-17 00:51:45,243] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f] with 2 results within PT0.002294S
INFO  [2023-01-17 00:51:45,244] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f, workerId=AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_f5428a4a-e461-4c42-b21e-a42f657824c2, startTime=2023-01-17T00:51:45.241729, finishTime=2023-01-17T00:51:45.243797) of size 2
INFO  [2023-01-17 00:51:45,244] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f, workerId=AND$20DURATION$20SUM$20Test.worker_AND$20DURATION$20SUM$20Test_aac76ce4-903a-4606-80f7-7d5e4e6f8bb3, startTime=2023-01-17T00:51:45.241607, finishTime=2023-01-17T00:51:45.243901) of size 2
INFO  [2023-01-17 00:51:45,244] com.bakdata.conquery.models.execution.ManagedExecution: DONE 69e239c2-f341-478a-877b-d35bd0814d3f ManagedQuery within PT0.006797S
127.0.0.1 - - [17/Jan/2023:00:51:45 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test/queries/AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f HTTP/1.1" 200 2128 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:45,276] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test], queryId=69e239c2-f341-478a-877b-d35bd0814d3f, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:45.237702, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@199b552a[Count = 0], startTime=2023-01-17T00:51:45.237934, finishTime=2023-01-17T00:51:45.244731, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ab6b5b3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2242d24a, com.bakdata.conquery.models.query.ColumnDescriptor@54a31d4d, com.bakdata.conquery.models.query.ColumnDescriptor@175220f6, com.bakdata.conquery.models.query.ColumnDescriptor@68360482]) download on dataset Dataset[label=null, name=AND DURATION SUM Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:45,276] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test], queryId=69e239c2-f341-478a-877b-d35bd0814d3f, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:45.237702, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@199b552a[Count = 0], startTime=2023-01-17T00:51:45.237934, finishTime=2023-01-17T00:51:45.244731, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ab6b5b3), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2242d24a, com.bakdata.conquery.models.query.ColumnDescriptor@54a31d4d, com.bakdata.conquery.models.query.ColumnDescriptor@175220f6, com.bakdata.conquery.models.query.ColumnDescriptor@68360482]) on dataset Dataset[label=null, name=AND DURATION SUM Test]
127.0.0.1 - - [17/Jan/2023:00:51:45 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test/result/AND$20DURATION$20SUM$20Test.69e239c2-f341-478a-877b-d35bd0814d3f.csv?pretty=false HTTP/1.1" 200 152 "-" "Conquery (test client)" 25
INFO  [2023-01-17 00:51:45,299] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 5 rows
INFO  [2023-01-17 00:51:45,299] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test
INFO  [2023-01-17 00:51:45,300] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-17 00:51:45,300] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test, name=AND DURATION SUM Test]
INFO  [2023-01-17 00:51:45,300] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test_f5428a4a-e461-4c42-b21e-a42f657824c2
INFO  [2023-01-17 00:51:45,300] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test_aac76ce4-903a-4606-80f7-7d5e4e6f8bb3
INFO  [2023-01-17 00:51:45,397] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test_aac76ce4-903a-4606-80f7-7d5e4e6f8bb3
INFO  [2023-01-17 00:51:45,397] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test
INFO  [2023-01-17 00:51:45,397] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test_f5428a4a-e461-4c42-b21e-a42f657824c2
INFO  [2023-01-17 00:51:45,497] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test
INFO  [2023-01-17 00:51:45,497] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,521] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:45,521] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:45,521] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:45,521] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:45,522] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-17 00:51:45,522] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:45,522] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-17 00:51:45,522] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:45,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_ff8930f6-299b-437d-8298-eddd443417fc are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:45,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_ff8930f6-299b-437d-8298-eddd443417fc are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:45,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:45,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_6c59251f-a412-48f6-8f71-e1038e1c62d2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:45,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_6c59251f-a412-48f6-8f71-e1038e1c62d2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:45,525] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:45,529] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,628] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[AND$20DURATION$20SUM$20Test[1].secondary]
INFO  [2023-01-17 00:51:45,629] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[AND$20DURATION$20SUM$20Test[1].ignored]
INFO  [2023-01-17 00:51:45,629] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,630] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].secondary
INFO  [2023-01-17 00:51:45,630] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].secondary
INFO  [2023-01-17 00:51:45,630] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].ignored
INFO  [2023-01-17 00:51:45,630] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId AND$20DURATION$20SUM$20Test[1].ignored
INFO  [2023-01-17 00:51:45,736] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,736] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table1
INFO  [2023-01-17 00:51:45,736] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table1
INFO  [2023-01-17 00:51:45,737] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table2
INFO  [2023-01-17 00:51:45,737] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[1].table2
INFO  [2023-01-17 00:51:45,855] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:45,966] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:45,967] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:45,967] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:45,967] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 373 B in total
INFO  [2023-01-17 00:51:45,967] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[1]/table1.import.json, tag=Optional.empty)
██████████████████████████████████                ▌  68%	est. time remaining: 0.021418628sINFO  [2023-01-17 00:51:46,012] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=12, min=1, average=4.000000, max=10}
INFO  [2023-01-17 00:51:46,012] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=15340, maxValue=15343), dateReader=com.bakdata.conquery.util.DateReader@55093240)
INFO  [2023-01-17 00:51:46,012] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=12, nullLines=2), encoding=null, prefix=A, suffix=A)
INFO  [2023-01-17 00:51:46,012] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[secondary] with StringParser(super=Parser(lines=12, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:46,015] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:46,015] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[1]/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000667027sINFO  [2023-01-17 00:51:46,034] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=6, min=1, average=2.000000, max=4}
INFO  [2023-01-17 00:51:46,035] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=B, suffix=B)
INFO  [2023-01-17 00:51:46,035] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@6d6614cd)
INFO  [2023-01-17 00:51:46,037] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[1]/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:46,037] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:46,037] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:46,037] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[1]/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:46,066] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into AND$20DURATION$20SUM$20Test[1].table1
127.0.0.1 - - [17/Jan/2023:00:51:46 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DURATION+SUM+Test%5B1%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:46,067] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:46,068] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:46,068] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:46,071] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:46,072] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table1.table1], containing 12 entries.
INFO  [2023-01-17 00:51:46,073] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table1.table1], containing 12 entries.
WARN  [2023-01-17 00:51:46,073] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:46,073] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[1].table1.table1.0
INFO  [2023-01-17 00:51:46,082] com.bakdata.conquery.models.jobs.ImportJob: Importing table2 into AND$20DURATION$20SUM$20Test[1].table2
INFO  [2023-01-17 00:51:46,083] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:46,083] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:46,083] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:46,083] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:46 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DURATION+SUM+Test%5B1%5D%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:46,083] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:46,083] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table2.table2], containing 6 entries.
WARN  [2023-01-17 00:51:46,083] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:46,084] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[1].table2.table2], containing 6 entries.
INFO  [2023-01-17 00:51:46,084] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[1].table2.table2.0
INFO  [2023-01-17 00:51:46,204] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,209] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,228] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,228] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:46,334] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-17 00:51:46,356] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:46,357] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c80913d5-bc24-4ca1-9b88-b3fdfeb9b171] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[1]))]]
INFO  [2023-01-17 00:51:46,361] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery AND$20DURATION$20SUM$20Test[1].c80913d5-bc24-4ca1-9b88-b3fdfeb9b171
INFO  [2023-01-17 00:51:46,361] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery AND$20DURATION$20SUM$20Test[1].c80913d5-bc24-4ca1-9b88-b3fdfeb9b171
WARN  [2023-01-17 00:51:46,361] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:51:46,361] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[1].c80913d5-bc24-4ca1-9b88-b3fdfeb9b171] with 0 results within PT0.000208S
INFO  [2023-01-17 00:51:46,362] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[1].c80913d5-bc24-4ca1-9b88-b3fdfeb9b171, workerId=AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_ff8930f6-299b-437d-8298-eddd443417fc, startTime=2023-01-17T00:51:46.361571, finishTime=2023-01-17T00:51:46.361779) of size 0
127.0.0.1 - - [17/Jan/2023:00:51:46 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B1%5D/queries HTTP/1.1" 201 2114 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:51:46,363] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[1].c80913d5-bc24-4ca1-9b88-b3fdfeb9b171] with 2 results within PT0.001821S
INFO  [2023-01-17 00:51:46,364] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[1].c80913d5-bc24-4ca1-9b88-b3fdfeb9b171, workerId=AND$20DURATION$20SUM$20Test[1].worker_AND$20DURATION$20SUM$20Test[1]_6c59251f-a412-48f6-8f71-e1038e1c62d2, startTime=2023-01-17T00:51:46.361494, finishTime=2023-01-17T00:51:46.363315) of size 2
INFO  [2023-01-17 00:51:46,364] com.bakdata.conquery.models.execution.ManagedExecution: DONE c80913d5-bc24-4ca1-9b88-b3fdfeb9b171 ManagedQuery within PT0.006898S
127.0.0.1 - - [17/Jan/2023:00:51:46 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B1%5D/queries/AND$20DURATION$20SUM$20Test%5B1%5D.c80913d5-bc24-4ca1-9b88-b3fdfeb9b171 HTTP/1.1" 200 2697 "-" "Conquery (test client)" 28
INFO  [2023-01-17 00:51:46,416] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[1]], queryId=c80913d5-bc24-4ca1-9b88-b3fdfeb9b171, label=tree1-a tree2-b	@§$, creationTime=2023-01-17T00:51:46.357095, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@59ed5dd1[Count = 0], startTime=2023-01-17T00:51:46.357354, finishTime=2023-01-17T00:51:46.364252, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4bf1c7eb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3258bec0, com.bakdata.conquery.models.query.ColumnDescriptor@2cd01659, com.bakdata.conquery.models.query.ColumnDescriptor@1fe0d5c7, com.bakdata.conquery.models.query.ColumnDescriptor@5a6b974f, com.bakdata.conquery.models.query.ColumnDescriptor@2db29c7c]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:46,416] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[1]], queryId=c80913d5-bc24-4ca1-9b88-b3fdfeb9b171, label=tree1-a tree2-b	@§$, creationTime=2023-01-17T00:51:46.357095, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@59ed5dd1[Count = 0], startTime=2023-01-17T00:51:46.357354, finishTime=2023-01-17T00:51:46.364252, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4bf1c7eb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3258bec0, com.bakdata.conquery.models.query.ColumnDescriptor@2cd01659, com.bakdata.conquery.models.query.ColumnDescriptor@1fe0d5c7, com.bakdata.conquery.models.query.ColumnDescriptor@5a6b974f, com.bakdata.conquery.models.query.ColumnDescriptor@2db29c7c]) on dataset Dataset[label=null, name=AND DURATION SUM Test[1]]
127.0.0.1 - - [17/Jan/2023:00:51:46 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B1%5D/result/AND$20DURATION$20SUM$20Test%5B1%5D.c80913d5-bc24-4ca1-9b88-b3fdfeb9b171.csv?pretty=false HTTP/1.1" 200 174 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:51:46,420] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 7 rows
INFO  [2023-01-17 00:51:46,420] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[1]
INFO  [2023-01-17 00:51:46,420] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-17 00:51:46,420] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[1], name=AND DURATION SUM Test[1]]
INFO  [2023-01-17 00:51:46,420] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[1]_ff8930f6-299b-437d-8298-eddd443417fc
INFO  [2023-01-17 00:51:46,420] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[1]_6c59251f-a412-48f6-8f71-e1038e1c62d2
INFO  [2023-01-17 00:51:46,423] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[1]
INFO  [2023-01-17 00:51:46,425] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[1]_ff8930f6-299b-437d-8298-eddd443417fc
INFO  [2023-01-17 00:51:46,425] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[1]_6c59251f-a412-48f6-8f71-e1038e1c62d2
INFO  [2023-01-17 00:51:46,502] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[1]
INFO  [2023-01-17 00:51:46,502] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,534] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:46,535] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND NEGATION Test
INFO  [2023-01-17 00:51:46,535] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:46,535] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:46,536] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-17 00:51:46,536] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-17 00:51:46,536] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:46,536] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_e5abe0e3-21ce-4f6e-9531-afdbc841c021 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_e5abe0e3-21ce-4f6e-9531-afdbc841c021 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_59ae943a-c44f-4320-b7c8-77049c56ff0e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_59ae943a-c44f-4320-b7c8-77049c56ff0e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:46,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:46,642] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,648] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,649] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20Test.table
INFO  [2023-01-17 00:51:46,649] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20Test.table
INFO  [2023-01-17 00:51:46,764] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,874] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:46,874] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:46,874] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 202 B in total
INFO  [2023-01-17 00:51:46,874] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000347794sINFO  [2023-01-17 00:51:46,910] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=11, min=1, average=2.200000, max=3}
INFO  [2023-01-17 00:51:46,910] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=11, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:46,910] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=11, nullLines=0), subType=IntegerParser(super=Parser(lines=11, nullLines=0), minValue=15340, maxValue=15342), dateReader=com.bakdata.conquery.util.DateReader@b1f21e1)
INFO  [2023-01-17 00:51:46,913] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:46,913] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:46,913] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:46,938] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20NEGATION$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:46 +0000] "POST /admin/datasets/AND%20NEGATION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+NEGATION+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:51:46,938] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:46,939] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:46,940] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:46,940] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:46,943] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:46,943] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20Test.table.table], containing 11 entries.
INFO  [2023-01-17 00:51:46,943] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20Test.table.table], containing 11 entries.
WARN  [2023-01-17 00:51:46,944] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:46,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20Test.table.table.0
INFO  [2023-01-17 00:51:46,944] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20Test.table.table.1
INFO  [2023-01-17 00:51:47,050] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,055] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,079] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,080] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:47,080] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:47,185] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND NEGATION Test QUERY INIT
INFO  [2023-01-17 00:51:47,197] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20NEGATION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:47,197] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION Test))]]
INFO  [2023-01-17 00:51:47,200] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f
INFO  [2023-01-17 00:51:47,200] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f
127.0.0.1 - - [17/Jan/2023:00:51:47 +0000] "POST /api/datasets/AND$20NEGATION$20Test/queries HTTP/1.1" 201 1612 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:47,240] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f] with 1 results within PT0.039349S
INFO  [2023-01-17 00:51:47,240] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f, workerId=AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_59ae943a-c44f-4320-b7c8-77049c56ff0e, startTime=2023-01-17T00:51:47.200652, finishTime=2023-01-17T00:51:47.240001) of size 1
INFO  [2023-01-17 00:51:47,252] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f] with 1 results within PT0.051794S
INFO  [2023-01-17 00:51:47,253] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f, workerId=AND$20NEGATION$20Test.worker_AND$20NEGATION$20Test_e5abe0e3-21ce-4f6e-9531-afdbc841c021, startTime=2023-01-17T00:51:47.200644, finishTime=2023-01-17T00:51:47.252438) of size 1
INFO  [2023-01-17 00:51:47,253] com.bakdata.conquery.models.execution.ManagedExecution: DONE cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f ManagedQuery within PT0.055366S
127.0.0.1 - - [17/Jan/2023:00:51:47 +0000] "GET /api/datasets/AND$20NEGATION$20Test/queries/AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f HTTP/1.1" 200 1888 "-" "Conquery (test client)" 43
INFO  [2023-01-17 00:51:47,278] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION Test], queryId=cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:47.197579, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@782d7e6[Count = 0], startTime=2023-01-17T00:51:47.197763, finishTime=2023-01-17T00:51:47.253129, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b089d0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6193a5ac, com.bakdata.conquery.models.query.ColumnDescriptor@5143fe68, com.bakdata.conquery.models.query.ColumnDescriptor@40ef3e41]) download on dataset Dataset[label=null, name=AND NEGATION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:47,278] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION Test], queryId=cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:47.197579, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@782d7e6[Count = 0], startTime=2023-01-17T00:51:47.197763, finishTime=2023-01-17T00:51:47.253129, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5b089d0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6193a5ac, com.bakdata.conquery.models.query.ColumnDescriptor@5143fe68, com.bakdata.conquery.models.query.ColumnDescriptor@40ef3e41]) on dataset Dataset[label=null, name=AND NEGATION Test]
127.0.0.1 - - [17/Jan/2023:00:51:47 +0000] "GET /api/datasets/AND%20NEGATION%20Test/result/AND$20NEGATION$20Test.cbb0446f-792e-4ab9-a551-5ba4d2ea7c8f.csv?pretty=false HTTP/1.1" 200 90 "-" "Conquery (test client)" 72
INFO  [2023-01-17 00:51:47,348] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND NEGATION Test on 3 rows
INFO  [2023-01-17 00:51:47,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND NEGATION Test
INFO  [2023-01-17 00:51:47,349] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-17 00:51:47,349] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION Test, name=AND NEGATION Test]
INFO  [2023-01-17 00:51:47,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION Test_e5abe0e3-21ce-4f6e-9531-afdbc841c021
INFO  [2023-01-17 00:51:47,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION Test_59ae943a-c44f-4320-b7c8-77049c56ff0e
INFO  [2023-01-17 00:51:47,447] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND NEGATION Test
INFO  [2023-01-17 00:51:47,447] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION Test_e5abe0e3-21ce-4f6e-9531-afdbc841c021
INFO  [2023-01-17 00:51:47,447] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION Test_59ae943a-c44f-4320-b7c8-77049c56ff0e
INFO  [2023-01-17 00:51:47,547] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20NEGATION$20Test
INFO  [2023-01-17 00:51:47,547] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,586] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND NEGATION Test
INFO  [2023-01-17 00:51:47,586] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-17 00:51:47,586] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:47,586] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:47,587] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-17 00:51:47,587] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-17 00:51:47,587] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:47,588] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:47,589] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_09ac4171-d662-46ba-ab80-6d76bcc8fb12 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:47,589] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_09ac4171-d662-46ba-ab80-6d76bcc8fb12 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:47,590] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:47,590] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_4026ccd2-a842-4132-873e-2c662c8c8f36 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:47,590] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_4026ccd2-a842-4132-873e-2c662c8c8f36 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:47,590] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:47,594] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,693] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,700] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,700] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:47,700] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:47,818] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,927] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:47,927] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:47,927] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 172 B in total
INFO  [2023-01-17 00:51:47,927] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION DATE LOGIC Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000408893sINFO  [2023-01-17 00:51:47,969] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=9, min=1, average=1.800000, max=2}
INFO  [2023-01-17 00:51:47,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=9, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:47,969] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@159fd180)
INFO  [2023-01-17 00:51:47,971] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:47,971] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:47,971] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:47,993] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20NEGATION$20DATE$20LOGIC$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:47 +0000] "POST /admin/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+NEGATION+DATE+LOGIC+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:51:47,994] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:47,994] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:47,995] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:47,995] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:47,996] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:47,997] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test.table.table], containing 9 entries.
INFO  [2023-01-17 00:51:47,997] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test.table.table], containing 9 entries.
WARN  [2023-01-17 00:51:47,997] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:47,997] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test.table.table.0
INFO  [2023-01-17 00:51:47,997] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test.table.table.1
INFO  [2023-01-17 00:51:48,103] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,108] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,117] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,118] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:48,118] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:48,224] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND NEGATION DATE LOGIC Test QUERY INIT
INFO  [2023-01-17 00:51:48,236] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20NEGATION$20DATE$20LOGIC$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:48,236] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[336034f0-e5cd-4756-a9c3-53d7219259c8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test))]]
INFO  [2023-01-17 00:51:48,240] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8
INFO  [2023-01-17 00:51:48,240] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8
INFO  [2023-01-17 00:51:48,242] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8] with 1 results within PT0.001502S
INFO  [2023-01-17 00:51:48,242] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8] with 1 results within PT0.001515S
127.0.0.1 - - [17/Jan/2023:00:51:48 +0000] "POST /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test/queries HTTP/1.1" 201 1669 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:48,242] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_09ac4171-d662-46ba-ab80-6d76bcc8fb12, startTime=2023-01-17T00:51:48.240595, finishTime=2023-01-17T00:51:48.242097) of size 1
INFO  [2023-01-17 00:51:48,242] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test.worker_AND$20NEGATION$20DATE$20LOGIC$20Test_4026ccd2-a842-4132-873e-2c662c8c8f36, startTime=2023-01-17T00:51:48.240610, finishTime=2023-01-17T00:51:48.242125) of size 1
INFO  [2023-01-17 00:51:48,243] com.bakdata.conquery.models.execution.ManagedExecution: DONE 336034f0-e5cd-4756-a9c3-53d7219259c8 ManagedQuery within PT0.006291S
127.0.0.1 - - [17/Jan/2023:00:51:48 +0000] "GET /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test/queries/AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8 HTTP/1.1" 200 2004 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:48,264] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test], queryId=336034f0-e5cd-4756-a9c3-53d7219259c8, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:48.236442, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3046eda9[Count = 0], startTime=2023-01-17T00:51:48.236698, finishTime=2023-01-17T00:51:48.242989, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@36580701), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6d5568b7, com.bakdata.conquery.models.query.ColumnDescriptor@743cf561, com.bakdata.conquery.models.query.ColumnDescriptor@731075ea]) download on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:48,264] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test], queryId=336034f0-e5cd-4756-a9c3-53d7219259c8, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:48.236442, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3046eda9[Count = 0], startTime=2023-01-17T00:51:48.236698, finishTime=2023-01-17T00:51:48.242989, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@36580701), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6d5568b7, com.bakdata.conquery.models.query.ColumnDescriptor@743cf561, com.bakdata.conquery.models.query.ColumnDescriptor@731075ea]) on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test]
127.0.0.1 - - [17/Jan/2023:00:51:48 +0000] "GET /api/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test/result/AND$20NEGATION$20DATE$20LOGIC$20Test.336034f0-e5cd-4756-a9c3-53d7219259c8.csv?pretty=false HTTP/1.1" 200 113 "-" "Conquery (test client)" 33
INFO  [2023-01-17 00:51:48,296] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND NEGATION DATE LOGIC Test on 3 rows
INFO  [2023-01-17 00:51:48,296] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND NEGATION DATE LOGIC Test
INFO  [2023-01-17 00:51:48,296] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-17 00:51:48,296] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test, name=AND NEGATION DATE LOGIC Test]
INFO  [2023-01-17 00:51:48,296] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test_4026ccd2-a842-4132-873e-2c662c8c8f36
INFO  [2023-01-17 00:51:48,296] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test_09ac4171-d662-46ba-ab80-6d76bcc8fb12
INFO  [2023-01-17 00:51:48,394] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test_4026ccd2-a842-4132-873e-2c662c8c8f36
INFO  [2023-01-17 00:51:48,394] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test_09ac4171-d662-46ba-ab80-6d76bcc8fb12
INFO  [2023-01-17 00:51:48,394] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND NEGATION DATE LOGIC Test
INFO  [2023-01-17 00:51:48,398] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20NEGATION$20DATE$20LOGIC$20Test
INFO  [2023-01-17 00:51:48,398] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,523] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-17 00:51:48,524] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-17 00:51:48,524] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:48,524] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:48,525] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-17 00:51:48,525] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:48,525] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-17 00:51:48,525] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:48,526] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,527] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_d429a535-58cc-4d37-98db-a05ea564acf3 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:48,527] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_d429a535-58cc-4d37-98db-a05ea564acf3 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:48,527] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:48,527] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_11816f56-0aa0-4d05-b7ea-dc363c171c8a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:48,527] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_11816f56-0aa0-4d05-b7ea-dc363c171c8a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:48,527] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:48,631] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,638] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,638] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test[1].table
INFO  [2023-01-17 00:51:48,638] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20NEGATION$20DATE$20LOGIC$20Test[1].table
INFO  [2023-01-17 00:51:48,755] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,865] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:48,865] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:48,865] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 206 B in total
INFO  [2023-01-17 00:51:48,866] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION DATE LOGIC Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00031062sINFO  [2023-01-17 00:51:48,897] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=11, min=1, average=2.200000, max=4}
INFO  [2023-01-17 00:51:48,897] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=11, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:48,897] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=11, nullLines=0), subType=IntegerParser(super=Parser(lines=11, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@43fa8adb)
INFO  [2023-01-17 00:51:48,901] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION DATE LOGIC Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:48,901] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:48,901] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND NEGATION DATE LOGIC Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:48,925] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20NEGATION$20DATE$20LOGIC$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:51:48 +0000] "POST /admin/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+NEGATION+DATE+LOGIC+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 16
INFO  [2023-01-17 00:51:48,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:48,927] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:48,928] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:48,928] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:48,931] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:48,931] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table], containing 11 entries.
INFO  [2023-01-17 00:51:48,931] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table], containing 11 entries.
WARN  [2023-01-17 00:51:48,932] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:48,933] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table.1
INFO  [2023-01-17 00:51:48,933] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20NEGATION$20DATE$20LOGIC$20Test[1].table.table.0
INFO  [2023-01-17 00:51:49,038] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,043] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,059] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,059] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:49,059] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:49,165] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND NEGATION DATE LOGIC Test QUERY INIT
INFO  [2023-01-17 00:51:49,184] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20NEGATION$20DATE$20LOGIC$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:49,185] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[bfe27798-3a09-43de-93fd-cba88e7db909] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test[1]))]]
INFO  [2023-01-17 00:51:49,190] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test[1].bfe27798-3a09-43de-93fd-cba88e7db909
INFO  [2023-01-17 00:51:49,190] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20NEGATION$20DATE$20LOGIC$20Test[1].bfe27798-3a09-43de-93fd-cba88e7db909
127.0.0.1 - - [17/Jan/2023:00:51:49 +0000] "POST /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D/queries HTTP/1.1" 201 1785 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:49,192] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test[1].bfe27798-3a09-43de-93fd-cba88e7db909] with 1 results within PT0.002815S
INFO  [2023-01-17 00:51:49,192] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20NEGATION$20DATE$20LOGIC$20Test[1].bfe27798-3a09-43de-93fd-cba88e7db909] with 1 results within PT0.002815S
INFO  [2023-01-17 00:51:49,193] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].bfe27798-3a09-43de-93fd-cba88e7db909, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_11816f56-0aa0-4d05-b7ea-dc363c171c8a, startTime=2023-01-17T00:51:49.190139, finishTime=2023-01-17T00:51:49.192954) of size 1
INFO  [2023-01-17 00:51:49,208] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].bfe27798-3a09-43de-93fd-cba88e7db909, workerId=AND$20NEGATION$20DATE$20LOGIC$20Test[1].worker_AND$20NEGATION$20DATE$20LOGIC$20Test[1]_d429a535-58cc-4d37-98db-a05ea564acf3, startTime=2023-01-17T00:51:49.190124, finishTime=2023-01-17T00:51:49.192939) of size 1
INFO  [2023-01-17 00:51:49,208] com.bakdata.conquery.models.execution.ManagedExecution: DONE bfe27798-3a09-43de-93fd-cba88e7db909 ManagedQuery within PT0.023188S
127.0.0.1 - - [17/Jan/2023:00:51:49 +0000] "GET /api/datasets/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D/queries/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D.bfe27798-3a09-43de-93fd-cba88e7db909 HTTP/1.1" 200 2440 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:51:49,236] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]], queryId=bfe27798-3a09-43de-93fd-cba88e7db909, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:49.184889, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2bc0c89e[Count = 0], startTime=2023-01-17T00:51:49.185192, finishTime=2023-01-17T00:51:49.208380, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6a5ec317), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5a59142f, com.bakdata.conquery.models.query.ColumnDescriptor@52195322, com.bakdata.conquery.models.query.ColumnDescriptor@7f30f5c7]) download on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:49,236] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]], queryId=bfe27798-3a09-43de-93fd-cba88e7db909, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:49.184889, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2bc0c89e[Count = 0], startTime=2023-01-17T00:51:49.185192, finishTime=2023-01-17T00:51:49.208380, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6a5ec317), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND NEGATION DATE LOGIC Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5a59142f, com.bakdata.conquery.models.query.ColumnDescriptor@52195322, com.bakdata.conquery.models.query.ColumnDescriptor@7f30f5c7]) on dataset Dataset[label=null, name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-17 00:51:49,240] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND NEGATION DATE LOGIC Test on 3 rows
127.0.0.1 - - [17/Jan/2023:00:51:49 +0000] "GET /api/datasets/AND%20NEGATION%20DATE%20LOGIC%20Test%5B1%5D/result/AND$20NEGATION$20DATE$20LOGIC$20Test%5B1%5D.bfe27798-3a09-43de-93fd-cba88e7db909.csv?pretty=false HTTP/1.1" 200 89 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:49,240] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND NEGATION DATE LOGIC Test[1]
INFO  [2023-01-17 00:51:49,240] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-17 00:51:49,240] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND NEGATION DATE LOGIC Test[1], name=AND NEGATION DATE LOGIC Test[1]]
INFO  [2023-01-17 00:51:49,240] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test[1]_11816f56-0aa0-4d05-b7ea-dc363c171c8a
INFO  [2023-01-17 00:51:49,240] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND NEGATION DATE LOGIC Test[1]_d429a535-58cc-4d37-98db-a05ea564acf3
INFO  [2023-01-17 00:51:49,338] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test[1]_11816f56-0aa0-4d05-b7ea-dc363c171c8a
INFO  [2023-01-17 00:51:49,338] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND NEGATION DATE LOGIC Test[1]_d429a535-58cc-4d37-98db-a05ea564acf3
INFO  [2023-01-17 00:51:49,338] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND NEGATION DATE LOGIC Test[1]
INFO  [2023-01-17 00:51:49,433] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20NEGATION$20DATE$20LOGIC$20Test[1]
INFO  [2023-01-17 00:51:49,433] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,465] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND NEGATION DATE LOGIC Test
INFO  [2023-01-17 00:51:49,466] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND Test
INFO  [2023-01-17 00:51:49,466] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:49,466] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:49,467] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-17 00:51:49,467] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-17 00:51:49,467] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:49,467] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:49,468] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,469] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test[1].worker_AND$20Test[1]_253bbff4-0f28-4607-a86f-27488ff49b2a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:49,469] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test[1].worker_AND$20Test[1]_253bbff4-0f28-4607-a86f-27488ff49b2a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:49,469] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:49,469] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20Test[1].worker_AND$20Test[1]_8bda616c-dc0e-4585-b238-01f0baba876f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:49,469] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20Test[1].worker_AND$20Test[1]_8bda616c-dc0e-4585-b238-01f0baba876f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:49,469] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:49,573] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,580] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,581] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test[1].table
INFO  [2023-01-17 00:51:49,581] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20Test[1].table
INFO  [2023-01-17 00:51:49,698] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,808] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:49,808] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:49,808] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 152 B in total
INFO  [2023-01-17 00:51:49,809] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000302882sINFO  [2023-01-17 00:51:49,839] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=1, average=2.000000, max=3}
INFO  [2023-01-17 00:51:49,839] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:49,839] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=15341), dateReader=com.bakdata.conquery.util.DateReader@1ee72e15)
INFO  [2023-01-17 00:51:49,842] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:49,842] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:49,842] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:49,854] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:51:49 +0000] "POST /admin/datasets/AND%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:51:49,854] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,855] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:49,856] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:49,856] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:49,859] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:49,859] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test[1].table.table], containing 8 entries.
INFO  [2023-01-17 00:51:49,859] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20Test[1].table.table], containing 8 entries.
WARN  [2023-01-17 00:51:49,860] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:49,860] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test[1].table.table.0
INFO  [2023-01-17 00:51:49,861] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20Test[1].table.table.1
INFO  [2023-01-17 00:51:49,965] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,971] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,985] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:49,986] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:49,986] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:50,092] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND Test QUERY INIT
INFO  [2023-01-17 00:51:50,109] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:50,110] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[26515cb0-15d5-4e9c-9397-155ae91dc710] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND Test[1]))]]
INFO  [2023-01-17 00:51:50,113] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test[1].26515cb0-15d5-4e9c-9397-155ae91dc710
INFO  [2023-01-17 00:51:50,113] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20Test[1].26515cb0-15d5-4e9c-9397-155ae91dc710
127.0.0.1 - - [17/Jan/2023:00:51:50 +0000] "POST /api/datasets/AND$20Test%5B1%5D/queries HTTP/1.1" 201 1664 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:50,114] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test[1].26515cb0-15d5-4e9c-9397-155ae91dc710] with 1 results within PT0.000962S
INFO  [2023-01-17 00:51:50,115] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20Test[1].26515cb0-15d5-4e9c-9397-155ae91dc710] with 1 results within PT0.00118S
INFO  [2023-01-17 00:51:50,115] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test[1].26515cb0-15d5-4e9c-9397-155ae91dc710, workerId=AND$20Test[1].worker_AND$20Test[1]_253bbff4-0f28-4607-a86f-27488ff49b2a, startTime=2023-01-17T00:51:50.113909, finishTime=2023-01-17T00:51:50.114871) of size 1
INFO  [2023-01-17 00:51:50,115] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20Test[1].26515cb0-15d5-4e9c-9397-155ae91dc710, workerId=AND$20Test[1].worker_AND$20Test[1]_8bda616c-dc0e-4585-b238-01f0baba876f, startTime=2023-01-17T00:51:50.113894, finishTime=2023-01-17T00:51:50.115074) of size 1
INFO  [2023-01-17 00:51:50,115] com.bakdata.conquery.models.execution.ManagedExecution: DONE 26515cb0-15d5-4e9c-9397-155ae91dc710 ManagedQuery within PT0.005709S
127.0.0.1 - - [17/Jan/2023:00:51:50 +0000] "GET /api/datasets/AND$20Test%5B1%5D/queries/AND$20Test%5B1%5D.26515cb0-15d5-4e9c-9397-155ae91dc710 HTTP/1.1" 200 2111 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:51:50,161] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test[1]], queryId=26515cb0-15d5-4e9c-9397-155ae91dc710, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:50.109869, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77bab6be[Count = 0], startTime=2023-01-17T00:51:50.110116, finishTime=2023-01-17T00:51:50.115825, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5bf0005b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6beb47b, com.bakdata.conquery.models.query.ColumnDescriptor@6289c6fa]) download on dataset Dataset[label=null, name=AND Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:50,161] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND Test[1]], queryId=26515cb0-15d5-4e9c-9397-155ae91dc710, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:50.109869, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77bab6be[Count = 0], startTime=2023-01-17T00:51:50.110116, finishTime=2023-01-17T00:51:50.115825, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5bf0005b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6beb47b, com.bakdata.conquery.models.query.ColumnDescriptor@6289c6fa]) on dataset Dataset[label=null, name=AND Test[1]]
127.0.0.1 - - [17/Jan/2023:00:51:50 +0000] "GET /api/datasets/AND%20Test%5B1%5D/result/AND$20Test%5B1%5D.26515cb0-15d5-4e9c-9397-155ae91dc710.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:50,164] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND Test on 3 rows
INFO  [2023-01-17 00:51:50,164] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND Test[1]
INFO  [2023-01-17 00:51:50,164] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-17 00:51:50,164] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND Test[1], name=AND Test[1]]
INFO  [2023-01-17 00:51:50,164] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test[1]_253bbff4-0f28-4607-a86f-27488ff49b2a
INFO  [2023-01-17 00:51:50,164] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND Test[1]_8bda616c-dc0e-4585-b238-01f0baba876f
INFO  [2023-01-17 00:51:50,173] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test[1]_8bda616c-dc0e-4585-b238-01f0baba876f
INFO  [2023-01-17 00:51:50,173] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND Test[1]
INFO  [2023-01-17 00:51:50,174] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND Test[1]_253bbff4-0f28-4607-a86f-27488ff49b2a
INFO  [2023-01-17 00:51:50,261] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20Test[1]
INFO  [2023-01-17 00:51:50,261] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,292] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND Test
INFO  [2023-01-17 00:51:50,292] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test OR Test
INFO  [2023-01-17 00:51:50,292] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:50,292] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:50,293] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-17 00:51:50,293] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-17 00:51:50,293] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:50,293] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:50,295] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20Test.worker_OR$20Test_5d5bd5e0-b7d6-4f9f-8c57-6312dc2caedb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:50,295] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20Test.worker_OR$20Test_5d5bd5e0-b7d6-4f9f-8c57-6312dc2caedb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:50,295] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:50,295] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20Test.worker_OR$20Test_b0fd1019-94b8-45dc-af40-cb074783b23e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:50,295] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20Test.worker_OR$20Test_b0fd1019-94b8-45dc-af40-cb074783b23e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:50,295] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:50,300] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,400] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,407] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,407] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20Test.table
INFO  [2023-01-17 00:51:50,407] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20Test.table
INFO  [2023-01-17 00:51:50,522] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,632] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:50,632] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:50,632] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 167 B in total
INFO  [2023-01-17 00:51:50,632] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000330238sINFO  [2023-01-17 00:51:50,665] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=9, min=1, average=1.800000, max=3}
INFO  [2023-01-17 00:51:50,665] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=9, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:50,666] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6c851401)
INFO  [2023-01-17 00:51:50,668] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:50,668] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:50,668] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:50,695] com.bakdata.conquery.models.jobs.ImportJob: Importing table into OR$20Test.table
INFO  [2023-01-17 00:51:50,695] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:50 +0000] "POST /admin/datasets/OR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_OR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:51:50,697] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:50,698] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:50,698] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:50,700] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:50,701] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20Test.table.table], containing 9 entries.
INFO  [2023-01-17 00:51:50,701] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20Test.table.table], containing 9 entries.
WARN  [2023-01-17 00:51:50,702] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:50,702] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20Test.table.table.0
INFO  [2023-01-17 00:51:50,702] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20Test.table.table.1
INFO  [2023-01-17 00:51:50,807] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,813] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,826] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:50,827] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:50,827] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:50,933] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: OR Test QUERY INIT
INFO  [2023-01-17 00:51:50,950] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[OR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:50,950] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f27d78ed-580b-457f-9f97-7c1c54601225] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_OR Test))]]
INFO  [2023-01-17 00:51:50,954] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225
INFO  [2023-01-17 00:51:50,954] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225
INFO  [2023-01-17 00:51:50,955] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225] with 1 results within PT0.00103S
INFO  [2023-01-17 00:51:50,955] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225] with 3 results within PT0.001468S
127.0.0.1 - - [17/Jan/2023:00:51:50 +0000] "POST /api/datasets/OR$20Test/queries HTTP/1.1" 201 1480 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:50,956] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225, workerId=OR$20Test.worker_OR$20Test_5d5bd5e0-b7d6-4f9f-8c57-6312dc2caedb, startTime=2023-01-17T00:51:50.954437, finishTime=2023-01-17T00:51:50.955467) of size 1
INFO  [2023-01-17 00:51:50,956] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225, workerId=OR$20Test.worker_OR$20Test_b0fd1019-94b8-45dc-af40-cb074783b23e, startTime=2023-01-17T00:51:50.954232, finishTime=2023-01-17T00:51:50.955700) of size 3
INFO  [2023-01-17 00:51:50,956] com.bakdata.conquery.models.execution.ManagedExecution: DONE f27d78ed-580b-457f-9f97-7c1c54601225 ManagedQuery within PT0.005598S
127.0.0.1 - - [17/Jan/2023:00:51:50 +0000] "GET /api/datasets/OR$20Test/queries/OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225 HTTP/1.1" 200 1706 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:50,985] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR Test], queryId=f27d78ed-580b-457f-9f97-7c1c54601225, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:50.950560, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6ff28ef4[Count = 0], startTime=2023-01-17T00:51:50.950792, finishTime=2023-01-17T00:51:50.956390, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@25a82bec), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@190bfae7, com.bakdata.conquery.models.query.ColumnDescriptor@2c8e6c42, com.bakdata.conquery.models.query.ColumnDescriptor@57f0c209]) download on dataset Dataset[label=null, name=OR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:50,986] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR Test], queryId=f27d78ed-580b-457f-9f97-7c1c54601225, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:50.950560, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6ff28ef4[Count = 0], startTime=2023-01-17T00:51:50.950792, finishTime=2023-01-17T00:51:50.956390, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@25a82bec), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@190bfae7, com.bakdata.conquery.models.query.ColumnDescriptor@2c8e6c42, com.bakdata.conquery.models.query.ColumnDescriptor@57f0c209]) on dataset Dataset[label=null, name=OR Test]
127.0.0.1 - - [17/Jan/2023:00:51:51 +0000] "GET /api/datasets/OR%20Test/result/OR$20Test.f27d78ed-580b-457f-9f97-7c1c54601225.csv?pretty=false HTTP/1.1" 200 142 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:51:51,004] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest OR Test on 5 rows
INFO  [2023-01-17 00:51:51,005] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast OR Test
INFO  [2023-01-17 00:51:51,005] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-17 00:51:51,005] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR Test, name=OR Test]
INFO  [2023-01-17 00:51:51,005] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR Test_5d5bd5e0-b7d6-4f9f-8c57-6312dc2caedb
INFO  [2023-01-17 00:51:51,005] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR Test_b0fd1019-94b8-45dc-af40-cb074783b23e
INFO  [2023-01-17 00:51:51,103] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR Test_5d5bd5e0-b7d6-4f9f-8c57-6312dc2caedb
INFO  [2023-01-17 00:51:51,103] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow OR Test
INFO  [2023-01-17 00:51:51,103] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR Test_b0fd1019-94b8-45dc-af40-cb074783b23e
INFO  [2023-01-17 00:51:51,203] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of OR$20Test
INFO  [2023-01-17 00:51:51,203] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,233] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test OR Test
INFO  [2023-01-17 00:51:51,233] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test OR_AND Select test
INFO  [2023-01-17 00:51:51,233] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:51,233] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:51,234] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-17 00:51:51,234] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-17 00:51:51,234] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:51,234] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:51,236] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_f485562e-d4ab-4a38-bf71-ea26352be738 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:51,236] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_f485562e-d4ab-4a38-bf71-ea26352be738 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:51,236] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:51,237] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_09d15e40-1b0d-4716-84d3-f83d33b5bde4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:51,237] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR_AND$20Select$20test.worker_OR_AND$20Select$20test_09d15e40-1b0d-4716-84d3-f83d33b5bde4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:51,237] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:51,241] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,340] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,347] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,347] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR_AND$20Select$20test.table
INFO  [2023-01-17 00:51:51,347] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR_AND$20Select$20test.table
INFO  [2023-01-17 00:51:51,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,580] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:51,580] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:51,580] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 154 B in total
INFO  [2023-01-17 00:51:51,580] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR_AND Select test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000325249sINFO  [2023-01-17 00:51:51,613] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=1, average=2.000000, max=3}
INFO  [2023-01-17 00:51:51,613] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:51,613] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@77ddbc0)
INFO  [2023-01-17 00:51:51,616] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR_AND Select test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:51,616] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:51,616] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR_AND Select test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:51,630] com.bakdata.conquery.models.jobs.ImportJob: Importing table into OR_AND$20Select$20test.table
INFO  [2023-01-17 00:51:51,631] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:51 +0000] "POST /admin/datasets/OR_AND%20Select%20test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_OR_AND+Select+test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:51,632] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:51,633] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:51,633] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:51,635] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:51,635] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR_AND$20Select$20test.table.table], containing 8 entries.
INFO  [2023-01-17 00:51:51,635] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR_AND$20Select$20test.table.table], containing 8 entries.
WARN  [2023-01-17 00:51:51,636] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:51,636] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR_AND$20Select$20test.table.table.1
INFO  [2023-01-17 00:51:51,636] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR_AND$20Select$20test.table.table.0
INFO  [2023-01-17 00:51:51,741] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,746] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,761] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:51,761] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:51,761] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:51,868] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: OR_AND Select test QUERY INIT
INFO  [2023-01-17 00:51:51,887] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[OR_AND$20Select$20test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:51,887] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1774968e-f5ca-4e80-b201-e3c9e669bc46] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_OR_AND Select test))]]
INFO  [2023-01-17 00:51:51,891] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46
INFO  [2023-01-17 00:51:51,891] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46
INFO  [2023-01-17 00:51:51,892] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46] with 1 results within PT0.000988S
INFO  [2023-01-17 00:51:51,893] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46] with 3 results within PT0.001451S
127.0.0.1 - - [17/Jan/2023:00:51:51 +0000] "POST /api/datasets/OR_AND$20Select$20test/queries HTTP/1.1" 201 2310 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:51,893] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46, workerId=OR_AND$20Select$20test.worker_OR_AND$20Select$20test_09d15e40-1b0d-4716-84d3-f83d33b5bde4, startTime=2023-01-17T00:51:51.891539, finishTime=2023-01-17T00:51:51.892527) of size 1
INFO  [2023-01-17 00:51:51,893] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46, workerId=OR_AND$20Select$20test.worker_OR_AND$20Select$20test_f485562e-d4ab-4a38-bf71-ea26352be738, startTime=2023-01-17T00:51:51.891615, finishTime=2023-01-17T00:51:51.893066) of size 3
INFO  [2023-01-17 00:51:51,893] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1774968e-f5ca-4e80-b201-e3c9e669bc46 ManagedQuery within PT0.006142S
127.0.0.1 - - [17/Jan/2023:00:51:51 +0000] "GET /api/datasets/OR_AND$20Select$20test/queries/OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46 HTTP/1.1" 200 2589 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:51,936] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR_AND Select test], queryId=1774968e-f5ca-4e80-b201-e3c9e669bc46, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:51.887387, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4e4ebfc4[Count = 0], startTime=2023-01-17T00:51:51.887669, finishTime=2023-01-17T00:51:51.893811, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@88956a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR_AND Select test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2b93b909, com.bakdata.conquery.models.query.ColumnDescriptor@18784fc2, com.bakdata.conquery.models.query.ColumnDescriptor@23f36396, com.bakdata.conquery.models.query.ColumnDescriptor@67148147]) download on dataset Dataset[label=null, name=OR_AND Select test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:51,936] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR_AND Select test], queryId=1774968e-f5ca-4e80-b201-e3c9e669bc46, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:51.887387, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4e4ebfc4[Count = 0], startTime=2023-01-17T00:51:51.887669, finishTime=2023-01-17T00:51:51.893811, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@88956a), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR_AND Select test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2b93b909, com.bakdata.conquery.models.query.ColumnDescriptor@18784fc2, com.bakdata.conquery.models.query.ColumnDescriptor@23f36396, com.bakdata.conquery.models.query.ColumnDescriptor@67148147]) on dataset Dataset[label=null, name=OR_AND Select test]
127.0.0.1 - - [17/Jan/2023:00:51:51 +0000] "GET /api/datasets/OR_AND%20Select%20test/result/OR_AND$20Select$20test.1774968e-f5ca-4e80-b201-e3c9e669bc46.csv?pretty=false HTTP/1.1" 200 168 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:51,939] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest OR_AND Select test on 5 rows
INFO  [2023-01-17 00:51:51,939] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast OR_AND Select test
INFO  [2023-01-17 00:51:51,939] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-17 00:51:51,939] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR_AND Select test, name=OR_AND Select test]
INFO  [2023-01-17 00:51:51,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR_AND Select test_09d15e40-1b0d-4716-84d3-f83d33b5bde4
INFO  [2023-01-17 00:51:51,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR_AND Select test_f485562e-d4ab-4a38-bf71-ea26352be738
INFO  [2023-01-17 00:51:52,035] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow OR_AND Select test
INFO  [2023-01-17 00:51:52,036] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR_AND Select test_f485562e-d4ab-4a38-bf71-ea26352be738
INFO  [2023-01-17 00:51:52,036] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR_AND Select test_09d15e40-1b0d-4716-84d3-f83d33b5bde4
INFO  [2023-01-17 00:51:52,037] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of OR_AND$20Select$20test
INFO  [2023-01-17 00:51:52,037] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,167] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test OR_AND Select test
INFO  [2023-01-17 00:51:52,167] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test OR DATE LOGIC Test
INFO  [2023-01-17 00:51:52,167] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:52,167] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:52,168] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-17 00:51:52,168] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-17 00:51:52,168] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:52,168] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:52,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_d16a1cd0-a794-44ad-94fb-48b513f9717e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:52,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_d16a1cd0-a794-44ad-94fb-48b513f9717e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:52,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:52,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_62bf1f4d-97e3-4ccd-ac2a-504d9dffc0bb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:52,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_62bf1f4d-97e3-4ccd-ac2a-504d9dffc0bb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:52,170] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:52,174] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,274] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,281] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,282] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:52,282] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table OR$20DATE$20LOGIC$20Test.table
INFO  [2023-01-17 00:51:52,397] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,507] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:52,508] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:52,508] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 240 B in total
INFO  [2023-01-17 00:51:52,508] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR DATE LOGIC Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000384702sINFO  [2023-01-17 00:51:52,547] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=13, min=2, average=2.600000, max=4}
INFO  [2023-01-17 00:51:52,547] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=13, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:52,547] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=0), subType=IntegerParser(super=Parser(lines=13, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@10c635a6)
INFO  [2023-01-17 00:51:52,549] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:52,549] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:52,549] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_OR DATE LOGIC Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:52,568] com.bakdata.conquery.models.jobs.ImportJob: Importing table into OR$20DATE$20LOGIC$20Test.table
127.0.0.1 - - [17/Jan/2023:00:51:52 +0000] "POST /admin/datasets/OR%20DATE%20LOGIC%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_OR+DATE+LOGIC+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:51:52,568] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,569] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:52,570] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:52,570] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:52,573] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:52,573] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
INFO  [2023-01-17 00:51:52,573] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[OR$20DATE$20LOGIC$20Test.table.table], containing 13 entries.
WARN  [2023-01-17 00:51:52,574] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:52,574] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20DATE$20LOGIC$20Test.table.table.0
INFO  [2023-01-17 00:51:52,575] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received OR$20DATE$20LOGIC$20Test.table.table.1
INFO  [2023-01-17 00:51:52,683] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,688] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,702] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:52,702] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:52,702] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:52,808] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: OR DATE LOGIC Test QUERY INIT
INFO  [2023-01-17 00:51:52,825] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[OR$20DATE$20LOGIC$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:52,825] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[95ccbb6a-d262-4e3b-a057-93c3e0d82eae] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_OR DATE LOGIC Test))]]
INFO  [2023-01-17 00:51:52,829] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae
INFO  [2023-01-17 00:51:52,829] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae
127.0.0.1 - - [17/Jan/2023:00:51:52 +0000] "POST /api/datasets/OR$20DATE$20LOGIC$20Test/queries HTTP/1.1" 201 1558 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:52,830] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae] with 2 results within PT0.001286S
INFO  [2023-01-17 00:51:52,831] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae] with 3 results within PT0.001607S
INFO  [2023-01-17 00:51:52,831] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae, workerId=OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_d16a1cd0-a794-44ad-94fb-48b513f9717e, startTime=2023-01-17T00:51:52.829468, finishTime=2023-01-17T00:51:52.831075) of size 3
INFO  [2023-01-17 00:51:52,831] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae, workerId=OR$20DATE$20LOGIC$20Test.worker_OR$20DATE$20LOGIC$20Test_62bf1f4d-97e3-4ccd-ac2a-504d9dffc0bb, startTime=2023-01-17T00:51:52.829658, finishTime=2023-01-17T00:51:52.830944) of size 2
INFO  [2023-01-17 00:51:52,831] com.bakdata.conquery.models.execution.ManagedExecution: DONE 95ccbb6a-d262-4e3b-a057-93c3e0d82eae ManagedQuery within PT0.006S
127.0.0.1 - - [17/Jan/2023:00:51:52 +0000] "GET /api/datasets/OR$20DATE$20LOGIC$20Test/queries/OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae HTTP/1.1" 200 1845 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:52,858] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR DATE LOGIC Test], queryId=95ccbb6a-d262-4e3b-a057-93c3e0d82eae, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:52.825576, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@27148532[Count = 0], startTime=2023-01-17T00:51:52.825964, finishTime=2023-01-17T00:51:52.831964, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@47229ac0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@37fdd3a, com.bakdata.conquery.models.query.ColumnDescriptor@78a73dfb, com.bakdata.conquery.models.query.ColumnDescriptor@7b990ac8]) download on dataset Dataset[label=null, name=OR DATE LOGIC Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:52,858] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=OR DATE LOGIC Test], queryId=95ccbb6a-d262-4e3b-a057-93c3e0d82eae, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:52.825576, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@27148532[Count = 0], startTime=2023-01-17T00:51:52.825964, finishTime=2023-01-17T00:51:52.831964, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@47229ac0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_OR DATE LOGIC Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@37fdd3a, com.bakdata.conquery.models.query.ColumnDescriptor@78a73dfb, com.bakdata.conquery.models.query.ColumnDescriptor@7b990ac8]) on dataset Dataset[label=null, name=OR DATE LOGIC Test]
127.0.0.1 - - [17/Jan/2023:00:51:52 +0000] "GET /api/datasets/OR%20DATE%20LOGIC%20Test/result/OR$20DATE$20LOGIC$20Test.95ccbb6a-d262-4e3b-a057-93c3e0d82eae.csv?pretty=false HTTP/1.1" 200 216 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:52,878] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest OR DATE LOGIC Test on 6 rows
INFO  [2023-01-17 00:51:52,878] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast OR DATE LOGIC Test
INFO  [2023-01-17 00:51:52,878] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-17 00:51:52,878] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=OR DATE LOGIC Test, name=OR DATE LOGIC Test]
INFO  [2023-01-17 00:51:52,878] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR DATE LOGIC Test_d16a1cd0-a794-44ad-94fb-48b513f9717e
INFO  [2023-01-17 00:51:52,878] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_OR DATE LOGIC Test_62bf1f4d-97e3-4ccd-ac2a-504d9dffc0bb
INFO  [2023-01-17 00:51:52,879] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR DATE LOGIC Test_d16a1cd0-a794-44ad-94fb-48b513f9717e
INFO  [2023-01-17 00:51:52,879] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow OR DATE LOGIC Test
INFO  [2023-01-17 00:51:52,879] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_OR DATE LOGIC Test_62bf1f4d-97e3-4ccd-ac2a-504d9dffc0bb
INFO  [2023-01-17 00:51:52,979] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of OR$20DATE$20LOGIC$20Test
INFO  [2023-01-17 00:51:52,979] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,008] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test OR DATE LOGIC Test
INFO  [2023-01-17 00:51:53,009] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:53,009] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:53,009] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:53,010] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-17 00:51:53,010] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-17 00:51:53,010] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:53,010] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_1b59641c-d102-44c7-9415-a397ac4f0a12 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_1b59641c-d102-44c7-9415-a397ac4f0a12 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_d3282f2a-b9d3-4f89-884f-142fc5afcc33 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_d3282f2a-b9d3-4f89-884f-142fc5afcc33 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:53,012] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:53,116] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,128] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,128] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[2].table
INFO  [2023-01-17 00:51:53,129] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[2].table
INFO  [2023-01-17 00:51:53,244] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,355] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:53,355] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:53,355] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 307 B in total
INFO  [2023-01-17 00:51:53,356] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[2]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000402808sINFO  [2023-01-17 00:51:53,396] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-17 00:51:53,397] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:53,397] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@3776362d)
INFO  [2023-01-17 00:51:53,401] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:53,401] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:53,401] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:53,422] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test[2].table
127.0.0.1 - - [17/Jan/2023:00:51:53 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DURATION+SUM+Test%5B2%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:51:53,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,424] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:53,425] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:53,425] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:53,428] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:53,429] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[2].table.table], containing 17 entries.
INFO  [2023-01-17 00:51:53,429] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[2].table.table], containing 17 entries.
WARN  [2023-01-17 00:51:53,430] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:53,430] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[2].table.table.0
INFO  [2023-01-17 00:51:53,430] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[2].table.table.1
INFO  [2023-01-17 00:51:53,536] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,541] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,555] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,556] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:53,556] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:53,662] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-17 00:51:53,678] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:53,679] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4d8589bd-061a-4694-bc0a-0a2a0d55ff9c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[2]))]]
INFO  [2023-01-17 00:51:53,683] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[2].4d8589bd-061a-4694-bc0a-0a2a0d55ff9c
INFO  [2023-01-17 00:51:53,683] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[2].4d8589bd-061a-4694-bc0a-0a2a0d55ff9c
INFO  [2023-01-17 00:51:53,684] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[2].4d8589bd-061a-4694-bc0a-0a2a0d55ff9c] with 2 results within PT0.001219S
INFO  [2023-01-17 00:51:53,684] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[2].4d8589bd-061a-4694-bc0a-0a2a0d55ff9c] with 2 results within PT0.001538S
127.0.0.1 - - [17/Jan/2023:00:51:53 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B2%5D/queries HTTP/1.1" 201 1848 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:53,685] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[2].4d8589bd-061a-4694-bc0a-0a2a0d55ff9c, workerId=AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_1b59641c-d102-44c7-9415-a397ac4f0a12, startTime=2023-01-17T00:51:53.683119, finishTime=2023-01-17T00:51:53.684657) of size 2
INFO  [2023-01-17 00:51:53,685] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[2].4d8589bd-061a-4694-bc0a-0a2a0d55ff9c, workerId=AND$20DURATION$20SUM$20Test[2].worker_AND$20DURATION$20SUM$20Test[2]_d3282f2a-b9d3-4f89-884f-142fc5afcc33, startTime=2023-01-17T00:51:53.683355, finishTime=2023-01-17T00:51:53.684574) of size 2
INFO  [2023-01-17 00:51:53,685] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4d8589bd-061a-4694-bc0a-0a2a0d55ff9c ManagedQuery within PT0.006075S
127.0.0.1 - - [17/Jan/2023:00:51:53 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B2%5D/queries/AND$20DURATION$20SUM$20Test%5B2%5D.4d8589bd-061a-4694-bc0a-0a2a0d55ff9c HTTP/1.1" 200 2431 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:53,711] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[2]], queryId=4d8589bd-061a-4694-bc0a-0a2a0d55ff9c, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:53.679255, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f2ca4f8[Count = 0], startTime=2023-01-17T00:51:53.679539, finishTime=2023-01-17T00:51:53.685614, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6f968997), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@57e93529, com.bakdata.conquery.models.query.ColumnDescriptor@7981288c, com.bakdata.conquery.models.query.ColumnDescriptor@13604820, com.bakdata.conquery.models.query.ColumnDescriptor@4861b1ea]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:53,711] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[2]], queryId=4d8589bd-061a-4694-bc0a-0a2a0d55ff9c, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:53.679255, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f2ca4f8[Count = 0], startTime=2023-01-17T00:51:53.679539, finishTime=2023-01-17T00:51:53.685614, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6f968997), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@57e93529, com.bakdata.conquery.models.query.ColumnDescriptor@7981288c, com.bakdata.conquery.models.query.ColumnDescriptor@13604820, com.bakdata.conquery.models.query.ColumnDescriptor@4861b1ea]) on dataset Dataset[label=null, name=AND DURATION SUM Test[2]]
127.0.0.1 - - [17/Jan/2023:00:51:53 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B2%5D/result/AND$20DURATION$20SUM$20Test%5B2%5D.4d8589bd-061a-4694-bc0a-0a2a0d55ff9c.csv?pretty=false HTTP/1.1" 200 173 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:53,731] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 5 rows
INFO  [2023-01-17 00:51:53,731] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[2]
INFO  [2023-01-17 00:51:53,732] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-17 00:51:53,732] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[2], name=AND DURATION SUM Test[2]]
INFO  [2023-01-17 00:51:53,732] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[2]_1b59641c-d102-44c7-9415-a397ac4f0a12
INFO  [2023-01-17 00:51:53,732] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[2]_d3282f2a-b9d3-4f89-884f-142fc5afcc33
INFO  [2023-01-17 00:51:53,811] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[2]
INFO  [2023-01-17 00:51:53,811] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[2]_1b59641c-d102-44c7-9415-a397ac4f0a12
INFO  [2023-01-17 00:51:53,812] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[2]_d3282f2a-b9d3-4f89-884f-142fc5afcc33
INFO  [2023-01-17 00:51:53,830] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[2]
INFO  [2023-01-17 00:51:53,830] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:53,962] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:53,962] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:53,962] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:53,962] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:53,963] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-17 00:51:53,963] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:53,963] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-17 00:51:53,964] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:53,965] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_649a296d-1b4e-4cce-9546-94de0528da66 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:53,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_649a296d-1b4e-4cce-9546-94de0528da66 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:53,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:53,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_1dc274be-18ae-4a6a-829a-83f29a7831d1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:53,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_1dc274be-18ae-4a6a-829a-83f29a7831d1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:53,966] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:53,970] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,069] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,076] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,076] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[3].table
INFO  [2023-01-17 00:51:54,077] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[3].table
INFO  [2023-01-17 00:51:54,199] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,311] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:54,311] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:54,312] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 308 B in total
INFO  [2023-01-17 00:51:54,312] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[3]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000401872sINFO  [2023-01-17 00:51:54,352] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-17 00:51:54,352] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:54,352] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@731719de)
INFO  [2023-01-17 00:51:54,355] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[3]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:54,355] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:54,355] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[3]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:54,376] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test[3].table
INFO  [2023-01-17 00:51:54,376] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:54 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B3%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DURATION+SUM+Test%5B3%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:51:54,377] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:54,379] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:54,379] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:54,382] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:54,382] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[3].table.table], containing 17 entries.
INFO  [2023-01-17 00:51:54,382] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[3].table.table], containing 17 entries.
WARN  [2023-01-17 00:51:54,383] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:54,383] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[3].table.table.0
INFO  [2023-01-17 00:51:54,384] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[3].table.table.1
INFO  [2023-01-17 00:51:54,489] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,494] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,508] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,508] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:54,509] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:54,614] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-17 00:51:54,630] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[3]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:54,631] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[0e382ae1-516e-48f1-b65e-cb1e1774fef3] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[3]))]]
INFO  [2023-01-17 00:51:54,634] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[3].0e382ae1-516e-48f1-b65e-cb1e1774fef3
INFO  [2023-01-17 00:51:54,634] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[3].0e382ae1-516e-48f1-b65e-cb1e1774fef3
INFO  [2023-01-17 00:51:54,635] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[3].0e382ae1-516e-48f1-b65e-cb1e1774fef3] with 2 results within PT0.001465S
127.0.0.1 - - [17/Jan/2023:00:51:54 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B3%5D/queries HTTP/1.1" 201 1685 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:54,636] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[3].0e382ae1-516e-48f1-b65e-cb1e1774fef3] with 2 results within PT0.001792S
INFO  [2023-01-17 00:51:54,636] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[3].0e382ae1-516e-48f1-b65e-cb1e1774fef3, workerId=AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_1dc274be-18ae-4a6a-829a-83f29a7831d1, startTime=2023-01-17T00:51:54.634435, finishTime=2023-01-17T00:51:54.635900) of size 2
INFO  [2023-01-17 00:51:54,636] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[3].0e382ae1-516e-48f1-b65e-cb1e1774fef3, workerId=AND$20DURATION$20SUM$20Test[3].worker_AND$20DURATION$20SUM$20Test[3]_649a296d-1b4e-4cce-9546-94de0528da66, startTime=2023-01-17T00:51:54.634459, finishTime=2023-01-17T00:51:54.636251) of size 2
INFO  [2023-01-17 00:51:54,636] com.bakdata.conquery.models.execution.ManagedExecution: DONE 0e382ae1-516e-48f1-b65e-cb1e1774fef3 ManagedQuery within PT0.00587S
127.0.0.1 - - [17/Jan/2023:00:51:54 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B3%5D/queries/AND$20DURATION$20SUM$20Test%5B3%5D.0e382ae1-516e-48f1-b65e-cb1e1774fef3 HTTP/1.1" 200 2268 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:54,660] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[3]], queryId=0e382ae1-516e-48f1-b65e-cb1e1774fef3, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:54.630882, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3e029bef[Count = 0], startTime=2023-01-17T00:51:54.631094, finishTime=2023-01-17T00:51:54.636964, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b6a91cd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[3])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@642ec5bd, com.bakdata.conquery.models.query.ColumnDescriptor@61448b64, com.bakdata.conquery.models.query.ColumnDescriptor@4b33a940]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[3]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:54,660] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[3]], queryId=0e382ae1-516e-48f1-b65e-cb1e1774fef3, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:54.630882, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3e029bef[Count = 0], startTime=2023-01-17T00:51:54.631094, finishTime=2023-01-17T00:51:54.636964, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@3b6a91cd), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[3])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@642ec5bd, com.bakdata.conquery.models.query.ColumnDescriptor@61448b64, com.bakdata.conquery.models.query.ColumnDescriptor@4b33a940]) on dataset Dataset[label=null, name=AND DURATION SUM Test[3]]
127.0.0.1 - - [17/Jan/2023:00:51:54 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B3%5D/result/AND$20DURATION$20SUM$20Test%5B3%5D.0e382ae1-516e-48f1-b65e-cb1e1774fef3.csv?pretty=false HTTP/1.1" 200 226 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:51:54,683] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 5 rows
INFO  [2023-01-17 00:51:54,683] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[3]
INFO  [2023-01-17 00:51:54,683] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-17 00:51:54,683] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[3], name=AND DURATION SUM Test[3]]
INFO  [2023-01-17 00:51:54,683] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[3]_1dc274be-18ae-4a6a-829a-83f29a7831d1
INFO  [2023-01-17 00:51:54,683] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[3]_649a296d-1b4e-4cce-9546-94de0528da66
INFO  [2023-01-17 00:51:54,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[3]_1dc274be-18ae-4a6a-829a-83f29a7831d1
INFO  [2023-01-17 00:51:54,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[3]_649a296d-1b4e-4cce-9546-94de0528da66
INFO  [2023-01-17 00:51:54,781] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[3]
INFO  [2023-01-17 00:51:54,784] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[3]
INFO  [2023-01-17 00:51:54,784] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:54,914] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:54,915] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:54,915] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:54,915] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:54,920] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-17 00:51:54,920] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-17 00:51:54,920] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:54,920] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:54,922] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_8e70185f-4eb1-4543-a88d-20769e9aa9b5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:54,922] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_8e70185f-4eb1-4543-a88d-20769e9aa9b5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:54,922] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:54,922] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_5f96d4ae-d6bf-4876-bd53-2ffcd6cdc30a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:54,922] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_5f96d4ae-d6bf-4876-bd53-2ffcd6cdc30a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:54,922] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:54,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,026] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,032] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,033] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[4].table
INFO  [2023-01-17 00:51:55,033] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table AND$20DURATION$20SUM$20Test[4].table
INFO  [2023-01-17 00:51:55,151] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,265] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:55,265] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:55,265] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 308 B in total
INFO  [2023-01-17 00:51:55,265] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[4]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000182501sINFO  [2023-01-17 00:51:55,283] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=17, min=2, average=3.400000, max=8}
INFO  [2023-01-17 00:51:55,284] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=17, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:55,284] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=17, nullLines=0), subType=IntegerParser(super=Parser(lines=17, nullLines=0), minValue=15340, maxValue=15344), dateReader=com.bakdata.conquery.util.DateReader@1add7fa7)
INFO  [2023-01-17 00:51:55,286] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[4]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:55,286] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:51:55,286] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_AND DURATION SUM Test[4]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:55,300] com.bakdata.conquery.models.jobs.ImportJob: Importing table into AND$20DURATION$20SUM$20Test[4].table
INFO  [2023-01-17 00:51:55,300] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:55 +0000] "POST /admin/datasets/AND%20DURATION%20SUM%20Test%5B4%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_AND+DURATION+SUM+Test%5B4%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:55,301] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:55,302] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:55,302] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:55,304] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:55,304] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[4].table.table], containing 17 entries.
INFO  [2023-01-17 00:51:55,304] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[AND$20DURATION$20SUM$20Test[4].table.table], containing 17 entries.
WARN  [2023-01-17 00:51:55,305] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:55,305] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[4].table.table.0
INFO  [2023-01-17 00:51:55,305] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received AND$20DURATION$20SUM$20Test[4].table.table.1
INFO  [2023-01-17 00:51:55,411] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,416] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,430] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:55,431] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:55,537] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: AND DURATION SUM Test QUERY INIT
INFO  [2023-01-17 00:51:55,554] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[AND$20DURATION$20SUM$20Test[4]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:55,554] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[69b21c36-56b9-4b68-9aa9-e9fb0c6e300b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[4]))]]
INFO  [2023-01-17 00:51:55,558] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[4].69b21c36-56b9-4b68-9aa9-e9fb0c6e300b
INFO  [2023-01-17 00:51:55,558] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery AND$20DURATION$20SUM$20Test[4].69b21c36-56b9-4b68-9aa9-e9fb0c6e300b
INFO  [2023-01-17 00:51:55,559] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[4].69b21c36-56b9-4b68-9aa9-e9fb0c6e300b] with 3 results within PT0.001128S
INFO  [2023-01-17 00:51:55,559] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[AND$20DURATION$20SUM$20Test[4].69b21c36-56b9-4b68-9aa9-e9fb0c6e300b] with 2 results within PT0.001029S
127.0.0.1 - - [17/Jan/2023:00:51:55 +0000] "POST /api/datasets/AND$20DURATION$20SUM$20Test%5B4%5D/queries HTTP/1.1" 201 1684 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:55,560] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[4].69b21c36-56b9-4b68-9aa9-e9fb0c6e300b, workerId=AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_8e70185f-4eb1-4543-a88d-20769e9aa9b5, startTime=2023-01-17T00:51:55.558158, finishTime=2023-01-17T00:51:55.559286) of size 3
INFO  [2023-01-17 00:51:55,560] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=AND$20DURATION$20SUM$20Test[4].69b21c36-56b9-4b68-9aa9-e9fb0c6e300b, workerId=AND$20DURATION$20SUM$20Test[4].worker_AND$20DURATION$20SUM$20Test[4]_5f96d4ae-d6bf-4876-bd53-2ffcd6cdc30a, startTime=2023-01-17T00:51:55.558357, finishTime=2023-01-17T00:51:55.559386) of size 2
INFO  [2023-01-17 00:51:55,560] com.bakdata.conquery.models.execution.ManagedExecution: DONE 69b21c36-56b9-4b68-9aa9-e9fb0c6e300b ManagedQuery within PT0.005639S
127.0.0.1 - - [17/Jan/2023:00:51:55 +0000] "GET /api/datasets/AND$20DURATION$20SUM$20Test%5B4%5D/queries/AND$20DURATION$20SUM$20Test%5B4%5D.69b21c36-56b9-4b68-9aa9-e9fb0c6e300b HTTP/1.1" 200 2267 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:55,585] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[4]], queryId=69b21c36-56b9-4b68-9aa9-e9fb0c6e300b, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:55.554425, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@11fecc1b[Count = 0], startTime=2023-01-17T00:51:55.554674, finishTime=2023-01-17T00:51:55.560313, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@249b31c6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[4])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@57938971, com.bakdata.conquery.models.query.ColumnDescriptor@7a378833, com.bakdata.conquery.models.query.ColumnDescriptor@1d4e1d7a]) download on dataset Dataset[label=null, name=AND DURATION SUM Test[4]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:55,585] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=AND DURATION SUM Test[4]], queryId=69b21c36-56b9-4b68-9aa9-e9fb0c6e300b, label=tree-a tree-b	@§$, creationTime=2023-01-17T00:51:55.554425, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@11fecc1b[Count = 0], startTime=2023-01-17T00:51:55.554674, finishTime=2023-01-17T00:51:55.560313, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@249b31c6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_AND DURATION SUM Test[4])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@57938971, com.bakdata.conquery.models.query.ColumnDescriptor@7a378833, com.bakdata.conquery.models.query.ColumnDescriptor@1d4e1d7a]) on dataset Dataset[label=null, name=AND DURATION SUM Test[4]]
127.0.0.1 - - [17/Jan/2023:00:51:55 +0000] "GET /api/datasets/AND%20DURATION%20SUM%20Test%5B4%5D/result/AND$20DURATION$20SUM$20Test%5B4%5D.69b21c36-56b9-4b68-9aa9-e9fb0c6e300b.csv?pretty=false HTTP/1.1" 200 255 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:51:55,602] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest AND DURATION SUM Test on 6 rows
INFO  [2023-01-17 00:51:55,602] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast AND DURATION SUM Test[4]
INFO  [2023-01-17 00:51:55,602] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-17 00:51:55,602] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=AND DURATION SUM Test[4], name=AND DURATION SUM Test[4]]
INFO  [2023-01-17 00:51:55,602] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[4]_5f96d4ae-d6bf-4876-bd53-2ffcd6cdc30a
INFO  [2023-01-17 00:51:55,603] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_AND DURATION SUM Test[4]_8e70185f-4eb1-4543-a88d-20769e9aa9b5
INFO  [2023-01-17 00:51:55,620] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow AND DURATION SUM Test[4]
INFO  [2023-01-17 00:51:55,621] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[4]_5f96d4ae-d6bf-4876-bd53-2ffcd6cdc30a
INFO  [2023-01-17 00:51:55,622] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_AND DURATION SUM Test[4]_8e70185f-4eb1-4543-a88d-20769e9aa9b5
INFO  [2023-01-17 00:51:55,706] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of AND$20DURATION$20SUM$20Test[4]
INFO  [2023-01-17 00:51:55,706] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,736] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test AND DURATION SUM Test
INFO  [2023-01-17 00:51:55,736] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-17 00:51:55,736] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:55,737] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:55,738] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-17 00:51:55,738] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:55,738] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-17 00:51:55,738] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:55,739] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_c4635294-792d-46cc-bd29-0dfdcac35767 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:55,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_c4635294-792d-46cc-bd29-0dfdcac35767 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:55,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:55,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_780fa209-6192-44be-80d5-cbfdeaafa349 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:55,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_780fa209-6192-44be-80d5-cbfdeaafa349 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:55,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:55,844] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,851] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:55,851] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose
INFO  [2023-01-17 00:51:55,851] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose
INFO  [2023-01-17 00:51:55,892] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose
INFO  [2023-01-17 00:51:55,892] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose
INFO  [2023-01-17 00:51:56,008] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,127] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:56,127] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:56,127] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:56,128] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 106 B in total
INFO  [2023-01-17 00:51:56,128] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_CONNECTORS_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████                ▌  68%	est. time remaining: 0.014983266sINFO  [2023-01-17 00:51:56,159] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=1, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:51:56,159] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@7f1cd04e)
INFO  [2023-01-17 00:51:56,159] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=1, nullLines=0), encoding=null, prefix=F20, suffix=F20)
INFO  [2023-01-17 00:51:56,159] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[aufnahmedatum] with DateParser(super=Parser(lines=1, nullLines=0), subType=IntegerParser(super=Parser(lines=1, nullLines=0), minValue=15340, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@669883f0)
INFO  [2023-01-17 00:51:56,161] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_CONNECTORS_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:56,161] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_CONNECTORS_QUERY Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:56,176] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=0, sum=0, min=2147483647, average=0.000000, max=-2147483648}
INFO  [2023-01-17 00:51:56,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au_ende] with DateParser(super=Parser(lines=0, nullLines=0), subType=IntegerParser(super=Parser(lines=0, nullLines=0), minValue=9223372036854775807, maxValue=-9223372036854775808), dateReader=com.bakdata.conquery.util.DateReader@921e0e5)
INFO  [2023-01-17 00:51:56,176] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au] with DateParser(super=Parser(lines=0, nullLines=0), subType=IntegerParser(super=Parser(lines=0, nullLines=0), minValue=9223372036854775807, maxValue=-9223372036854775808), dateReader=com.bakdata.conquery.util.DateReader@364703e9)
INFO  [2023-01-17 00:51:56,177] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=0, nullLines=0), encoding=null, prefix=null, suffix=null)
INFO  [2023-01-17 00:51:56,177] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au_beginn] with DateParser(super=Parser(lines=0, nullLines=0), subType=IntegerParser(super=Parser(lines=0, nullLines=0), minValue=9223372036854775807, maxValue=-9223372036854775808), dateReader=com.bakdata.conquery.util.DateReader@2fbd5709)
INFO  [2023-01-17 00:51:56,178] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_CONNECTORS_QUERY Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:56,178] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:56,178] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_CONNECTORS_QUERY Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:56,178] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_CONNECTORS_QUERY Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:56,194] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose
127.0.0.1 - - [17/Jan/2023:00:51:56 +0000] "POST /admin/datasets/MULTIPLE_CONNECTORS_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTIPLE_CONNECTORS_QUERY+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:56,196] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:56,196] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:56,196] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:56,198] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:51:56,199] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose.kh_diagnose], containing 1 entries.
INFO  [2023-01-17 00:51:56,199] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose.kh_diagnose], containing 1 entries.
WARN  [2023-01-17 00:51:56,200] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:56,200] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_CONNECTORS_QUERY$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-17 00:51:56,206] com.bakdata.conquery.models.jobs.ImportJob: Importing au_diagnose into MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose
INFO  [2023-01-17 00:51:56,206] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:56,206] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:56,206] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:56,206] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:51:56 +0000] "POST /admin/datasets/MULTIPLE_CONNECTORS_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTIPLE_CONNECTORS_QUERY+Test%2Fau_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:51:56,207] com.bakdata.conquery.models.jobs.ImportJob: Start sending 0 Buckets
WARN  [2023-01-17 00:51:56,207] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
WARN  [2023-01-17 00:51:56,207] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:56,207] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose.au_diagnose], containing 0 entries.
INFO  [2023-01-17 00:51:56,207] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_CONNECTORS_QUERY$20Test.au_diagnose.au_diagnose], containing 0 entries.
INFO  [2023-01-17 00:51:56,312] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,317] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,331] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,331] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:56,437] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTIPLE_CONNECTORS_QUERY Test QUERY INIT
INFO  [2023-01-17 00:51:56,453] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTIPLE_CONNECTORS_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:56,454] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[878208de-6064-4ffc-9035-d8c77da9b4a4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_CONNECTORS_QUERY Test))]]
INFO  [2023-01-17 00:51:56,457] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4
INFO  [2023-01-17 00:51:56,457] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4
WARN  [2023-01-17 00:51:56,458] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:51:56,458] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4] with 0 results within PT0.000195S
INFO  [2023-01-17 00:51:56,458] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4] with 1 results within PT0.000924S
INFO  [2023-01-17 00:51:56,458] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4, workerId=MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_780fa209-6192-44be-80d5-cbfdeaafa349, startTime=2023-01-17T00:51:56.457887, finishTime=2023-01-17T00:51:56.458082) of size 0
127.0.0.1 - - [17/Jan/2023:00:51:56 +0000] "POST /api/datasets/MULTIPLE_CONNECTORS_QUERY$20Test/queries HTTP/1.1" 201 1611 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:56,459] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4, workerId=MULTIPLE_CONNECTORS_QUERY$20Test.worker_MULTIPLE_CONNECTORS_QUERY$20Test_c4635294-792d-46cc-bd29-0dfdcac35767, startTime=2023-01-17T00:51:56.457661, finishTime=2023-01-17T00:51:56.458585) of size 1
INFO  [2023-01-17 00:51:56,459] com.bakdata.conquery.models.execution.ManagedExecution: DONE 878208de-6064-4ffc-9035-d8c77da9b4a4 ManagedQuery within PT0.005184S
127.0.0.1 - - [17/Jan/2023:00:51:56 +0000] "GET /api/datasets/MULTIPLE_CONNECTORS_QUERY$20Test/queries/MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4 HTTP/1.1" 200 1930 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:51:56,502] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test], queryId=878208de-6064-4ffc-9035-d8c77da9b4a4, label=F00-F99 F20-F29	@§$, creationTime=2023-01-17T00:51:56.453847, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@44716642[Count = 0], startTime=2023-01-17T00:51:56.454077, finishTime=2023-01-17T00:51:56.459261, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@20abdb94), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_CONNECTORS_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@56de4d44, com.bakdata.conquery.models.query.ColumnDescriptor@20d66afe]) download on dataset Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:56,502] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test], queryId=878208de-6064-4ffc-9035-d8c77da9b4a4, label=F00-F99 F20-F29	@§$, creationTime=2023-01-17T00:51:56.453847, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@44716642[Count = 0], startTime=2023-01-17T00:51:56.454077, finishTime=2023-01-17T00:51:56.459261, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@20abdb94), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_CONNECTORS_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@56de4d44, com.bakdata.conquery.models.query.ColumnDescriptor@20d66afe]) on dataset Dataset[label=null, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-17 00:51:56,504] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTIPLE_CONNECTORS_QUERY Test on 2 rows
127.0.0.1 - - [17/Jan/2023:00:51:56 +0000] "GET /api/datasets/MULTIPLE_CONNECTORS_QUERY%20Test/result/MULTIPLE_CONNECTORS_QUERY$20Test.878208de-6064-4ffc-9035-d8c77da9b4a4.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:51:56,504] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-17 00:51:56,505] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-17 00:51:56,505] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_CONNECTORS_QUERY Test_780fa209-6192-44be-80d5-cbfdeaafa349
INFO  [2023-01-17 00:51:56,505] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_CONNECTORS_QUERY Test, name=MULTIPLE_CONNECTORS_QUERY Test]
INFO  [2023-01-17 00:51:56,505] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_CONNECTORS_QUERY Test_c4635294-792d-46cc-bd29-0dfdcac35767
INFO  [2023-01-17 00:51:56,538] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-17 00:51:56,539] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_CONNECTORS_QUERY Test_780fa209-6192-44be-80d5-cbfdeaafa349
INFO  [2023-01-17 00:51:56,539] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_CONNECTORS_QUERY Test_c4635294-792d-46cc-bd29-0dfdcac35767
INFO  [2023-01-17 00:51:56,607] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTIPLE_CONNECTORS_QUERY$20Test
INFO  [2023-01-17 00:51:56,607] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,737] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTIPLE_CONNECTORS_QUERY Test
INFO  [2023-01-17 00:51:56,737] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-17 00:51:56,737] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:56,737] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:56,738] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-17 00:51:56,738] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:56,738] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-17 00:51:56,739] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:56,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,740] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_90771c9d-188a-4cd6-aa53-6a065ac1567e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:56,741] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_90771c9d-188a-4cd6-aa53-6a065ac1567e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:56,741] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:56,741] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_84dcb262-fa5d-4930-a0d4-e1efc5f17107 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:56,741] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_84dcb262-fa5d-4930-a0d4-e1efc5f17107 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:56,741] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:56,845] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,852] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:56,852] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose
INFO  [2023-01-17 00:51:56,852] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose
INFO  [2023-01-17 00:51:56,852] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose
INFO  [2023-01-17 00:51:56,896] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose
INFO  [2023-01-17 00:51:57,012] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,123] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:57,123] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:57,123] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:57,124] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 1.3 KiB in total
INFO  [2023-01-17 00:51:57,124] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/kh_diagnose.import.json, tag=Optional.empty)
████████████████████████                          ▌  48%	est. time remaining: 0.046092763sINFO  [2023-01-17 00:51:57,166] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=26, sum=37, min=1, average=1.423077, max=2}
INFO  [2023-01-17 00:51:57,167] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[entlassungsdatum] with DateParser(super=Parser(lines=37, nullLines=13), subType=IntegerParser(super=Parser(lines=37, nullLines=13), minValue=15430, maxValue=17317), dateReader=com.bakdata.conquery.util.DateReader@50bc2d96)
INFO  [2023-01-17 00:51:57,167] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=37, nullLines=0), encoding=null, prefix=F, suffix=)
INFO  [2023-01-17 00:51:57,170] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:57,170] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/au_diagnose.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000643273sINFO  [2023-01-17 00:51:57,189] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=27, sum=40, min=1, average=1.481481, max=2}
INFO  [2023-01-17 00:51:57,189] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[au_beginn] with DateParser(super=Parser(lines=40, nullLines=14), subType=IntegerParser(super=Parser(lines=40, nullLines=14), minValue=15492, maxValue=17410), dateReader=com.bakdata.conquery.util.DateReader@5243d40)
INFO  [2023-01-17 00:51:57,189] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[icd_code] with StringParser(super=Parser(lines=40, nullLines=0), encoding=null, prefix=F, suffix=)
INFO  [2023-01-17 00:51:57,192] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:57,192] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:57,192] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/kh_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:57,192] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTIPLE_TABLES_ICD_QUERY2 Test/au_diagnose.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:57,221] com.bakdata.conquery.models.jobs.ImportJob: Importing kh_diagnose into MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose
127.0.0.1 - - [17/Jan/2023:00:51:57 +0000] "POST /admin/datasets/MULTIPLE_TABLES_ICD_QUERY2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTIPLE_TABLES_ICD_QUERY2+Test%2Fkh_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:51:57,222] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:57,223] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries [DictionaryMapping(sourceDictionary=SuccinctTrie[size=12], targetDictionary=MapDictionary[size=12], numberOfNewIds=12)]
INFO  [2023-01-17 00:51:57,228] com.bakdata.conquery.models.jobs.ImportJob: Start sending 9 Buckets
INFO  [2023-01-17 00:51:57,229] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose], containing 37 entries.
INFO  [2023-01-17 00:51:57,229] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose], containing 37 entries.
INFO  [2023-01-17 00:51:57,230] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.0
INFO  [2023-01-17 00:51:57,230] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.1
INFO  [2023-01-17 00:51:57,230] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.2
WARN  [2023-01-17 00:51:57,230] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:57,230] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.3
INFO  [2023-01-17 00:51:57,230] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.4
INFO  [2023-01-17 00:51:57,230] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.5
INFO  [2023-01-17 00:51:57,231] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.8
INFO  [2023-01-17 00:51:57,240] com.bakdata.conquery.models.jobs.ImportJob: Importing au_diagnose into MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose
INFO  [2023-01-17 00:51:57,240] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:57,240] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries [DictionaryMapping(sourceDictionary=SuccinctTrie[size=12], targetDictionary=MapDictionary[size=12], numberOfNewIds=0)]
INFO  [2023-01-17 00:51:57,241] com.bakdata.conquery.models.jobs.ImportJob: Start sending 9 Buckets
127.0.0.1 - - [17/Jan/2023:00:51:57 +0000] "POST /admin/datasets/MULTIPLE_TABLES_ICD_QUERY2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTIPLE_TABLES_ICD_QUERY2+Test%2Fau_diagnose.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:51:57,241] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
WARN  [2023-01-17 00:51:57,241] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:57,241] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose], containing 40 entries.
INFO  [2023-01-17 00:51:57,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.3
INFO  [2023-01-17 00:51:57,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.4
INFO  [2023-01-17 00:51:57,242] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.5
INFO  [2023-01-17 00:51:57,243] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.8
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.6
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.kh_diagnose.kh_diagnose.7
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose], containing 40 entries.
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.0
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.1
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.2
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.6
INFO  [2023-01-17 00:51:57,272] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTIPLE_TABLES_ICD_QUERY2$20Test.au_diagnose.au_diagnose.7
INFO  [2023-01-17 00:51:57,377] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,382] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,392] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,392] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:57,392] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:57,498] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTIPLE_TABLES_ICD_QUERY2 Test QUERY INIT
INFO  [2023-01-17 00:51:57,514] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTIPLE_TABLES_ICD_QUERY2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:57,515] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ca8803cc-e142-458e-91d9-eae3ec0f3f7c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_TABLES_ICD_QUERY2 Test))]]
INFO  [2023-01-17 00:51:57,518] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c
INFO  [2023-01-17 00:51:57,519] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c
127.0.0.1 - - [17/Jan/2023:00:51:57 +0000] "POST /api/datasets/MULTIPLE_TABLES_ICD_QUERY2$20Test/queries HTTP/1.1" 201 1353 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:57,522] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c] with 1 results within PT0.003527S
INFO  [2023-01-17 00:51:57,523] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c] with 6 results within PT0.004175S
INFO  [2023-01-17 00:51:57,523] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c, workerId=MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_84dcb262-fa5d-4930-a0d4-e1efc5f17107, startTime=2023-01-17T00:51:57.519143, finishTime=2023-01-17T00:51:57.522670) of size 1
INFO  [2023-01-17 00:51:57,523] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c, workerId=MULTIPLE_TABLES_ICD_QUERY2$20Test.worker_MULTIPLE_TABLES_ICD_QUERY2$20Test_90771c9d-188a-4cd6-aa53-6a065ac1567e, startTime=2023-01-17T00:51:57.518927, finishTime=2023-01-17T00:51:57.523102) of size 6
INFO  [2023-01-17 00:51:57,523] com.bakdata.conquery.models.execution.ManagedExecution: DONE ca8803cc-e142-458e-91d9-eae3ec0f3f7c ManagedQuery within PT0.008299S
127.0.0.1 - - [17/Jan/2023:00:51:57 +0000] "GET /api/datasets/MULTIPLE_TABLES_ICD_QUERY2$20Test/queries/MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c HTTP/1.1" 200 1676 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:51:57,548] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test], queryId=ca8803cc-e142-458e-91d9-eae3ec0f3f7c, label=icd-f20	@§$, creationTime=2023-01-17T00:51:57.515256, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7bfb8d42[Count = 0], startTime=2023-01-17T00:51:57.515475, finishTime=2023-01-17T00:51:57.523774, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5312b789), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_TABLES_ICD_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6d3580d6, com.bakdata.conquery.models.query.ColumnDescriptor@1016f14a]) download on dataset Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:57,548] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test], queryId=ca8803cc-e142-458e-91d9-eae3ec0f3f7c, label=icd-f20	@§$, creationTime=2023-01-17T00:51:57.515256, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7bfb8d42[Count = 0], startTime=2023-01-17T00:51:57.515475, finishTime=2023-01-17T00:51:57.523774, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5312b789), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTIPLE_TABLES_ICD_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6d3580d6, com.bakdata.conquery.models.query.ColumnDescriptor@1016f14a]) on dataset Dataset[label=null, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
127.0.0.1 - - [17/Jan/2023:00:51:57 +0000] "GET /api/datasets/MULTIPLE_TABLES_ICD_QUERY2%20Test/result/MULTIPLE_TABLES_ICD_QUERY2$20Test.ca8803cc-e142-458e-91d9-eae3ec0f3f7c.csv?pretty=false HTTP/1.1" 200 312 "-" "Conquery (test client)" 25
INFO  [2023-01-17 00:51:57,572] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTIPLE_TABLES_ICD_QUERY2 Test on 8 rows
INFO  [2023-01-17 00:51:57,572] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-17 00:51:57,572] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-17 00:51:57,572] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTIPLE_TABLES_ICD_QUERY2 Test, name=MULTIPLE_TABLES_ICD_QUERY2 Test]
INFO  [2023-01-17 00:51:57,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_TABLES_ICD_QUERY2 Test_90771c9d-188a-4cd6-aa53-6a065ac1567e
INFO  [2023-01-17 00:51:57,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTIPLE_TABLES_ICD_QUERY2 Test_84dcb262-fa5d-4930-a0d4-e1efc5f17107
INFO  [2023-01-17 00:51:57,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_TABLES_ICD_QUERY2 Test_84dcb262-fa5d-4930-a0d4-e1efc5f17107
INFO  [2023-01-17 00:51:57,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-17 00:51:57,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTIPLE_TABLES_ICD_QUERY2 Test_90771c9d-188a-4cd6-aa53-6a065ac1567e
INFO  [2023-01-17 00:51:57,742] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTIPLE_TABLES_ICD_QUERY2$20Test
INFO  [2023-01-17 00:51:57,742] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,798] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTIPLE_TABLES_ICD_QUERY2 Test
INFO  [2023-01-17 00:51:57,798] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:57,798] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:57,798] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:57,800] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:57,800] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:57,800] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:57,800] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:57,802] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_96cc6e6d-6301-4b7b-bc86-135d8bac5c62 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:57,802] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_96cc6e6d-6301-4b7b-bc86-135d8bac5c62 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:57,802] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:57,802] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_5664fee8-fbd3-4a3f-afea-0283c449ba57 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:57,802] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_5664fee8-fbd3-4a3f-afea-0283c449ba57 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:57,802] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:57,806] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,906] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,913] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:57,914] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-17 00:51:57,914] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-17 00:51:57,914] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-17 00:51:57,956] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-17 00:51:58,081] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,190] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:58,190] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:58,190] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:58,190] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 214 B in total
INFO  [2023-01-17 00:51:58,190] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.03839623sINFO  [2023-01-17 00:51:58,229] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:51:58,229] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@2caee51c)
INFO  [2023-01-17 00:51:58,229] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:58,232] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:58,232] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000780274sINFO  [2023-01-17 00:51:58,272] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:51:58,272] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5d3bc5d2)
INFO  [2023-01-17 00:51:58,272] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:58,275] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:58,275] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:58,275] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:58,275] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:58,312] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-17 00:51:58,313] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:58,313] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:58,313] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:58,315] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:58,315] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
INFO  [2023-01-17 00:51:58,315] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
WARN  [2023-01-17 00:51:58,317] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:58,317] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:58,317] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table.test_table.1
127.0.0.1 - - [17/Jan/2023:00:51:58 +0000] "POST /admin/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 25
INFO  [2023-01-17 00:51:58,335] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table2 into MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-17 00:51:58,335] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:58,335] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:58,335] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
127.0.0.1 - - [17/Jan/2023:00:51:58 +0000] "POST /admin/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_CONCEPT_QUERY_SEPARATE_DATES+Test%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:51:58,335] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,336] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:58,336] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
INFO  [2023-01-17 00:51:58,336] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
WARN  [2023-01-17 00:51:58,336] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:58,336] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.0
INFO  [2023-01-17 00:51:58,336] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.1
INFO  [2023-01-17 00:51:58,441] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,446] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,460] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,461] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:58,461] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:51:58,567] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_CONCEPT_QUERY_SEPARATE_DATES Test QUERY INIT
INFO  [2023-01-17 00:51:58,582] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:58,583] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c7291e85-4945-4c80-a1fd-8bbdeb890825] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test))]]
INFO  [2023-01-17 00:51:58,587] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825
INFO  [2023-01-17 00:51:58,587] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825
INFO  [2023-01-17 00:51:58,588] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825] with 0 results within PT0.000849S
INFO  [2023-01-17 00:51:58,588] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825] with 2 results within PT0.001235S
INFO  [2023-01-17 00:51:58,589] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825, workerId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_96cc6e6d-6301-4b7b-bc86-135d8bac5c62, startTime=2023-01-17T00:51:58.587586, finishTime=2023-01-17T00:51:58.588435) of size 0
127.0.0.1 - - [17/Jan/2023:00:51:58 +0000] "POST /api/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test/queries HTTP/1.1" 201 2140 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:58,589] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825, workerId=MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test_5664fee8-fbd3-4a3f-afea-0283c449ba57, startTime=2023-01-17T00:51:58.587437, finishTime=2023-01-17T00:51:58.588672) of size 2
INFO  [2023-01-17 00:51:58,589] com.bakdata.conquery.models.execution.ManagedExecution: DONE c7291e85-4945-4c80-a1fd-8bbdeb890825 ManagedQuery within PT0.005531S
127.0.0.1 - - [17/Jan/2023:00:51:58 +0000] "GET /api/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test/queries/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825 HTTP/1.1" 200 2495 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:51:58,641] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test], queryId=c7291e85-4945-4c80-a1fd-8bbdeb890825, label=test_tree-test_child1 test_tree2-test_child1	@§$, creationTime=2023-01-17T00:51:58.583564, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@702fe84e[Count = 0], startTime=2023-01-17T00:51:58.583818, finishTime=2023-01-17T00:51:58.589349, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@68f47ac9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4856f9b4, com.bakdata.conquery.models.query.ColumnDescriptor@69cb75d9, com.bakdata.conquery.models.query.ColumnDescriptor@66a99d36, com.bakdata.conquery.models.query.ColumnDescriptor@2dc09118]) download on dataset Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:58,641] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test], queryId=c7291e85-4945-4c80-a1fd-8bbdeb890825, label=test_tree-test_child1 test_tree2-test_child1	@§$, creationTime=2023-01-17T00:51:58.583564, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@702fe84e[Count = 0], startTime=2023-01-17T00:51:58.583818, finishTime=2023-01-17T00:51:58.589349, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@68f47ac9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4856f9b4, com.bakdata.conquery.models.query.ColumnDescriptor@69cb75d9, com.bakdata.conquery.models.query.ColumnDescriptor@66a99d36, com.bakdata.conquery.models.query.ColumnDescriptor@2dc09118]) on dataset Dataset[label=null, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:58,644] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_CONCEPT_QUERY_SEPARATE_DATES Test on 3 rows
127.0.0.1 - - [17/Jan/2023:00:51:58 +0000] "GET /api/datasets/MULTI_CONCEPT_QUERY_SEPARATE_DATES%20Test/result/MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test.c7291e85-4945-4c80-a1fd-8bbdeb890825.csv?pretty=false HTTP/1.1" 200 218 "-" "Conquery (test client)" 28
INFO  [2023-01-17 00:51:58,644] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:58,644] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:58,644] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test, name=MULTI_CONCEPT_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:58,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_96cc6e6d-6301-4b7b-bc86-135d8bac5c62
INFO  [2023-01-17 00:51:58,645] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_5664fee8-fbd3-4a3f-afea-0283c449ba57
INFO  [2023-01-17 00:51:58,700] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:58,701] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_96cc6e6d-6301-4b7b-bc86-135d8bac5c62
INFO  [2023-01-17 00:51:58,701] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONCEPT_QUERY_SEPARATE_DATES Test_5664fee8-fbd3-4a3f-afea-0283c449ba57
INFO  [2023-01-17 00:51:58,740] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_CONCEPT_QUERY_SEPARATE_DATES$20Test
INFO  [2023-01-17 00:51:58,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,867] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_CONCEPT_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:58,868] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:58,868] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:58,868] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:58,870] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:58,870] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:58,871] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:58,871] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:58,873] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,875] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_ec90e5d8-476c-453e-86df-67b70400ebcb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:58,875] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_ec90e5d8-476c-453e-86df-67b70400ebcb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:58,875] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:58,875] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_c7813339-bc11-4ef7-97ef-17d61e14d648 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:58,875] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_c7813339-bc11-4ef7-97ef-17d61e14d648 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:58,875] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:58,976] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,984] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:58,984] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-17 00:51:58,984] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table
INFO  [2023-01-17 00:51:58,985] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-17 00:51:58,985] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2
INFO  [2023-01-17 00:51:59,111] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,224] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:51:59,225] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:59,225] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:51:59,225] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 224 B in total
INFO  [2023-01-17 00:51:59,225] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
█████████████████████████                         ▌  50%	est. time remaining: 0.03142718sINFO  [2023-01-17 00:51:59,257] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:51:59,257] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:59,257] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7f1e4a86)
INFO  [2023-01-17 00:51:59,261] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:59,261] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00067022sINFO  [2023-01-17 00:51:59,293] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:51:59,293] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=5, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:51:59,293] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3ade4f73)
INFO  [2023-01-17 00:51:59,295] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:59,295] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:51:59,295] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:59,295] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test/test_table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:51:59,311] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:51:59 +0000] "POST /admin/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:51:59,313] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:59,314] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:59,314] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:59,317] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:59,317] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
INFO  [2023-01-17 00:51:59,317] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table], containing 5 entries.
WARN  [2023-01-17 00:51:59,319] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:59,319] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table.0
INFO  [2023-01-17 00:51:59,319] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table.test_table.1
INFO  [2023-01-17 00:51:59,330] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table2 into MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2
127.0.0.1 - - [17/Jan/2023:00:51:59 +0000] "POST /admin/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_CONNECTOR_QUERY_SEPARATE_DATES+Test%2Ftest_table2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:51:59,331] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,331] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:51:59,331] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:51:59,331] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:51:59,331] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:51:59,331] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
INFO  [2023-01-17 00:51:59,332] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2], containing 5 entries.
WARN  [2023-01-17 00:51:59,332] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:51:59,332] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.0
INFO  [2023-01-17 00:51:59,332] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.test_table2.test_table2.1
INFO  [2023-01-17 00:51:59,438] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,443] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,456] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,456] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:59,457] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:51:59,563] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test QUERY INIT
INFO  [2023-01-17 00:51:59,579] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:51:59,579] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[fde55065-0893-4d9c-bab3-26f293b1d9fb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test))]]
INFO  [2023-01-17 00:51:59,583] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb
INFO  [2023-01-17 00:51:59,583] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb
INFO  [2023-01-17 00:51:59,584] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb] with 0 results within PT0.001074S
127.0.0.1 - - [17/Jan/2023:00:51:59 +0000] "POST /api/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test/queries HTTP/1.1" 201 1639 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:51:59,584] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb] with 2 results within PT0.001655S
INFO  [2023-01-17 00:51:59,585] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb, workerId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_c7813339-bc11-4ef7-97ef-17d61e14d648, startTime=2023-01-17T00:51:59.583305, finishTime=2023-01-17T00:51:59.584379) of size 0
INFO  [2023-01-17 00:51:59,585] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb, workerId=MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test_ec90e5d8-476c-453e-86df-67b70400ebcb, startTime=2023-01-17T00:51:59.583149, finishTime=2023-01-17T00:51:59.584804) of size 2
INFO  [2023-01-17 00:51:59,585] com.bakdata.conquery.models.execution.ManagedExecution: DONE fde55065-0893-4d9c-bab3-26f293b1d9fb ManagedQuery within PT0.00548S
127.0.0.1 - - [17/Jan/2023:00:51:59 +0000] "GET /api/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test/queries/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb HTTP/1.1" 200 2002 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:51:59,610] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test], queryId=fde55065-0893-4d9c-bab3-26f293b1d9fb, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:59.579725, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@47d19875[Count = 0], startTime=2023-01-17T00:51:59.579981, finishTime=2023-01-17T00:51:59.585461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6c6da340), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@30b7888a, com.bakdata.conquery.models.query.ColumnDescriptor@c62ea29, com.bakdata.conquery.models.query.ColumnDescriptor@473bc9e0]) download on dataset Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:51:59,611] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test], queryId=fde55065-0893-4d9c-bab3-26f293b1d9fb, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:51:59.579725, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@47d19875[Count = 0], startTime=2023-01-17T00:51:59.579981, finishTime=2023-01-17T00:51:59.585461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6c6da340), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@30b7888a, com.bakdata.conquery.models.query.ColumnDescriptor@c62ea29, com.bakdata.conquery.models.query.ColumnDescriptor@473bc9e0]) on dataset Dataset[label=null, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
127.0.0.1 - - [17/Jan/2023:00:51:59 +0000] "GET /api/datasets/MULTI_CONNECTOR_QUERY_SEPARATE_DATES%20Test/result/MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test.fde55065-0893-4d9c-bab3-26f293b1d9fb.csv?pretty=false HTTP/1.1" 200 152 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:51:59,630] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test on 3 rows
INFO  [2023-01-17 00:51:59,630] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:59,631] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:59,631] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test, name=MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test]
INFO  [2023-01-17 00:51:59,631] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_c7813339-bc11-4ef7-97ef-17d61e14d648
INFO  [2023-01-17 00:51:59,631] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_ec90e5d8-476c-453e-86df-67b70400ebcb
INFO  [2023-01-17 00:51:59,675] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_ec90e5d8-476c-453e-86df-67b70400ebcb
INFO  [2023-01-17 00:51:59,675] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:59,675] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test_c7813339-bc11-4ef7-97ef-17d61e14d648
INFO  [2023-01-17 00:51:59,738] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_CONNECTOR_QUERY_SEPARATE_DATES$20Test
INFO  [2023-01-17 00:51:59,738] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,863] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_CONNECTOR_QUERY_SEPARATE_DATES Test
INFO  [2023-01-17 00:51:59,868] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:51:59,868] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:51:59,868] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:51:59,869] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:59,869] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:59,869] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:51:59,869] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:51:59,871] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9426574d-b2b5-422d-b84a-4e2017e5763b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:59,871] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9426574d-b2b5-422d-b84a-4e2017e5763b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:59,871] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:59,871] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_b093e8d9-5602-4c48-82e3-c3dd12c8a8ef are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:51:59,871] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_b093e8d9-5602-4c48-82e3-c3dd12c8a8ef are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:51:59,871] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:51:59,875] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,974] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,981] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:51:59,982] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:51:59,982] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:00,095] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,216] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:00,217] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:00,217] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:52:00,217] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000279236sINFO  [2023-01-17 00:52:00,245] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:00,245] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:00,245] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@6183cb31)
INFO  [2023-01-17 00:52:00,250] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:00,250] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:00,250] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:00,274] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:00,275] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:00 +0000] "POST /admin/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:00,276] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:00,277] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:00,277] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:00,280] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:00,280] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:52:00,280] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:52:00,282] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:00,282] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:00,282] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:00,282] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:52:00,387] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,392] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,409] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,409] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:00,409] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:00,515] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:00,532] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:00,532] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[14637375-a6bd-4612-afca-25dcbc2f5a46] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:00,538] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46
INFO  [2023-01-17 00:52:00,538] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46
INFO  [2023-01-17 00:52:00,539] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46] with 0 results within PT0.001034S
127.0.0.1 - - [17/Jan/2023:00:52:00 +0000] "POST /api/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1559 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:00,540] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46] with 1 results within PT0.001404S
INFO  [2023-01-17 00:52:00,540] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46, workerId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_b093e8d9-5602-4c48-82e3-c3dd12c8a8ef, startTime=2023-01-17T00:52:00.538672, finishTime=2023-01-17T00:52:00.539706) of size 0
INFO  [2023-01-17 00:52:00,540] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46, workerId=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_9426574d-b2b5-422d-b84a-4e2017e5763b, startTime=2023-01-17T00:52:00.538738, finishTime=2023-01-17T00:52:00.540142) of size 1
INFO  [2023-01-17 00:52:00,540] com.bakdata.conquery.models.execution.ManagedExecution: DONE 14637375-a6bd-4612-afca-25dcbc2f5a46 ManagedQuery within PT0.007851S
127.0.0.1 - - [17/Jan/2023:00:52:00 +0000] "GET /api/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46 HTTP/1.1" 200 1962 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:00,566] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=14637375-a6bd-4612-afca-25dcbc2f5a46, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:00.532825, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@721166f4[Count = 0], startTime=2023-01-17T00:52:00.533035, finishTime=2023-01-17T00:52:00.540886, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@58a9ef11), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2ec061bf, com.bakdata.conquery.models.query.ColumnDescriptor@4ae69940]) download on dataset Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:00,566] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=14637375-a6bd-4612-afca-25dcbc2f5a46, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:00.532825, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@721166f4[Count = 0], startTime=2023-01-17T00:52:00.533035, finishTime=2023-01-17T00:52:00.540886, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@58a9ef11), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2ec061bf, com.bakdata.conquery.models.query.ColumnDescriptor@4ae69940]) on dataset Dataset[label=null, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:00 +0000] "GET /api/datasets/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.14637375-a6bd-4612-afca-25dcbc2f5a46.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:52:00,585] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 2 rows
INFO  [2023-01-17 00:52:00,585] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:00,585] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:00,585] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:00,586] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_b093e8d9-5602-4c48-82e3-c3dd12c8a8ef
INFO  [2023-01-17 00:52:00,586] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_9426574d-b2b5-422d-b84a-4e2017e5763b
INFO  [2023-01-17 00:52:00,684] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:00,684] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_9426574d-b2b5-422d-b84a-4e2017e5763b
INFO  [2023-01-17 00:52:00,684] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_b093e8d9-5602-4c48-82e3-c3dd12c8a8ef
INFO  [2023-01-17 00:52:00,784] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:00,784] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,815] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:00,815] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:00,816] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:00,816] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:00,817] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:00,817] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:00,817] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:00,817] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:00,818] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_0bc2bf12-1705-4cfc-8172-28c9b76fc5b8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:00,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_0bc2bf12-1705-4cfc-8172-28c9b76fc5b8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:00,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:00,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_ee7da551-9037-4b01-ae28-b4f19ddd73fc are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:00,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_ee7da551-9037-4b01-ae28-b4f19ddd73fc are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:00,819] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:00,923] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,929] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:00,930] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:00,930] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:01,048] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,159] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:01,159] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:01,159] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 157 B in total
INFO  [2023-01-17 00:52:01,159] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000342418sINFO  [2023-01-17 00:52:01,194] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:01,194] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:01,194] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@62159786)
INFO  [2023-01-17 00:52:01,197] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:01,197] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:01,197] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:01,210] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:01,211] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:01 +0000] "POST /admin/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:01,212] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:01,213] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:01,213] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:01,216] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:01,216] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:52:01,216] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:52:01,217] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:01,217] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:01,218] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:52:01,218] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:01,323] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,328] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,352] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,353] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:01,353] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:01,459] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:01,475] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:01,476] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[468caf50-f360-4e47-807b-566db762bd16] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:01,480] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16
INFO  [2023-01-17 00:52:01,481] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16
127.0.0.1 - - [17/Jan/2023:00:52:01 +0000] "POST /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 2180 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:01,482] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16] with 2 results within PT0.001217S
INFO  [2023-01-17 00:52:01,482] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16] with 4 results within PT0.001643S
INFO  [2023-01-17 00:52:01,482] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_0bc2bf12-1705-4cfc-8172-28c9b76fc5b8, startTime=2023-01-17T00:52:01.481011, finishTime=2023-01-17T00:52:01.482228) of size 2
INFO  [2023-01-17 00:52:01,483] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_ee7da551-9037-4b01-ae28-b4f19ddd73fc, startTime=2023-01-17T00:52:01.481122, finishTime=2023-01-17T00:52:01.482765) of size 4
INFO  [2023-01-17 00:52:01,483] com.bakdata.conquery.models.execution.ManagedExecution: DONE 468caf50-f360-4e47-807b-566db762bd16 ManagedQuery within PT0.007326S
127.0.0.1 - - [17/Jan/2023:00:52:01 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16 HTTP/1.1" 200 2619 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:01,513] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=468caf50-f360-4e47-807b-566db762bd16, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:01.475909, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@25dcfb98[Count = 0], startTime=2023-01-17T00:52:01.476118, finishTime=2023-01-17T00:52:01.483444, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@446d021c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e86343b, com.bakdata.conquery.models.query.ColumnDescriptor@54372539]) download on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:01,513] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=468caf50-f360-4e47-807b-566db762bd16, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:01.475909, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@25dcfb98[Count = 0], startTime=2023-01-17T00:52:01.476118, finishTime=2023-01-17T00:52:01.483444, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@446d021c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3e86343b, com.bakdata.conquery.models.query.ColumnDescriptor@54372539]) on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:01 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.468caf50-f360-4e47-807b-566db762bd16.csv?pretty=false HTTP/1.1" 200 43 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:52:01,531] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 7 rows
INFO  [2023-01-17 00:52:01,531] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:01,532] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:01,532] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:01,532] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_0bc2bf12-1705-4cfc-8172-28c9b76fc5b8
INFO  [2023-01-17 00:52:01,532] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_ee7da551-9037-4b01-ae28-b4f19ddd73fc
INFO  [2023-01-17 00:52:01,630] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:01,630] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_ee7da551-9037-4b01-ae28-b4f19ddd73fc
INFO  [2023-01-17 00:52:01,630] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_0bc2bf12-1705-4cfc-8172-28c9b76fc5b8
INFO  [2023-01-17 00:52:01,730] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:01,730] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,759] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:01,759] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-17 00:52:01,759] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:01,759] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:01,760] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-17 00:52:01,760] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-17 00:52:01,760] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:01,760] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:01,762] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_1d95a488-b1bf-48b4-b686-ac7816840195 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:01,762] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_1d95a488-b1bf-48b4-b686-ac7816840195 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:01,762] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:01,762] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_4bb6328a-53fe-4abd-a72f-e3c9c977158e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:01,762] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_4bb6328a-53fe-4abd-a72f-e3c9c977158e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:01,762] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:01,767] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,866] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,873] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:01,874] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1
INFO  [2023-01-17 00:52:01,874] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1
INFO  [2023-01-17 00:52:01,991] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,108] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:02,109] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:02,109] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:52:02,109] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000253338sINFO  [2023-01-17 00:52:02,134] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:02,134] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:02,134] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@6f52ab38)
INFO  [2023-01-17 00:52:02,137] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:02,137] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:02,137] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:02,156] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1
INFO  [2023-01-17 00:52:02,157] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:02 +0000] "POST /admin/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:02,157] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:02,158] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:02,158] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:02,161] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:02,161] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:52:02,161] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:52:02,163] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:02,163] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1.0
INFO  [2023-01-17 00:52:02,165] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1.1
INFO  [2023-01-17 00:52:02,165] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.table1.table1.2
INFO  [2023-01-17 00:52:02,270] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,275] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,291] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,291] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:02,291] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:02,397] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test QUERY INIT
INFO  [2023-01-17 00:52:02,413] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:02,413] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4885465d-0c9f-41ac-a0ff-5855f7ef1b75] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test))]]
INFO  [2023-01-17 00:52:02,419] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75
INFO  [2023-01-17 00:52:02,419] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75
127.0.0.1 - - [17/Jan/2023:00:52:02 +0000] "POST /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test/queries HTTP/1.1" 201 1640 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:02,420] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75] with 3 results within PT0.001123S
INFO  [2023-01-17 00:52:02,420] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75] with 4 results within PT0.001401S
INFO  [2023-01-17 00:52:02,420] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_4bb6328a-53fe-4abd-a72f-e3c9c977158e, startTime=2023-01-17T00:52:02.419040, finishTime=2023-01-17T00:52:02.420163) of size 3
INFO  [2023-01-17 00:52:02,421] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75, workerId=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test_1d95a488-b1bf-48b4-b686-ac7816840195, startTime=2023-01-17T00:52:02.419114, finishTime=2023-01-17T00:52:02.420515) of size 4
INFO  [2023-01-17 00:52:02,421] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4885465d-0c9f-41ac-a0ff-5855f7ef1b75 ManagedQuery within PT0.007199S
127.0.0.1 - - [17/Jan/2023:00:52:02 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test/queries/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75 HTTP/1.1" 200 2083 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:02,448] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test], queryId=4885465d-0c9f-41ac-a0ff-5855f7ef1b75, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:02.413819, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3574e7f9[Count = 0], startTime=2023-01-17T00:52:02.414044, finishTime=2023-01-17T00:52:02.421243, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@22143a20), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@ae61e28, com.bakdata.conquery.models.query.ColumnDescriptor@6aa222a2]) download on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:02,449] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test], queryId=4885465d-0c9f-41ac-a0ff-5855f7ef1b75, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:02.413819, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3574e7f9[Count = 0], startTime=2023-01-17T00:52:02.414044, finishTime=2023-01-17T00:52:02.421243, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@22143a20), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@ae61e28, com.bakdata.conquery.models.query.ColumnDescriptor@6aa222a2]) on dataset Dataset[label=null, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
127.0.0.1 - - [17/Jan/2023:00:52:02 +0000] "GET /api/datasets/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2%20Test/result/MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test.4885465d-0c9f-41ac-a0ff-5855f7ef1b75.csv?pretty=false HTTP/1.1" 200 48 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:52:02,472] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test on 8 rows
INFO  [2023-01-17 00:52:02,472] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-17 00:52:02,472] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-17 00:52:02,472] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test, name=MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test]
INFO  [2023-01-17 00:52:02,472] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_4bb6328a-53fe-4abd-a72f-e3c9c977158e
INFO  [2023-01-17 00:52:02,472] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_1d95a488-b1bf-48b4-b686-ac7816840195
INFO  [2023-01-17 00:52:02,570] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_1d95a488-b1bf-48b4-b686-ac7816840195
INFO  [2023-01-17 00:52:02,570] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test_4bb6328a-53fe-4abd-a72f-e3c9c977158e
INFO  [2023-01-17 00:52:02,570] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-17 00:52:02,670] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2$20Test
INFO  [2023-01-17 00:52:02,670] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,697] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY2 Test
INFO  [2023-01-17 00:52:02,697] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_QUERY Test
INFO  [2023-01-17 00:52:02,697] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:02,697] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:02,698] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-17 00:52:02,699] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-17 00:52:02,699] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:02,699] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:02,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_8777d7a6-1533-40a3-8019-4113addf2a95 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:02,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_8777d7a6-1533-40a3-8019-4113addf2a95 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:02,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:02,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_97af439a-b751-43fd-8840-052ec3c8fee8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:02,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_97af439a-b751-43fd-8840-052ec3c8fee8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:02,700] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:02,705] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,805] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,811] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:02,812] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test.table1
INFO  [2023-01-17 00:52:02,812] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test.table1
INFO  [2023-01-17 00:52:02,929] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,045] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:03,045] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:03,045] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 372 B in total
INFO  [2023-01-17 00:52:03,045] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000319791sINFO  [2023-01-17 00:52:03,078] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=12, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:52:03,078] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16106, maxValue=16469), dateReader=com.bakdata.conquery.util.DateReader@5710d4fc), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16121, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@17f37bc6), dateReader=com.bakdata.conquery.util.DateReader@420a1e78, onlyQuarters=false, maxValue=16616, minValue=16106, anyOpen=false)
INFO  [2023-01-17 00:52:03,078] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=12, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:03,080] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:03,080] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:03,080] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:03,097] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_QUERY$20Test.table1
INFO  [2023-01-17 00:52:03,098] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:03 +0000] "POST /admin/datasets/NUMBER_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:03,098] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:03,098] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:03,098] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:03,101] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:03,102] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test.table1.table1], containing 12 entries.
WARN  [2023-01-17 00:52:03,102] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:03,102] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:03,140] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test.table1.table1], containing 12 entries.
INFO  [2023-01-17 00:52:03,140] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:03,140] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:52:03,245] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,250] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,265] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:03,266] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:03,372] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:03,386] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:03,387] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9bd2cadd-074c-4d42-b256-b514a3246a29] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test))]]
INFO  [2023-01-17 00:52:03,392] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29
INFO  [2023-01-17 00:52:03,392] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29
127.0.0.1 - - [17/Jan/2023:00:52:03 +0000] "POST /api/datasets/NUMBER_QUERY$20Test/queries HTTP/1.1" 201 1358 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:03,393] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29] with 2 results within PT0.001296S
INFO  [2023-01-17 00:52:03,393] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29] with 2 results within PT0.001509S
INFO  [2023-01-17 00:52:03,394] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29, workerId=NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_97af439a-b751-43fd-8840-052ec3c8fee8, startTime=2023-01-17T00:52:03.392609, finishTime=2023-01-17T00:52:03.393905) of size 2
INFO  [2023-01-17 00:52:03,394] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29, workerId=NUMBER_QUERY$20Test.worker_NUMBER_QUERY$20Test_8777d7a6-1533-40a3-8019-4113addf2a95, startTime=2023-01-17T00:52:03.392452, finishTime=2023-01-17T00:52:03.393961) of size 2
INFO  [2023-01-17 00:52:03,394] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9bd2cadd-074c-4d42-b256-b514a3246a29 ManagedQuery within PT0.007307S
127.0.0.1 - - [17/Jan/2023:00:52:03 +0000] "GET /api/datasets/NUMBER_QUERY$20Test/queries/NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29 HTTP/1.1" 200 1625 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:03,436] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test], queryId=9bd2cadd-074c-4d42-b256-b514a3246a29, label=vs	@§$, creationTime=2023-01-17T00:52:03.387144, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@17019e36[Count = 0], startTime=2023-01-17T00:52:03.387341, finishTime=2023-01-17T00:52:03.394648, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@276d9706), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@13518bfe, com.bakdata.conquery.models.query.ColumnDescriptor@76a0f83b]) download on dataset Dataset[label=null, name=NUMBER_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:03,437] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test], queryId=9bd2cadd-074c-4d42-b256-b514a3246a29, label=vs	@§$, creationTime=2023-01-17T00:52:03.387144, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@17019e36[Count = 0], startTime=2023-01-17T00:52:03.387341, finishTime=2023-01-17T00:52:03.394648, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@276d9706), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@13518bfe, com.bakdata.conquery.models.query.ColumnDescriptor@76a0f83b]) on dataset Dataset[label=null, name=NUMBER_QUERY Test]
INFO  [2023-01-17 00:52:03,439] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_QUERY Test on 5 rows
127.0.0.1 - - [17/Jan/2023:00:52:03 +0000] "GET /api/datasets/NUMBER_QUERY%20Test/result/NUMBER_QUERY$20Test.9bd2cadd-074c-4d42-b256-b514a3246a29.csv?pretty=false HTTP/1.1" 200 145 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:03,439] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_QUERY Test
INFO  [2023-01-17 00:52:03,439] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-17 00:52:03,439] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test, name=NUMBER_QUERY Test]
INFO  [2023-01-17 00:52:03,440] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test_8777d7a6-1533-40a3-8019-4113addf2a95
INFO  [2023-01-17 00:52:03,440] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test_97af439a-b751-43fd-8840-052ec3c8fee8
INFO  [2023-01-17 00:52:03,499] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_QUERY Test
INFO  [2023-01-17 00:52:03,501] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test_97af439a-b751-43fd-8840-052ec3c8fee8
INFO  [2023-01-17 00:52:03,501] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test_8777d7a6-1533-40a3-8019-4113addf2a95
INFO  [2023-01-17 00:52:03,503] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_QUERY$20Test
INFO  [2023-01-17 00:52:03,503] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,677] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_QUERY Test
INFO  [2023-01-17 00:52:03,677] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING_QUERY Test
INFO  [2023-01-17 00:52:03,677] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:03,677] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:03,678] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-17 00:52:03,678] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:03,679] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-17 00:52:03,679] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:03,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_bf7d5fa8-cf84-4cec-b038-b10cbf83c4c1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:03,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_bf7d5fa8-cf84-4cec-b038-b10cbf83c4c1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:03,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:03,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_3fb07a16-3b6d-44f4-a210-5d8a8fd8861c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:03,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_3fb07a16-3b6d-44f4-a210-5d8a8fd8861c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:03,680] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:03,685] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,784] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,790] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:03,791] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_QUERY$20Test.table1
INFO  [2023-01-17 00:52:03,791] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_QUERY$20Test.table1
INFO  [2023-01-17 00:52:03,906] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,020] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:04,020] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:04,020] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 129 B in total
INFO  [2023-01-17 00:52:04,021] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000350345sINFO  [2023-01-17 00:52:04,056] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:04,056] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16251, maxValue=16555), dateReader=com.bakdata.conquery.util.DateReader@6dfb355d), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@64019dbe), dateReader=com.bakdata.conquery.util.DateReader@393db079, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-17 00:52:04,056] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=2), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-17 00:52:04,061] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:04,061] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:04,061] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:04,074] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_MISSING_QUERY$20Test.table1
INFO  [2023-01-17 00:52:04,076] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:04 +0000] "POST /admin/datasets/NUMBER_MISSING_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_MISSING_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:04,077] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:04,077] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:04,077] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:04,080] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:04,080] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-17 00:52:04,080] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-17 00:52:04,081] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:04,082] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:04,082] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:04,187] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,192] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,208] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,208] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:04,209] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:04,315] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:04,331] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:04,331] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d77fe151-aaec-423b-9dfe-42988514d92b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_QUERY Test))]]
INFO  [2023-01-17 00:52:04,337] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b
INFO  [2023-01-17 00:52:04,337] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b
INFO  [2023-01-17 00:52:04,338] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b] with 1 results within PT0.000811S
INFO  [2023-01-17 00:52:04,338] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b] with 0 results within PT0.000677S
127.0.0.1 - - [17/Jan/2023:00:52:04 +0000] "POST /api/datasets/NUMBER_MISSING_QUERY$20Test/queries HTTP/1.1" 201 1390 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:04,338] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b, workerId=NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_bf7d5fa8-cf84-4cec-b038-b10cbf83c4c1, startTime=2023-01-17T00:52:04.337428, finishTime=2023-01-17T00:52:04.338239) of size 1
INFO  [2023-01-17 00:52:04,339] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b, workerId=NUMBER_MISSING_QUERY$20Test.worker_NUMBER_MISSING_QUERY$20Test_3fb07a16-3b6d-44f4-a210-5d8a8fd8861c, startTime=2023-01-17T00:52:04.337859, finishTime=2023-01-17T00:52:04.338536) of size 0
INFO  [2023-01-17 00:52:04,339] com.bakdata.conquery.models.execution.ManagedExecution: DONE d77fe151-aaec-423b-9dfe-42988514d92b ManagedQuery within PT0.007331S
127.0.0.1 - - [17/Jan/2023:00:52:04 +0000] "GET /api/datasets/NUMBER_MISSING_QUERY$20Test/queries/NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b HTTP/1.1" 200 1689 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:04,364] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_QUERY Test], queryId=d77fe151-aaec-423b-9dfe-42988514d92b, label=vs	@§$, creationTime=2023-01-17T00:52:04.331557, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@26d18c0b[Count = 0], startTime=2023-01-17T00:52:04.331805, finishTime=2023-01-17T00:52:04.339136, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1b68d7da), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@10663b89, com.bakdata.conquery.models.query.ColumnDescriptor@3902cd6b]) download on dataset Dataset[label=null, name=NUMBER_MISSING_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:04,364] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_QUERY Test], queryId=d77fe151-aaec-423b-9dfe-42988514d92b, label=vs	@§$, creationTime=2023-01-17T00:52:04.331557, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@26d18c0b[Count = 0], startTime=2023-01-17T00:52:04.331805, finishTime=2023-01-17T00:52:04.339136, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1b68d7da), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@10663b89, com.bakdata.conquery.models.query.ColumnDescriptor@3902cd6b]) on dataset Dataset[label=null, name=NUMBER_MISSING_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:04 +0000] "GET /api/datasets/NUMBER_MISSING_QUERY%20Test/result/NUMBER_MISSING_QUERY$20Test.d77fe151-aaec-423b-9dfe-42988514d92b.csv?pretty=false HTTP/1.1" 200 40 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:04,379] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_MISSING_QUERY Test on 2 rows
INFO  [2023-01-17 00:52:04,379] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING_QUERY Test
INFO  [2023-01-17 00:52:04,380] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-17 00:52:04,380] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_QUERY Test, name=NUMBER_MISSING_QUERY Test]
INFO  [2023-01-17 00:52:04,380] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_QUERY Test_bf7d5fa8-cf84-4cec-b038-b10cbf83c4c1
INFO  [2023-01-17 00:52:04,380] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_QUERY Test_3fb07a16-3b6d-44f4-a210-5d8a8fd8861c
INFO  [2023-01-17 00:52:04,479] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING_QUERY Test
INFO  [2023-01-17 00:52:04,480] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_QUERY Test_bf7d5fa8-cf84-4cec-b038-b10cbf83c4c1
INFO  [2023-01-17 00:52:04,480] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_QUERY Test_3fb07a16-3b6d-44f4-a210-5d8a8fd8861c
INFO  [2023-01-17 00:52:04,482] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING_QUERY$20Test
INFO  [2023-01-17 00:52:04,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,618] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING_QUERY Test
INFO  [2023-01-17 00:52:04,619] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:04,619] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:04,619] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:04,620] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:04,620] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:04,620] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:04,620] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:04,621] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_b305ea5a-acd1-48bc-b49e-c26e92130612 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:04,621] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_b305ea5a-acd1-48bc-b49e-c26e92130612 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:04,621] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:04,622] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_ce7b29a5-35d5-43b1-a882-6b29582b68e7 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:04,622] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_ce7b29a5-35d5-43b1-a882-6b29582b68e7 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:04,622] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:04,626] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,725] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,732] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,733] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:04,733] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:04,861] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:04,973] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:04,973] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:04,973] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 129 B in total
INFO  [2023-01-17 00:52:04,973] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000316208sINFO  [2023-01-17 00:52:05,005] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:05,005] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16251, maxValue=16555), dateReader=com.bakdata.conquery.util.DateReader@3bf884bf), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@4dfd8492), dateReader=com.bakdata.conquery.util.DateReader@46942fa0, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-17 00:52:05,005] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=2), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-17 00:52:05,008] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:05,008] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:05,008] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:05,026] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:05,026] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:05 +0000] "POST /admin/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:05,027] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:05,027] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:05,027] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:05,029] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:05,030] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-17 00:52:05,030] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-17 00:52:05,030] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:05,030] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:05,031] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:05,135] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,141] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,155] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,156] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:05,156] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:05,262] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:05,277] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:05,278] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[95c5cde1-1c12-40c3-9a67-1f3b7d2e3288] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test))]]
INFO  [2023-01-17 00:52:05,282] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288
INFO  [2023-01-17 00:52:05,282] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288
INFO  [2023-01-17 00:52:05,283] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288] with 0 results within PT0.000646S
127.0.0.1 - - [17/Jan/2023:00:52:05 +0000] "POST /api/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test/queries HTTP/1.1" 201 1340 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:05,284] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288] with 2 results within PT0.001363S
INFO  [2023-01-17 00:52:05,284] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288, workerId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_ce7b29a5-35d5-43b1-a882-6b29582b68e7, startTime=2023-01-17T00:52:05.282737, finishTime=2023-01-17T00:52:05.283383) of size 0
INFO  [2023-01-17 00:52:05,284] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288, workerId=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test_b305ea5a-acd1-48bc-b49e-c26e92130612, startTime=2023-01-17T00:52:05.282640, finishTime=2023-01-17T00:52:05.284003) of size 2
INFO  [2023-01-17 00:52:05,284] com.bakdata.conquery.models.execution.ManagedExecution: DONE 95c5cde1-1c12-40c3-9a67-1f3b7d2e3288 ManagedQuery within PT0.006316S
127.0.0.1 - - [17/Jan/2023:00:52:05 +0000] "GET /api/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test/queries/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288 HTTP/1.1" 200 1727 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:05,309] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test], queryId=95c5cde1-1c12-40c3-9a67-1f3b7d2e3288, label=vs	@§$, creationTime=2023-01-17T00:52:05.278206, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@487719b8[Count = 0], startTime=2023-01-17T00:52:05.278403, finishTime=2023-01-17T00:52:05.284719, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66bbd767), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d610401, com.bakdata.conquery.models.query.ColumnDescriptor@390a52b1]) download on dataset Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:05,309] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test], queryId=95c5cde1-1c12-40c3-9a67-1f3b7d2e3288, label=vs	@§$, creationTime=2023-01-17T00:52:05.278206, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@487719b8[Count = 0], startTime=2023-01-17T00:52:05.278403, finishTime=2023-01-17T00:52:05.284719, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66bbd767), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3d610401, com.bakdata.conquery.models.query.ColumnDescriptor@390a52b1]) on dataset Dataset[label=null, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:05 +0000] "GET /api/datasets/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY%20Test/result/NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test.95c5cde1-1c12-40c3-9a67-1f3b7d2e3288.csv?pretty=false HTTP/1.1" 200 67 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:05,329] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:05,329] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:05,329] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:05,329] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:05,329] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_ce7b29a5-35d5-43b1-a882-6b29582b68e7
INFO  [2023-01-17 00:52:05,330] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_b305ea5a-acd1-48bc-b49e-c26e92130612
INFO  [2023-01-17 00:52:05,428] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_ce7b29a5-35d5-43b1-a882-6b29582b68e7
INFO  [2023-01-17 00:52:05,428] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test_b305ea5a-acd1-48bc-b49e-c26e92130612
INFO  [2023-01-17 00:52:05,428] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:05,431] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY$20Test
INFO  [2023-01-17 00:52:05,431] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,566] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING_NO_FILTER_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:05,566] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:05,566] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:05,566] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:05,567] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:05,567] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:05,567] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:05,567] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:05,569] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_7b8ffa7e-7788-4851-9abc-f6254617fc53 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:05,569] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_7b8ffa7e-7788-4851-9abc-f6254617fc53 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:05,569] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:05,569] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_74d7e523-0026-4a07-a8c3-27fff9c30661 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:05,569] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_74d7e523-0026-4a07-a8c3-27fff9c30661 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:05,569] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:05,573] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,673] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,680] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,680] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:05,680] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:05,794] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:05,901] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:05,902] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:05,902] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 129 B in total
INFO  [2023-01-17 00:52:05,902] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_NO_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000325458sINFO  [2023-01-17 00:52:05,935] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:05,935] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16251, maxValue=16555), dateReader=com.bakdata.conquery.util.DateReader@63db8c62), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@3f698965), dateReader=com.bakdata.conquery.util.DateReader@2e542ee2, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-17 00:52:05,935] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=2), requiredPrecision=4.9E-324, floatULP=1.401298464324817E-45)
INFO  [2023-01-17 00:52:05,937] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_NO_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:05,937] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:05,937] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_MISSING_NO_RESTRICTION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:05,957] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:05,957] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:05 +0000] "POST /admin/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_MISSING_NO_RESTRICTION_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:05,958] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:05,958] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:05,958] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:05,959] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:05,959] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-17 00:52:05,959] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-17 00:52:05,960] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:05,960] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:06,065] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,070] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,081] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,082] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:06,082] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:06,188] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_MISSING_NO_RESTRICTION_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:06,203] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:06,204] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[97cf4da5-36c8-434e-9f42-ecf024a41fe9] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_RESTRICTION_QUERY Test))]]
INFO  [2023-01-17 00:52:06,207] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9
INFO  [2023-01-17 00:52:06,207] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9
INFO  [2023-01-17 00:52:06,208] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9] with 1 results within PT0.000741S
127.0.0.1 - - [17/Jan/2023:00:52:06 +0000] "POST /api/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test/queries HTTP/1.1" 201 1278 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:06,209] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9] with 3 results within PT0.001206S
INFO  [2023-01-17 00:52:06,209] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9, workerId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_74d7e523-0026-4a07-a8c3-27fff9c30661, startTime=2023-01-17T00:52:06.207943, finishTime=2023-01-17T00:52:06.208684) of size 1
INFO  [2023-01-17 00:52:06,209] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9, workerId=NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.worker_NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test_7b8ffa7e-7788-4851-9abc-f6254617fc53, startTime=2023-01-17T00:52:06.207927, finishTime=2023-01-17T00:52:06.209133) of size 3
INFO  [2023-01-17 00:52:06,209] com.bakdata.conquery.models.execution.ManagedExecution: DONE 97cf4da5-36c8-434e-9f42-ecf024a41fe9 ManagedQuery within PT0.00569S
127.0.0.1 - - [17/Jan/2023:00:52:06 +0000] "GET /api/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test/queries/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9 HTTP/1.1" 200 1637 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:06,238] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test], queryId=97cf4da5-36c8-434e-9f42-ecf024a41fe9, label=vs	@§$, creationTime=2023-01-17T00:52:06.203974, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6378f572[Count = 0], startTime=2023-01-17T00:52:06.204175, finishTime=2023-01-17T00:52:06.209865, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@139e517e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6f32ef0c, com.bakdata.conquery.models.query.ColumnDescriptor@6a05e527]) download on dataset Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:06,238] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test], queryId=97cf4da5-36c8-434e-9f42-ecf024a41fe9, label=vs	@§$, creationTime=2023-01-17T00:52:06.203974, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6378f572[Count = 0], startTime=2023-01-17T00:52:06.204175, finishTime=2023-01-17T00:52:06.209865, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@139e517e), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_MISSING_NO_RESTRICTION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6f32ef0c, com.bakdata.conquery.models.query.ColumnDescriptor@6a05e527]) on dataset Dataset[label=null, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:06 +0000] "GET /api/datasets/NUMBER_MISSING_NO_RESTRICTION_QUERY%20Test/result/NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test.97cf4da5-36c8-434e-9f42-ecf024a41fe9.csv?pretty=false HTTP/1.1" 200 121 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:52:06,259] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_MISSING_NO_RESTRICTION_QUERY Test on 5 rows
INFO  [2023-01-17 00:52:06,259] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:06,260] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:06,260] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_MISSING_NO_RESTRICTION_QUERY Test, name=NUMBER_MISSING_NO_RESTRICTION_QUERY Test]
INFO  [2023-01-17 00:52:06,260] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_74d7e523-0026-4a07-a8c3-27fff9c30661
INFO  [2023-01-17 00:52:06,260] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_7b8ffa7e-7788-4851-9abc-f6254617fc53
INFO  [2023-01-17 00:52:06,268] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_74d7e523-0026-4a07-a8c3-27fff9c30661
INFO  [2023-01-17 00:52:06,279] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_MISSING_NO_RESTRICTION_QUERY Test_7b8ffa7e-7788-4851-9abc-f6254617fc53
INFO  [2023-01-17 00:52:06,292] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:06,360] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_MISSING_NO_RESTRICTION_QUERY$20Test
INFO  [2023-01-17 00:52:06,360] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,387] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_MISSING_NO_RESTRICTION_QUERY Test
INFO  [2023-01-17 00:52:06,388] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_NEGATION_QUERY Test
INFO  [2023-01-17 00:52:06,388] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:06,388] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:06,389] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-17 00:52:06,389] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-17 00:52:06,390] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:06,390] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:06,391] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_4eb81971-ecbb-4ff1-8b0a-4fd0d2d054e8 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:06,391] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_4eb81971-ecbb-4ff1-8b0a-4fd0d2d054e8 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:06,391] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:06,391] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_32d95f47-628a-430c-8aa6-26165f1826ed are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:06,391] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_32d95f47-628a-430c-8aa6-26165f1826ed are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:06,391] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:06,393] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,495] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,503] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,503] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_NEGATION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:06,503] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_NEGATION_QUERY$20Test.table1
INFO  [2023-01-17 00:52:06,623] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,734] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:06,734] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:06,734] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 372 B in total
INFO  [2023-01-17 00:52:06,734] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_NEGATION_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000356551sINFO  [2023-01-17 00:52:06,770] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=12, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:52:06,770] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16106, maxValue=16469), dateReader=com.bakdata.conquery.util.DateReader@3834e96b), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16121, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@2691a4c3), dateReader=com.bakdata.conquery.util.DateReader@5cb3f3e3, onlyQuarters=false, maxValue=16616, minValue=16106, anyOpen=false)
INFO  [2023-01-17 00:52:06,770] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=12, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:06,773] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_NEGATION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:06,773] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:06,773] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_NEGATION_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:06,800] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_NEGATION_QUERY$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:06 +0000] "POST /admin/datasets/NUMBER_NEGATION_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_NEGATION_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:06,801] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,801] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:06,801] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:06,801] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:06,802] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:06,802] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_NEGATION_QUERY$20Test.table1.table1], containing 12 entries.
INFO  [2023-01-17 00:52:06,803] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_NEGATION_QUERY$20Test.table1.table1], containing 12 entries.
WARN  [2023-01-17 00:52:06,803] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:06,803] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_NEGATION_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:06,803] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_NEGATION_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:06,803] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_NEGATION_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:52:06,908] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,913] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,929] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:06,930] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:06,930] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:07,036] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_NEGATION_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:07,052] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_NEGATION_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:07,053] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[092f1c96-2e4e-4faf-a322-b08e085bbdff] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_NEGATION_QUERY Test))]]
INFO  [2023-01-17 00:52:07,058] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff
INFO  [2023-01-17 00:52:07,058] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff
127.0.0.1 - - [17/Jan/2023:00:52:07 +0000] "POST /api/datasets/NUMBER_NEGATION_QUERY$20Test/queries HTTP/1.1" 201 1435 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:07,060] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff] with 1 results within PT0.001641S
INFO  [2023-01-17 00:52:07,060] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff] with 3 results within PT0.002032S
INFO  [2023-01-17 00:52:07,061] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff, workerId=NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_32d95f47-628a-430c-8aa6-26165f1826ed, startTime=2023-01-17T00:52:07.059018, finishTime=2023-01-17T00:52:07.060659) of size 1
INFO  [2023-01-17 00:52:07,061] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff, workerId=NUMBER_NEGATION_QUERY$20Test.worker_NUMBER_NEGATION_QUERY$20Test_4eb81971-ecbb-4ff1-8b0a-4fd0d2d054e8, startTime=2023-01-17T00:52:07.058937, finishTime=2023-01-17T00:52:07.060969) of size 3
INFO  [2023-01-17 00:52:07,061] com.bakdata.conquery.models.execution.ManagedExecution: DONE 092f1c96-2e4e-4faf-a322-b08e085bbdff ManagedQuery within PT0.00857S
127.0.0.1 - - [17/Jan/2023:00:52:07 +0000] "GET /api/datasets/NUMBER_NEGATION_QUERY$20Test/queries/NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff HTTP/1.1" 200 1738 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:07,088] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_NEGATION_QUERY Test], queryId=092f1c96-2e4e-4faf-a322-b08e085bbdff, label=vs	@§$, creationTime=2023-01-17T00:52:07.052937, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77cd246c[Count = 0], startTime=2023-01-17T00:52:07.053166, finishTime=2023-01-17T00:52:07.061736, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@f0d92bb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_NEGATION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5529c54d, com.bakdata.conquery.models.query.ColumnDescriptor@20c774ce]) download on dataset Dataset[label=null, name=NUMBER_NEGATION_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:07,088] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_NEGATION_QUERY Test], queryId=092f1c96-2e4e-4faf-a322-b08e085bbdff, label=vs	@§$, creationTime=2023-01-17T00:52:07.052937, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@77cd246c[Count = 0], startTime=2023-01-17T00:52:07.053166, finishTime=2023-01-17T00:52:07.061736, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@f0d92bb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_NEGATION_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5529c54d, com.bakdata.conquery.models.query.ColumnDescriptor@20c774ce]) on dataset Dataset[label=null, name=NUMBER_NEGATION_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:07 +0000] "GET /api/datasets/NUMBER_NEGATION_QUERY%20Test/result/NUMBER_NEGATION_QUERY$20Test.092f1c96-2e4e-4faf-a322-b08e085bbdff.csv?pretty=false HTTP/1.1" 200 37 "-" "Conquery (test client)" 24
INFO  [2023-01-17 00:52:07,111] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_NEGATION_QUERY Test on 5 rows
INFO  [2023-01-17 00:52:07,111] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_NEGATION_QUERY Test
INFO  [2023-01-17 00:52:07,111] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-17 00:52:07,111] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_NEGATION_QUERY Test, name=NUMBER_NEGATION_QUERY Test]
INFO  [2023-01-17 00:52:07,111] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_NEGATION_QUERY Test_32d95f47-628a-430c-8aa6-26165f1826ed
INFO  [2023-01-17 00:52:07,111] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_NEGATION_QUERY Test_4eb81971-ecbb-4ff1-8b0a-4fd0d2d054e8
INFO  [2023-01-17 00:52:07,210] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_NEGATION_QUERY Test
INFO  [2023-01-17 00:52:07,210] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_NEGATION_QUERY Test_4eb81971-ecbb-4ff1-8b0a-4fd0d2d054e8
INFO  [2023-01-17 00:52:07,210] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_NEGATION_QUERY Test_32d95f47-628a-430c-8aa6-26165f1826ed
INFO  [2023-01-17 00:52:07,310] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_NEGATION_QUERY$20Test
INFO  [2023-01-17 00:52:07,310] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,336] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_NEGATION_QUERY Test
INFO  [2023-01-17 00:52:07,336] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test NUMBER_QUERY Test
INFO  [2023-01-17 00:52:07,336] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:07,336] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:07,351] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-17 00:52:07,351] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:07,355] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-17 00:52:07,355] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:07,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_35bfe178-e08e-49de-8ff7-90aa8b01739f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:07,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_35bfe178-e08e-49de-8ff7-90aa8b01739f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:07,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:07,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_cf2ebaa6-97f8-45f3-883b-aa5cf1991480 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:07,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_cf2ebaa6-97f8-45f3-883b-aa5cf1991480 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:07,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:07,462] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,474] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,476] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test[1].table1
INFO  [2023-01-17 00:52:07,477] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table NUMBER_QUERY$20Test[1].table1
INFO  [2023-01-17 00:52:07,617] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,738] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:07,744] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:07,744] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 372 B in total
INFO  [2023-01-17 00:52:07,744] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_QUERY Test[1]/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000405806sINFO  [2023-01-17 00:52:07,785] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=12, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:52:07,785] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16106, maxValue=16469), dateReader=com.bakdata.conquery.util.DateReader@3b308de1), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16121, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@2ea9e55a), dateReader=com.bakdata.conquery.util.DateReader@46170292, onlyQuarters=false, maxValue=16616, minValue=16106, anyOpen=false)
INFO  [2023-01-17 00:52:07,785] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=12, nullLines=1), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:07,787] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_QUERY Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:07,787] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:07,787] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_NUMBER_QUERY Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:07,809] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into NUMBER_QUERY$20Test[1].table1
127.0.0.1 - - [17/Jan/2023:00:52:07 +0000] "POST /admin/datasets/NUMBER_QUERY%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_NUMBER_QUERY+Test%5B1%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:07,810] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,810] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:07,810] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:07,810] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:07,811] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:52:07,812] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:07,817] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test[1].table1.table1], containing 12 entries.
INFO  [2023-01-17 00:52:07,820] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[NUMBER_QUERY$20Test[1].table1.table1], containing 12 entries.
INFO  [2023-01-17 00:52:07,821] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received NUMBER_QUERY$20Test[1].table1.table1.0
INFO  [2023-01-17 00:52:07,926] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,931] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,968] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:07,968] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:08,077] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: NUMBER_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:08,109] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[NUMBER_QUERY$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:08,110] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b3432595-3a86-4ddb-9d1b-a8f89227ff71] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test[1]))]]
127.0.0.1 - - [17/Jan/2023:00:52:08 +0000] "POST /api/datasets/NUMBER_QUERY$20Test%5B1%5D/queries HTTP/1.1" 201 1370 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:52:08,131] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test[1].b3432595-3a86-4ddb-9d1b-a8f89227ff71
WARN  [2023-01-17 00:52:08,131] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:08,132] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery NUMBER_QUERY$20Test[1].b3432595-3a86-4ddb-9d1b-a8f89227ff71
INFO  [2023-01-17 00:52:08,132] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test[1].b3432595-3a86-4ddb-9d1b-a8f89227ff71] with 0 results within PT0.000199S
INFO  [2023-01-17 00:52:08,133] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[NUMBER_QUERY$20Test[1].b3432595-3a86-4ddb-9d1b-a8f89227ff71] with 4 results within PT0.001486S
INFO  [2023-01-17 00:52:08,135] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test[1].b3432595-3a86-4ddb-9d1b-a8f89227ff71, workerId=NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_35bfe178-e08e-49de-8ff7-90aa8b01739f, startTime=2023-01-17T00:52:08.131827, finishTime=2023-01-17T00:52:08.132026) of size 0
INFO  [2023-01-17 00:52:08,135] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=NUMBER_QUERY$20Test[1].b3432595-3a86-4ddb-9d1b-a8f89227ff71, workerId=NUMBER_QUERY$20Test[1].worker_NUMBER_QUERY$20Test[1]_cf2ebaa6-97f8-45f3-883b-aa5cf1991480, startTime=2023-01-17T00:52:08.132044, finishTime=2023-01-17T00:52:08.133530) of size 4
INFO  [2023-01-17 00:52:08,135] com.bakdata.conquery.models.execution.ManagedExecution: DONE b3432595-3a86-4ddb-9d1b-a8f89227ff71 ManagedQuery within PT0.02567S
127.0.0.1 - - [17/Jan/2023:00:52:08 +0000] "GET /api/datasets/NUMBER_QUERY$20Test%5B1%5D/queries/NUMBER_QUERY$20Test%5B1%5D.b3432595-3a86-4ddb-9d1b-a8f89227ff71 HTTP/1.1" 200 1890 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:08,157] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test[1]], queryId=b3432595-3a86-4ddb-9d1b-a8f89227ff71, label=vs	@§$, creationTime=2023-01-17T00:52:08.109957, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3f9d8d9c[Count = 0], startTime=2023-01-17T00:52:08.110241, finishTime=2023-01-17T00:52:08.135911, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4bc41796), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@759292ac], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@5a79e8fc], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@3b43b516]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@773b614f, com.bakdata.conquery.models.query.ColumnDescriptor@2499e359]) download on dataset Dataset[label=null, name=NUMBER_QUERY Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:08,157] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=NUMBER_QUERY Test[1]], queryId=b3432595-3a86-4ddb-9d1b-a8f89227ff71, label=vs	@§$, creationTime=2023-01-17T00:52:08.109957, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3f9d8d9c[Count = 0], startTime=2023-01-17T00:52:08.110241, finishTime=2023-01-17T00:52:08.135911, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4bc41796), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_NUMBER_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@759292ac], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@5a79e8fc], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@3b43b516]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@773b614f, com.bakdata.conquery.models.query.ColumnDescriptor@2499e359]) on dataset Dataset[label=null, name=NUMBER_QUERY Test[1]]
127.0.0.1 - - [17/Jan/2023:00:52:08 +0000] "GET /api/datasets/NUMBER_QUERY%20Test%5B1%5D/result/NUMBER_QUERY$20Test%5B1%5D.b3432595-3a86-4ddb-9d1b-a8f89227ff71.csv?pretty=false HTTP/1.1" 200 143 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:52:08,178] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest NUMBER_QUERY Test on 5 rows
INFO  [2023-01-17 00:52:08,178] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast NUMBER_QUERY Test[1]
INFO  [2023-01-17 00:52:08,180] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-17 00:52:08,180] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test[1]_35bfe178-e08e-49de-8ff7-90aa8b01739f
INFO  [2023-01-17 00:52:08,180] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=NUMBER_QUERY Test[1], name=NUMBER_QUERY Test[1]]
INFO  [2023-01-17 00:52:08,180] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_NUMBER_QUERY Test[1]_cf2ebaa6-97f8-45f3-883b-aa5cf1991480
INFO  [2023-01-17 00:52:08,248] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow NUMBER_QUERY Test[1]
INFO  [2023-01-17 00:52:08,252] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test[1]_35bfe178-e08e-49de-8ff7-90aa8b01739f
INFO  [2023-01-17 00:52:08,256] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_NUMBER_QUERY Test[1]_cf2ebaa6-97f8-45f3-883b-aa5cf1991480
INFO  [2023-01-17 00:52:08,313] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of NUMBER_QUERY$20Test[1]
INFO  [2023-01-17 00:52:08,313] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,381] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test NUMBER_QUERY Test
INFO  [2023-01-17 00:52:08,382] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:08,382] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:08,382] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:08,383] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:08,383] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:08,383] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:08,383] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:08,385] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_1b21e960-81ce-4b06-aed9-4a8faaa6ff5a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:08,385] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_1b21e960-81ce-4b06-aed9-4a8faaa6ff5a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:08,385] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:08,385] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_5231d8b8-a9e7-4158-a172-234714acba63 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:08,385] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_5231d8b8-a9e7-4158-a172-234714acba63 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:08,385] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:08,389] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,489] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,496] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,496] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:08,496] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:08,616] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,727] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:08,728] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:08,728] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 178 B in total
INFO  [2023-01-17 00:52:08,728] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TEMPORAL_BEFORE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000288333sINFO  [2023-01-17 00:52:08,757] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=9, min=2, average=2.250000, max=3}
INFO  [2023-01-17 00:52:08,757] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:08,757] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=14975, maxValue=15006), dateReader=com.bakdata.conquery.util.DateReader@273fd476)
INFO  [2023-01-17 00:52:08,761] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TEMPORAL_BEFORE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:08,761] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:08,761] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TEMPORAL_BEFORE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:08,773] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:08 +0000] "POST /admin/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_TEMPORAL_BEFORE_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:08,774] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,775] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:08,776] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:08,776] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:08,778] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:08,778] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1], containing 9 entries.
INFO  [2023-01-17 00:52:08,778] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1], containing 9 entries.
WARN  [2023-01-17 00:52:08,780] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:08,780] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:08,780] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:08,885] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,911] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:08,912] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:08,912] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:09,018] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: TEMPORAL_BEFORE_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:09,035] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:09,036] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TEMPORAL_BEFORE_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:09,040] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82
INFO  [2023-01-17 00:52:09,040] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "POST /api/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1932 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:09,058] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82] with 1 results within PT0.018313S
INFO  [2023-01-17 00:52:09,058] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82] with 1 results within PT0.018349S
INFO  [2023-01-17 00:52:09,059] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82, workerId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_1b21e960-81ce-4b06-aed9-4a8faaa6ff5a, startTime=2023-01-17T00:52:09.040163, finishTime=2023-01-17T00:52:09.058476) of size 1
INFO  [2023-01-17 00:52:09,059] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82, workerId=TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.worker_TEMPORAL_BEFORE_CONCEPT_QUERY$20Test_5231d8b8-a9e7-4158-a172-234714acba63, startTime=2023-01-17T00:52:09.040137, finishTime=2023-01-17T00:52:09.058486) of size 1
INFO  [2023-01-17 00:52:09,059] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82 ManagedQuery within PT0.022963S
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "GET /api/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test/queries/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82 HTTP/1.1" 200 2268 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:09,079] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test], queryId=5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:09.036087, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@587fa0da[Count = 0], startTime=2023-01-17T00:52:09.036269, finishTime=2023-01-17T00:52:09.059232, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66140a3d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TEMPORAL_BEFORE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@78e712ba, com.bakdata.conquery.models.query.ColumnDescriptor@2f9111a4]) download on dataset Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:09,080] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test], queryId=5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:09.036087, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@587fa0da[Count = 0], startTime=2023-01-17T00:52:09.036269, finishTime=2023-01-17T00:52:09.059232, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66140a3d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TEMPORAL_BEFORE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@78e712ba, com.bakdata.conquery.models.query.ColumnDescriptor@2f9111a4]) on dataset Dataset[label=null, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "GET /api/datasets/TEMPORAL_BEFORE_CONCEPT_QUERY%20Test/result/TEMPORAL_BEFORE_CONCEPT_QUERY$20Test.5b51e263-b0a9-49b0-a7b1-c03d8d7f8b82.csv?pretty=false HTTP/1.1" 200 23 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:09,082] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest TEMPORAL_BEFORE_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:09,082] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:09,082] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:09,082] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TEMPORAL_BEFORE_CONCEPT_QUERY Test, name=TEMPORAL_BEFORE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:09,082] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_1b21e960-81ce-4b06-aed9-4a8faaa6ff5a
INFO  [2023-01-17 00:52:09,082] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_5231d8b8-a9e7-4158-a172-234714acba63
INFO  [2023-01-17 00:52:09,083] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:09,084] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_1b21e960-81ce-4b06-aed9-4a8faaa6ff5a
INFO  [2023-01-17 00:52:09,084] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TEMPORAL_BEFORE_CONCEPT_QUERY Test_5231d8b8-a9e7-4158-a172-234714acba63
INFO  [2023-01-17 00:52:09,180] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of TEMPORAL_BEFORE_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:09,180] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,217] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test TEMPORAL_BEFORE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:09,218] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-17 00:52:09,218] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:09,218] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:09,220] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-17 00:52:09,220] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-17 00:52:09,220] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:09,220] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_16adecd1-6af1-4d2d-b9e6-37f5db3c2291 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_16adecd1-6af1-4d2d-b9e6-37f5db3c2291 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_ba7c4355-04ca-4ca7-ba5f-5db6a37367b9 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_ba7c4355-04ca-4ca7-ba5f-5db6a37367b9 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:09,222] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:09,326] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,332] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,333] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITHOUT$20DATES$20Test.table
INFO  [2023-01-17 00:52:09,333] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITHOUT$20DATES$20Test.table
INFO  [2023-01-17 00:52:09,448] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,566] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:09,566] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:09,566] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 178 B in total
INFO  [2023-01-17 00:52:09,566] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL_EXPORT WITHOUT DATES Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.0003057sINFO  [2023-01-17 00:52:09,597] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=9, min=1, average=4.500000, max=8}
INFO  [2023-01-17 00:52:09,597] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:09,597] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=11109, maxValue=12935), dateReader=com.bakdata.conquery.util.DateReader@581a59e3)
INFO  [2023-01-17 00:52:09,600] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL_EXPORT WITHOUT DATES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:09,600] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:09,600] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL_EXPORT WITHOUT DATES Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:09,618] com.bakdata.conquery.models.jobs.ImportJob: Importing table into REL_EXPORT$20WITHOUT$20DATES$20Test.table
INFO  [2023-01-17 00:52:09,619] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "POST /admin/datasets/REL_EXPORT%20WITHOUT%20DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL_EXPORT+WITHOUT+DATES+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:09,620] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:09,621] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:09,621] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:09,625] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:09,625] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITHOUT$20DATES$20Test.table.table], containing 9 entries.
INFO  [2023-01-17 00:52:09,625] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITHOUT$20DATES$20Test.table.table], containing 9 entries.
WARN  [2023-01-17 00:52:09,627] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:09,627] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL_EXPORT$20WITHOUT$20DATES$20Test.table.table.0
INFO  [2023-01-17 00:52:09,732] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,737] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,754] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:09,755] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 2 Concepts
INFO  [2023-01-17 00:52:09,861] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: REL_EXPORT WITHOUT DATES Test QUERY INIT
INFO  [2023-01-17 00:52:09,878] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[REL_EXPORT$20WITHOUT$20DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:09,879] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[63bcc15b-77de-4cfe-9c09-0b0abbdd649d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITHOUT DATES Test))]]
INFO  [2023-01-17 00:52:09,883] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d
INFO  [2023-01-17 00:52:09,884] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d
WARN  [2023-01-17 00:52:09,884] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:09,884] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d] with 0 results within PT0.000248S
WARN  [2023-01-17 00:52:09,884] com.bakdata.conquery.models.forms.managed.RelativeFormQueryPlan: Sampled empty result for Entity[0]: `EARLIEST({-∞/+∞})`
INFO  [2023-01-17 00:52:09,885] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d] with 1 results within PT0.001392S
INFO  [2023-01-17 00:52:09,885] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d, workerId=REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_ba7c4355-04ca-4ca7-ba5f-5db6a37367b9, startTime=2023-01-17T00:52:09.884576, finishTime=2023-01-17T00:52:09.884824) of size 0
INFO  [2023-01-17 00:52:09,886] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d, workerId=REL_EXPORT$20WITHOUT$20DATES$20Test.worker_REL_EXPORT$20WITHOUT$20DATES$20Test_16adecd1-6af1-4d2d-b9e6-37f5db3c2291, startTime=2023-01-17T00:52:09.883916, finishTime=2023-01-17T00:52:09.885308) of size 1
INFO  [2023-01-17 00:52:09,886] com.bakdata.conquery.models.execution.ManagedExecution: DONE 63bcc15b-77de-4cfe-9c09-0b0abbdd649d ManagedQuery within PT0.006891S
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "POST /api/datasets/REL_EXPORT$20WITHOUT$20DATES$20Test/queries HTTP/1.1" 201 3327 "-" "Conquery (test client)" 11
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "GET /api/datasets/REL_EXPORT$20WITHOUT$20DATES$20Test/queries/REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d HTTP/1.1" 200 3658 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:09,913] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test], queryId=63bcc15b-77de-4cfe-9c09-0b0abbdd649d, label=concept_dateless-child1 concept-child1	@§$, creationTime=2023-01-17T00:52:09.879001, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3a5c7086[Count = 0], startTime=2023-01-17T00:52:09.879248, finishTime=2023-01-17T00:52:09.886139, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@185c8ea6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITHOUT DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@e2e7a1b, com.bakdata.conquery.models.query.ColumnDescriptor@48914f37, com.bakdata.conquery.models.query.ColumnDescriptor@1110dee3, com.bakdata.conquery.models.query.ColumnDescriptor@6dc54bd9, com.bakdata.conquery.models.query.ColumnDescriptor@10460ad8, com.bakdata.conquery.models.query.ColumnDescriptor@526c0710, com.bakdata.conquery.models.query.ColumnDescriptor@451c2c78, com.bakdata.conquery.models.query.ColumnDescriptor@2402c81]) download on dataset Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:09,913] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test], queryId=63bcc15b-77de-4cfe-9c09-0b0abbdd649d, label=concept_dateless-child1 concept-child1	@§$, creationTime=2023-01-17T00:52:09.879001, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3a5c7086[Count = 0], startTime=2023-01-17T00:52:09.879248, finishTime=2023-01-17T00:52:09.886139, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@185c8ea6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITHOUT DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@e2e7a1b, com.bakdata.conquery.models.query.ColumnDescriptor@48914f37, com.bakdata.conquery.models.query.ColumnDescriptor@1110dee3, com.bakdata.conquery.models.query.ColumnDescriptor@6dc54bd9, com.bakdata.conquery.models.query.ColumnDescriptor@10460ad8, com.bakdata.conquery.models.query.ColumnDescriptor@526c0710, com.bakdata.conquery.models.query.ColumnDescriptor@451c2c78, com.bakdata.conquery.models.query.ColumnDescriptor@2402c81]) on dataset Dataset[label=null, name=REL_EXPORT WITHOUT DATES Test]
127.0.0.1 - - [17/Jan/2023:00:52:09 +0000] "GET /api/datasets/REL_EXPORT%20WITHOUT%20DATES%20Test/result/REL_EXPORT$20WITHOUT$20DATES$20Test.63bcc15b-77de-4cfe-9c09-0b0abbdd649d.csv?pretty=false HTTP/1.1" 200 130 "-" "Conquery (test client)" 28
INFO  [2023-01-17 00:52:09,939] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest REL_EXPORT WITHOUT DATES Test on 2 rows
INFO  [2023-01-17 00:52:09,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-17 00:52:09,940] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-17 00:52:09,940] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITHOUT DATES Test, name=REL_EXPORT WITHOUT DATES Test]
INFO  [2023-01-17 00:52:09,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITHOUT DATES Test_ba7c4355-04ca-4ca7-ba5f-5db6a37367b9
INFO  [2023-01-17 00:52:09,940] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITHOUT DATES Test_16adecd1-6af1-4d2d-b9e6-37f5db3c2291
INFO  [2023-01-17 00:52:10,037] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-17 00:52:10,037] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITHOUT DATES Test_ba7c4355-04ca-4ca7-ba5f-5db6a37367b9
INFO  [2023-01-17 00:52:10,037] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITHOUT DATES Test_16adecd1-6af1-4d2d-b9e6-37f5db3c2291
INFO  [2023-01-17 00:52:10,137] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL_EXPORT$20WITHOUT$20DATES$20Test
INFO  [2023-01-17 00:52:10,137] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,160] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL_EXPORT WITHOUT DATES Test
INFO  [2023-01-17 00:52:10,160] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REL_EXPORT WITH DATES Test
INFO  [2023-01-17 00:52:10,160] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:10,161] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:10,162] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-17 00:52:10,162] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-17 00:52:10,162] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:10,162] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_3ce1afbd-23b0-46c3-9d8c-82a71985f8ee are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_3ce1afbd-23b0-46c3-9d8c-82a71985f8ee are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_5bc75d8e-9fbb-4b62-8108-e4617bcbf257 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_5bc75d8e-9fbb-4b62-8108-e4617bcbf257 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:10,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:10,168] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,267] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,274] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,274] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITH$20DATES$20Test.test_table
INFO  [2023-01-17 00:52:10,275] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REL_EXPORT$20WITH$20DATES$20Test.test_table
INFO  [2023-01-17 00:52:10,387] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,494] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:10,494] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:10,494] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 183 B in total
INFO  [2023-01-17 00:52:10,494] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL_EXPORT WITH DATES Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000392644sINFO  [2023-01-17 00:52:10,534] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=9, min=1, average=4.500000, max=8}
INFO  [2023-01-17 00:52:10,534] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:10,534] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=11109, maxValue=12935), dateReader=com.bakdata.conquery.util.DateReader@167d5cc1)
INFO  [2023-01-17 00:52:10,537] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL_EXPORT WITH DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:10,537] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:10,537] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REL_EXPORT WITH DATES Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:10,557] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into REL_EXPORT$20WITH$20DATES$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:52:10 +0000] "POST /admin/datasets/REL_EXPORT%20WITH%20DATES%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REL_EXPORT+WITH+DATES+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:10,557] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,558] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:10,559] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:10,559] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:10,561] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:10,561] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITH$20DATES$20Test.test_table.test_table], containing 9 entries.
INFO  [2023-01-17 00:52:10,561] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REL_EXPORT$20WITH$20DATES$20Test.test_table.test_table], containing 9 entries.
WARN  [2023-01-17 00:52:10,562] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:10,563] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REL_EXPORT$20WITH$20DATES$20Test.test_table.test_table.0
INFO  [2023-01-17 00:52:10,668] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,673] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,688] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:10,688] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:10,794] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: REL_EXPORT WITH DATES Test QUERY INIT
INFO  [2023-01-17 00:52:10,810] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[REL_EXPORT$20WITH$20DATES$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:10,811] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c7994661-4e50-4f10-884e-14980d908268] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITH DATES Test))]]
INFO  [2023-01-17 00:52:10,815] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268
INFO  [2023-01-17 00:52:10,815] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started RelativeFormQuery REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268
WARN  [2023-01-17 00:52:10,816] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:10,816] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268] with 0 results within PT0.000229S
INFO  [2023-01-17 00:52:10,816] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268, workerId=REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_5bc75d8e-9fbb-4b62-8108-e4617bcbf257, startTime=2023-01-17T00:52:10.815844, finishTime=2023-01-17T00:52:10.816073) of size 0
INFO  [2023-01-17 00:52:10,817] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268] with 1 results within PT0.001561S
INFO  [2023-01-17 00:52:10,818] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268, workerId=REL_EXPORT$20WITH$20DATES$20Test.worker_REL_EXPORT$20WITH$20DATES$20Test_3ce1afbd-23b0-46c3-9d8c-82a71985f8ee, startTime=2023-01-17T00:52:10.815780, finishTime=2023-01-17T00:52:10.817341) of size 1
127.0.0.1 - - [17/Jan/2023:00:52:10 +0000] "POST /api/datasets/REL_EXPORT$20WITH$20DATES$20Test/queries HTTP/1.1" 201 3331 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:10,818] com.bakdata.conquery.models.execution.ManagedExecution: DONE c7994661-4e50-4f10-884e-14980d908268 ManagedQuery within PT0.006919S
127.0.0.1 - - [17/Jan/2023:00:52:10 +0000] "GET /api/datasets/REL_EXPORT$20WITH$20DATES$20Test/queries/REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268 HTTP/1.1" 200 3650 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:10,846] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITH DATES Test], queryId=c7994661-4e50-4f10-884e-14980d908268, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:52:10.810921, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@40cf5d1d[Count = 0], startTime=2023-01-17T00:52:10.811204, finishTime=2023-01-17T00:52:10.818123, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1ef4bf8f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITH DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@43da7347, com.bakdata.conquery.models.query.ColumnDescriptor@6191be82, com.bakdata.conquery.models.query.ColumnDescriptor@5b0c3018, com.bakdata.conquery.models.query.ColumnDescriptor@60bff431, com.bakdata.conquery.models.query.ColumnDescriptor@70efe32a, com.bakdata.conquery.models.query.ColumnDescriptor@1ce6361b, com.bakdata.conquery.models.query.ColumnDescriptor@2650596f, com.bakdata.conquery.models.query.ColumnDescriptor@39512fe8]) download on dataset Dataset[label=null, name=REL_EXPORT WITH DATES Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:10,846] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REL_EXPORT WITH DATES Test], queryId=c7994661-4e50-4f10-884e-14980d908268, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:52:10.810921, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@40cf5d1d[Count = 0], startTime=2023-01-17T00:52:10.811204, finishTime=2023-01-17T00:52:10.818123, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1ef4bf8f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REL_EXPORT WITH DATES Test)), query=com.bakdata.conquery.models.forms.managed.RelativeFormQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@43da7347, com.bakdata.conquery.models.query.ColumnDescriptor@6191be82, com.bakdata.conquery.models.query.ColumnDescriptor@5b0c3018, com.bakdata.conquery.models.query.ColumnDescriptor@60bff431, com.bakdata.conquery.models.query.ColumnDescriptor@70efe32a, com.bakdata.conquery.models.query.ColumnDescriptor@1ce6361b, com.bakdata.conquery.models.query.ColumnDescriptor@2650596f, com.bakdata.conquery.models.query.ColumnDescriptor@39512fe8]) on dataset Dataset[label=null, name=REL_EXPORT WITH DATES Test]
127.0.0.1 - - [17/Jan/2023:00:52:10 +0000] "GET /api/datasets/REL_EXPORT%20WITH%20DATES%20Test/result/REL_EXPORT$20WITH$20DATES$20Test.c7994661-4e50-4f10-884e-14980d908268.csv?pretty=false HTTP/1.1" 200 599 "-" "Conquery (test client)" 25
INFO  [2023-01-17 00:52:10,870] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest REL_EXPORT WITH DATES Test on 10 rows
INFO  [2023-01-17 00:52:10,870] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REL_EXPORT WITH DATES Test
INFO  [2023-01-17 00:52:10,870] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-17 00:52:10,870] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REL_EXPORT WITH DATES Test, name=REL_EXPORT WITH DATES Test]
INFO  [2023-01-17 00:52:10,870] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITH DATES Test_5bc75d8e-9fbb-4b62-8108-e4617bcbf257
INFO  [2023-01-17 00:52:10,871] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REL_EXPORT WITH DATES Test_3ce1afbd-23b0-46c3-9d8c-82a71985f8ee
INFO  [2023-01-17 00:52:10,967] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REL_EXPORT WITH DATES Test
INFO  [2023-01-17 00:52:10,967] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITH DATES Test_3ce1afbd-23b0-46c3-9d8c-82a71985f8ee
INFO  [2023-01-17 00:52:10,967] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REL_EXPORT WITH DATES Test_5bc75d8e-9fbb-4b62-8108-e4617bcbf257
INFO  [2023-01-17 00:52:11,067] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REL_EXPORT$20WITH$20DATES$20Test
INFO  [2023-01-17 00:52:11,067] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,094] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REL_EXPORT WITH DATES Test
INFO  [2023-01-17 00:52:11,095] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test REUSED_QUERY Test
INFO  [2023-01-17 00:52:11,095] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:11,095] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:11,096] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-17 00:52:11,096] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-17 00:52:11,096] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:11,096] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:11,098] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_db17f510-3fee-48fc-bc32-17abc9768ec2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:11,098] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_db17f510-3fee-48fc-bc32-17abc9768ec2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:11,098] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:11,098] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_39399b37-99dc-4def-ba2a-b75d74898fa0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:11,098] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_39399b37-99dc-4def-ba2a-b75d74898fa0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:11,098] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:11,102] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,202] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,209] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,209] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REUSED_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:11,209] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table REUSED_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:11,326] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,437] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:11,437] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:11,437] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 374 B in total
INFO  [2023-01-17 00:52:11,437] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00038775sINFO  [2023-01-17 00:52:11,477] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=21, sum=21, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:11,477] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=21, nullLines=0), encoding=null, prefix=a, suffix=a)
INFO  [2023-01-17 00:52:11,477] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=21, nullLines=0), subType=IntegerParser(super=Parser(lines=21, nullLines=0), minValue=11323, maxValue=11323), dateReader=com.bakdata.conquery.util.DateReader@6c8c2e5d)
INFO  [2023-01-17 00:52:11,479] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:11,479] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:11,479] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:11,491] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into REUSED_QUERY$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:52:11 +0000] "POST /admin/datasets/REUSED_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_REUSED_QUERY+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:11,492] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,493] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:11,493] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:11,493] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:11,496] com.bakdata.conquery.models.jobs.ImportJob: Start sending 7 Buckets
INFO  [2023-01-17 00:52:11,496] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REUSED_QUERY$20Test.test_table.test_table], containing 21 entries.
INFO  [2023-01-17 00:52:11,496] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[REUSED_QUERY$20Test.test_table.test_table], containing 21 entries.
INFO  [2023-01-17 00:52:11,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.0
INFO  [2023-01-17 00:52:11,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.1
INFO  [2023-01-17 00:52:11,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.2
INFO  [2023-01-17 00:52:11,498] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.3
WARN  [2023-01-17 00:52:11,498] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:11,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.5
INFO  [2023-01-17 00:52:11,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.4
INFO  [2023-01-17 00:52:11,540] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received REUSED_QUERY$20Test.test_table.test_table.6
INFO  [2023-01-17 00:52:11,644] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,713] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,718] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,726] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:11,726] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:11,726] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:11,832] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: REUSED_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:11,849] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[REUSED_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:11,850] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[00000000-0000-0000-0000-000000000001] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_REUSED_QUERY Test))]]
INFO  [2023-01-17 00:52:11,853] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001
INFO  [2023-01-17 00:52:11,853] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001
127.0.0.1 - - [17/Jan/2023:00:52:11 +0000] "POST /api/datasets/REUSED_QUERY$20Test/queries HTTP/1.1" 201 1315 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:11,856] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001] with 5 results within PT0.00311S
INFO  [2023-01-17 00:52:11,857] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001, workerId=REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_db17f510-3fee-48fc-bc32-17abc9768ec2, startTime=2023-01-17T00:52:11.853681, finishTime=2023-01-17T00:52:11.856791) of size 5
INFO  [2023-01-17 00:52:11,857] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001] with 6 results within PT0.003884S
INFO  [2023-01-17 00:52:11,858] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001, workerId=REUSED_QUERY$20Test.worker_REUSED_QUERY$20Test_39399b37-99dc-4def-ba2a-b75d74898fa0, startTime=2023-01-17T00:52:11.853557, finishTime=2023-01-17T00:52:11.857441) of size 6
INFO  [2023-01-17 00:52:11,858] com.bakdata.conquery.models.execution.ManagedExecution: DONE 00000000-0000-0000-0000-000000000001 ManagedQuery within PT0.007712S
127.0.0.1 - - [17/Jan/2023:00:52:11 +0000] "GET /api/datasets/REUSED_QUERY$20Test/queries/REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001 HTTP/1.1" 200 1583 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:11,894] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REUSED_QUERY Test], queryId=00000000-0000-0000-0000-000000000001, label=Uploaded-List	@§$, creationTime=2023-01-17T00:52:11.694308, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@48d47093[Count = 0], startTime=2023-01-17T00:52:11.850591, finishTime=2023-01-17T00:52:11.858303, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7cec65ee), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=11, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@708aafa3, com.bakdata.conquery.models.query.ColumnDescriptor@68e366b4]) download on dataset Dataset[label=null, name=REUSED_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:11,895] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=REUSED_QUERY Test], queryId=00000000-0000-0000-0000-000000000001, label=Uploaded-List	@§$, creationTime=2023-01-17T00:52:11.694308, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@48d47093[Count = 0], startTime=2023-01-17T00:52:11.850591, finishTime=2023-01-17T00:52:11.858303, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7cec65ee), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=11, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@708aafa3, com.bakdata.conquery.models.query.ColumnDescriptor@68e366b4]) on dataset Dataset[label=null, name=REUSED_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:11 +0000] "GET /api/datasets/REUSED_QUERY%20Test/result/REUSED_QUERY$20Test.00000000-0000-0000-0000-000000000001.csv?pretty=false HTTP/1.1" 200 331 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:52:11,915] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest REUSED_QUERY Test on 12 rows
INFO  [2023-01-17 00:52:11,916] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast REUSED_QUERY Test
INFO  [2023-01-17 00:52:11,916] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-17 00:52:11,916] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=REUSED_QUERY Test, name=REUSED_QUERY Test]
INFO  [2023-01-17 00:52:11,916] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REUSED_QUERY Test_db17f510-3fee-48fc-bc32-17abc9768ec2
INFO  [2023-01-17 00:52:11,916] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_REUSED_QUERY Test_39399b37-99dc-4def-ba2a-b75d74898fa0
INFO  [2023-01-17 00:52:12,014] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REUSED_QUERY Test_db17f510-3fee-48fc-bc32-17abc9768ec2
INFO  [2023-01-17 00:52:12,014] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_REUSED_QUERY Test_39399b37-99dc-4def-ba2a-b75d74898fa0
INFO  [2023-01-17 00:52:12,014] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow REUSED_QUERY Test
INFO  [2023-01-17 00:52:12,114] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of REUSED_QUERY$20Test
INFO  [2023-01-17 00:52:12,114] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,131] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test REUSED_QUERY Test
INFO  [2023-01-17 00:52:12,132] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID Test
INFO  [2023-01-17 00:52:12,132] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:12,132] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:12,133] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-17 00:52:12,133] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-17 00:52:12,133] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:12,133] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:12,134] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,135] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_2a3720ca-9faa-4dc9-9cec-4dd0db591fe4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:12,135] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_2a3720ca-9faa-4dc9-9cec-4dd0db591fe4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:12,135] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:12,135] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_180d328d-a9b1-422e-96a4-836e808f235c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:12,135] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_180d328d-a9b1-422e-96a4-836e808f235c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:12,135] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:12,239] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test.secondary]
INFO  [2023-01-17 00:52:12,240] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test.ignored]
INFO  [2023-01-17 00:52:12,240] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,240] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.secondary
INFO  [2023-01-17 00:52:12,240] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.secondary
INFO  [2023-01-17 00:52:12,284] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.ignored
INFO  [2023-01-17 00:52:12,284] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test.ignored
INFO  [2023-01-17 00:52:12,390] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,390] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table1
INFO  [2023-01-17 00:52:12,390] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table1
INFO  [2023-01-17 00:52:12,390] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table12
INFO  [2023-01-17 00:52:12,390] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test.table12
INFO  [2023-01-17 00:52:12,505] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,617] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:12,617] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:12,617] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:12,618] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 433 B in total
INFO  [2023-01-17 00:52:12,618] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test/table1.import.json, tag=Optional.empty)
████████████████████████▌                         ▌  49%	est. time remaining: 0.044936988sINFO  [2023-01-17 00:52:12,661] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-17 00:52:12,661] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:12,661] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:12,661] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@3b17025f), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@3dd69c4e), dateReader=com.bakdata.conquery.util.DateReader@7644abc3, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-17 00:52:12,665] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:12,665] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test/table12.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000658007sINFO  [2023-01-17 00:52:12,684] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-17 00:52:12,684] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-17 00:52:12,684] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:12,684] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@3549dbde), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@3f1649d5), dateReader=com.bakdata.conquery.util.DateReader@43f191a2, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-17 00:52:12,687] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test/table12.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:12,687] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:52:12,687] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:12,687] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test/table12.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:12,711] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:12 +0000] "POST /admin/datasets/SECONDARY_ID%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:52:12,721] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:12,722] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:12,722] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:12,725] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:12,725] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-17 00:52:12,725] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table1.table1], containing 6 entries.
WARN  [2023-01-17 00:52:12,726] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:12,727] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test.table1.table1.0
INFO  [2023-01-17 00:52:12,741] com.bakdata.conquery.models.jobs.ImportJob: Importing table12 into SECONDARY_ID$20Test.table12
INFO  [2023-01-17 00:52:12,741] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:52:12 +0000] "POST /admin/datasets/SECONDARY_ID%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID+Test%2Ftable12.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:12,741] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:12,741] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:12,742] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,742] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:12,742] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table12.table12], containing 6 entries.
INFO  [2023-01-17 00:52:12,742] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test.table12.table12], containing 6 entries.
WARN  [2023-01-17 00:52:12,742] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:12,742] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test.table12.table12.0
INFO  [2023-01-17 00:52:12,847] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,852] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,869] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:12,870] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:12,976] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID Test QUERY INIT
INFO  [2023-01-17 00:52:12,992] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:12,993] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test))]]
INFO  [2023-01-17 00:52:12,999] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1
INFO  [2023-01-17 00:52:12,999] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1
WARN  [2023-01-17 00:52:12,999] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:12,999] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1] with 0 results within PT0.000594S
127.0.0.1 - - [17/Jan/2023:00:52:12 +0000] "POST /api/datasets/SECONDARY_ID$20Test/queries HTTP/1.1" 201 1683 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:13,001] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1, workerId=SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_2a3720ca-9faa-4dc9-9cec-4dd0db591fe4, startTime=2023-01-17T00:52:12.999164, finishTime=2023-01-17T00:52:12.999758) of size 0
INFO  [2023-01-17 00:52:13,002] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1] with 2 results within PT0.003547S
INFO  [2023-01-17 00:52:13,003] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1, workerId=SECONDARY_ID$20Test.worker_SECONDARY_ID$20Test_180d328d-a9b1-422e-96a4-836e808f235c, startTime=2023-01-17T00:52:12.999179, finishTime=2023-01-17T00:52:13.002726) of size 2
INFO  [2023-01-17 00:52:13,003] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1 ManagedQuery within PT0.010384S
127.0.0.1 - - [17/Jan/2023:00:52:13 +0000] "GET /api/datasets/SECONDARY_ID$20Test/queries/SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1 HTTP/1.1" 200 1951 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:13,047] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test], queryId=5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1, label=number	@§$, creationTime=2023-01-17T00:52:12.992746, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2ff8de89[Count = 0], startTime=2023-01-17T00:52:12.993092, finishTime=2023-01-17T00:52:13.003476, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@67c78229), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@508a3c48, com.bakdata.conquery.models.query.ColumnDescriptor@70bdaa31, com.bakdata.conquery.models.query.ColumnDescriptor@19583e39]) download on dataset Dataset[label=null, name=SECONDARY_ID Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:13,047] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test], queryId=5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1, label=number	@§$, creationTime=2023-01-17T00:52:12.992746, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2ff8de89[Count = 0], startTime=2023-01-17T00:52:12.993092, finishTime=2023-01-17T00:52:13.003476, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@67c78229), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@508a3c48, com.bakdata.conquery.models.query.ColumnDescriptor@70bdaa31, com.bakdata.conquery.models.query.ColumnDescriptor@19583e39]) on dataset Dataset[label=null, name=SECONDARY_ID Test]
127.0.0.1 - - [17/Jan/2023:00:52:13 +0000] "GET /api/datasets/SECONDARY_ID%20Test/result/SECONDARY_ID$20Test.5779dd5d-bfe9-4eb6-a1c8-8cd543a446f1.csv?pretty=false HTTP/1.1" 200 226 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:13,049] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID Test on 6 rows
INFO  [2023-01-17 00:52:13,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID Test
INFO  [2023-01-17 00:52:13,049] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-17 00:52:13,049] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test, name=SECONDARY_ID Test]
INFO  [2023-01-17 00:52:13,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test_180d328d-a9b1-422e-96a4-836e808f235c
INFO  [2023-01-17 00:52:13,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test_2a3720ca-9faa-4dc9-9cec-4dd0db591fe4
INFO  [2023-01-17 00:52:13,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test_2a3720ca-9faa-4dc9-9cec-4dd0db591fe4
INFO  [2023-01-17 00:52:13,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID Test
INFO  [2023-01-17 00:52:13,142] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test_180d328d-a9b1-422e-96a4-836e808f235c
INFO  [2023-01-17 00:52:13,142] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID$20Test
INFO  [2023-01-17 00:52:13,143] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:13,275] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID Test
INFO  [2023-01-17 00:52:13,275] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID Test
INFO  [2023-01-17 00:52:13,276] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:13,276] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:13,308] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-17 00:52:13,308] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-17 00:52:13,308] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:13,308] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_356410ec-79da-4bd8-a8da-0b3f73871e69 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_356410ec-79da-4bd8-a8da-0b3f73871e69 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_95868285-9259-4360-a048-01154f61df0c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_95868285-9259-4360-a048-01154f61df0c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:13,310] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:13,414] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[1].secondary]
INFO  [2023-01-17 00:52:13,414] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[1].ignored]
INFO  [2023-01-17 00:52:13,415] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:13,415] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].secondary
INFO  [2023-01-17 00:52:13,415] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].secondary
INFO  [2023-01-17 00:52:13,415] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].ignored
INFO  [2023-01-17 00:52:13,456] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[1].ignored
INFO  [2023-01-17 00:52:13,562] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:13,562] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table1
INFO  [2023-01-17 00:52:13,562] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table12
INFO  [2023-01-17 00:52:13,562] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table1
INFO  [2023-01-17 00:52:13,562] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[1].table12
INFO  [2023-01-17 00:52:13,675] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:13,783] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:13,783] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:13,783] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:13,783] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 295 B in total
INFO  [2023-01-17 00:52:13,783] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[1]/table1.import.json, tag=Optional.empty)
████████████████████████▌                         ▌  49%	est. time remaining: 0.029043518sINFO  [2023-01-17 00:52:13,811] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=4, min=4, average=4.000000, max=4}
INFO  [2023-01-17 00:52:13,811] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:13,811] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=4, nullLines=1), encoding=null, prefix=f_a, suffix=)
INFO  [2023-01-17 00:52:13,811] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@4e02b147), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14790, maxValue=20999), dateReader=com.bakdata.conquery.util.DateReader@35ee1472), dateReader=com.bakdata.conquery.util.DateReader@38bbbbfb, onlyQuarters=false, maxValue=20999, minValue=14790, anyOpen=false)
INFO  [2023-01-17 00:52:13,814] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:13,814] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[1]/table12.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000506696sINFO  [2023-01-17 00:52:13,834] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=1, sum=4, min=4, average=4.000000, max=4}
INFO  [2023-01-17 00:52:13,834] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=4, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-17 00:52:13,834] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=4, nullLines=1), encoding=null, prefix=f_a, suffix=)
INFO  [2023-01-17 00:52:13,834] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=4, nullLines=0), minParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@778851cf), maxParser=DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@2cdb4ef0), dateReader=com.bakdata.conquery.util.DateReader@6953829d, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-17 00:52:13,837] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[1]/table12.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:13,838] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:52:13,838] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[1]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:13,838] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[1]/table12.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:13,854] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID$20Test[1].table1
127.0.0.1 - - [17/Jan/2023:00:52:13 +0000] "POST /admin/datasets/SECONDARY_ID%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID+Test%5B1%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:13,856] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:13,857] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:13,857] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:13,861] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:13,861] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table1.table1], containing 4 entries.
INFO  [2023-01-17 00:52:13,861] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table1.table1], containing 4 entries.
WARN  [2023-01-17 00:52:13,863] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:13,863] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test[1].table1.table1.0
INFO  [2023-01-17 00:52:13,871] com.bakdata.conquery.models.jobs.ImportJob: Importing table12 into SECONDARY_ID$20Test[1].table12
INFO  [2023-01-17 00:52:13,872] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:13,872] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:13 +0000] "POST /admin/datasets/SECONDARY_ID%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID+Test%5B1%5D%2Ftable12.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:13,872] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:13,872] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:13,872] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:52:13,872] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:13,872] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table12.table12], containing 4 entries.
INFO  [2023-01-17 00:52:13,872] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[1].table12.table12], containing 4 entries.
INFO  [2023-01-17 00:52:13,873] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test[1].table12.table12.0
INFO  [2023-01-17 00:52:13,978] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:13,983] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,015] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,016] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:14,121] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID Test QUERY INIT
INFO  [2023-01-17 00:52:14,137] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:14,138] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[da1a775c-cec3-49fd-8d95-88093d4050be] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[1]))]]
INFO  [2023-01-17 00:52:14,144] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[1].da1a775c-cec3-49fd-8d95-88093d4050be
INFO  [2023-01-17 00:52:14,144] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[1].da1a775c-cec3-49fd-8d95-88093d4050be
WARN  [2023-01-17 00:52:14,144] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:14,145] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[1].da1a775c-cec3-49fd-8d95-88093d4050be] with 0 results within PT0.000431S
127.0.0.1 - - [17/Jan/2023:00:52:14 +0000] "POST /api/datasets/SECONDARY_ID$20Test%5B1%5D/queries HTTP/1.1" 201 1870 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:14,145] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[1].da1a775c-cec3-49fd-8d95-88093d4050be, workerId=SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_356410ec-79da-4bd8-a8da-0b3f73871e69, startTime=2023-01-17T00:52:14.144577, finishTime=2023-01-17T00:52:14.145008) of size 0
INFO  [2023-01-17 00:52:14,146] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[1].da1a775c-cec3-49fd-8d95-88093d4050be] with 1 results within PT0.001655S
INFO  [2023-01-17 00:52:14,146] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[1].da1a775c-cec3-49fd-8d95-88093d4050be, workerId=SECONDARY_ID$20Test[1].worker_SECONDARY_ID$20Test[1]_95868285-9259-4360-a048-01154f61df0c, startTime=2023-01-17T00:52:14.144566, finishTime=2023-01-17T00:52:14.146221) of size 1
INFO  [2023-01-17 00:52:14,147] com.bakdata.conquery.models.execution.ManagedExecution: DONE da1a775c-cec3-49fd-8d95-88093d4050be ManagedQuery within PT0.00844S
127.0.0.1 - - [17/Jan/2023:00:52:14 +0000] "GET /api/datasets/SECONDARY_ID$20Test%5B1%5D/queries/SECONDARY_ID$20Test%5B1%5D.da1a775c-cec3-49fd-8d95-88093d4050be HTTP/1.1" 200 2389 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:14,172] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[1]], queryId=da1a775c-cec3-49fd-8d95-88093d4050be, label=vs	@§$, creationTime=2023-01-17T00:52:14.138325, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@12ddd80e[Count = 0], startTime=2023-01-17T00:52:14.138565, finishTime=2023-01-17T00:52:14.147005, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4e0005fb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5685ea21, com.bakdata.conquery.models.query.ColumnDescriptor@20df6fc0, com.bakdata.conquery.models.query.ColumnDescriptor@5c3ede7d]) download on dataset Dataset[label=null, name=SECONDARY_ID Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:14,172] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[1]], queryId=da1a775c-cec3-49fd-8d95-88093d4050be, label=vs	@§$, creationTime=2023-01-17T00:52:14.138325, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@12ddd80e[Count = 0], startTime=2023-01-17T00:52:14.138565, finishTime=2023-01-17T00:52:14.147005, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4e0005fb), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[1])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5685ea21, com.bakdata.conquery.models.query.ColumnDescriptor@20df6fc0, com.bakdata.conquery.models.query.ColumnDescriptor@5c3ede7d]) on dataset Dataset[label=null, name=SECONDARY_ID Test[1]]
127.0.0.1 - - [17/Jan/2023:00:52:14 +0000] "GET /api/datasets/SECONDARY_ID%20Test%5B1%5D/result/SECONDARY_ID$20Test%5B1%5D.da1a775c-cec3-49fd-8d95-88093d4050be.csv?pretty=false HTTP/1.1" 200 88 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:52:14,193] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID Test on 3 rows
INFO  [2023-01-17 00:52:14,193] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID Test[1]
INFO  [2023-01-17 00:52:14,194] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-17 00:52:14,194] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[1], name=SECONDARY_ID Test[1]]
INFO  [2023-01-17 00:52:14,194] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[1]_356410ec-79da-4bd8-a8da-0b3f73871e69
INFO  [2023-01-17 00:52:14,194] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[1]_95868285-9259-4360-a048-01154f61df0c
INFO  [2023-01-17 00:52:14,214] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID Test[1]
INFO  [2023-01-17 00:52:14,214] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[1]_95868285-9259-4360-a048-01154f61df0c
INFO  [2023-01-17 00:52:14,214] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[1]_356410ec-79da-4bd8-a8da-0b3f73871e69
INFO  [2023-01-17 00:52:14,273] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID$20Test[1]
INFO  [2023-01-17 00:52:14,273] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,321] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID Test
INFO  [2023-01-17 00:52:14,321] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-17 00:52:14,321] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:14,321] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:14,322] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-17 00:52:14,322] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-17 00:52:14,322] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:14,322] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:14,323] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_7632b0a2-d17e-4911-8539-2543b96cf608 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:14,323] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_7632b0a2-d17e-4911-8539-2543b96cf608 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:14,323] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:14,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_8a8d95a1-97db-4644-80d8-aa4791101e78 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:14,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_8a8d95a1-97db-4644-80d8-aa4791101e78 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:14,324] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:14,328] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,428] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID_EXCLUDED$20Test.secondary]
INFO  [2023-01-17 00:52:14,429] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,429] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_EXCLUDED$20Test.secondary
INFO  [2023-01-17 00:52:14,429] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_EXCLUDED$20Test.secondary
INFO  [2023-01-17 00:52:14,536] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,536] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_EXCLUDED$20Test.table1
INFO  [2023-01-17 00:52:14,536] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_EXCLUDED$20Test.table1
INFO  [2023-01-17 00:52:14,657] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,767] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:14,768] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:14,768] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 135 B in total
INFO  [2023-01-17 00:52:14,768] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_EXCLUDED Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000410194sINFO  [2023-01-17 00:52:14,809] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=6, min=2, average=2.000000, max=2}
INFO  [2023-01-17 00:52:14,809] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:14,809] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18262, maxValue=18262), dateReader=com.bakdata.conquery.util.DateReader@c52982f)
INFO  [2023-01-17 00:52:14,809] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=f, suffix=)
INFO  [2023-01-17 00:52:14,813] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_EXCLUDED Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:14,813] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:14,813] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_EXCLUDED Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:14,827] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID_EXCLUDED$20Test.table1
INFO  [2023-01-17 00:52:14,827] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:14 +0000] "POST /admin/datasets/SECONDARY_ID_EXCLUDED%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID_EXCLUDED+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:14,828] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:14,828] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:14,828] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:14,829] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:52:14,830] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:14,830] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_EXCLUDED$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-17 00:52:14,831] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_EXCLUDED$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-17 00:52:14,832] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID_EXCLUDED$20Test.table1.table1.0
INFO  [2023-01-17 00:52:14,937] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,942] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,958] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:14,959] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:15,065] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID_EXCLUDED Test QUERY INIT
INFO  [2023-01-17 00:52:15,082] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID_EXCLUDED$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:15,083] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[cdff88c9-84d2-4589-9184-25479aaa5a0a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_EXCLUDED Test))]]
INFO  [2023-01-17 00:52:15,088] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a
INFO  [2023-01-17 00:52:15,088] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a
WARN  [2023-01-17 00:52:15,088] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:15,088] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a] with 0 results within PT0.000156S
INFO  [2023-01-17 00:52:15,089] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a, workerId=SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_7632b0a2-d17e-4911-8539-2543b96cf608, startTime=2023-01-17T00:52:15.088425, finishTime=2023-01-17T00:52:15.088581) of size 0
127.0.0.1 - - [17/Jan/2023:00:52:15 +0000] "POST /api/datasets/SECONDARY_ID_EXCLUDED$20Test/queries HTTP/1.1" 201 1809 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:15,089] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a] with 3 results within PT0.001373S
INFO  [2023-01-17 00:52:15,090] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a, workerId=SECONDARY_ID_EXCLUDED$20Test.worker_SECONDARY_ID_EXCLUDED$20Test_8a8d95a1-97db-4644-80d8-aa4791101e78, startTime=2023-01-17T00:52:15.088482, finishTime=2023-01-17T00:52:15.089855) of size 3
INFO  [2023-01-17 00:52:15,090] com.bakdata.conquery.models.execution.ManagedExecution: DONE cdff88c9-84d2-4589-9184-25479aaa5a0a ManagedQuery within PT0.007148S
127.0.0.1 - - [17/Jan/2023:00:52:15 +0000] "GET /api/datasets/SECONDARY_ID_EXCLUDED$20Test/queries/SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a HTTP/1.1" 200 2112 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:15,115] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test], queryId=cdff88c9-84d2-4589-9184-25479aaa5a0a, label=vs concept	@§$, creationTime=2023-01-17T00:52:15.083198, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5d2ec780[Count = 0], startTime=2023-01-17T00:52:15.083426, finishTime=2023-01-17T00:52:15.090574, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5d4e96e1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_EXCLUDED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@604168c6, com.bakdata.conquery.models.query.ColumnDescriptor@3224e64, com.bakdata.conquery.models.query.ColumnDescriptor@281b8b47]) download on dataset Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:15,116] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test], queryId=cdff88c9-84d2-4589-9184-25479aaa5a0a, label=vs concept	@§$, creationTime=2023-01-17T00:52:15.083198, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5d2ec780[Count = 0], startTime=2023-01-17T00:52:15.083426, finishTime=2023-01-17T00:52:15.090574, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5d4e96e1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_EXCLUDED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@604168c6, com.bakdata.conquery.models.query.ColumnDescriptor@3224e64, com.bakdata.conquery.models.query.ColumnDescriptor@281b8b47]) on dataset Dataset[label=null, name=SECONDARY_ID_EXCLUDED Test]
127.0.0.1 - - [17/Jan/2023:00:52:15 +0000] "GET /api/datasets/SECONDARY_ID_EXCLUDED%20Test/result/SECONDARY_ID_EXCLUDED$20Test.cdff88c9-84d2-4589-9184-25479aaa5a0a.csv?pretty=false HTTP/1.1" 200 173 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:52:15,134] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID_EXCLUDED Test on 6 rows
INFO  [2023-01-17 00:52:15,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-17 00:52:15,134] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-17 00:52:15,134] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_EXCLUDED Test, name=SECONDARY_ID_EXCLUDED Test]
INFO  [2023-01-17 00:52:15,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_EXCLUDED Test_7632b0a2-d17e-4911-8539-2543b96cf608
INFO  [2023-01-17 00:52:15,134] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_EXCLUDED Test_8a8d95a1-97db-4644-80d8-aa4791101e78
INFO  [2023-01-17 00:52:15,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-17 00:52:15,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_EXCLUDED Test_7632b0a2-d17e-4911-8539-2543b96cf608
INFO  [2023-01-17 00:52:15,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_EXCLUDED Test_8a8d95a1-97db-4644-80d8-aa4791101e78
INFO  [2023-01-17 00:52:15,335] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID_EXCLUDED$20Test
INFO  [2023-01-17 00:52:15,335] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:15,365] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID_EXCLUDED Test
INFO  [2023-01-17 00:52:15,366] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID Test
INFO  [2023-01-17 00:52:15,366] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:15,366] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:15,367] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-17 00:52:15,367] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-17 00:52:15,367] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:15,367] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:15,368] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:15,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_75a6b641-e63b-4a76-bcc7-d9af0fcdcd98 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:15,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_75a6b641-e63b-4a76-bcc7-d9af0fcdcd98 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:15,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:15,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_a71b779e-56e8-4378-9f71-7f5534d33a9d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:15,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_a71b779e-56e8-4378-9f71-7f5534d33a9d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:15,369] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:15,473] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[2].secondary]
INFO  [2023-01-17 00:52:15,474] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID$20Test[2].ignored]
INFO  [2023-01-17 00:52:15,474] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:15,474] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].secondary
INFO  [2023-01-17 00:52:15,474] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].ignored
INFO  [2023-01-17 00:52:15,474] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].secondary
INFO  [2023-01-17 00:52:15,516] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID$20Test[2].ignored
INFO  [2023-01-17 00:52:15,622] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:15,622] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[2].table1
INFO  [2023-01-17 00:52:15,622] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID$20Test[2].table1
INFO  [2023-01-17 00:52:15,738] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:15,845] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:15,846] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:15,846] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 180 B in total
INFO  [2023-01-17 00:52:15,846] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[2]/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000806879sINFO  [2023-01-17 00:52:15,927] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=5, min=1, average=2.500000, max=4}
INFO  [2023-01-17 00:52:15,928] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:15,928] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=5, nullLines=0), minParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@7779ca08), maxParser=DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@3db951f3), dateReader=com.bakdata.conquery.util.DateReader@53aba070, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-17 00:52:15,928] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=5, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:15,939] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[2]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:15,939] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:15,939] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID Test[2]/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:15,961] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SECONDARY_ID$20Test[2].table1
INFO  [2023-01-17 00:52:15,962] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:15 +0000] "POST /admin/datasets/SECONDARY_ID%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID+Test%5B2%5D%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:15,962] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:15,963] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:15,963] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:15,964] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:15,965] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[2].table1.table1], containing 5 entries.
INFO  [2023-01-17 00:52:15,965] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID$20Test[2].table1.table1], containing 5 entries.
WARN  [2023-01-17 00:52:15,965] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:15,965] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID$20Test[2].table1.table1.0
INFO  [2023-01-17 00:52:16,071] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,076] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,094] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,094] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:16,199] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID Test QUERY INIT
INFO  [2023-01-17 00:52:16,215] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:16,216] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[04138548-cc32-4fd5-aa35-5f3549b19761] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[2]))]]
INFO  [2023-01-17 00:52:16,222] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[2].04138548-cc32-4fd5-aa35-5f3549b19761
INFO  [2023-01-17 00:52:16,222] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID$20Test[2].04138548-cc32-4fd5-aa35-5f3549b19761
WARN  [2023-01-17 00:52:16,222] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:16,222] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[2].04138548-cc32-4fd5-aa35-5f3549b19761] with 0 results within PT0.000183S
127.0.0.1 - - [17/Jan/2023:00:52:16 +0000] "POST /api/datasets/SECONDARY_ID$20Test%5B2%5D/queries HTTP/1.1" 201 1552 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:16,223] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[2].04138548-cc32-4fd5-aa35-5f3549b19761, workerId=SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_a71b779e-56e8-4378-9f71-7f5534d33a9d, startTime=2023-01-17T00:52:16.222755, finishTime=2023-01-17T00:52:16.222938) of size 0
INFO  [2023-01-17 00:52:16,224] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID$20Test[2].04138548-cc32-4fd5-aa35-5f3549b19761] with 1 results within PT0.001462S
INFO  [2023-01-17 00:52:16,224] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID$20Test[2].04138548-cc32-4fd5-aa35-5f3549b19761, workerId=SECONDARY_ID$20Test[2].worker_SECONDARY_ID$20Test[2]_75a6b641-e63b-4a76-bcc7-d9af0fcdcd98, startTime=2023-01-17T00:52:16.222640, finishTime=2023-01-17T00:52:16.224102) of size 1
INFO  [2023-01-17 00:52:16,224] com.bakdata.conquery.models.execution.ManagedExecution: DONE 04138548-cc32-4fd5-aa35-5f3549b19761 ManagedQuery within PT0.007797S
127.0.0.1 - - [17/Jan/2023:00:52:16 +0000] "GET /api/datasets/SECONDARY_ID$20Test%5B2%5D/queries/SECONDARY_ID$20Test%5B2%5D.04138548-cc32-4fd5-aa35-5f3549b19761 HTTP/1.1" 200 2070 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:16,252] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[2]], queryId=04138548-cc32-4fd5-aa35-5f3549b19761, label=Uploaded-List number	@§$, creationTime=2023-01-17T00:52:16.215919, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2bc72e91[Count = 0], startTime=2023-01-17T00:52:16.216873, finishTime=2023-01-17T00:52:16.224670, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@498774e8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[2])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@702b0349, com.bakdata.conquery.models.query.ColumnDescriptor@1cafbf8d, com.bakdata.conquery.models.query.ColumnDescriptor@e0eb92f]) download on dataset Dataset[label=null, name=SECONDARY_ID Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:16,252] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID Test[2]], queryId=04138548-cc32-4fd5-aa35-5f3549b19761, label=Uploaded-List number	@§$, creationTime=2023-01-17T00:52:16.215919, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2bc72e91[Count = 0], startTime=2023-01-17T00:52:16.216873, finishTime=2023-01-17T00:52:16.224670, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@498774e8), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID Test[2])), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@702b0349, com.bakdata.conquery.models.query.ColumnDescriptor@1cafbf8d, com.bakdata.conquery.models.query.ColumnDescriptor@e0eb92f]) on dataset Dataset[label=null, name=SECONDARY_ID Test[2]]
127.0.0.1 - - [17/Jan/2023:00:52:16 +0000] "GET /api/datasets/SECONDARY_ID%20Test%5B2%5D/result/SECONDARY_ID$20Test%5B2%5D.04138548-cc32-4fd5-aa35-5f3549b19761.csv?pretty=false HTTP/1.1" 200 64 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:16,271] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID Test on 3 rows
INFO  [2023-01-17 00:52:16,271] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID Test[2]
INFO  [2023-01-17 00:52:16,271] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-17 00:52:16,271] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID Test[2], name=SECONDARY_ID Test[2]]
INFO  [2023-01-17 00:52:16,272] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[2]_a71b779e-56e8-4378-9f71-7f5534d33a9d
INFO  [2023-01-17 00:52:16,272] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID Test[2]_75a6b641-e63b-4a76-bcc7-d9af0fcdcd98
INFO  [2023-01-17 00:52:16,273] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[2]_75a6b641-e63b-4a76-bcc7-d9af0fcdcd98
INFO  [2023-01-17 00:52:16,273] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID Test[2]
INFO  [2023-01-17 00:52:16,273] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID Test[2]_a71b779e-56e8-4378-9f71-7f5534d33a9d
INFO  [2023-01-17 00:52:16,370] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID$20Test[2]
INFO  [2023-01-17 00:52:16,370] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,400] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID Test
INFO  [2023-01-17 00:52:16,400] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SECONDARY_ID_MIXED Test
INFO  [2023-01-17 00:52:16,400] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:16,400] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:16,401] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-17 00:52:16,401] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-17 00:52:16,401] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:16,401] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_d8df44e1-d500-4fe5-9e31-6b6f6400611a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_d8df44e1-d500-4fe5-9e31-6b6f6400611a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_d94ef881-3242-4e27-836a-03e3da068879 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_d94ef881-3242-4e27-836a-03e3da068879 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:16,403] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:16,507] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID_MIXED$20Test.secondary]
INFO  [2023-01-17 00:52:16,508] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[SECONDARY_ID_MIXED$20Test.ignored]
INFO  [2023-01-17 00:52:16,508] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,509] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.secondary
INFO  [2023-01-17 00:52:16,509] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.secondary
INFO  [2023-01-17 00:52:16,509] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.ignored
INFO  [2023-01-17 00:52:16,509] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId SECONDARY_ID_MIXED$20Test.ignored
INFO  [2023-01-17 00:52:16,616] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,616] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table
INFO  [2023-01-17 00:52:16,616] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table
INFO  [2023-01-17 00:52:16,616] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table2
INFO  [2023-01-17 00:52:16,616] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SECONDARY_ID_MIXED$20Test.table2
INFO  [2023-01-17 00:52:16,740] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,850] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:16,850] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:16,850] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:16,850] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 465 B in total
INFO  [2023-01-17 00:52:16,850] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_MIXED Test/table.import.json, tag=Optional.empty)
██████████████████████████▌                       ▌  53%	est. time remaining: 0.039541391sINFO  [2023-01-17 00:52:16,895] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-17 00:52:16,895] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@24381ab3), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@54811b25), dateReader=com.bakdata.conquery.util.DateReader@37ada8e4, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-17 00:52:16,895] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[ignored] with StringParser(super=Parser(lines=6, nullLines=0), encoding=null, prefix=a, suffix=a)
INFO  [2023-01-17 00:52:16,895] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:16,895] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:16,899] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_MIXED Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:16,899] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_MIXED Test/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.0006753sINFO  [2023-01-17 00:52:16,919] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-17 00:52:16,919] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-17 00:52:16,919] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:16,919] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@39e1a643), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@4fbbc341), dateReader=com.bakdata.conquery.util.DateReader@3a89f741, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-17 00:52:16,922] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_MIXED Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:16,922] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:52:16,922] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_MIXED Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:16,922] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SECONDARY_ID_MIXED Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:16,938] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SECONDARY_ID_MIXED$20Test.table
127.0.0.1 - - [17/Jan/2023:00:52:16 +0000] "POST /admin/datasets/SECONDARY_ID_MIXED%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID_MIXED+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:16,940] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:16,941] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:16,941] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:16,944] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:16,944] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:52:16,945] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:52:16,946] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:16,946] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID_MIXED$20Test.table.table.0
INFO  [2023-01-17 00:52:16,964] com.bakdata.conquery.models.jobs.ImportJob: Importing table2 into SECONDARY_ID_MIXED$20Test.table2
INFO  [2023-01-17 00:52:16,964] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:16,964] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
127.0.0.1 - - [17/Jan/2023:00:52:16 +0000] "POST /admin/datasets/SECONDARY_ID_MIXED%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SECONDARY_ID_MIXED+Test%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 18
WARN  [2023-01-17 00:52:16,964] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:16,964] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:16,964] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
WARN  [2023-01-17 00:52:16,964] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:16,965] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table2.table2], containing 6 entries.
INFO  [2023-01-17 00:52:17,008] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SECONDARY_ID_MIXED$20Test.table2.table2], containing 6 entries.
INFO  [2023-01-17 00:52:17,008] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SECONDARY_ID_MIXED$20Test.table2.table2.0
INFO  [2023-01-17 00:52:17,113] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,119] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,135] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,136] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:17,242] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SECONDARY_ID_MIXED Test QUERY INIT
INFO  [2023-01-17 00:52:17,256] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SECONDARY_ID_MIXED$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:17,257] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1db2b718-4f14-43ae-b28f-278c7892bda0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_MIXED Test))]]
INFO  [2023-01-17 00:52:17,280] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0
INFO  [2023-01-17 00:52:17,280] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started SecondaryIdQuery SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0
WARN  [2023-01-17 00:52:17,280] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:17,280] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0] with 0 results within PT0.000166S
INFO  [2023-01-17 00:52:17,280] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0, workerId=SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_d8df44e1-d500-4fe5-9e31-6b6f6400611a, startTime=2023-01-17T00:52:17.280418, finishTime=2023-01-17T00:52:17.280584) of size 0
127.0.0.1 - - [17/Jan/2023:00:52:17 +0000] "POST /api/datasets/SECONDARY_ID_MIXED$20Test/queries HTTP/1.1" 201 1694 "-" "Conquery (test client)" 27
INFO  [2023-01-17 00:52:17,281] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0] with 2 results within PT0.001239S
INFO  [2023-01-17 00:52:17,282] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0, workerId=SECONDARY_ID_MIXED$20Test.worker_SECONDARY_ID_MIXED$20Test_d94ef881-3242-4e27-836a-03e3da068879, startTime=2023-01-17T00:52:17.280494, finishTime=2023-01-17T00:52:17.281733) of size 2
INFO  [2023-01-17 00:52:17,282] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1db2b718-4f14-43ae-b28f-278c7892bda0 ManagedQuery within PT0.025313S
127.0.0.1 - - [17/Jan/2023:00:52:17 +0000] "GET /api/datasets/SECONDARY_ID_MIXED$20Test/queries/SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0 HTTP/1.1" 200 1986 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:17,300] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_MIXED Test], queryId=1db2b718-4f14-43ae-b28f-278c7892bda0, label=concept	@§$, creationTime=2023-01-17T00:52:17.256811, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@37eaa6a8[Count = 0], startTime=2023-01-17T00:52:17.257059, finishTime=2023-01-17T00:52:17.282372, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1289d8a0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_MIXED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@33b67542, com.bakdata.conquery.models.query.ColumnDescriptor@6a263ab0, com.bakdata.conquery.models.query.ColumnDescriptor@81b0eba]) download on dataset Dataset[label=null, name=SECONDARY_ID_MIXED Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:17,300] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SECONDARY_ID_MIXED Test], queryId=1db2b718-4f14-43ae-b28f-278c7892bda0, label=concept	@§$, creationTime=2023-01-17T00:52:17.256811, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@37eaa6a8[Count = 0], startTime=2023-01-17T00:52:17.257059, finishTime=2023-01-17T00:52:17.282372, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1289d8a0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SECONDARY_ID_MIXED Test)), query=com.bakdata.conquery.apiv1.query.SecondaryIdQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@33b67542, com.bakdata.conquery.models.query.ColumnDescriptor@6a263ab0, com.bakdata.conquery.models.query.ColumnDescriptor@81b0eba]) on dataset Dataset[label=null, name=SECONDARY_ID_MIXED Test]
127.0.0.1 - - [17/Jan/2023:00:52:17 +0000] "GET /api/datasets/SECONDARY_ID_MIXED%20Test/result/SECONDARY_ID_MIXED$20Test.1db2b718-4f14-43ae-b28f-278c7892bda0.csv?pretty=false HTTP/1.1" 200 309 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:17,303] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SECONDARY_ID_MIXED Test on 5 rows
INFO  [2023-01-17 00:52:17,303] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SECONDARY_ID_MIXED Test
INFO  [2023-01-17 00:52:17,303] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-17 00:52:17,303] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_MIXED Test_d8df44e1-d500-4fe5-9e31-6b6f6400611a
INFO  [2023-01-17 00:52:17,303] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SECONDARY_ID_MIXED Test, name=SECONDARY_ID_MIXED Test]
INFO  [2023-01-17 00:52:17,304] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SECONDARY_ID_MIXED Test_d94ef881-3242-4e27-836a-03e3da068879
INFO  [2023-01-17 00:52:17,402] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SECONDARY_ID_MIXED Test
INFO  [2023-01-17 00:52:17,403] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_MIXED Test_d8df44e1-d500-4fe5-9e31-6b6f6400611a
INFO  [2023-01-17 00:52:17,403] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SECONDARY_ID_MIXED Test_d94ef881-3242-4e27-836a-03e3da068879
INFO  [2023-01-17 00:52:17,476] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SECONDARY_ID_MIXED$20Test
INFO  [2023-01-17 00:52:17,476] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,541] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SECONDARY_ID_MIXED Test
INFO  [2023-01-17 00:52:17,542] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-17 00:52:17,542] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:17,542] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:17,543] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-17 00:52:17,543] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:17,543] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-17 00:52:17,543] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:17,544] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,545] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_834a067b-bcdc-481b-8107-1a0cab5796ca are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:17,545] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_834a067b-bcdc-481b-8107-1a0cab5796ca are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:17,545] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:17,545] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_b72ecb27-63bd-44db-959e-a1bd88fd8088 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:17,545] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_b72ecb27-63bd-44db-959e-a1bd88fd8088 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:17,545] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:17,649] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,656] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,656] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:17,656] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:17,773] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:17,882] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:17,882] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:17,882] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 87 B in total
INFO  [2023-01-17 00:52:17,883] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_CQEXTERNAL_QUERY Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000321385sINFO  [2023-01-17 00:52:17,915] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=6, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:17,915] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@60073ca0)
INFO  [2023-01-17 00:52:17,917] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_CQEXTERNAL_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:17,917] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:17,917] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_CQEXTERNAL_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:17,934] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_CQEXTERNAL_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:17,935] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:17 +0000] "POST /admin/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_CQEXTERNAL_QUERY+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:17,936] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:17,936] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:17,936] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:17,938] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:17,938] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table], containing 6 entries.
INFO  [2023-01-17 00:52:17,938] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table], containing 6 entries.
WARN  [2023-01-17 00:52:17,940] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:17,940] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table.0
INFO  [2023-01-17 00:52:17,940] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test.test_table.test_table.1
INFO  [2023-01-17 00:52:18,046] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,051] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,059] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,059] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:18,060] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:18,165] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_CQEXTERNAL_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:18,178] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_CQEXTERNAL_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:18,178] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4c9ac0a6-fb6c-43ca-89c2-310125045795] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test))]]
INFO  [2023-01-17 00:52:18,181] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795
INFO  [2023-01-17 00:52:18,181] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795
127.0.0.1 - - [17/Jan/2023:00:52:18 +0000] "POST /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test/queries HTTP/1.1" 201 984 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:18,183] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795] with 2 results within PT0.001158S
INFO  [2023-01-17 00:52:18,183] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795] with 0 results within PT0.001274S
INFO  [2023-01-17 00:52:18,183] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_834a067b-bcdc-481b-8107-1a0cab5796ca, startTime=2023-01-17T00:52:18.181843, finishTime=2023-01-17T00:52:18.183117) of size 0
INFO  [2023-01-17 00:52:18,183] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test.worker_SIMPLE_CQEXTERNAL_QUERY$20Test_b72ecb27-63bd-44db-959e-a1bd88fd8088, startTime=2023-01-17T00:52:18.181830, finishTime=2023-01-17T00:52:18.182988) of size 2
INFO  [2023-01-17 00:52:18,183] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4c9ac0a6-fb6c-43ca-89c2-310125045795 ManagedQuery within PT0.004885S
127.0.0.1 - - [17/Jan/2023:00:52:18 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test/queries/SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795 HTTP/1.1" 200 1294 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:18,215] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test], queryId=4c9ac0a6-fb6c-43ca-89c2-310125045795, label=Uploaded-List	@§$, creationTime=2023-01-17T00:52:18.178268, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f723c35[Count = 0], startTime=2023-01-17T00:52:18.178905, finishTime=2023-01-17T00:52:18.183790, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7ac54a25), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7f08068d, com.bakdata.conquery.models.query.ColumnDescriptor@23eab5a1]) download on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:18,215] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test], queryId=4c9ac0a6-fb6c-43ca-89c2-310125045795, label=Uploaded-List	@§$, creationTime=2023-01-17T00:52:18.178268, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f723c35[Count = 0], startTime=2023-01-17T00:52:18.178905, finishTime=2023-01-17T00:52:18.183790, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7ac54a25), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7f08068d, com.bakdata.conquery.models.query.ColumnDescriptor@23eab5a1]) on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:18 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test/result/SIMPLE_CQEXTERNAL_QUERY$20Test.4c9ac0a6-fb6c-43ca-89c2-310125045795.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 17
INFO  [2023-01-17 00:52:18,231] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_CQEXTERNAL_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:18,231] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-17 00:52:18,231] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-17 00:52:18,232] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test, name=SIMPLE_CQEXTERNAL_QUERY Test]
INFO  [2023-01-17 00:52:18,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test_834a067b-bcdc-481b-8107-1a0cab5796ca
INFO  [2023-01-17 00:52:18,232] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test_b72ecb27-63bd-44db-959e-a1bd88fd8088
INFO  [2023-01-17 00:52:18,244] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-17 00:52:18,244] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test_834a067b-bcdc-481b-8107-1a0cab5796ca
INFO  [2023-01-17 00:52:18,244] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test_b72ecb27-63bd-44db-959e-a1bd88fd8088
INFO  [2023-01-17 00:52:18,340] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_CQEXTERNAL_QUERY$20Test
INFO  [2023-01-17 00:52:18,340] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,366] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-17 00:52:18,366] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-17 00:52:18,366] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:18,366] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:18,368] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-17 00:52:18,368] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-17 00:52:18,368] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:18,368] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:18,371] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_fda5f25b-cd85-479f-8323-3cbe847fc5d1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:18,371] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_fda5f25b-cd85-479f-8323-3cbe847fc5d1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:18,371] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:18,371] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_d57f004a-addb-4c69-86c6-6eacaf2005ef are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:18,371] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_d57f004a-addb-4c69-86c6-6eacaf2005ef are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:18,371] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:18,372] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,474] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,483] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table
INFO  [2023-01-17 00:52:18,483] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table
INFO  [2023-01-17 00:52:18,602] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,711] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:18,711] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:18,711] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 87 B in total
INFO  [2023-01-17 00:52:18,711] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_CQEXTERNAL_QUERY Test[1]/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000349199sINFO  [2023-01-17 00:52:18,746] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=6, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:18,746] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=15340, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@69e772b8)
INFO  [2023-01-17 00:52:18,748] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_CQEXTERNAL_QUERY Test[1]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:18,748] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:18,748] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_CQEXTERNAL_QUERY Test[1]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:18,767] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table
127.0.0.1 - - [17/Jan/2023:00:52:18 +0000] "POST /admin/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_CQEXTERNAL_QUERY+Test%5B1%5D%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:18,768] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,768] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:18,768] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:18,768] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:18,769] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:18,769] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table], containing 6 entries.
INFO  [2023-01-17 00:52:18,769] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table], containing 6 entries.
WARN  [2023-01-17 00:52:18,769] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:18,770] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table.1
INFO  [2023-01-17 00:52:18,770] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_CQEXTERNAL_QUERY$20Test[1].test_table.test_table.0
INFO  [2023-01-17 00:52:18,874] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,898] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:18,899] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:18,899] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:19,005] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_CQEXTERNAL_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:19,019] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_CQEXTERNAL_QUERY$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:19,020] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d1d7e2f9-e4e1-4679-a03e-8682868c65b0] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test[1]))]]
INFO  [2023-01-17 00:52:19,023] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test[1].d1d7e2f9-e4e1-4679-a03e-8682868c65b0
INFO  [2023-01-17 00:52:19,023] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_CQEXTERNAL_QUERY$20Test[1].d1d7e2f9-e4e1-4679-a03e-8682868c65b0
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "POST /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D/queries HTTP/1.1" 201 1029 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:19,024] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test[1].d1d7e2f9-e4e1-4679-a03e-8682868c65b0] with 2 results within PT0.001069S
INFO  [2023-01-17 00:52:19,025] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_CQEXTERNAL_QUERY$20Test[1].d1d7e2f9-e4e1-4679-a03e-8682868c65b0] with 0 results within PT0.001167S
INFO  [2023-01-17 00:52:19,025] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].d1d7e2f9-e4e1-4679-a03e-8682868c65b0, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_d57f004a-addb-4c69-86c6-6eacaf2005ef, startTime=2023-01-17T00:52:19.023829, finishTime=2023-01-17T00:52:19.024898) of size 2
INFO  [2023-01-17 00:52:19,025] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].d1d7e2f9-e4e1-4679-a03e-8682868c65b0, workerId=SIMPLE_CQEXTERNAL_QUERY$20Test[1].worker_SIMPLE_CQEXTERNAL_QUERY$20Test[1]_fda5f25b-cd85-479f-8323-3cbe847fc5d1, startTime=2023-01-17T00:52:19.023853, finishTime=2023-01-17T00:52:19.025020) of size 0
INFO  [2023-01-17 00:52:19,025] com.bakdata.conquery.models.execution.ManagedExecution: DONE d1d7e2f9-e4e1-4679-a03e-8682868c65b0 ManagedQuery within PT0.005214S
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D/queries/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D.d1d7e2f9-e4e1-4679-a03e-8682868c65b0 HTTP/1.1" 200 1636 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:19,041] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]], queryId=d1d7e2f9-e4e1-4679-a03e-8682868c65b0, label=Uploaded-List	@§$, creationTime=2023-01-17T00:52:19.019838, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4a73419f[Count = 0], startTime=2023-01-17T00:52:19.020580, finishTime=2023-01-17T00:52:19.025794, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@ff0c549), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@336267f9, com.bakdata.conquery.models.query.ColumnDescriptor@3889efac]) download on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:19,041] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]], queryId=d1d7e2f9-e4e1-4679-a03e-8682868c65b0, label=Uploaded-List	@§$, creationTime=2023-01-17T00:52:19.019838, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4a73419f[Count = 0], startTime=2023-01-17T00:52:19.020580, finishTime=2023-01-17T00:52:19.025794, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@ff0c549), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_CQEXTERNAL_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@336267f9, com.bakdata.conquery.models.query.ColumnDescriptor@3889efac]) on dataset Dataset[label=null, name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "GET /api/datasets/SIMPLE_CQEXTERNAL_QUERY%20Test%5B1%5D/result/SIMPLE_CQEXTERNAL_QUERY$20Test%5B1%5D.d1d7e2f9-e4e1-4679-a03e-8682868c65b0.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:52:19,063] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_CQEXTERNAL_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:19,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_CQEXTERNAL_QUERY Test[1]
INFO  [2023-01-17 00:52:19,064] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-17 00:52:19,064] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_CQEXTERNAL_QUERY Test[1], name=SIMPLE_CQEXTERNAL_QUERY Test[1]]
INFO  [2023-01-17 00:52:19,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_d57f004a-addb-4c69-86c6-6eacaf2005ef
INFO  [2023-01-17 00:52:19,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_fda5f25b-cd85-479f-8323-3cbe847fc5d1
INFO  [2023-01-17 00:52:19,068] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_CQEXTERNAL_QUERY Test[1]
INFO  [2023-01-17 00:52:19,069] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_d57f004a-addb-4c69-86c6-6eacaf2005ef
INFO  [2023-01-17 00:52:19,069] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_CQEXTERNAL_QUERY Test[1]_fda5f25b-cd85-479f-8323-3cbe847fc5d1
INFO  [2023-01-17 00:52:19,070] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_CQEXTERNAL_QUERY$20Test[1]
INFO  [2023-01-17 00:52:19,070] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,204] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_CQEXTERNAL_QUERY Test
INFO  [2023-01-17 00:52:19,205] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:52:19,205] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:19,205] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:19,206] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-17 00:52:19,206] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-17 00:52:19,206] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:19,206] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:19,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_3e97dd85-5639-4410-990d-8e60f570cd0d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:19,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_3e97dd85-5639-4410-990d-8e60f570cd0d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:19,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:19,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_42d5a955-d70c-4c96-9375-ae537e4f6525 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:19,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_42d5a955-d70c-4c96-9375-ae537e4f6525 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:19,207] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:19,211] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,311] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,318] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,319] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[1].table
INFO  [2023-01-17 00:52:19,319] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[1].table
INFO  [2023-01-17 00:52:19,435] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,546] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:19,547] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:19,547] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 194 B in total
INFO  [2023-01-17 00:52:19,547] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000353901sINFO  [2023-01-17 00:52:19,583] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=7, sum=8, min=1, average=1.142857, max=2}
INFO  [2023-01-17 00:52:19,583] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[column] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:19,583] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[include] with StringParser(super=Parser(lines=8, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:19,583] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4933dac)
INFO  [2023-01-17 00:52:19,586] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:19,586] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:19,586] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:19,615] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SIMPLE_TREECONCEPT_QUERY$20Test[1].table
INFO  [2023-01-17 00:52:19,615] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_TREECONCEPT_QUERY+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:52:19,616] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:19,617] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:19,617] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:19,621] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:19,621] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table], containing 8 entries.
INFO  [2023-01-17 00:52:19,621] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table], containing 8 entries.
WARN  [2023-01-17 00:52:19,622] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:19,622] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table.0
INFO  [2023-01-17 00:52:19,622] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table.1
INFO  [2023-01-17 00:52:19,623] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[1].table.table.2
INFO  [2023-01-17 00:52:19,744] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,749] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,763] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:19,763] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:19,763] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:19,869] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:19,884] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:19,885] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[367abd58-72f3-4eef-98bf-698b282b12ad] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[1]))]]
INFO  [2023-01-17 00:52:19,888] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[1].367abd58-72f3-4eef-98bf-698b282b12ad
INFO  [2023-01-17 00:52:19,888] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[1].367abd58-72f3-4eef-98bf-698b282b12ad
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D/queries HTTP/1.1" 201 1134 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:19,889] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[1].367abd58-72f3-4eef-98bf-698b282b12ad] with 0 results within PT0.001151S
INFO  [2023-01-17 00:52:19,889] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[1].367abd58-72f3-4eef-98bf-698b282b12ad] with 2 results within PT0.001337S
INFO  [2023-01-17 00:52:19,889] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[1].367abd58-72f3-4eef-98bf-698b282b12ad, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_3e97dd85-5639-4410-990d-8e60f570cd0d, startTime=2023-01-17T00:52:19.888233, finishTime=2023-01-17T00:52:19.889384) of size 0
INFO  [2023-01-17 00:52:19,890] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[1].367abd58-72f3-4eef-98bf-698b282b12ad, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[1].worker_SIMPLE_TREECONCEPT_QUERY$20Test[1]_42d5a955-d70c-4c96-9375-ae537e4f6525, startTime=2023-01-17T00:52:19.888248, finishTime=2023-01-17T00:52:19.889585) of size 2
INFO  [2023-01-17 00:52:19,890] com.bakdata.conquery.models.execution.ManagedExecution: DONE 367abd58-72f3-4eef-98bf-698b282b12ad ManagedQuery within PT0.005084S
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D/queries/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D.367abd58-72f3-4eef-98bf-698b282b12ad HTTP/1.1" 200 1749 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:19,916] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]], queryId=367abd58-72f3-4eef-98bf-698b282b12ad, label=concept-a1	@§$, creationTime=2023-01-17T00:52:19.884974, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@63caffd[Count = 0], startTime=2023-01-17T00:52:19.885200, finishTime=2023-01-17T00:52:19.890284, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6d0dcc72), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4fff5409, com.bakdata.conquery.models.query.ColumnDescriptor@7edcfcbe]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:19,934] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]], queryId=367abd58-72f3-4eef-98bf-698b282b12ad, label=concept-a1	@§$, creationTime=2023-01-17T00:52:19.884974, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@63caffd[Count = 0], startTime=2023-01-17T00:52:19.885200, finishTime=2023-01-17T00:52:19.890284, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6d0dcc72), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[1])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4fff5409, com.bakdata.conquery.models.query.ColumnDescriptor@7edcfcbe]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[1]]
127.0.0.1 - - [17/Jan/2023:00:52:19 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B1%5D/result/SIMPLE_TREECONCEPT_QUERY$20Test%5B1%5D.367abd58-72f3-4eef-98bf-698b282b12ad.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:19,936] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:19,937] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY Test[1]
INFO  [2023-01-17 00:52:19,937] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-17 00:52:19,937] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[1], name=SIMPLE_TREECONCEPT_QUERY Test[1]]
INFO  [2023-01-17 00:52:19,937] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[1]_3e97dd85-5639-4410-990d-8e60f570cd0d
INFO  [2023-01-17 00:52:19,937] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[1]_42d5a955-d70c-4c96-9375-ae537e4f6525
INFO  [2023-01-17 00:52:20,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[1]_3e97dd85-5639-4410-990d-8e60f570cd0d
INFO  [2023-01-17 00:52:20,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY Test[1]
INFO  [2023-01-17 00:52:20,011] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[1]_42d5a955-d70c-4c96-9375-ae537e4f6525
INFO  [2023-01-17 00:52:20,034] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY$20Test[1]
INFO  [2023-01-17 00:52:20,034] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,169] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:52:20,169] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-17 00:52:20,169] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:20,169] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:20,170] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-17 00:52:20,171] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:20,170] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-17 00:52:20,171] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_20f613c4-3250-4739-9330-fba4239ebb79 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_20f613c4-3250-4739-9330-fba4239ebb79 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_4c37f804-b6d9-40aa-8b34-951c6e29f875 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_4c37f804-b6d9-40aa-8b34-951c6e29f875 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:20,173] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:20,177] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,276] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,283] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,283] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy
INFO  [2023-01-17 00:52:20,283] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy
INFO  [2023-01-17 00:52:20,417] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,558] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:20,558] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:20,559] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 112 B in total
INFO  [2023-01-17 00:52:20,559] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_GROOVY_QUERY Test/test_table_groovy.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00048954sINFO  [2023-01-17 00:52:20,608] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:20,608] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:20,608] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[groovy_column] with IntegerParser(super=Parser(lines=4, nullLines=0), minValue=1, maxValue=2)
INFO  [2023-01-17 00:52:20,608] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@d2ef59c)
INFO  [2023-01-17 00:52:20,611] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_GROOVY_QUERY Test/test_table_groovy.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:20,611] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:20,611] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_GROOVY_QUERY Test/test_table_groovy.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:20,626] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table_groovy into SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy
INFO  [2023-01-17 00:52:20,627] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:20 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_TREECONCEPT_GROOVY_QUERY+Test%2Ftest_table_groovy.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:20,628] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:20,629] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:20,629] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:20,632] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:20,632] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy], containing 4 entries.
INFO  [2023-01-17 00:52:20,632] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy], containing 4 entries.
WARN  [2023-01-17 00:52:20,634] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:20,634] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy.0
INFO  [2023-01-17 00:52:20,634] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.test_table_groovy.test_table_groovy.1
INFO  [2023-01-17 00:52:20,739] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,744] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,754] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:20,754] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:20,754] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:20,860] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_GROOVY_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:20,876] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:20,876] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a476af5f-7b14-466e-953a-c4d1089fbf55] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_GROOVY_QUERY Test))]]
INFO  [2023-01-17 00:52:20,879] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55
INFO  [2023-01-17 00:52:20,879] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55
INFO  [2023-01-17 00:52:20,880] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55] with 0 results within PT0.000785S
INFO  [2023-01-17 00:52:20,880] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55] with 1 results within PT0.001166S
127.0.0.1 - - [17/Jan/2023:00:52:20 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test/queries HTTP/1.1" 201 1194 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:20,881] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55, workerId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_20f613c4-3250-4739-9330-fba4239ebb79, startTime=2023-01-17T00:52:20.879557, finishTime=2023-01-17T00:52:20.880342) of size 0
INFO  [2023-01-17 00:52:20,881] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55, workerId=SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.worker_SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test_4c37f804-b6d9-40aa-8b34-951c6e29f875, startTime=2023-01-17T00:52:20.879519, finishTime=2023-01-17T00:52:20.880685) of size 1
INFO  [2023-01-17 00:52:20,881] com.bakdata.conquery.models.execution.ManagedExecution: DONE a476af5f-7b14-466e-953a-c4d1089fbf55 ManagedQuery within PT0.004825S
127.0.0.1 - - [17/Jan/2023:00:52:20 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test/queries/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55 HTTP/1.1" 200 1537 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:20,907] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test], queryId=a476af5f-7b14-466e-953a-c4d1089fbf55, label=test_child1_1	@§$, creationTime=2023-01-17T00:52:20.876413, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1a6f2dcf[Count = 0], startTime=2023-01-17T00:52:20.876592, finishTime=2023-01-17T00:52:20.881417, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@71e59889), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_GROOVY_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@a764dd2, com.bakdata.conquery.models.query.ColumnDescriptor@7c0c2871]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:20,907] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test], queryId=a476af5f-7b14-466e-953a-c4d1089fbf55, label=test_child1_1	@§$, creationTime=2023-01-17T00:52:20.876413, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1a6f2dcf[Count = 0], startTime=2023-01-17T00:52:20.876592, finishTime=2023-01-17T00:52:20.881417, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@71e59889), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_GROOVY_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@a764dd2, com.bakdata.conquery.models.query.ColumnDescriptor@7c0c2871]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:20 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_GROOVY_QUERY%20Test/result/SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test.a476af5f-7b14-466e-953a-c4d1089fbf55.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:20,927] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_GROOVY_QUERY Test on 2 rows
INFO  [2023-01-17 00:52:20,928] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-17 00:52:20,928] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-17 00:52:20,928] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_GROOVY_QUERY Test, name=SIMPLE_TREECONCEPT_GROOVY_QUERY Test]
INFO  [2023-01-17 00:52:20,928] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_20f613c4-3250-4739-9330-fba4239ebb79
INFO  [2023-01-17 00:52:20,928] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_4c37f804-b6d9-40aa-8b34-951c6e29f875
INFO  [2023-01-17 00:52:20,971] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-17 00:52:20,972] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_20f613c4-3250-4739-9330-fba4239ebb79
INFO  [2023-01-17 00:52:20,972] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_GROOVY_QUERY Test_4c37f804-b6d9-40aa-8b34-951c6e29f875
INFO  [2023-01-17 00:52:21,034] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_GROOVY_QUERY$20Test
INFO  [2023-01-17 00:52:21,035] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,160] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_GROOVY_QUERY Test
INFO  [2023-01-17 00:52:21,160] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Concept Condition is Present
INFO  [2023-01-17 00:52:21,160] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:21,160] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:21,161] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-17 00:52:21,161] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:21,161] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-17 00:52:21,162] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:21,163] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_17310abf-27ed-4f40-9c9d-e49fa2e41aff are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:21,163] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_17310abf-27ed-4f40-9c9d-e49fa2e41aff are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:21,163] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:21,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_25f4aff5-0d6b-4f24-810e-1114f1d43126 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:21,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_25f4aff5-0d6b-4f24-810e-1114f1d43126 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:21,164] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:21,168] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,268] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,275] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,275] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Concept$20Condition$20is$20Present.table
INFO  [2023-01-17 00:52:21,275] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Concept$20Condition$20is$20Present.table
INFO  [2023-01-17 00:52:21,401] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,513] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:21,513] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:21,513] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 173 B in total
INFO  [2023-01-17 00:52:21,513] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Concept Condition is Present/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000305572sINFO  [2023-01-17 00:52:21,544] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=7, min=1, average=1.166667, max=2}
INFO  [2023-01-17 00:52:21,544] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:21,544] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[include] with StringParser(super=Parser(lines=7, nullLines=3), encoding=null, prefix=1, suffix=1)
INFO  [2023-01-17 00:52:21,544] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7be404a)
INFO  [2023-01-17 00:52:21,547] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Concept Condition is Present/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:21,547] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:21,547] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Concept Condition is Present/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:21,560] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Concept$20Condition$20is$20Present.table
INFO  [2023-01-17 00:52:21,561] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:21 +0000] "POST /admin/datasets/Concept%20Condition%20is%20Present/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Concept+Condition+is+Present%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:21,562] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:21,563] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:21,563] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:21,566] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:21,566] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Concept$20Condition$20is$20Present.table.table], containing 7 entries.
INFO  [2023-01-17 00:52:21,566] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Concept$20Condition$20is$20Present.table.table], containing 7 entries.
WARN  [2023-01-17 00:52:21,567] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:21,567] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Concept$20Condition$20is$20Present.table.table.0
INFO  [2023-01-17 00:52:21,568] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Concept$20Condition$20is$20Present.table.table.1
INFO  [2023-01-17 00:52:21,673] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,678] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,691] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:21,691] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:21,691] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:21,797] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Concept Condition is Present QUERY INIT
INFO  [2023-01-17 00:52:21,828] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Concept$20Condition$20is$20Present] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:21,828] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[3b370353-c11f-4756-b65e-1638c35fa2b9] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Concept Condition is Present))]]
INFO  [2023-01-17 00:52:21,831] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9
INFO  [2023-01-17 00:52:21,831] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9
127.0.0.1 - - [17/Jan/2023:00:52:21 +0000] "POST /api/datasets/Concept$20Condition$20is$20Present/queries HTTP/1.1" 201 1139 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:21,832] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9] with 0 results within PT0.000936S
INFO  [2023-01-17 00:52:21,832] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9] with 2 results within PT0.001027S
INFO  [2023-01-17 00:52:21,833] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9, workerId=Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_25f4aff5-0d6b-4f24-810e-1114f1d43126, startTime=2023-01-17T00:52:21.831946, finishTime=2023-01-17T00:52:21.832882) of size 0
INFO  [2023-01-17 00:52:21,833] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9, workerId=Concept$20Condition$20is$20Present.worker_Concept$20Condition$20is$20Present_17310abf-27ed-4f40-9c9d-e49fa2e41aff, startTime=2023-01-17T00:52:21.831901, finishTime=2023-01-17T00:52:21.832928) of size 2
INFO  [2023-01-17 00:52:21,833] com.bakdata.conquery.models.execution.ManagedExecution: DONE 3b370353-c11f-4756-b65e-1638c35fa2b9 ManagedQuery within PT0.004727S
127.0.0.1 - - [17/Jan/2023:00:52:21 +0000] "GET /api/datasets/Concept$20Condition$20is$20Present/queries/Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9 HTTP/1.1" 200 1466 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:21,864] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Concept Condition is Present], queryId=3b370353-c11f-4756-b65e-1638c35fa2b9, label=concept-a1	@§$, creationTime=2023-01-17T00:52:21.828707, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@fd2695a[Count = 0], startTime=2023-01-17T00:52:21.828897, finishTime=2023-01-17T00:52:21.833624, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@510e5bba), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Concept Condition is Present)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6ca9451, com.bakdata.conquery.models.query.ColumnDescriptor@41cc414e]) download on dataset Dataset[label=null, name=Concept Condition is Present] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:21,864] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Concept Condition is Present], queryId=3b370353-c11f-4756-b65e-1638c35fa2b9, label=concept-a1	@§$, creationTime=2023-01-17T00:52:21.828707, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@fd2695a[Count = 0], startTime=2023-01-17T00:52:21.828897, finishTime=2023-01-17T00:52:21.833624, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@510e5bba), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Concept Condition is Present)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6ca9451, com.bakdata.conquery.models.query.ColumnDescriptor@41cc414e]) on dataset Dataset[label=null, name=Concept Condition is Present]
127.0.0.1 - - [17/Jan/2023:00:52:21 +0000] "GET /api/datasets/Concept%20Condition%20is%20Present/result/Concept$20Condition$20is$20Present.3b370353-c11f-4756-b65e-1638c35fa2b9.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:21,866] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Concept Condition is Present on 3 rows
INFO  [2023-01-17 00:52:21,866] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Concept Condition is Present
INFO  [2023-01-17 00:52:21,866] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-17 00:52:21,866] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Concept Condition is Present, name=Concept Condition is Present]
INFO  [2023-01-17 00:52:21,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Concept Condition is Present_17310abf-27ed-4f40-9c9d-e49fa2e41aff
INFO  [2023-01-17 00:52:21,867] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Concept Condition is Present_25f4aff5-0d6b-4f24-810e-1114f1d43126
INFO  [2023-01-17 00:52:21,962] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Concept Condition is Present
INFO  [2023-01-17 00:52:21,963] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Concept Condition is Present_17310abf-27ed-4f40-9c9d-e49fa2e41aff
INFO  [2023-01-17 00:52:21,963] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Concept Condition is Present_25f4aff5-0d6b-4f24-810e-1114f1d43126
INFO  [2023-01-17 00:52:21,968] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Concept$20Condition$20is$20Present
INFO  [2023-01-17 00:52:21,968] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,097] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Concept Condition is Present
INFO  [2023-01-17 00:52:22,097] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:52:22,097] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:22,097] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:22,136] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-17 00:52:22,136] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-17 00:52:22,136] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:22,136] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:22,138] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_0983fc87-d54d-4a95-86b1-07af35b84e5f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:22,138] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_0983fc87-d54d-4a95-86b1-07af35b84e5f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:22,138] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:22,138] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_82e0c32b-9a57-42a4-b3ff-d2579253df56 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:22,138] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_82e0c32b-9a57-42a4-b3ff-d2579253df56 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:22,138] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:22,142] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,241] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,248] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,249] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table
INFO  [2023-01-17 00:52:22,249] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table
INFO  [2023-01-17 00:52:22,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,473] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:22,473] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:22,473] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 94 B in total
INFO  [2023-01-17 00:52:22,473] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test[2]/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000399894sINFO  [2023-01-17 00:52:22,514] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:22,514] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:22,514] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@3e2ef33f)
INFO  [2023-01-17 00:52:22,516] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test[2]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:22,517] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:22,517] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY Test[2]/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:22,538] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table
INFO  [2023-01-17 00:52:22,538] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:22 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_TREECONCEPT_QUERY+Test%5B2%5D%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:22,539] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:22,539] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:22,539] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:22,541] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:22,541] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:52:22,541] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table], containing 4 entries.
WARN  [2023-01-17 00:52:22,542] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:22,542] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table.0
INFO  [2023-01-17 00:52:22,542] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY$20Test[2].test_table.test_table.1
INFO  [2023-01-17 00:52:22,648] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,653] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,682] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:22,683] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:22,683] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:22,788] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:22,803] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:22,804] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[868ff1c5-8a28-4eb9-b95d-6a14424fc934] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[2]))]]
INFO  [2023-01-17 00:52:22,806] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[2].868ff1c5-8a28-4eb9-b95d-6a14424fc934
INFO  [2023-01-17 00:52:22,807] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY$20Test[2].868ff1c5-8a28-4eb9-b95d-6a14424fc934
INFO  [2023-01-17 00:52:22,807] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[2].868ff1c5-8a28-4eb9-b95d-6a14424fc934] with 0 results within PT0.00067S
INFO  [2023-01-17 00:52:22,808] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY$20Test[2].868ff1c5-8a28-4eb9-b95d-6a14424fc934] with 2 results within PT0.001047S
127.0.0.1 - - [17/Jan/2023:00:52:22 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D/queries HTTP/1.1" 201 1164 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:22,808] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[2].868ff1c5-8a28-4eb9-b95d-6a14424fc934, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_0983fc87-d54d-4a95-86b1-07af35b84e5f, startTime=2023-01-17T00:52:22.807110, finishTime=2023-01-17T00:52:22.807780) of size 0
INFO  [2023-01-17 00:52:22,808] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY$20Test[2].868ff1c5-8a28-4eb9-b95d-6a14424fc934, workerId=SIMPLE_TREECONCEPT_QUERY$20Test[2].worker_SIMPLE_TREECONCEPT_QUERY$20Test[2]_82e0c32b-9a57-42a4-b3ff-d2579253df56, startTime=2023-01-17T00:52:22.807005, finishTime=2023-01-17T00:52:22.808052) of size 2
INFO  [2023-01-17 00:52:22,808] com.bakdata.conquery.models.execution.ManagedExecution: DONE 868ff1c5-8a28-4eb9-b95d-6a14424fc934 ManagedQuery within PT0.004572S
127.0.0.1 - - [17/Jan/2023:00:52:22 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D/queries/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D.868ff1c5-8a28-4eb9-b95d-6a14424fc934 HTTP/1.1" 200 1779 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:22,832] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]], queryId=868ff1c5-8a28-4eb9-b95d-6a14424fc934, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:52:22.803920, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5f010a6c[Count = 0], startTime=2023-01-17T00:52:22.804145, finishTime=2023-01-17T00:52:22.808717, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@57dc57ce), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3c634902, com.bakdata.conquery.models.query.ColumnDescriptor@4e717da2]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:22,833] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]], queryId=868ff1c5-8a28-4eb9-b95d-6a14424fc934, label=test_tree-test_child1	@§$, creationTime=2023-01-17T00:52:22.803920, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5f010a6c[Count = 0], startTime=2023-01-17T00:52:22.804145, finishTime=2023-01-17T00:52:22.808717, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@57dc57ce), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3c634902, com.bakdata.conquery.models.query.ColumnDescriptor@4e717da2]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY Test[2]]
127.0.0.1 - - [17/Jan/2023:00:52:22 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY%20Test%5B2%5D/result/SIMPLE_TREECONCEPT_QUERY$20Test%5B2%5D.868ff1c5-8a28-4eb9-b95d-6a14424fc934.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:22,850] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:22,850] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY Test[2]
INFO  [2023-01-17 00:52:22,850] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-17 00:52:22,850] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY Test[2], name=SIMPLE_TREECONCEPT_QUERY Test[2]]
INFO  [2023-01-17 00:52:22,850] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[2]_0983fc87-d54d-4a95-86b1-07af35b84e5f
INFO  [2023-01-17 00:52:22,850] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY Test[2]_82e0c32b-9a57-42a4-b3ff-d2579253df56
INFO  [2023-01-17 00:52:22,948] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[2]_82e0c32b-9a57-42a4-b3ff-d2579253df56
INFO  [2023-01-17 00:52:22,948] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY Test[2]_0983fc87-d54d-4a95-86b1-07af35b84e5f
INFO  [2023-01-17 00:52:22,948] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY Test[2]
INFO  [2023-01-17 00:52:23,048] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY$20Test[2]
INFO  [2023-01-17 00:52:23,048] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,088] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY Test
INFO  [2023-01-17 00:52:23,088] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-17 00:52:23,088] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:23,089] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:23,089] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-17 00:52:23,090] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:23,090] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-17 00:52:23,090] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_6bdcfe91-7ada-45e1-a758-5fba1a997f67 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_6bdcfe91-7ada-45e1-a758-5fba1a997f67 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_e0f7dd25-ddeb-4fb7-8d5a-2096029cab54 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_e0f7dd25-ddeb-4fb7-8d5a-2096029cab54 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:23,091] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:23,195] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,202] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,203] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table
INFO  [2023-01-17 00:52:23,203] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table
INFO  [2023-01-17 00:52:23,325] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,433] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:23,433] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:23,434] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 96 B in total
INFO  [2023-01-17 00:52:23,434] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000393904sINFO  [2023-01-17 00:52:23,473] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:23,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:23,474] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@49ef0bc0)
INFO  [2023-01-17 00:52:23,476] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:23,476] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:23,476] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:23,498] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table
127.0.0.1 - - [17/Jan/2023:00:52:23 +0000] "POST /admin/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:23,498] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,499] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:23,499] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:23,499] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:23,501] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:23,501] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table], containing 4 entries.
INFO  [2023-01-17 00:52:23,501] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table], containing 4 entries.
WARN  [2023-01-17 00:52:23,502] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:23,502] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table.0
INFO  [2023-01-17 00:52:23,502] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.test_table.test_table.1
INFO  [2023-01-17 00:52:23,607] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,612] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,625] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:23,626] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:23,626] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:23,732] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test QUERY INIT
INFO  [2023-01-17 00:52:23,747] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:23,747] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[86e03951-16bd-4746-bbff-561c55a4702c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test))]]
INFO  [2023-01-17 00:52:23,750] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c
INFO  [2023-01-17 00:52:23,750] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c
INFO  [2023-01-17 00:52:23,751] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c] with 0 results within PT0.000751S
INFO  [2023-01-17 00:52:23,751] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c] with 2 results within PT0.001066S
127.0.0.1 - - [17/Jan/2023:00:52:23 +0000] "POST /api/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test/queries HTTP/1.1" 201 1198 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:23,751] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c, workerId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_e0f7dd25-ddeb-4fb7-8d5a-2096029cab54, startTime=2023-01-17T00:52:23.750603, finishTime=2023-01-17T00:52:23.751354) of size 0
INFO  [2023-01-17 00:52:23,752] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c, workerId=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test_6bdcfe91-7ada-45e1-a758-5fba1a997f67, startTime=2023-01-17T00:52:23.750590, finishTime=2023-01-17T00:52:23.751656) of size 2
INFO  [2023-01-17 00:52:23,752] com.bakdata.conquery.models.execution.ManagedExecution: DONE 86e03951-16bd-4746-bbff-561c55a4702c ManagedQuery within PT0.004616S
127.0.0.1 - - [17/Jan/2023:00:52:23 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test/queries/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c HTTP/1.1" 200 1585 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:23,777] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test], queryId=86e03951-16bd-4746-bbff-561c55a4702c, label=test_tree-Ä1	@§$, creationTime=2023-01-17T00:52:23.747504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2ac84e24[Count = 0], startTime=2023-01-17T00:52:23.747713, finishTime=2023-01-17T00:52:23.752329, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@565cbe20), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7ec44ede, com.bakdata.conquery.models.query.ColumnDescriptor@435c7928]) download on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:23,778] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test], queryId=86e03951-16bd-4746-bbff-561c55a4702c, label=test_tree-Ä1	@§$, creationTime=2023-01-17T00:52:23.747504, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2ac84e24[Count = 0], startTime=2023-01-17T00:52:23.747713, finishTime=2023-01-17T00:52:23.752329, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@565cbe20), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7ec44ede, com.bakdata.conquery.models.query.ColumnDescriptor@435c7928]) on dataset Dataset[label=null, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
127.0.0.1 - - [17/Jan/2023:00:52:23 +0000] "GET /api/datasets/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA%20Test/result/SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test.86e03951-16bd-4746-bbff-561c55a4702c.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:23,794] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test on 3 rows
INFO  [2023-01-17 00:52:23,794] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-17 00:52:23,795] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-17 00:52:23,795] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test, name=SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test]
INFO  [2023-01-17 00:52:23,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_e0f7dd25-ddeb-4fb7-8d5a-2096029cab54
INFO  [2023-01-17 00:52:23,795] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_6bdcfe91-7ada-45e1-a758-5fba1a997f67
INFO  [2023-01-17 00:52:23,893] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-17 00:52:23,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_e0f7dd25-ddeb-4fb7-8d5a-2096029cab54
INFO  [2023-01-17 00:52:23,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test_6bdcfe91-7ada-45e1-a758-5fba1a997f67
INFO  [2023-01-17 00:52:23,902] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA$20Test
INFO  [2023-01-17 00:52:23,902] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,032] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_TREECONCEPT_QUERY_SPECIAL_CHAR_DATA Test
INFO  [2023-01-17 00:52:24,032] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:24,032] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:24,032] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:24,033] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:24,033] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:24,033] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:24,033] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:24,035] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_b98f329d-9a0d-4ba1-9b81-7d9115107a2a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:24,035] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_b98f329d-9a0d-4ba1-9b81-7d9115107a2a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:24,035] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:24,035] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_edf2b363-c2f8-4bda-b770-2d70c7bad303 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:24,035] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_edf2b363-c2f8-4bda-b770-2d70c7bad303 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:24,035] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:24,039] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,139] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,145] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,146] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:24,146] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:24,265] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,373] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:24,373] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:24,373] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 89 B in total
INFO  [2023-01-17 00:52:24,373] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_VIRTUAL_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000399342sINFO  [2023-01-17 00:52:24,414] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:24,414] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:24,414] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@47c99aad)
INFO  [2023-01-17 00:52:24,417] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_VIRTUAL_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:24,417] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:24,417] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SIMPLE_VIRTUAL_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:24,437] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:24 +0000] "POST /admin/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SIMPLE_VIRTUAL_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:24,438] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,438] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:24,438] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:24,438] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:24,440] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:24,440] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1], containing 4 entries.
INFO  [2023-01-17 00:52:24,440] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1], containing 4 entries.
WARN  [2023-01-17 00:52:24,440] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:24,441] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:24,441] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:24,546] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,551] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,566] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,567] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:24,576] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:24,681] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SIMPLE_VIRTUAL_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:24,696] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:24,697] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[634374fd-4bb7-4004-8215-9f07ea987e4c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_VIRTUAL_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:24,700] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c
INFO  [2023-01-17 00:52:24,700] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c
INFO  [2023-01-17 00:52:24,701] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c] with 0 results within PT0.00086S
INFO  [2023-01-17 00:52:24,701] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c] with 2 results within PT0.001204S
127.0.0.1 - - [17/Jan/2023:00:52:24 +0000] "POST /api/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1326 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:24,702] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c, workerId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_b98f329d-9a0d-4ba1-9b81-7d9115107a2a, startTime=2023-01-17T00:52:24.700754, finishTime=2023-01-17T00:52:24.701614) of size 0
INFO  [2023-01-17 00:52:24,702] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c, workerId=SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.worker_SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test_edf2b363-c2f8-4bda-b770-2d70c7bad303, startTime=2023-01-17T00:52:24.700644, finishTime=2023-01-17T00:52:24.701848) of size 2
INFO  [2023-01-17 00:52:24,702] com.bakdata.conquery.models.execution.ManagedExecution: DONE 634374fd-4bb7-4004-8215-9f07ea987e4c ManagedQuery within PT0.005146S
127.0.0.1 - - [17/Jan/2023:00:52:24 +0000] "GET /api/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test/queries/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c HTTP/1.1" 200 1657 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:24,727] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test], queryId=634374fd-4bb7-4004-8215-9f07ea987e4c, label=geschlecht_select	@§$, creationTime=2023-01-17T00:52:24.697268, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@688eca2e[Count = 0], startTime=2023-01-17T00:52:24.697487, finishTime=2023-01-17T00:52:24.702633, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4cdf29c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_VIRTUAL_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@44b49c43, com.bakdata.conquery.models.query.ColumnDescriptor@1f57f985]) download on dataset Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:24,728] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test], queryId=634374fd-4bb7-4004-8215-9f07ea987e4c, label=geschlecht_select	@§$, creationTime=2023-01-17T00:52:24.697268, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@688eca2e[Count = 0], startTime=2023-01-17T00:52:24.697487, finishTime=2023-01-17T00:52:24.702633, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4cdf29c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SIMPLE_VIRTUAL_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@44b49c43, com.bakdata.conquery.models.query.ColumnDescriptor@1f57f985]) on dataset Dataset[label=null, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:24 +0000] "GET /api/datasets/SIMPLE_VIRTUAL_CONCEPT_QUERY%20Test/result/SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test.634374fd-4bb7-4004-8215-9f07ea987e4c.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 22
INFO  [2023-01-17 00:52:24,748] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SIMPLE_VIRTUAL_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:24,748] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:24,748] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:24,748] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SIMPLE_VIRTUAL_CONCEPT_QUERY Test, name=SIMPLE_VIRTUAL_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:24,749] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_b98f329d-9a0d-4ba1-9b81-7d9115107a2a
INFO  [2023-01-17 00:52:24,749] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_edf2b363-c2f8-4bda-b770-2d70c7bad303
INFO  [2023-01-17 00:52:24,847] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_b98f329d-9a0d-4ba1-9b81-7d9115107a2a
INFO  [2023-01-17 00:52:24,847] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SIMPLE_VIRTUAL_CONCEPT_QUERY Test_edf2b363-c2f8-4bda-b770-2d70c7bad303
INFO  [2023-01-17 00:52:24,847] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:24,947] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SIMPLE_VIRTUAL_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:24,947] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:24,981] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SIMPLE_VIRTUAL_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:24,981] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:24,982] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:24,982] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:24,983] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:24,983] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:24,983] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:24,983] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:24,985] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_09391b0e-4ecf-4a9a-ac77-42859c0ba7b4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:24,985] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_09391b0e-4ecf-4a9a-ac77-42859c0ba7b4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:24,985] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:24,985] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_813bae9c-d96a-44ab-a4cb-f38687e2e4c2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:24,985] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_813bae9c-d96a-44ab-a4cb-f38687e2e4c2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:24,985] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:24,989] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,088] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,095] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,096] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:25,096] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:25,210] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,318] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:25,318] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:25,318] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:52:25,319] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000370265sINFO  [2023-01-17 00:52:25,356] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:25,356] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@32003e0e)
INFO  [2023-01-17 00:52:25,356] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:25,361] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:25,361] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:25,361] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:25,409] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:25,410] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:25 +0000] "POST /admin/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:25,411] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:25,411] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:25,411] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:25,413] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:25,413] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:52:25,413] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:52:25,414] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:25,414] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:25,414] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:25,414] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:52:25,519] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,524] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,539] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,540] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:25,540] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:25,645] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:25,659] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:25,660] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1d0d554e-6496-4350-88f4-bdfa22816816] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:25,663] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816
INFO  [2023-01-17 00:52:25,663] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816
127.0.0.1 - - [17/Jan/2023:00:52:25 +0000] "POST /api/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1554 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:25,664] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816] with 1 results within PT0.001609S
INFO  [2023-01-17 00:52:25,665] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816] with 0 results within PT0.001729S
INFO  [2023-01-17 00:52:25,665] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816, workerId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_813bae9c-d96a-44ab-a4cb-f38687e2e4c2, startTime=2023-01-17T00:52:25.663296, finishTime=2023-01-17T00:52:25.664905) of size 1
INFO  [2023-01-17 00:52:25,665] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816, workerId=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_09391b0e-4ecf-4a9a-ac77-42859c0ba7b4, startTime=2023-01-17T00:52:25.663303, finishTime=2023-01-17T00:52:25.665032) of size 0
INFO  [2023-01-17 00:52:25,665] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1d0d554e-6496-4350-88f4-bdfa22816816 ManagedQuery within PT0.005597S
127.0.0.1 - - [17/Jan/2023:00:52:25 +0000] "GET /api/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816 HTTP/1.1" 200 1961 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:25,706] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=1d0d554e-6496-4350-88f4-bdfa22816816, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:25.659980, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@360be921[Count = 0], startTime=2023-01-17T00:52:25.660116, finishTime=2023-01-17T00:52:25.665713, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6f80db9d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@259d3d8a, com.bakdata.conquery.models.query.ColumnDescriptor@2041f288]) download on dataset Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:25,706] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=1d0d554e-6496-4350-88f4-bdfa22816816, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:25.659980, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@360be921[Count = 0], startTime=2023-01-17T00:52:25.660116, finishTime=2023-01-17T00:52:25.665713, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@6f80db9d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@259d3d8a, com.bakdata.conquery.models.query.ColumnDescriptor@2041f288]) on dataset Dataset[label=null, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:25 +0000] "GET /api/datasets/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.1d0d554e-6496-4350-88f4-bdfa22816816.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:25,708] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 2 rows
INFO  [2023-01-17 00:52:25,709] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:25,709] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:25,709] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:25,709] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_09391b0e-4ecf-4a9a-ac77-42859c0ba7b4
INFO  [2023-01-17 00:52:25,709] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_813bae9c-d96a-44ab-a4cb-f38687e2e4c2
INFO  [2023-01-17 00:52:25,786] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:25,790] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_09391b0e-4ecf-4a9a-ac77-42859c0ba7b4
INFO  [2023-01-17 00:52:25,800] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_813bae9c-d96a-44ab-a4cb-f38687e2e4c2
INFO  [2023-01-17 00:52:25,814] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:25,814] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,945] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:25,946] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:25,946] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:25,946] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:25,947] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:25,947] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:25,947] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:25,947] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:25,948] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:25,949] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_efafd659-c1b5-4d95-8153-4e9bd1d93a8e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:25,949] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_efafd659-c1b5-4d95-8153-4e9bd1d93a8e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:25,949] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:25,949] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_eeacaf36-4cb5-4206-955e-3a599d45cf46 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:25,949] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_eeacaf36-4cb5-4206-955e-3a599d45cf46 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:25,949] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:26,053] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,059] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,060] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:26,060] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:26,185] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,305] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:26,305] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:26,305] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:52:26,305] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000364289sINFO  [2023-01-17 00:52:26,342] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=8, sum=8, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:26,342] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:26,342] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=13828, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7bfe847a)
INFO  [2023-01-17 00:52:26,345] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:26,345] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:26,345] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:26,364] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:26 +0000] "POST /admin/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:26,364] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,366] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:26,367] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:26,367] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:26,370] com.bakdata.conquery.models.jobs.ImportJob: Start sending 3 Buckets
INFO  [2023-01-17 00:52:26,370] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:52:26,370] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:52:26,371] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:26,371] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:26,372] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.2
INFO  [2023-01-17 00:52:26,372] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:26,477] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,482] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,498] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,498] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:26,499] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:26,604] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:26,620] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:26,620] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[d41f0ff6-35c7-40f7-9456-95d93c9a5e3e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:26,625] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e
INFO  [2023-01-17 00:52:26,625] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e
127.0.0.1 - - [17/Jan/2023:00:52:26 +0000] "POST /api/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1632 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:26,627] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e] with 3 results within PT0.001558S
INFO  [2023-01-17 00:52:26,627] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e] with 4 results within PT0.001921S
INFO  [2023-01-17 00:52:26,628] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e, workerId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_efafd659-c1b5-4d95-8153-4e9bd1d93a8e, startTime=2023-01-17T00:52:26.625927, finishTime=2023-01-17T00:52:26.627485) of size 3
INFO  [2023-01-17 00:52:26,628] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e, workerId=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test_eeacaf36-4cb5-4206-955e-3a599d45cf46, startTime=2023-01-17T00:52:26.625783, finishTime=2023-01-17T00:52:26.627704) of size 4
INFO  [2023-01-17 00:52:26,628] com.bakdata.conquery.models.execution.ManagedExecution: DONE d41f0ff6-35c7-40f7-9456-95d93c9a5e3e ManagedQuery within PT0.007366S
127.0.0.1 - - [17/Jan/2023:00:52:26 +0000] "GET /api/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test/queries/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e HTTP/1.1" 200 2074 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:26,651] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=d41f0ff6-35c7-40f7-9456-95d93c9a5e3e, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:26.620805, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@32dba654[Count = 0], startTime=2023-01-17T00:52:26.621014, finishTime=2023-01-17T00:52:26.628380, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4863edcf), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5850e1bb, com.bakdata.conquery.models.query.ColumnDescriptor@2fe86b66]) download on dataset Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:26,651] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test], queryId=d41f0ff6-35c7-40f7-9456-95d93c9a5e3e, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:26.620805, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@32dba654[Count = 0], startTime=2023-01-17T00:52:26.621014, finishTime=2023-01-17T00:52:26.628380, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4863edcf), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=7, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5850e1bb, com.bakdata.conquery.models.query.ColumnDescriptor@2fe86b66]) on dataset Dataset[label=null, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:26 +0000] "GET /api/datasets/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY%20Test/result/SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test.d41f0ff6-35c7-40f7-9456-95d93c9a5e3e.csv?pretty=false HTTP/1.1" 200 48 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:52:26,668] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test on 8 rows
INFO  [2023-01-17 00:52:26,669] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:26,669] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:26,669] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test, name=SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:26,669] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_efafd659-c1b5-4d95-8153-4e9bd1d93a8e
INFO  [2023-01-17 00:52:26,669] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_eeacaf36-4cb5-4206-955e-3a599d45cf46
INFO  [2023-01-17 00:52:26,747] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:26,748] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_efafd659-c1b5-4d95-8153-4e9bd1d93a8e
INFO  [2023-01-17 00:52:26,748] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test_eeacaf36-4cb5-4206-955e-3a599d45cf46
INFO  [2023-01-17 00:52:26,772] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:26,772] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:26,905] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SINGLE_SELECT_NEGATION_DATE_RESTRICTION_OR_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:26,905] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:26,905] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:26,905] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:26,906] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:26,906] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:26,906] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:26,906] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:26,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_446952e7-b4bd-4c0b-83a7-a4f0a3cb0a62 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:26,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_446952e7-b4bd-4c0b-83a7-a4f0a3cb0a62 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:26,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:26,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_322942ac-6abc-4539-af56-771da038247c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:26,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_322942ac-6abc-4539-af56-771da038247c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:26,908] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:26,912] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,012] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,019] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,019] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:27,019] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
INFO  [2023-01-17 00:52:27,140] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,249] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:27,250] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:27,250] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 197 B in total
INFO  [2023-01-17 00:52:27,250] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000404166sINFO  [2023-01-17 00:52:27,291] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=13, min=2, average=2.166667, max=3}
INFO  [2023-01-17 00:52:27,291] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=4), subType=IntegerParser(super=Parser(lines=13, nullLines=4), minValue=14246, maxValue=14642), dateReader=com.bakdata.conquery.util.DateReader@6fcf0171)
INFO  [2023-01-17 00:52:27,291] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=13, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:27,293] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:27,293] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:27,293] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_EMPTY_DATE_CONCEPT_QUERY Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:27,311] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:27 +0000] "POST /admin/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SUM_EMPTY_DATE_CONCEPT_QUERY+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:27,311] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,312] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:27,312] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:27,312] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:27,314] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:27,315] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 13 entries.
INFO  [2023-01-17 00:52:27,315] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1], containing 13 entries.
WARN  [2023-01-17 00:52:27,316] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:27,316] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.0
INFO  [2023-01-17 00:52:27,316] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.table1.table1.1
INFO  [2023-01-17 00:52:27,422] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,427] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,442] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,443] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:27,443] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:27,549] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_EMPTY_DATE_CONCEPT_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:27,564] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:27,565] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[878e303a-a869-4211-9a52-840da3685e40] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_EMPTY_DATE_CONCEPT_QUERY Test))]]
INFO  [2023-01-17 00:52:27,570] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40
INFO  [2023-01-17 00:52:27,570] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40
127.0.0.1 - - [17/Jan/2023:00:52:27 +0000] "POST /api/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries HTTP/1.1" 201 1406 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:27,572] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40] with 1 results within PT0.001651S
INFO  [2023-01-17 00:52:27,572] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40] with 1 results within PT0.001636S
INFO  [2023-01-17 00:52:27,573] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40, workerId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_446952e7-b4bd-4c0b-83a7-a4f0a3cb0a62, startTime=2023-01-17T00:52:27.570688, finishTime=2023-01-17T00:52:27.572339) of size 1
INFO  [2023-01-17 00:52:27,573] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40, workerId=SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.worker_SUM_EMPTY_DATE_CONCEPT_QUERY$20Test_322942ac-6abc-4539-af56-771da038247c, startTime=2023-01-17T00:52:27.570738, finishTime=2023-01-17T00:52:27.572374) of size 1
INFO  [2023-01-17 00:52:27,573] com.bakdata.conquery.models.execution.ManagedExecution: DONE 878e303a-a869-4211-9a52-840da3685e40 ManagedQuery within PT0.007939S
127.0.0.1 - - [17/Jan/2023:00:52:27 +0000] "GET /api/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test/queries/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40 HTTP/1.1" 200 1737 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:27,596] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=878e303a-a869-4211-9a52-840da3685e40, label=vs	@§$, creationTime=2023-01-17T00:52:27.564980, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6760ac04[Count = 0], startTime=2023-01-17T00:52:27.565176, finishTime=2023-01-17T00:52:27.573115, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@bf9f6b1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5db31a61, com.bakdata.conquery.models.query.ColumnDescriptor@358d8300]) download on dataset Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:27,596] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test], queryId=878e303a-a869-4211-9a52-840da3685e40, label=vs	@§$, creationTime=2023-01-17T00:52:27.564980, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6760ac04[Count = 0], startTime=2023-01-17T00:52:27.565176, finishTime=2023-01-17T00:52:27.573115, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@bf9f6b1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_EMPTY_DATE_CONCEPT_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5db31a61, com.bakdata.conquery.models.query.ColumnDescriptor@358d8300]) on dataset Dataset[label=null, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:27 +0000] "GET /api/datasets/SUM_EMPTY_DATE_CONCEPT_QUERY%20Test/result/SUM_EMPTY_DATE_CONCEPT_QUERY$20Test.878e303a-a869-4211-9a52-840da3685e40.csv?pretty=false HTTP/1.1" 200 114 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:52:27,614] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SUM_EMPTY_DATE_CONCEPT_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:27,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:27,615] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:27,615] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_EMPTY_DATE_CONCEPT_QUERY Test, name=SUM_EMPTY_DATE_CONCEPT_QUERY Test]
INFO  [2023-01-17 00:52:27,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_446952e7-b4bd-4c0b-83a7-a4f0a3cb0a62
INFO  [2023-01-17 00:52:27,615] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_322942ac-6abc-4539-af56-771da038247c
INFO  [2023-01-17 00:52:27,713] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:27,713] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_322942ac-6abc-4539-af56-771da038247c
INFO  [2023-01-17 00:52:27,714] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_EMPTY_DATE_CONCEPT_QUERY Test_446952e7-b4bd-4c0b-83a7-a4f0a3cb0a62
INFO  [2023-01-17 00:52:27,717] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_EMPTY_DATE_CONCEPT_QUERY$20Test
INFO  [2023-01-17 00:52:27,717] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,848] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_EMPTY_DATE_CONCEPT_QUERY Test
INFO  [2023-01-17 00:52:27,849] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test TABLE_EXPORT Test
INFO  [2023-01-17 00:52:27,849] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:27,849] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:27,850] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-17 00:52:27,850] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-17 00:52:27,850] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:27,850] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:27,852] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_17cf6c94-20f8-4427-b4b4-f50d9071b069 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:27,852] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_17cf6c94-20f8-4427-b4b4-f50d9071b069 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:27,852] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:27,852] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_65ca198c-c6e5-4f1c-b4c5-bfedb0990704 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:27,852] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_65ca198c-c6e5-4f1c-b4c5-bfedb0990704 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:27,852] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:27,856] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,956] com.bakdata.conquery.resources.admin.rest.AdminDatasetProcessor: Received new SecondaryId[TABLE_EXPORT$20Test.sid]
INFO  [2023-01-17 00:52:27,958] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:27,958] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId TABLE_EXPORT$20Test.sid
INFO  [2023-01-17 00:52:27,958] com.bakdata.conquery.models.messages.namespaces.specific.UpdateSecondaryId: Received update of SecondaryId TABLE_EXPORT$20Test.sid
INFO  [2023-01-17 00:52:28,065] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,065] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table1
INFO  [2023-01-17 00:52:28,065] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table2
INFO  [2023-01-17 00:52:28,065] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table1
INFO  [2023-01-17 00:52:28,065] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table TABLE_EXPORT$20Test.table2
INFO  [2023-01-17 00:52:28,182] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,292] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:28,292] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:28,292] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:28,292] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 433 B in total
INFO  [2023-01-17 00:52:28,292] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TABLE_EXPORT Test/table1.import.json, tag=Optional.empty)
████████████████████████▌                         ▌  49%	est. time remaining: 0.042014389sINFO  [2023-01-17 00:52:28,333] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-17 00:52:28,333] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=1.1920928955078125E-7)
INFO  [2023-01-17 00:52:28,333] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:28,333] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@5e675d49), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14790, maxValue=16982), dateReader=com.bakdata.conquery.util.DateReader@77cccef4), dateReader=com.bakdata.conquery.util.DateReader@6aaff3fc, onlyQuarters=false, maxValue=16982, minValue=14790, anyOpen=false)
INFO  [2023-01-17 00:52:28,336] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TABLE_EXPORT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:28,336] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TABLE_EXPORT Test/table2.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000622203sINFO  [2023-01-17 00:52:28,355] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=2, sum=6, min=1, average=3.000000, max=5}
INFO  [2023-01-17 00:52:28,355] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=6, nullLines=0), requiredPrecision=4.9E-324, floatULP=9.5367431640625E-7)
INFO  [2023-01-17 00:52:28,355] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sid] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=f_, suffix=)
INFO  [2023-01-17 00:52:28,355] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@4afe31ce), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=18443, maxValue=20634), dateReader=com.bakdata.conquery.util.DateReader@41e9c6ce), dateReader=com.bakdata.conquery.util.DateReader@722a60b3, onlyQuarters=false, maxValue=20634, minValue=18443, anyOpen=false)
INFO  [2023-01-17 00:52:28,358] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TABLE_EXPORT Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:28,358] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 2 Jobs:
INFO  [2023-01-17 00:52:28,358] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TABLE_EXPORT Test/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:28,358] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_TABLE_EXPORT Test/table2.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:28,379] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into TABLE_EXPORT$20Test.table1
127.0.0.1 - - [17/Jan/2023:00:52:28 +0000] "POST /admin/datasets/TABLE_EXPORT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_TABLE_EXPORT+Test%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:28,381] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:28,382] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:28,382] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:28,386] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:28,386] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table1.table1], containing 6 entries.
INFO  [2023-01-17 00:52:28,386] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table1.table1], containing 6 entries.
WARN  [2023-01-17 00:52:28,387] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:28,387] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TABLE_EXPORT$20Test.table1.table1.0
INFO  [2023-01-17 00:52:28,410] com.bakdata.conquery.models.jobs.ImportJob: Importing table2 into TABLE_EXPORT$20Test.table2
INFO  [2023-01-17 00:52:28,410] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
127.0.0.1 - - [17/Jan/2023:00:52:28 +0000] "POST /admin/datasets/TABLE_EXPORT%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_TABLE_EXPORT+Test%2Ftable2.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:28,410] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,410] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:28,410] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:28,411] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:28,411] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table2.table2], containing 6 entries.
INFO  [2023-01-17 00:52:28,411] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[TABLE_EXPORT$20Test.table2.table2], containing 6 entries.
WARN  [2023-01-17 00:52:28,411] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:28,411] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received TABLE_EXPORT$20Test.table2.table2.0
INFO  [2023-01-17 00:52:28,527] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,532] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,552] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,553] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:28,659] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: TABLE_EXPORT Test QUERY INIT
INFO  [2023-01-17 00:52:28,678] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[TABLE_EXPORT$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:28,679] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7362badb-cb79-4aa6-89fe-b8fe6214a3e4] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_TABLE_EXPORT Test))]]
INFO  [2023-01-17 00:52:28,684] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started TableExportQuery TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4
INFO  [2023-01-17 00:52:28,684] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started TableExportQuery TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4
WARN  [2023-01-17 00:52:28,684] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:28,684] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4] with 0 results within PT0.00027S
127.0.0.1 - - [17/Jan/2023:00:52:28 +0000] "POST /api/datasets/TABLE_EXPORT$20Test/queries HTTP/1.1" 201 2067 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:28,685] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4, workerId=TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_17cf6c94-20f8-4427-b4b4-f50d9071b069, startTime=2023-01-17T00:52:28.684678, finishTime=2023-01-17T00:52:28.684948) of size 0
INFO  [2023-01-17 00:52:28,685] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4] with 2 results within PT0.001202S
INFO  [2023-01-17 00:52:28,686] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4, workerId=TABLE_EXPORT$20Test.worker_TABLE_EXPORT$20Test_65ca198c-c6e5-4f1c-b4c5-bfedb0990704, startTime=2023-01-17T00:52:28.684688, finishTime=2023-01-17T00:52:28.685890) of size 2
INFO  [2023-01-17 00:52:28,687] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7362badb-cb79-4aa6-89fe-b8fe6214a3e4 ManagedQuery within PT0.007753S
127.0.0.1 - - [17/Jan/2023:00:52:28 +0000] "GET /api/datasets/TABLE_EXPORT$20Test/queries/TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4 HTTP/1.1" 200 2334 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:28,730] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TABLE_EXPORT Test], queryId=7362badb-cb79-4aa6-89fe-b8fe6214a3e4, label=concept	@§$, creationTime=2023-01-17T00:52:28.678923, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@fe7b279[Count = 0], startTime=2023-01-17T00:52:28.679245, finishTime=2023-01-17T00:52:28.686998, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@11f5e7c9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TABLE_EXPORT Test)), query=com.bakdata.conquery.apiv1.query.TableExportQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2eaacea0, com.bakdata.conquery.models.query.ColumnDescriptor@10123d03, com.bakdata.conquery.models.query.ColumnDescriptor@22543102, com.bakdata.conquery.models.query.ColumnDescriptor@1ca1512e, com.bakdata.conquery.models.query.ColumnDescriptor@648643d4]) download on dataset Dataset[label=null, name=TABLE_EXPORT Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:28,730] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=TABLE_EXPORT Test], queryId=7362badb-cb79-4aa6-89fe-b8fe6214a3e4, label=concept	@§$, creationTime=2023-01-17T00:52:28.678923, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@fe7b279[Count = 0], startTime=2023-01-17T00:52:28.679245, finishTime=2023-01-17T00:52:28.686998, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@11f5e7c9), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_TABLE_EXPORT Test)), query=com.bakdata.conquery.apiv1.query.TableExportQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2eaacea0, com.bakdata.conquery.models.query.ColumnDescriptor@10123d03, com.bakdata.conquery.models.query.ColumnDescriptor@22543102, com.bakdata.conquery.models.query.ColumnDescriptor@1ca1512e, com.bakdata.conquery.models.query.ColumnDescriptor@648643d4]) on dataset Dataset[label=null, name=TABLE_EXPORT Test]
127.0.0.1 - - [17/Jan/2023:00:52:28 +0000] "GET /api/datasets/TABLE_EXPORT%20Test/result/TABLE_EXPORT$20Test.7362badb-cb79-4aa6-89fe-b8fe6214a3e4.csv?pretty=false HTTP/1.1" 200 297 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:28,733] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest TABLE_EXPORT Test on 8 rows
INFO  [2023-01-17 00:52:28,733] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast TABLE_EXPORT Test
INFO  [2023-01-17 00:52:28,733] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-17 00:52:28,733] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TABLE_EXPORT Test_17cf6c94-20f8-4427-b4b4-f50d9071b069
INFO  [2023-01-17 00:52:28,733] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=TABLE_EXPORT Test, name=TABLE_EXPORT Test]
INFO  [2023-01-17 00:52:28,733] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_TABLE_EXPORT Test_65ca198c-c6e5-4f1c-b4c5-bfedb0990704
INFO  [2023-01-17 00:52:28,751] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow TABLE_EXPORT Test
INFO  [2023-01-17 00:52:28,751] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TABLE_EXPORT Test_65ca198c-c6e5-4f1c-b4c5-bfedb0990704
INFO  [2023-01-17 00:52:28,751] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_TABLE_EXPORT Test_17cf6c94-20f8-4427-b4b4-f50d9071b069
INFO  [2023-01-17 00:52:28,827] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of TABLE_EXPORT$20Test
INFO  [2023-01-17 00:52:28,827] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,859] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test TABLE_EXPORT Test
INFO  [2023-01-17 00:52:28,859] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before (with Aggregation)
INFO  [2023-01-17 00:52:28,859] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:28,859] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:28,860] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-17 00:52:28,860] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-17 00:52:28,861] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:28,861] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_b183ed37-c402-4d20-94b9-e1712a0402d5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_b183ed37-c402-4d20-94b9-e1712a0402d5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_cce5cce8-d024-410a-ae01-0d30ab574c6c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_cce5cce8-d024-410a-ae01-0d30ab574c6c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:28,862] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:28,966] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,973] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:28,973] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20(with$20Aggregation).table
INFO  [2023-01-17 00:52:28,973] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20(with$20Aggregation).table
INFO  [2023-01-17 00:52:29,092] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,201] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:29,201] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:29,201] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 153 B in total
INFO  [2023-01-17 00:52:29,201] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before (with Aggregation)/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000395307sINFO  [2023-01-17 00:52:29,241] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=2, average=2.000000, max=2}
INFO  [2023-01-17 00:52:29,242] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:29,242] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14975, maxValue=14980), dateReader=com.bakdata.conquery.util.DateReader@10577920)
INFO  [2023-01-17 00:52:29,244] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before (with Aggregation)/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:29,244] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:29,244] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before (with Aggregation)/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:29,261] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Temporal$20Before$20(with$20Aggregation).table
127.0.0.1 - - [17/Jan/2023:00:52:29 +0000] "POST /admin/datasets/Temporal%20Before%20(with%20Aggregation)/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Temporal+Before+%28with+Aggregation%29%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:29,262] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,263] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:29,264] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:29,264] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:29,266] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:29,266] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20(with$20Aggregation).table.table], containing 8 entries.
INFO  [2023-01-17 00:52:29,266] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20(with$20Aggregation).table.table], containing 8 entries.
WARN  [2023-01-17 00:52:29,268] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:29,268] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20(with$20Aggregation).table.table.0
INFO  [2023-01-17 00:52:29,268] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20(with$20Aggregation).table.table.1
INFO  [2023-01-17 00:52:29,373] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,378] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,395] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,396] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:29,396] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:29,501] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before (with Aggregation) QUERY INIT
INFO  [2023-01-17 00:52:29,518] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before$20(with$20Aggregation)] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:29,519] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[29b6c6bf-d641-41e3-8640-0af78ea54a65] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before (with Aggregation)))]]
INFO  [2023-01-17 00:52:29,523] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65
INFO  [2023-01-17 00:52:29,524] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65
127.0.0.1 - - [17/Jan/2023:00:52:29 +0000] "POST /api/datasets/Temporal$20Before$20(with$20Aggregation)/queries HTTP/1.1" 201 2251 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:29,525] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65] with 0 results within PT0.001174S
INFO  [2023-01-17 00:52:29,525] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65] with 1 results within PT0.002018S
INFO  [2023-01-17 00:52:29,526] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65, workerId=Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_cce5cce8-d024-410a-ae01-0d30ab574c6c, startTime=2023-01-17T00:52:29.524393, finishTime=2023-01-17T00:52:29.525567) of size 0
INFO  [2023-01-17 00:52:29,526] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65, workerId=Temporal$20Before$20(with$20Aggregation).worker_Temporal$20Before$20(with$20Aggregation)_b183ed37-c402-4d20-94b9-e1712a0402d5, startTime=2023-01-17T00:52:29.523841, finishTime=2023-01-17T00:52:29.525859) of size 1
INFO  [2023-01-17 00:52:29,526] com.bakdata.conquery.models.execution.ManagedExecution: DONE 29b6c6bf-d641-41e3-8640-0af78ea54a65 ManagedQuery within PT0.007207S
127.0.0.1 - - [17/Jan/2023:00:52:29 +0000] "GET /api/datasets/Temporal$20Before$20(with$20Aggregation)/queries/Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65 HTTP/1.1" 200 2602 "-" "Conquery (test client)" 25
INFO  [2023-01-17 00:52:29,571] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before (with Aggregation)], queryId=29b6c6bf-d641-41e3-8640-0af78ea54a65, label=concept	@§$, creationTime=2023-01-17T00:52:29.519261, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4f779de8[Count = 0], startTime=2023-01-17T00:52:29.519466, finishTime=2023-01-17T00:52:29.526673, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@74db8f70), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before (with Aggregation))), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@40991524, com.bakdata.conquery.models.query.ColumnDescriptor@39a53528, com.bakdata.conquery.models.query.ColumnDescriptor@4a4e6729, com.bakdata.conquery.models.query.ColumnDescriptor@6246d11f]) download on dataset Dataset[label=null, name=Temporal Before (with Aggregation)] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:29,571] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before (with Aggregation)], queryId=29b6c6bf-d641-41e3-8640-0af78ea54a65, label=concept	@§$, creationTime=2023-01-17T00:52:29.519261, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4f779de8[Count = 0], startTime=2023-01-17T00:52:29.519466, finishTime=2023-01-17T00:52:29.526673, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@74db8f70), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before (with Aggregation))), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@40991524, com.bakdata.conquery.models.query.ColumnDescriptor@39a53528, com.bakdata.conquery.models.query.ColumnDescriptor@4a4e6729, com.bakdata.conquery.models.query.ColumnDescriptor@6246d11f]) on dataset Dataset[label=null, name=Temporal Before (with Aggregation)]
127.0.0.1 - - [17/Jan/2023:00:52:29 +0000] "GET /api/datasets/Temporal%20Before%20(with%20Aggregation)/result/Temporal$20Before$20(with$20Aggregation).29b6c6bf-d641-41e3-8640-0af78ea54a65.csv?pretty=false HTTP/1.1" 200 73 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:29,573] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before (with Aggregation) on 2 rows
INFO  [2023-01-17 00:52:29,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before (with Aggregation)
INFO  [2023-01-17 00:52:29,574] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-17 00:52:29,574] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before (with Aggregation), name=Temporal Before (with Aggregation)]
INFO  [2023-01-17 00:52:29,574] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before (with Aggregation)_cce5cce8-d024-410a-ae01-0d30ab574c6c
INFO  [2023-01-17 00:52:29,574] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before (with Aggregation)_b183ed37-c402-4d20-94b9-e1712a0402d5
INFO  [2023-01-17 00:52:29,661] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before (with Aggregation)
INFO  [2023-01-17 00:52:29,662] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before (with Aggregation)_cce5cce8-d024-410a-ae01-0d30ab574c6c
INFO  [2023-01-17 00:52:29,662] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before (with Aggregation)_b183ed37-c402-4d20-94b9-e1712a0402d5
INFO  [2023-01-17 00:52:29,668] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before$20(with$20Aggregation)
INFO  [2023-01-17 00:52:29,668] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,801] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before (with Aggregation)
INFO  [2023-01-17 00:52:29,801] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before
INFO  [2023-01-17 00:52:29,801] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:29,801] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:29,802] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-17 00:52:29,803] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-17 00:52:29,803] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:29,803] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:29,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before.worker_Temporal$20Before_87fbc8f5-a2a3-4942-8544-051ac7c1d58d are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:29,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before.worker_Temporal$20Before_87fbc8f5-a2a3-4942-8544-051ac7c1d58d are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:29,804] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:29,805] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before.worker_Temporal$20Before_aebcf3fa-6f63-41ff-8392-7a5cfe0e5f26 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:29,805] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before.worker_Temporal$20Before_aebcf3fa-6f63-41ff-8392-7a5cfe0e5f26 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:29,805] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:29,809] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,908] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,915] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:29,915] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before.table
INFO  [2023-01-17 00:52:29,915] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before.table
INFO  [2023-01-17 00:52:30,028] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,140] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:30,140] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:30,140] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 153 B in total
INFO  [2023-01-17 00:52:30,140] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000414234sINFO  [2023-01-17 00:52:30,182] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=2, average=2.000000, max=2}
INFO  [2023-01-17 00:52:30,182] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:30,182] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14975, maxValue=14980), dateReader=com.bakdata.conquery.util.DateReader@74bef376)
INFO  [2023-01-17 00:52:30,184] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:30,184] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:30,184] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:30,204] com.bakdata.conquery.models.jobs.ImportJob: Importing table into Temporal$20Before.table
127.0.0.1 - - [17/Jan/2023:00:52:30 +0000] "POST /admin/datasets/Temporal%20Before/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Temporal+Before%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:52:30,205] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,205] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:30,206] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:30,206] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:30,207] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:30,207] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before.table.table], containing 8 entries.
INFO  [2023-01-17 00:52:30,207] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before.table.table], containing 8 entries.
WARN  [2023-01-17 00:52:30,208] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:30,208] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before.table.table.1
INFO  [2023-01-17 00:52:30,208] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before.table.table.0
INFO  [2023-01-17 00:52:30,313] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,318] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,332] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,333] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:30,333] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:30,439] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before QUERY INIT
INFO  [2023-01-17 00:52:30,454] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:30,455] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5a00b55b-4497-42cf-9dd1-47d990c68386] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before))]]
INFO  [2023-01-17 00:52:30,458] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386
INFO  [2023-01-17 00:52:30,458] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386
INFO  [2023-01-17 00:52:30,459] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386] with 0 results within PT0.000854S
127.0.0.1 - - [17/Jan/2023:00:52:30 +0000] "POST /api/datasets/Temporal$20Before/queries HTTP/1.1" 201 1604 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:30,460] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386, workerId=Temporal$20Before.worker_Temporal$20Before_aebcf3fa-6f63-41ff-8392-7a5cfe0e5f26, startTime=2023-01-17T00:52:30.458827, finishTime=2023-01-17T00:52:30.459681) of size 0
INFO  [2023-01-17 00:52:30,460] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386] with 1 results within PT0.001389S
INFO  [2023-01-17 00:52:30,460] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386, workerId=Temporal$20Before.worker_Temporal$20Before_87fbc8f5-a2a3-4942-8544-051ac7c1d58d, startTime=2023-01-17T00:52:30.458847, finishTime=2023-01-17T00:52:30.460236) of size 1
INFO  [2023-01-17 00:52:30,460] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5a00b55b-4497-42cf-9dd1-47d990c68386 ManagedQuery within PT0.005629S
127.0.0.1 - - [17/Jan/2023:00:52:30 +0000] "GET /api/datasets/Temporal$20Before/queries/Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386 HTTP/1.1" 200 1863 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:30,488] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before], queryId=5a00b55b-4497-42cf-9dd1-47d990c68386, label=concept	@§$, creationTime=2023-01-17T00:52:30.454994, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@64481b0a[Count = 0], startTime=2023-01-17T00:52:30.455189, finishTime=2023-01-17T00:52:30.460818, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@22b3c464), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2722d4d4, com.bakdata.conquery.models.query.ColumnDescriptor@e0adeab]) download on dataset Dataset[label=null, name=Temporal Before] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:30,488] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before], queryId=5a00b55b-4497-42cf-9dd1-47d990c68386, label=concept	@§$, creationTime=2023-01-17T00:52:30.454994, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@64481b0a[Count = 0], startTime=2023-01-17T00:52:30.455189, finishTime=2023-01-17T00:52:30.460818, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@22b3c464), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2722d4d4, com.bakdata.conquery.models.query.ColumnDescriptor@e0adeab]) on dataset Dataset[label=null, name=Temporal Before]
127.0.0.1 - - [17/Jan/2023:00:52:30 +0000] "GET /api/datasets/Temporal%20Before/result/Temporal$20Before.5a00b55b-4497-42cf-9dd1-47d990c68386.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:30,507] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before on 2 rows
INFO  [2023-01-17 00:52:30,507] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before
INFO  [2023-01-17 00:52:30,507] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-17 00:52:30,507] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before, name=Temporal Before]
INFO  [2023-01-17 00:52:30,507] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before_aebcf3fa-6f63-41ff-8392-7a5cfe0e5f26
INFO  [2023-01-17 00:52:30,508] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before_87fbc8f5-a2a3-4942-8544-051ac7c1d58d
INFO  [2023-01-17 00:52:30,605] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before_aebcf3fa-6f63-41ff-8392-7a5cfe0e5f26
INFO  [2023-01-17 00:52:30,605] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before
INFO  [2023-01-17 00:52:30,605] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before_87fbc8f5-a2a3-4942-8544-051ac7c1d58d
INFO  [2023-01-17 00:52:30,608] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before
INFO  [2023-01-17 00:52:30,608] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,739] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before
INFO  [2023-01-17 00:52:30,739] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before or Same
INFO  [2023-01-17 00:52:30,739] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:30,739] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:30,740] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-17 00:52:30,740] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-17 00:52:30,740] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:30,740] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:30,742] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_620fbec3-ec62-4954-a9c0-05f839cafa36 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:30,742] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_620fbec3-ec62-4954-a9c0-05f839cafa36 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:30,742] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:30,742] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_544b9664-7e11-45fc-add9-310c9c072095 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:30,742] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_544b9664-7e11-45fc-add9-310c9c072095 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:30,742] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:30,746] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,846] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,853] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:30,853] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Same.table1
INFO  [2023-01-17 00:52:30,853] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Same.table1
INFO  [2023-01-17 00:52:30,973] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,081] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:31,081] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:31,081] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 231 B in total
INFO  [2023-01-17 00:52:31,081] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before or Same/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000469827sINFO  [2023-01-17 00:52:31,129] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=12, min=2, average=2.400000, max=4}
INFO  [2023-01-17 00:52:31,129] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=12, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:31,129] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=14442, maxValue=15655), dateReader=com.bakdata.conquery.util.DateReader@6594387b)
INFO  [2023-01-17 00:52:31,132] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before or Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:31,132] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:31,132] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before or Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:31,152] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Before$20or$20Same.table1
INFO  [2023-01-17 00:52:31,152] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:31 +0000] "POST /admin/datasets/Temporal%20Before%20or%20Same/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Temporal+Before+or+Same%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:31,154] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:31,155] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:31,155] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:31,158] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:31,158] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Same.table1.table1], containing 12 entries.
INFO  [2023-01-17 00:52:31,158] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Same.table1.table1], containing 12 entries.
WARN  [2023-01-17 00:52:31,160] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:31,160] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Same.table1.table1.0
INFO  [2023-01-17 00:52:31,160] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Same.table1.table1.1
INFO  [2023-01-17 00:52:31,266] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,271] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,287] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,287] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:31,287] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:31,392] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before or Same QUERY INIT
INFO  [2023-01-17 00:52:31,407] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before$20or$20Same] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:31,408] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[4b272d8f-fd38-48af-bf14-ba00a8330f86] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Same))]]
INFO  [2023-01-17 00:52:31,411] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86
INFO  [2023-01-17 00:52:31,412] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86
127.0.0.1 - - [17/Jan/2023:00:52:31 +0000] "POST /api/datasets/Temporal$20Before$20or$20Same/queries HTTP/1.1" 201 1848 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:31,413] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86] with 0 results within PT0.001589S
INFO  [2023-01-17 00:52:31,413] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86] with 3 results within PT0.00148S
INFO  [2023-01-17 00:52:31,413] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86, workerId=Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_620fbec3-ec62-4954-a9c0-05f839cafa36, startTime=2023-01-17T00:52:31.411673, finishTime=2023-01-17T00:52:31.413262) of size 0
INFO  [2023-01-17 00:52:31,414] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86, workerId=Temporal$20Before$20or$20Same.worker_Temporal$20Before$20or$20Same_544b9664-7e11-45fc-add9-310c9c072095, startTime=2023-01-17T00:52:31.412081, finishTime=2023-01-17T00:52:31.413561) of size 3
INFO  [2023-01-17 00:52:31,414] com.bakdata.conquery.models.execution.ManagedExecution: DONE 4b272d8f-fd38-48af-bf14-ba00a8330f86 ManagedQuery within PT0.006137S
127.0.0.1 - - [17/Jan/2023:00:52:31 +0000] "GET /api/datasets/Temporal$20Before$20or$20Same/queries/Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86 HTTP/1.1" 200 2155 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:31,437] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Same], queryId=4b272d8f-fd38-48af-bf14-ba00a8330f86, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:31.407824, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2a2950c2[Count = 0], startTime=2023-01-17T00:52:31.408044, finishTime=2023-01-17T00:52:31.414181, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1ff5fb2b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@533f827e, com.bakdata.conquery.models.query.ColumnDescriptor@49e97ff6]) download on dataset Dataset[label=null, name=Temporal Before or Same] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:31,438] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Same], queryId=4b272d8f-fd38-48af-bf14-ba00a8330f86, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:31.407824, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@2a2950c2[Count = 0], startTime=2023-01-17T00:52:31.408044, finishTime=2023-01-17T00:52:31.414181, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1ff5fb2b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@533f827e, com.bakdata.conquery.models.query.ColumnDescriptor@49e97ff6]) on dataset Dataset[label=null, name=Temporal Before or Same]
127.0.0.1 - - [17/Jan/2023:00:52:31 +0000] "GET /api/datasets/Temporal%20Before%20or%20Same/result/Temporal$20Before$20or$20Same.4b272d8f-fd38-48af-bf14-ba00a8330f86.csv?pretty=false HTTP/1.1" 200 115 "-" "Conquery (test client)" 21
INFO  [2023-01-17 00:52:31,457] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before or Same on 4 rows
INFO  [2023-01-17 00:52:31,457] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before or Same
INFO  [2023-01-17 00:52:31,458] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-17 00:52:31,458] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Same, name=Temporal Before or Same]
INFO  [2023-01-17 00:52:31,458] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Same_620fbec3-ec62-4954-a9c0-05f839cafa36
INFO  [2023-01-17 00:52:31,458] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Same_544b9664-7e11-45fc-add9-310c9c072095
INFO  [2023-01-17 00:52:31,556] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before or Same
INFO  [2023-01-17 00:52:31,556] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Same_544b9664-7e11-45fc-add9-310c9c072095
INFO  [2023-01-17 00:52:31,556] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Same_620fbec3-ec62-4954-a9c0-05f839cafa36
INFO  [2023-01-17 00:52:31,561] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before$20or$20Same
INFO  [2023-01-17 00:52:31,561] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,693] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before or Same
INFO  [2023-01-17 00:52:31,693] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Days Before
INFO  [2023-01-17 00:52:31,693] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:31,693] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:31,694] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-17 00:52:31,694] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:31,694] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-17 00:52:31,694] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:31,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_356fa8eb-a850-4e18-bb61-a7fbd07f7a72 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:31,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_356fa8eb-a850-4e18-bb61-a7fbd07f7a72 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:31,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:31,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_4c6b4914-611b-4dbb-886e-74522698e022 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:31,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Days$20Before.worker_Temporal$20Days$20Before_4c6b4914-611b-4dbb-886e-74522698e022 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:31,696] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:31,700] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,800] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,807] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:31,807] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Days$20Before.table1
INFO  [2023-01-17 00:52:31,807] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Days$20Before.table1
INFO  [2023-01-17 00:52:31,921] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,030] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:32,030] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:32,030] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 158 B in total
INFO  [2023-01-17 00:52:32,031] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Days Before/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000531587sINFO  [2023-01-17 00:52:32,084] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=8, min=2, average=2.000000, max=2}
INFO  [2023-01-17 00:52:32,084] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=8, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:32,084] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=8, nullLines=0), subType=IntegerParser(super=Parser(lines=8, nullLines=0), minValue=14975, maxValue=14980), dateReader=com.bakdata.conquery.util.DateReader@70311726)
INFO  [2023-01-17 00:52:32,087] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Days Before/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:32,087] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:32,087] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Days Before/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:32,108] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Days$20Before.table1
INFO  [2023-01-17 00:52:32,108] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:32 +0000] "POST /admin/datasets/Temporal%20Days%20Before/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Temporal+Days+Before%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:52:32,109] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:32,110] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:32,110] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:32,112] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:32,112] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Days$20Before.table1.table1], containing 8 entries.
WARN  [2023-01-17 00:52:32,113] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:32,113] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Days$20Before.table1.table1.0
INFO  [2023-01-17 00:52:32,152] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Days$20Before.table1.table1], containing 8 entries.
INFO  [2023-01-17 00:52:32,152] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Days$20Before.table1.table1.1
INFO  [2023-01-17 00:52:32,257] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,262] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,279] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,280] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:32,280] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:32,385] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Days Before QUERY INIT
INFO  [2023-01-17 00:52:32,403] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Days$20Before] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:32,403] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[26fc65ea-a200-4433-aca7-855cda6e2ee8] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Days Before))]]
INFO  [2023-01-17 00:52:32,408] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8
INFO  [2023-01-17 00:52:32,408] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8
127.0.0.1 - - [17/Jan/2023:00:52:32 +0000] "POST /api/datasets/Temporal$20Days$20Before/queries HTTP/1.1" 201 1835 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:32,409] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8] with 1 results within PT0.001595S
INFO  [2023-01-17 00:52:32,410] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8] with 0 results within PT0.001813S
INFO  [2023-01-17 00:52:32,410] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8, workerId=Temporal$20Days$20Before.worker_Temporal$20Days$20Before_4c6b4914-611b-4dbb-886e-74522698e022, startTime=2023-01-17T00:52:32.408325, finishTime=2023-01-17T00:52:32.409920) of size 1
INFO  [2023-01-17 00:52:32,410] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8, workerId=Temporal$20Days$20Before.worker_Temporal$20Days$20Before_356fa8eb-a850-4e18-bb61-a7fbd07f7a72, startTime=2023-01-17T00:52:32.408205, finishTime=2023-01-17T00:52:32.410018) of size 0
INFO  [2023-01-17 00:52:32,410] com.bakdata.conquery.models.execution.ManagedExecution: DONE 26fc65ea-a200-4433-aca7-855cda6e2ee8 ManagedQuery within PT0.006835S
127.0.0.1 - - [17/Jan/2023:00:52:32 +0000] "GET /api/datasets/Temporal$20Days$20Before/queries/Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8 HTTP/1.1" 200 2122 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:32,435] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Days Before], queryId=26fc65ea-a200-4433-aca7-855cda6e2ee8, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:32.403512, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@43985189[Count = 0], startTime=2023-01-17T00:52:32.403732, finishTime=2023-01-17T00:52:32.410567, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@58434cff), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Days Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1dbad1f, com.bakdata.conquery.models.query.ColumnDescriptor@7c9dd548]) download on dataset Dataset[label=null, name=Temporal Days Before] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:32,435] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Days Before], queryId=26fc65ea-a200-4433-aca7-855cda6e2ee8, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:32.403512, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@43985189[Count = 0], startTime=2023-01-17T00:52:32.403732, finishTime=2023-01-17T00:52:32.410567, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@58434cff), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Days Before)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=1, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@1dbad1f, com.bakdata.conquery.models.query.ColumnDescriptor@7c9dd548]) on dataset Dataset[label=null, name=Temporal Days Before]
127.0.0.1 - - [17/Jan/2023:00:52:32 +0000] "GET /api/datasets/Temporal%20Days%20Before/result/Temporal$20Days$20Before.26fc65ea-a200-4433-aca7-855cda6e2ee8.csv?pretty=false HTTP/1.1" 200 39 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:52:32,454] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Days Before on 2 rows
INFO  [2023-01-17 00:52:32,455] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Days Before
INFO  [2023-01-17 00:52:32,455] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-17 00:52:32,455] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Days Before, name=Temporal Days Before]
INFO  [2023-01-17 00:52:32,455] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Days Before_4c6b4914-611b-4dbb-886e-74522698e022
INFO  [2023-01-17 00:52:32,455] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Days Before_356fa8eb-a850-4e18-bb61-a7fbd07f7a72
INFO  [2023-01-17 00:52:32,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Days Before_4c6b4914-611b-4dbb-886e-74522698e022
INFO  [2023-01-17 00:52:32,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Days Before
INFO  [2023-01-17 00:52:32,498] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Days Before_356fa8eb-a850-4e18-bb61-a7fbd07f7a72
INFO  [2023-01-17 00:52:32,513] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Days$20Before
INFO  [2023-01-17 00:52:32,513] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,686] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Days Before
INFO  [2023-01-17 00:52:32,687] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Before or Never
INFO  [2023-01-17 00:52:32,687] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:32,687] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:32,688] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-17 00:52:32,688] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-17 00:52:32,688] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:32,688] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_9f5c3705-d709-4fbe-b7c6-e9c80d23b72c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_9f5c3705-d709-4fbe-b7c6-e9c80d23b72c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_521c0307-fbaf-44c9-90f4-080ff15223e4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_521c0307-fbaf-44c9-90f4-080ff15223e4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:32,689] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:32,794] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,800] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:32,801] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Never.table1
INFO  [2023-01-17 00:52:32,801] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Before$20or$20Never.table1
INFO  [2023-01-17 00:52:32,920] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,029] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:33,029] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:33,029] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 144 B in total
INFO  [2023-01-17 00:52:33,029] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before or Never/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000384019sINFO  [2023-01-17 00:52:33,068] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=7, min=1, average=1.750000, max=2}
INFO  [2023-01-17 00:52:33,068] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=7, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:33,068] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=14976, maxValue=15656), dateReader=com.bakdata.conquery.util.DateReader@5b84ee79)
INFO  [2023-01-17 00:52:33,071] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before or Never/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:33,071] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:33,071] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Before or Never/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:33,090] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Before$20or$20Never.table1
INFO  [2023-01-17 00:52:33,091] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:33 +0000] "POST /admin/datasets/Temporal%20Before%20or%20Never/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Temporal+Before+or+Never%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:33,092] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:33,093] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:33,093] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:33,096] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:33,096] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Never.table1.table1], containing 7 entries.
INFO  [2023-01-17 00:52:33,096] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Before$20or$20Never.table1.table1], containing 7 entries.
INFO  [2023-01-17 00:52:33,097] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Never.table1.table1.0
WARN  [2023-01-17 00:52:33,097] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:33,098] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Before$20or$20Never.table1.table1.1
INFO  [2023-01-17 00:52:33,202] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,208] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,221] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,221] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:33,221] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:33,328] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Before or Never QUERY INIT
INFO  [2023-01-17 00:52:33,344] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Before$20or$20Never] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:33,344] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[80c929c2-05c4-4348-a1fa-fc744d30d0ae] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Never))]]
INFO  [2023-01-17 00:52:33,347] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae
INFO  [2023-01-17 00:52:33,347] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae
127.0.0.1 - - [17/Jan/2023:00:52:33 +0000] "POST /api/datasets/Temporal$20Before$20or$20Never/queries HTTP/1.1" 201 1873 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:33,348] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae] with 0 results within PT0.001396S
INFO  [2023-01-17 00:52:33,349] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae] with 2 results within PT0.001778S
INFO  [2023-01-17 00:52:33,349] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae, workerId=Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_9f5c3705-d709-4fbe-b7c6-e9c80d23b72c, startTime=2023-01-17T00:52:33.347495, finishTime=2023-01-17T00:52:33.348891) of size 0
INFO  [2023-01-17 00:52:33,349] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae, workerId=Temporal$20Before$20or$20Never.worker_Temporal$20Before$20or$20Never_521c0307-fbaf-44c9-90f4-080ff15223e4, startTime=2023-01-17T00:52:33.347499, finishTime=2023-01-17T00:52:33.349277) of size 2
INFO  [2023-01-17 00:52:33,349] com.bakdata.conquery.models.execution.ManagedExecution: DONE 80c929c2-05c4-4348-a1fa-fc744d30d0ae ManagedQuery within PT0.005528S
127.0.0.1 - - [17/Jan/2023:00:52:33 +0000] "GET /api/datasets/Temporal$20Before$20or$20Never/queries/Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae HTTP/1.1" 200 2183 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:33,373] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Never], queryId=80c929c2-05c4-4348-a1fa-fc744d30d0ae, label=geschlecht_select	@§$, creationTime=2023-01-17T00:52:33.344235, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@246986d1[Count = 0], startTime=2023-01-17T00:52:33.344362, finishTime=2023-01-17T00:52:33.349890, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@103d3b5c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Never)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@27327024, com.bakdata.conquery.models.query.ColumnDescriptor@5c40da9f]) download on dataset Dataset[label=null, name=Temporal Before or Never] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:33,373] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Before or Never], queryId=80c929c2-05c4-4348-a1fa-fc744d30d0ae, label=geschlecht_select	@§$, creationTime=2023-01-17T00:52:33.344235, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@246986d1[Count = 0], startTime=2023-01-17T00:52:33.344362, finishTime=2023-01-17T00:52:33.349890, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@103d3b5c), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Before or Never)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@27327024, com.bakdata.conquery.models.query.ColumnDescriptor@5c40da9f]) on dataset Dataset[label=null, name=Temporal Before or Never]
127.0.0.1 - - [17/Jan/2023:00:52:33 +0000] "GET /api/datasets/Temporal%20Before%20or%20Never/result/Temporal$20Before$20or$20Never.80c929c2-05c4-4348-a1fa-fc744d30d0ae.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:52:33,393] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Before or Never on 3 rows
INFO  [2023-01-17 00:52:33,393] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Before or Never
INFO  [2023-01-17 00:52:33,393] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-17 00:52:33,393] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Before or Never, name=Temporal Before or Never]
INFO  [2023-01-17 00:52:33,393] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Never_9f5c3705-d709-4fbe-b7c6-e9c80d23b72c
INFO  [2023-01-17 00:52:33,394] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Before or Never_521c0307-fbaf-44c9-90f4-080ff15223e4
INFO  [2023-01-17 00:52:33,491] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Before or Never
INFO  [2023-01-17 00:52:33,491] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Never_521c0307-fbaf-44c9-90f4-080ff15223e4
INFO  [2023-01-17 00:52:33,491] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Before or Never_9f5c3705-d709-4fbe-b7c6-e9c80d23b72c
INFO  [2023-01-17 00:52:33,498] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Before$20or$20Never
INFO  [2023-01-17 00:52:33,498] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,627] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Before or Never
INFO  [2023-01-17 00:52:33,628] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test Temporal Same
INFO  [2023-01-17 00:52:33,628] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:33,628] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:33,629] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-17 00:52:33,629] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-17 00:52:33,629] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:33,629] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:33,630] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Same.worker_Temporal$20Same_399b97a3-e5f4-42d9-bc45-56692b230d5f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:33,630] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Same.worker_Temporal$20Same_399b97a3-e5f4-42d9-bc45-56692b230d5f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:33,630] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:33,630] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker Temporal$20Same.worker_Temporal$20Same_d603a239-213d-49f8-b893-ccefea66c3aa are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:33,630] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker Temporal$20Same.worker_Temporal$20Same_d603a239-213d-49f8-b893-ccefea66c3aa are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:33,630] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:33,635] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,734] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,741] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,742] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Same.table1
INFO  [2023-01-17 00:52:33,742] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table Temporal$20Same.table1
INFO  [2023-01-17 00:52:33,861] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:33,970] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:33,971] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:33,971] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 178 B in total
INFO  [2023-01-17 00:52:33,971] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Same/table1.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000396003sINFO  [2023-01-17 00:52:34,011] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=9, min=2, average=2.250000, max=3}
INFO  [2023-01-17 00:52:34,011] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geschlecht] with StringParser(super=Parser(lines=9, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:34,011] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=14975, maxValue=15006), dateReader=com.bakdata.conquery.util.DateReader@58a2933c)
INFO  [2023-01-17 00:52:34,013] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:34,013] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:34,013] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_Temporal Same/table1.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:34,033] com.bakdata.conquery.models.jobs.ImportJob: Importing table1 into Temporal$20Same.table1
INFO  [2023-01-17 00:52:34,033] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:34 +0000] "POST /admin/datasets/Temporal%20Same/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_Temporal+Same%2Ftable1.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:34,035] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:34,036] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:34,036] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:34,039] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:34,039] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Same.table1.table1], containing 9 entries.
INFO  [2023-01-17 00:52:34,039] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[Temporal$20Same.table1.table1], containing 9 entries.
WARN  [2023-01-17 00:52:34,041] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:34,041] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Same.table1.table1.0
INFO  [2023-01-17 00:52:34,041] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received Temporal$20Same.table1.table1.1
INFO  [2023-01-17 00:52:34,146] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,151] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,165] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,166] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:34,166] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:34,271] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: Temporal Same QUERY INIT
INFO  [2023-01-17 00:52:34,292] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[Temporal$20Same] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:34,292] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Same))]]
INFO  [2023-01-17 00:52:34,296] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5
INFO  [2023-01-17 00:52:34,296] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5
127.0.0.1 - - [17/Jan/2023:00:52:34 +0000] "POST /api/datasets/Temporal$20Same/queries HTTP/1.1" 201 1744 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:34,310] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5] with 0 results within PT0.01387S
INFO  [2023-01-17 00:52:34,310] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5, workerId=Temporal$20Same.worker_Temporal$20Same_d603a239-213d-49f8-b893-ccefea66c3aa, startTime=2023-01-17T00:52:34.296531, finishTime=2023-01-17T00:52:34.310401) of size 0
INFO  [2023-01-17 00:52:34,336] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5] with 2 results within PT0.039512S
INFO  [2023-01-17 00:52:34,336] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5, workerId=Temporal$20Same.worker_Temporal$20Same_399b97a3-e5f4-42d9-bc45-56692b230d5f, startTime=2023-01-17T00:52:34.296527, finishTime=2023-01-17T00:52:34.336039) of size 2
INFO  [2023-01-17 00:52:34,336] com.bakdata.conquery.models.execution.ManagedExecution: DONE dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5 ManagedQuery within PT0.043734S
127.0.0.1 - - [17/Jan/2023:00:52:34 +0000] "GET /api/datasets/Temporal$20Same/queries/Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5 HTTP/1.1" 200 1995 "-" "Conquery (test client)" 28
INFO  [2023-01-17 00:52:34,348] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Same], queryId=dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:34.292816, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f0e4d4f[Count = 0], startTime=2023-01-17T00:52:34.292996, finishTime=2023-01-17T00:52:34.336730, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2087ab56), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@a42a9a4, com.bakdata.conquery.models.query.ColumnDescriptor@4e4c0309]) download on dataset Dataset[label=null, name=Temporal Same] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:34,348] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=Temporal Same], queryId=dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5, label=Geschlecht-SELECT	@§$, creationTime=2023-01-17T00:52:34.292816, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@f0e4d4f[Count = 0], startTime=2023-01-17T00:52:34.292996, finishTime=2023-01-17T00:52:34.336730, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2087ab56), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_Temporal Same)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@a42a9a4, com.bakdata.conquery.models.query.ColumnDescriptor@4e4c0309]) on dataset Dataset[label=null, name=Temporal Same]
127.0.0.1 - - [17/Jan/2023:00:52:34 +0000] "GET /api/datasets/Temporal%20Same/result/Temporal$20Same.dd6598c9-5ec7-42f5-8b8f-b5d24ff856a5.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 74
INFO  [2023-01-17 00:52:34,420] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest Temporal Same on 3 rows
INFO  [2023-01-17 00:52:34,420] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast Temporal Same
INFO  [2023-01-17 00:52:34,421] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-17 00:52:34,421] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=Temporal Same, name=Temporal Same]
INFO  [2023-01-17 00:52:34,421] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Same_399b97a3-e5f4-42d9-bc45-56692b230d5f
INFO  [2023-01-17 00:52:34,421] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_Temporal Same_d603a239-213d-49f8-b893-ccefea66c3aa
INFO  [2023-01-17 00:52:34,429] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow Temporal Same
INFO  [2023-01-17 00:52:34,430] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Same_399b97a3-e5f4-42d9-bc45-56692b230d5f
INFO  [2023-01-17 00:52:34,430] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_Temporal Same_d603a239-213d-49f8-b893-ccefea66c3aa
INFO  [2023-01-17 00:52:34,441] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of Temporal$20Same
INFO  [2023-01-17 00:52:34,441] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,624] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test Temporal Same
INFO  [2023-01-17 00:52:34,624] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test VALIDITY_DATE_QUERY Test
INFO  [2023-01-17 00:52:34,624] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:34,624] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:34,625] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-17 00:52:34,625] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-17 00:52:34,625] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:34,625] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_bd26f296-42df-479e-91ee-ca1ed6f60bd0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_bd26f296-42df-479e-91ee-ca1ed6f60bd0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_bcfc498a-5978-4ccc-b36f-5cb0bb338c44 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_bcfc498a-5978-4ccc-b36f-5cb0bb338c44 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:34,627] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:34,735] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,741] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,742] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALIDITY_DATE_QUERY$20Test.table
INFO  [2023-01-17 00:52:34,742] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALIDITY_DATE_QUERY$20Test.table
INFO  [2023-01-17 00:52:34,852] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:34,962] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:34,962] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:34,962] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 149 B in total
INFO  [2023-01-17 00:52:34,962] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VALIDITY_DATE_QUERY Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000335987sINFO  [2023-01-17 00:52:34,996] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=4, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:34,997] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[other_date] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=15170, maxValue=16384), dateReader=com.bakdata.conquery.util.DateReader@3f196196)
INFO  [2023-01-17 00:52:34,997] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:34,997] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@7d78f7d8)
INFO  [2023-01-17 00:52:34,999] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VALIDITY_DATE_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:34,999] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:34,999] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VALIDITY_DATE_QUERY Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:35,015] com.bakdata.conquery.models.jobs.ImportJob: Importing table into VALIDITY_DATE_QUERY$20Test.table
INFO  [2023-01-17 00:52:35,015] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:35 +0000] "POST /admin/datasets/VALIDITY_DATE_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_VALIDITY_DATE_QUERY+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:35,016] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:35,017] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:35,017] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:35,018] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:35,018] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALIDITY_DATE_QUERY$20Test.table.table], containing 4 entries.
INFO  [2023-01-17 00:52:35,018] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALIDITY_DATE_QUERY$20Test.table.table], containing 4 entries.
WARN  [2023-01-17 00:52:35,019] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:35,019] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALIDITY_DATE_QUERY$20Test.table.table.0
INFO  [2023-01-17 00:52:35,019] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALIDITY_DATE_QUERY$20Test.table.table.1
INFO  [2023-01-17 00:52:35,124] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,130] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,142] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,143] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:35,143] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:35,249] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: VALIDITY_DATE_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:35,264] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[VALIDITY_DATE_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:35,264] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[14f81a61-0cac-4a59-b682-eb3d64c1ea73] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_VALIDITY_DATE_QUERY Test))]]
INFO  [2023-01-17 00:52:35,267] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73
INFO  [2023-01-17 00:52:35,267] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73
INFO  [2023-01-17 00:52:35,268] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73] with 0 results within PT0.000814S
INFO  [2023-01-17 00:52:35,268] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73] with 2 results within PT0.001009S
127.0.0.1 - - [17/Jan/2023:00:52:35 +0000] "POST /api/datasets/VALIDITY_DATE_QUERY$20Test/queries HTTP/1.1" 201 1196 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:35,268] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73, workerId=VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_bcfc498a-5978-4ccc-b36f-5cb0bb338c44, startTime=2023-01-17T00:52:35.267498, finishTime=2023-01-17T00:52:35.268312) of size 0
INFO  [2023-01-17 00:52:35,269] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73, workerId=VALIDITY_DATE_QUERY$20Test.worker_VALIDITY_DATE_QUERY$20Test_bd26f296-42df-479e-91ee-ca1ed6f60bd0, startTime=2023-01-17T00:52:35.267534, finishTime=2023-01-17T00:52:35.268543) of size 2
INFO  [2023-01-17 00:52:35,269] com.bakdata.conquery.models.execution.ManagedExecution: DONE 14f81a61-0cac-4a59-b682-eb3d64c1ea73 ManagedQuery within PT0.004684S
127.0.0.1 - - [17/Jan/2023:00:52:35 +0000] "GET /api/datasets/VALIDITY_DATE_QUERY$20Test/queries/VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73 HTTP/1.1" 200 1491 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:35,292] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALIDITY_DATE_QUERY Test], queryId=14f81a61-0cac-4a59-b682-eb3d64c1ea73, label=concept-test_child1	@§$, creationTime=2023-01-17T00:52:35.264404, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@674335cf[Count = 0], startTime=2023-01-17T00:52:35.264595, finishTime=2023-01-17T00:52:35.269279, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7740bac4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALIDITY_DATE_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@35369ad2, com.bakdata.conquery.models.query.ColumnDescriptor@79aafba0]) download on dataset Dataset[label=null, name=VALIDITY_DATE_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:35,293] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALIDITY_DATE_QUERY Test], queryId=14f81a61-0cac-4a59-b682-eb3d64c1ea73, label=concept-test_child1	@§$, creationTime=2023-01-17T00:52:35.264404, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@674335cf[Count = 0], startTime=2023-01-17T00:52:35.264595, finishTime=2023-01-17T00:52:35.269279, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7740bac4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALIDITY_DATE_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@35369ad2, com.bakdata.conquery.models.query.ColumnDescriptor@79aafba0]) on dataset Dataset[label=null, name=VALIDITY_DATE_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:35 +0000] "GET /api/datasets/VALIDITY_DATE_QUERY%20Test/result/VALIDITY_DATE_QUERY$20Test.14f81a61-0cac-4a59-b682-eb3d64c1ea73.csv?pretty=false HTTP/1.1" 200 65 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:35,309] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest VALIDITY_DATE_QUERY Test on 3 rows
INFO  [2023-01-17 00:52:35,310] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast VALIDITY_DATE_QUERY Test
INFO  [2023-01-17 00:52:35,310] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-17 00:52:35,310] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALIDITY_DATE_QUERY Test, name=VALIDITY_DATE_QUERY Test]
INFO  [2023-01-17 00:52:35,310] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALIDITY_DATE_QUERY Test_bcfc498a-5978-4ccc-b36f-5cb0bb338c44
INFO  [2023-01-17 00:52:35,310] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALIDITY_DATE_QUERY Test_bd26f296-42df-479e-91ee-ca1ed6f60bd0
INFO  [2023-01-17 00:52:35,326] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow VALIDITY_DATE_QUERY Test
INFO  [2023-01-17 00:52:35,326] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALIDITY_DATE_QUERY Test_bd26f296-42df-479e-91ee-ca1ed6f60bd0
INFO  [2023-01-17 00:52:35,327] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALIDITY_DATE_QUERY Test_bcfc498a-5978-4ccc-b36f-5cb0bb338c44
INFO  [2023-01-17 00:52:35,419] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of VALIDITY_DATE_QUERY$20Test
INFO  [2023-01-17 00:52:35,420] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,448] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test VALIDITY_DATE_QUERY Test
INFO  [2023-01-17 00:52:35,449] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-17 00:52:35,449] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:35,449] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:35,450] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-17 00:52:35,450] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:35,450] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-17 00:52:35,450] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_0950d1d3-d244-4033-9c33-d80ba335a05f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_0950d1d3-d244-4033-9c33-d80ba335a05f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_c1535b46-c465-4d1c-9791-e1ab0e32ce77 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_c1535b46-c465-4d1c-9791-e1ab0e32ce77 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:35,451] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:35,555] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,562] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,563] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:35,563] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:35,680] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:35,790] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:35,790] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:35,790] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 390 B in total
INFO  [2023-01-17 00:52:35,790] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VIRTUAL_CONCEPT_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000221742sINFO  [2023-01-17 00:52:35,812] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=22, sum=22, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:35,812] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[test_column] with StringParser(super=Parser(lines=22, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:35,813] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=22, nullLines=0), subType=IntegerParser(super=Parser(lines=22, nullLines=0), minValue=13149, maxValue=16071), dateReader=com.bakdata.conquery.util.DateReader@4633c9f8)
INFO  [2023-01-17 00:52:35,815] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VIRTUAL_CONCEPT_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:35,815] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:35,815] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VIRTUAL_CONCEPT_REUSED_QUERY Test/test_table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:35,833] com.bakdata.conquery.models.jobs.ImportJob: Importing test_table into VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table
INFO  [2023-01-17 00:52:35,833] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:35 +0000] "POST /admin/datasets/VIRTUAL_CONCEPT_REUSED_QUERY%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_VIRTUAL_CONCEPT_REUSED_QUERY+Test%2Ftest_table.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:35,834] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:35,835] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:35,835] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:35,838] com.bakdata.conquery.models.jobs.ImportJob: Start sending 8 Buckets
INFO  [2023-01-17 00:52:35,838] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table], containing 22 entries.
INFO  [2023-01-17 00:52:35,838] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table], containing 22 entries.
INFO  [2023-01-17 00:52:35,839] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.0
INFO  [2023-01-17 00:52:35,839] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.1
INFO  [2023-01-17 00:52:35,840] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.2
WARN  [2023-01-17 00:52:35,840] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:35,840] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.3
INFO  [2023-01-17 00:52:35,884] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.4
INFO  [2023-01-17 00:52:35,884] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.5
INFO  [2023-01-17 00:52:35,884] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.6
INFO  [2023-01-17 00:52:35,884] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VIRTUAL_CONCEPT_REUSED_QUERY$20Test.test_table.test_table.7
INFO  [2023-01-17 00:52:35,988] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,040] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,045] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,056] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,056] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:36,056] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:36,162] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: VIRTUAL_CONCEPT_REUSED_QUERY Test QUERY INIT
INFO  [2023-01-17 00:52:36,173] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[VIRTUAL_CONCEPT_REUSED_QUERY$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:36,174] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f8ad5fc1-c31f-4d21-9110-995f1aeabfd1] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_VIRTUAL_CONCEPT_REUSED_QUERY Test))]]
127.0.0.1 - - [17/Jan/2023:00:52:36 +0000] "POST /api/datasets/VIRTUAL_CONCEPT_REUSED_QUERY$20Test/queries HTTP/1.1" 201 1507 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:36,177] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1
INFO  [2023-01-17 00:52:36,177] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1
INFO  [2023-01-17 00:52:36,181] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1] with 2 results within PT0.004372S
INFO  [2023-01-17 00:52:36,182] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1, workerId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_c1535b46-c465-4d1c-9791-e1ab0e32ce77, startTime=2023-01-17T00:52:36.177475, finishTime=2023-01-17T00:52:36.181847) of size 2
INFO  [2023-01-17 00:52:36,183] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1] with 2 results within PT0.006171S
INFO  [2023-01-17 00:52:36,184] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1, workerId=VIRTUAL_CONCEPT_REUSED_QUERY$20Test.worker_VIRTUAL_CONCEPT_REUSED_QUERY$20Test_0950d1d3-d244-4033-9c33-d80ba335a05f, startTime=2023-01-17T00:52:36.177502, finishTime=2023-01-17T00:52:36.183673) of size 2
INFO  [2023-01-17 00:52:36,184] com.bakdata.conquery.models.execution.ManagedExecution: DONE f8ad5fc1-c31f-4d21-9110-995f1aeabfd1 ManagedQuery within PT0.010006S
127.0.0.1 - - [17/Jan/2023:00:52:36 +0000] "GET /api/datasets/VIRTUAL_CONCEPT_REUSED_QUERY$20Test/queries/VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1 HTTP/1.1" 200 1839 "-" "Conquery (test client)" 1
INFO  [2023-01-17 00:52:36,193] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test], queryId=f8ad5fc1-c31f-4d21-9110-995f1aeabfd1, label=Query test_concept	@§$, creationTime=2023-01-17T00:52:36.173851, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@17b6045d[Count = 0], startTime=2023-01-17T00:52:36.174355, finishTime=2023-01-17T00:52:36.184361, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@b965205), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VIRTUAL_CONCEPT_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@64f9cf2f, com.bakdata.conquery.models.query.ColumnDescriptor@40a92c25]) download on dataset Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:36,193] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test], queryId=f8ad5fc1-c31f-4d21-9110-995f1aeabfd1, label=Query test_concept	@§$, creationTime=2023-01-17T00:52:36.173851, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@17b6045d[Count = 0], startTime=2023-01-17T00:52:36.174355, finishTime=2023-01-17T00:52:36.184361, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@b965205), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VIRTUAL_CONCEPT_REUSED_QUERY Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@64f9cf2f, com.bakdata.conquery.models.query.ColumnDescriptor@40a92c25]) on dataset Dataset[label=null, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
127.0.0.1 - - [17/Jan/2023:00:52:36 +0000] "GET /api/datasets/VIRTUAL_CONCEPT_REUSED_QUERY%20Test/result/VIRTUAL_CONCEPT_REUSED_QUERY$20Test.f8ad5fc1-c31f-4d21-9110-995f1aeabfd1.csv?pretty=false HTTP/1.1" 200 120 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:36,207] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest VIRTUAL_CONCEPT_REUSED_QUERY Test on 5 rows
INFO  [2023-01-17 00:52:36,207] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-17 00:52:36,207] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-17 00:52:36,207] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VIRTUAL_CONCEPT_REUSED_QUERY Test, name=VIRTUAL_CONCEPT_REUSED_QUERY Test]
INFO  [2023-01-17 00:52:36,207] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_0950d1d3-d244-4033-9c33-d80ba335a05f
INFO  [2023-01-17 00:52:36,207] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_c1535b46-c465-4d1c-9791-e1ab0e32ce77
INFO  [2023-01-17 00:52:36,250] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-17 00:52:36,251] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_c1535b46-c465-4d1c-9791-e1ab0e32ce77
INFO  [2023-01-17 00:52:36,251] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VIRTUAL_CONCEPT_REUSED_QUERY Test_0950d1d3-d244-4033-9c33-d80ba335a05f
INFO  [2023-01-17 00:52:36,340] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of VIRTUAL_CONCEPT_REUSED_QUERY$20Test
INFO  [2023-01-17 00:52:36,340] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,361] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test VIRTUAL_CONCEPT_REUSED_QUERY Test
INFO  [2023-01-17 00:52:36,362] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_AGGREGATOR Test
INFO  [2023-01-17 00:52:36,362] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:36,362] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:36,363] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:36,363] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:36,363] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:36,363] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_0876f2f4-daae-4e29-b721-7cd674f732d1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_0876f2f4-daae-4e29-b721-7cd674f732d1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_87e3637f-a26c-4d6d-bd39-626b4f919e04 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_87e3637f-a26c-4d6d-bd39-626b4f919e04 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:36,364] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:36,468] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,475] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,475] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:36,475] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:36,591] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,699] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:36,700] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:36,700] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 98 B in total
INFO  [2023-01-17 00:52:36,700] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000436993sINFO  [2023-01-17 00:52:36,744] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:52:36,744] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:36,744] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@4963a846)
INFO  [2023-01-17 00:52:36,747] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:36,747] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:36,747] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:36,760] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_AGGREGATOR$20Test.table
127.0.0.1 - - [17/Jan/2023:00:52:36 +0000] "POST /admin/datasets/COUNT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:36,760] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,761] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:36,761] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:36,761] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:36,764] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:36,764] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:52:36,764] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:52:36,765] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:36,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:36,765] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:36,870] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,875] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:36,890] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:36,890] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:36,996] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:37,011] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:37,011] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[bfb0de42-1f23-4753-8fb2-6757527fdee5] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:37,014] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5
INFO  [2023-01-17 00:52:37,014] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "POST /api/datasets/COUNT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1312 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:37,015] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5] with 1 results within PT0.00084S
INFO  [2023-01-17 00:52:37,015] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5] with 3 results within PT0.000987S
INFO  [2023-01-17 00:52:37,016] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5, workerId=COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_0876f2f4-daae-4e29-b721-7cd674f732d1, startTime=2023-01-17T00:52:37.014695, finishTime=2023-01-17T00:52:37.015535) of size 1
INFO  [2023-01-17 00:52:37,016] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5, workerId=COUNT_AGGREGATOR$20Test.worker_COUNT_AGGREGATOR$20Test_87e3637f-a26c-4d6d-bd39-626b4f919e04, startTime=2023-01-17T00:52:37.014700, finishTime=2023-01-17T00:52:37.015687) of size 3
INFO  [2023-01-17 00:52:37,016] com.bakdata.conquery.models.execution.ManagedExecution: DONE bfb0de42-1f23-4753-8fb2-6757527fdee5 ManagedQuery within PT0.004499S
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "GET /api/datasets/COUNT_AGGREGATOR$20Test/queries/COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5 HTTP/1.1" 200 1595 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:37,039] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_AGGREGATOR Test], queryId=bfb0de42-1f23-4753-8fb2-6757527fdee5, label=concept	@§$, creationTime=2023-01-17T00:52:37.011759, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7f00fa47[Count = 0], startTime=2023-01-17T00:52:37.011933, finishTime=2023-01-17T00:52:37.016432, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@254536d0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@207d0a11, com.bakdata.conquery.models.query.ColumnDescriptor@3bfde2f7, com.bakdata.conquery.models.query.ColumnDescriptor@4357f50d]) download on dataset Dataset[label=null, name=COUNT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:37,040] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_AGGREGATOR Test], queryId=bfb0de42-1f23-4753-8fb2-6757527fdee5, label=concept	@§$, creationTime=2023-01-17T00:52:37.011759, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7f00fa47[Count = 0], startTime=2023-01-17T00:52:37.011933, finishTime=2023-01-17T00:52:37.016432, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@254536d0), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@207d0a11, com.bakdata.conquery.models.query.ColumnDescriptor@3bfde2f7, com.bakdata.conquery.models.query.ColumnDescriptor@4357f50d]) on dataset Dataset[label=null, name=COUNT_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "GET /api/datasets/COUNT_AGGREGATOR%20Test/result/COUNT_AGGREGATOR$20Test.bfb0de42-1f23-4753-8fb2-6757527fdee5.csv?pretty=false HTTP/1.1" 200 139 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:52:37,061] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COUNT_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:37,061] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_AGGREGATOR Test
INFO  [2023-01-17 00:52:37,062] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:37,062] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_AGGREGATOR Test, name=COUNT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:37,062] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_AGGREGATOR Test_0876f2f4-daae-4e29-b721-7cd674f732d1
INFO  [2023-01-17 00:52:37,062] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_AGGREGATOR Test_87e3637f-a26c-4d6d-bd39-626b4f919e04
INFO  [2023-01-17 00:52:37,063] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_AGGREGATOR Test
INFO  [2023-01-17 00:52:37,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_AGGREGATOR Test_0876f2f4-daae-4e29-b721-7cd674f732d1
INFO  [2023-01-17 00:52:37,064] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_AGGREGATOR Test_87e3637f-a26c-4d6d-bd39-626b4f919e04
INFO  [2023-01-17 00:52:37,065] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:37,065] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,196] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_AGGREGATOR Test
INFO  [2023-01-17 00:52:37,196] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-17 00:52:37,196] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:37,196] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:37,197] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:37,197] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:37,197] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:37,197] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:37,198] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,199] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_cefde8fb-cc9c-460f-b182-fc082ffedeb2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:37,199] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_cefde8fb-cc9c-460f-b182-fc082ffedeb2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:37,199] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:37,199] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_8e79dff4-12c7-4b45-a126-d9039d515c62 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:37,199] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_8e79dff4-12c7-4b45-a126-d9039d515c62 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:37,199] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:37,303] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,310] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,310] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_DISTINCT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:37,310] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_DISTINCT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:37,436] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,546] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:37,546] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:37,546] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 115 B in total
INFO  [2023-01-17 00:52:37,546] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_DISTINCT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000315936sINFO  [2023-01-17 00:52:37,578] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:52:37,579] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=6, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:37,579] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@68ed6621)
INFO  [2023-01-17 00:52:37,582] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_DISTINCT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:37,582] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:37,582] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_DISTINCT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:37,600] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_DISTINCT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:37,600] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "POST /admin/datasets/COUNT_DISTINCT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT_DISTINCT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:37,601] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:37,601] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:37,601] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:37,603] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:37,603] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_DISTINCT_AGGREGATOR$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:52:37,603] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_DISTINCT_AGGREGATOR$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:52:37,604] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:37,604] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_DISTINCT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:37,604] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_DISTINCT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:37,709] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,714] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,727] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:37,728] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:37,728] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:37,846] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_DISTINCT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:37,859] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_DISTINCT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:37,859] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[f291a401-f441-4b4b-83cb-74bed4dea48d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_DISTINCT_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:37,861] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d
INFO  [2023-01-17 00:52:37,861] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "POST /api/datasets/COUNT_DISTINCT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1357 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:37,862] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d] with 1 results within PT0.000829S
INFO  [2023-01-17 00:52:37,862] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d] with 3 results within PT0.000982S
INFO  [2023-01-17 00:52:37,862] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d, workerId=COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_cefde8fb-cc9c-460f-b182-fc082ffedeb2, startTime=2023-01-17T00:52:37.861307, finishTime=2023-01-17T00:52:37.862136) of size 1
INFO  [2023-01-17 00:52:37,862] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d, workerId=COUNT_DISTINCT_AGGREGATOR$20Test.worker_COUNT_DISTINCT_AGGREGATOR$20Test_8e79dff4-12c7-4b45-a126-d9039d515c62, startTime=2023-01-17T00:52:37.861272, finishTime=2023-01-17T00:52:37.862254) of size 3
INFO  [2023-01-17 00:52:37,862] com.bakdata.conquery.models.execution.ManagedExecution: DONE f291a401-f441-4b4b-83cb-74bed4dea48d ManagedQuery within PT0.003672S
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "GET /api/datasets/COUNT_DISTINCT_AGGREGATOR$20Test/queries/COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d HTTP/1.1" 200 1676 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:37,885] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test], queryId=f291a401-f441-4b4b-83cb-74bed4dea48d, label=concept	@§$, creationTime=2023-01-17T00:52:37.859184, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@342d8120[Count = 0], startTime=2023-01-17T00:52:37.859303, finishTime=2023-01-17T00:52:37.862975, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@29dcba2d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_DISTINCT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@60b0a148, com.bakdata.conquery.models.query.ColumnDescriptor@1c03ccbe, com.bakdata.conquery.models.query.ColumnDescriptor@44ff7a9f]) download on dataset Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:37,886] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test], queryId=f291a401-f441-4b4b-83cb-74bed4dea48d, label=concept	@§$, creationTime=2023-01-17T00:52:37.859184, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@342d8120[Count = 0], startTime=2023-01-17T00:52:37.859303, finishTime=2023-01-17T00:52:37.862975, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@29dcba2d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_DISTINCT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@60b0a148, com.bakdata.conquery.models.query.ColumnDescriptor@1c03ccbe, com.bakdata.conquery.models.query.ColumnDescriptor@44ff7a9f]) on dataset Dataset[label=null, name=COUNT_DISTINCT_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:37 +0000] "GET /api/datasets/COUNT_DISTINCT_AGGREGATOR%20Test/result/COUNT_DISTINCT_AGGREGATOR$20Test.f291a401-f441-4b4b-83cb-74bed4dea48d.csv?pretty=false HTTP/1.1" 200 139 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:37,888] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COUNT_DISTINCT_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:37,888] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-17 00:52:37,888] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:37,888] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_DISTINCT_AGGREGATOR Test, name=COUNT_DISTINCT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:37,888] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_DISTINCT_AGGREGATOR Test_cefde8fb-cc9c-460f-b182-fc082ffedeb2
INFO  [2023-01-17 00:52:37,889] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_DISTINCT_AGGREGATOR Test_8e79dff4-12c7-4b45-a126-d9039d515c62
INFO  [2023-01-17 00:52:37,897] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-17 00:52:37,898] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_DISTINCT_AGGREGATOR Test_cefde8fb-cc9c-460f-b182-fc082ffedeb2
INFO  [2023-01-17 00:52:37,898] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_DISTINCT_AGGREGATOR Test_8e79dff4-12c7-4b45-a126-d9039d515c62
INFO  [2023-01-17 00:52:37,904] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_DISTINCT_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:37,904] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,034] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_DISTINCT_AGGREGATOR Test
INFO  [2023-01-17 00:52:38,034] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-17 00:52:38,034] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:38,034] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:38,035] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-17 00:52:38,035] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:38,035] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-17 00:52:38,035] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_01cbe94a-ed2e-4790-bb08-7ec49d959b05 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_01cbe94a-ed2e-4790-bb08-7ec49d959b05 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_e5fe4dad-483e-470f-a0f4-7a1e5c525904 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_e5fe4dad-483e-470f-a0f4-7a1e5c525904 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:38,037] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:38,141] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,148] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,149] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:38,149] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table COUNT_QUARTERS_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:38,274] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,387] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:38,388] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:38,388] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 460 B in total
INFO  [2023-01-17 00:52:38,388] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000287413sINFO  [2023-01-17 00:52:38,417] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=12, min=1, average=2.400000, max=4}
INFO  [2023-01-17 00:52:38,417] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=12, nullLines=0), minParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=16436, maxValue=16436), dateReader=com.bakdata.conquery.util.DateReader@3078b6fd), maxParser=DateParser(super=Parser(lines=12, nullLines=0), subType=IntegerParser(super=Parser(lines=12, nullLines=0), minValue=17166, maxValue=17166), dateReader=com.bakdata.conquery.util.DateReader@2a0a56fd), dateReader=com.bakdata.conquery.util.DateReader@6c6db7ab, onlyQuarters=false, maxValue=17166, minValue=16436, anyOpen=false)
INFO  [2023-01-17 00:52:38,417] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[behandlungsdatum] with DateParser(super=Parser(lines=12, nullLines=1), subType=IntegerParser(super=Parser(lines=12, nullLines=1), minValue=16467, maxValue=16740), dateReader=com.bakdata.conquery.util.DateReader@65ebde26)
INFO  [2023-01-17 00:52:38,419] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:38,419] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:38,419] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_COUNT_QUARTERS_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:38,430] com.bakdata.conquery.models.jobs.ImportJob: Importing table into COUNT_QUARTERS_AGGREGATOR$20Test.table
127.0.0.1 - - [17/Jan/2023:00:52:38 +0000] "POST /admin/datasets/COUNT_QUARTERS_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_COUNT_QUARTERS_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:38,430] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,431] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:38,432] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:38,432] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:38,434] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:38,434] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS_AGGREGATOR$20Test.table.table], containing 12 entries.
INFO  [2023-01-17 00:52:38,434] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[COUNT_QUARTERS_AGGREGATOR$20Test.table.table], containing 12 entries.
WARN  [2023-01-17 00:52:38,435] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:38,435] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:38,435] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received COUNT_QUARTERS_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:38,540] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,545] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,558] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,559] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:38,559] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:38,665] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: COUNT_QUARTERS_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:38,682] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[COUNT_QUARTERS_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:38,683] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[eb0cb0d7-d445-49be-911c-178acb6fee30] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:38,686] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30
INFO  [2023-01-17 00:52:38,686] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30
INFO  [2023-01-17 00:52:38,687] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30] with 2 results within PT0.001274S
127.0.0.1 - - [17/Jan/2023:00:52:38 +0000] "POST /api/datasets/COUNT_QUARTERS_AGGREGATOR$20Test/queries HTTP/1.1" 201 1356 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:38,688] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30] with 3 results within PT0.001622S
INFO  [2023-01-17 00:52:38,688] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30, workerId=COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_e5fe4dad-483e-470f-a0f4-7a1e5c525904, startTime=2023-01-17T00:52:38.686415, finishTime=2023-01-17T00:52:38.687689) of size 2
INFO  [2023-01-17 00:52:38,688] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30, workerId=COUNT_QUARTERS_AGGREGATOR$20Test.worker_COUNT_QUARTERS_AGGREGATOR$20Test_01cbe94a-ed2e-4790-bb08-7ec49d959b05, startTime=2023-01-17T00:52:38.686443, finishTime=2023-01-17T00:52:38.688065) of size 3
INFO  [2023-01-17 00:52:38,688] com.bakdata.conquery.models.execution.ManagedExecution: DONE eb0cb0d7-d445-49be-911c-178acb6fee30 ManagedQuery within PT0.005592S
127.0.0.1 - - [17/Jan/2023:00:52:38 +0000] "GET /api/datasets/COUNT_QUARTERS_AGGREGATOR$20Test/queries/COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30 HTTP/1.1" 200 1675 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:38,713] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test], queryId=eb0cb0d7-d445-49be-911c-178acb6fee30, label=concept	@§$, creationTime=2023-01-17T00:52:38.682917, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@712417ab[Count = 0], startTime=2023-01-17T00:52:38.683160, finishTime=2023-01-17T00:52:38.688752, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4a2da44b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7cbc67f, com.bakdata.conquery.models.query.ColumnDescriptor@69ec9619, com.bakdata.conquery.models.query.ColumnDescriptor@38245dc0]) download on dataset Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:38,713] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test], queryId=eb0cb0d7-d445-49be-911c-178acb6fee30, label=concept	@§$, creationTime=2023-01-17T00:52:38.682917, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@712417ab[Count = 0], startTime=2023-01-17T00:52:38.683160, finishTime=2023-01-17T00:52:38.688752, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4a2da44b), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_COUNT_QUARTERS_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7cbc67f, com.bakdata.conquery.models.query.ColumnDescriptor@69ec9619, com.bakdata.conquery.models.query.ColumnDescriptor@38245dc0]) on dataset Dataset[label=null, name=COUNT_QUARTERS_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:38 +0000] "GET /api/datasets/COUNT_QUARTERS_AGGREGATOR%20Test/result/COUNT_QUARTERS_AGGREGATOR$20Test.eb0cb0d7-d445-49be-911c-178acb6fee30.csv?pretty=false HTTP/1.1" 200 167 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:38,715] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest COUNT_QUARTERS_AGGREGATOR Test on 6 rows
INFO  [2023-01-17 00:52:38,715] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-17 00:52:38,716] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-17 00:52:38,716] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=COUNT_QUARTERS_AGGREGATOR Test, name=COUNT_QUARTERS_AGGREGATOR Test]
INFO  [2023-01-17 00:52:38,716] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS_AGGREGATOR Test_01cbe94a-ed2e-4790-bb08-7ec49d959b05
INFO  [2023-01-17 00:52:38,716] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_COUNT_QUARTERS_AGGREGATOR Test_e5fe4dad-483e-470f-a0f4-7a1e5c525904
INFO  [2023-01-17 00:52:38,746] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-17 00:52:38,747] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS_AGGREGATOR Test_e5fe4dad-483e-470f-a0f4-7a1e5c525904
INFO  [2023-01-17 00:52:38,747] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_COUNT_QUARTERS_AGGREGATOR Test_01cbe94a-ed2e-4790-bb08-7ec49d959b05
INFO  [2023-01-17 00:52:38,847] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of COUNT_QUARTERS_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:38,847] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,865] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test COUNT_QUARTERS_AGGREGATOR Test
INFO  [2023-01-17 00:52:38,865] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-17 00:52:38,865] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:38,865] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:38,867] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-17 00:52:38,867] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-17 00:52:38,867] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:38,867] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:38,873] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,874] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_5259fa0e-c991-4d36-9cdf-7e19f884b8cd are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:38,874] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_5259fa0e-c991-4d36-9cdf-7e19f884b8cd are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:38,874] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:38,874] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_1fcecc68-4bff-43f1-b745-da0540f9c1a2 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:38,874] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_1fcecc68-4bff-43f1-b745-da0540f9c1a2 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:38,874] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:38,973] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,979] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:38,980] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:38,980] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:39,097] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,207] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:39,207] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:39,207] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 381 B in total
INFO  [2023-01-17 00:52:39,207] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000185649sINFO  [2023-01-17 00:52:39,226] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:39,226] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@68957147)
INFO  [2023-01-17 00:52:39,226] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@52919ad3), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@a35f570), dateReader=com.bakdata.conquery.util.DateReader@5ec04de8, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:52:39,228] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:39,228] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:39,228] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:39,242] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DATE_DISTANCE_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:39,242] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:39 +0000] "POST /admin/datasets/DATE_DISTANCE_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:39,243] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:39,243] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:39,243] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:39,245] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:52:39,246] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:39,246] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:39,247] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.0
WARN  [2023-01-17 00:52:39,247] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:39,247] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:39,247] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.2
INFO  [2023-01-17 00:52:39,247] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR$20Test.table.table.3
INFO  [2023-01-17 00:52:39,353] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,358] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,371] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,372] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:39,372] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:39,478] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:39,493] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:39,493] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[29f15b4c-795d-4229-ade0-1dec53acd353] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:39,496] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353
INFO  [2023-01-17 00:52:39,496] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353
127.0.0.1 - - [17/Jan/2023:00:52:39 +0000] "POST /api/datasets/DATE_DISTANCE_AGGREGATOR$20Test/queries HTTP/1.1" 201 1453 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:39,497] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353] with 4 results within PT0.001438S
INFO  [2023-01-17 00:52:39,498] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353] with 6 results within PT0.001724S
INFO  [2023-01-17 00:52:39,498] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353, workerId=DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_1fcecc68-4bff-43f1-b745-da0540f9c1a2, startTime=2023-01-17T00:52:39.496546, finishTime=2023-01-17T00:52:39.497984) of size 4
INFO  [2023-01-17 00:52:39,498] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353, workerId=DATE_DISTANCE_AGGREGATOR$20Test.worker_DATE_DISTANCE_AGGREGATOR$20Test_5259fa0e-c991-4d36-9cdf-7e19f884b8cd, startTime=2023-01-17T00:52:39.496409, finishTime=2023-01-17T00:52:39.498133) of size 6
INFO  [2023-01-17 00:52:39,499] com.bakdata.conquery.models.execution.ManagedExecution: DONE 29f15b4c-795d-4229-ade0-1dec53acd353 ManagedQuery within PT0.005293S
127.0.0.1 - - [17/Jan/2023:00:52:39 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR$20Test/queries/DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353 HTTP/1.1" 200 1768 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:39,522] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test], queryId=29f15b4c-795d-4229-ade0-1dec53acd353, label=concept	@§$, creationTime=2023-01-17T00:52:39.493554, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@719b1a0a[Count = 0], startTime=2023-01-17T00:52:39.493697, finishTime=2023-01-17T00:52:39.498990, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@459aad82), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7a73bec3, com.bakdata.conquery.models.query.ColumnDescriptor@29d93b12, com.bakdata.conquery.models.query.ColumnDescriptor@3b3f69]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:39,522] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test], queryId=29f15b4c-795d-4229-ade0-1dec53acd353, label=concept	@§$, creationTime=2023-01-17T00:52:39.493554, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@719b1a0a[Count = 0], startTime=2023-01-17T00:52:39.493697, finishTime=2023-01-17T00:52:39.498990, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@459aad82), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7a73bec3, com.bakdata.conquery.models.query.ColumnDescriptor@29d93b12, com.bakdata.conquery.models.query.ColumnDescriptor@3b3f69]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:39 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR%20Test/result/DATE_DISTANCE_AGGREGATOR$20Test.29f15b4c-795d-4229-ade0-1dec53acd353.csv?pretty=false HTTP/1.1" 200 314 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:39,524] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGGREGATOR Test on 11 rows
INFO  [2023-01-17 00:52:39,524] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-17 00:52:39,524] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-17 00:52:39,524] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR Test, name=DATE_DISTANCE_AGGREGATOR Test]
INFO  [2023-01-17 00:52:39,524] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR Test_1fcecc68-4bff-43f1-b745-da0540f9c1a2
INFO  [2023-01-17 00:52:39,524] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR Test_5259fa0e-c991-4d36-9cdf-7e19f884b8cd
INFO  [2023-01-17 00:52:39,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-17 00:52:39,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR Test_5259fa0e-c991-4d36-9cdf-7e19f884b8cd
INFO  [2023-01-17 00:52:39,573] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR Test_1fcecc68-4bff-43f1-b745-da0540f9c1a2
INFO  [2023-01-17 00:52:39,648] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:39,648] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,781] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGGREGATOR Test
INFO  [2023-01-17 00:52:39,782] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-17 00:52:39,782] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:39,782] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:39,783] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-17 00:52:39,783] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-17 00:52:39,783] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:39,783] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:39,785] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_32e0728f-1932-40d5-9cb1-32cf1fd97edf are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:39,785] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_32e0728f-1932-40d5-9cb1-32cf1fd97edf are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:39,785] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:39,785] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_d964f11d-24b7-41db-b861-17c6f2e6291e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:39,785] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_d964f11d-24b7-41db-b861-17c6f2e6291e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:39,785] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:39,789] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,888] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,895] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:39,896] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR2$20Test.table
INFO  [2023-01-17 00:52:39,896] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DATE_DISTANCE_AGGREGATOR2$20Test.table
INFO  [2023-01-17 00:52:40,013] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,124] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:40,124] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:40,124] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 381 B in total
INFO  [2023-01-17 00:52:40,124] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGGREGATOR2 Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000396774sINFO  [2023-01-17 00:52:40,165] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:40,165] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@29a71023)
INFO  [2023-01-17 00:52:40,165] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@5d1fe47f), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@306ba0d), dateReader=com.bakdata.conquery.util.DateReader@263d4168, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:52:40,167] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGGREGATOR2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:40,167] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:40,167] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DATE_DISTANCE_AGGREGATOR2 Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:40,183] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DATE_DISTANCE_AGGREGATOR2$20Test.table
127.0.0.1 - - [17/Jan/2023:00:52:40 +0000] "POST /admin/datasets/DATE_DISTANCE_AGGREGATOR2%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DATE_DISTANCE_AGGREGATOR2+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:40,183] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,184] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:40,184] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:40,184] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:40,185] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:52:40,185] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR2$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:40,185] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DATE_DISTANCE_AGGREGATOR2$20Test.table.table], containing 10 entries.
WARN  [2023-01-17 00:52:40,186] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:40,186] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.0
INFO  [2023-01-17 00:52:40,186] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.1
INFO  [2023-01-17 00:52:40,186] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.2
INFO  [2023-01-17 00:52:40,187] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DATE_DISTANCE_AGGREGATOR2$20Test.table.table.3
INFO  [2023-01-17 00:52:40,292] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,299] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,308] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,309] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:40,309] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:40,414] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DATE_DISTANCE_AGGREGATOR2 Test QUERY INIT
INFO  [2023-01-17 00:52:40,429] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DATE_DISTANCE_AGGREGATOR2$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:40,430] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[9474126d-ade3-4091-9312-3f5d09d0e505] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR2 Test))]]
INFO  [2023-01-17 00:52:40,433] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505
INFO  [2023-01-17 00:52:40,433] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505
127.0.0.1 - - [17/Jan/2023:00:52:40 +0000] "POST /api/datasets/DATE_DISTANCE_AGGREGATOR2$20Test/queries HTTP/1.1" 201 1458 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:40,435] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505] with 4 results within PT0.001585S
INFO  [2023-01-17 00:52:40,435] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505] with 6 results within PT0.001953S
INFO  [2023-01-17 00:52:40,436] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505, workerId=DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_d964f11d-24b7-41db-b861-17c6f2e6291e, startTime=2023-01-17T00:52:40.433886, finishTime=2023-01-17T00:52:40.435471) of size 4
INFO  [2023-01-17 00:52:40,436] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505, workerId=DATE_DISTANCE_AGGREGATOR2$20Test.worker_DATE_DISTANCE_AGGREGATOR2$20Test_32e0728f-1932-40d5-9cb1-32cf1fd97edf, startTime=2023-01-17T00:52:40.433768, finishTime=2023-01-17T00:52:40.435721) of size 6
INFO  [2023-01-17 00:52:40,436] com.bakdata.conquery.models.execution.ManagedExecution: DONE 9474126d-ade3-4091-9312-3f5d09d0e505 ManagedQuery within PT0.006135S
127.0.0.1 - - [17/Jan/2023:00:52:40 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR2$20Test/queries/DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505 HTTP/1.1" 200 1778 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:40,462] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test], queryId=9474126d-ade3-4091-9312-3f5d09d0e505, label=concept	@§$, creationTime=2023-01-17T00:52:40.430145, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1f7f1100[Count = 0], startTime=2023-01-17T00:52:40.430318, finishTime=2023-01-17T00:52:40.436453, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66e81ce), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4253bed, com.bakdata.conquery.models.query.ColumnDescriptor@1dfae593, com.bakdata.conquery.models.query.ColumnDescriptor@7aea003d]) download on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:40,462] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test], queryId=9474126d-ade3-4091-9312-3f5d09d0e505, label=concept	@§$, creationTime=2023-01-17T00:52:40.430145, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1f7f1100[Count = 0], startTime=2023-01-17T00:52:40.430318, finishTime=2023-01-17T00:52:40.436453, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@66e81ce), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DATE_DISTANCE_AGGREGATOR2 Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@4253bed, com.bakdata.conquery.models.query.ColumnDescriptor@1dfae593, com.bakdata.conquery.models.query.ColumnDescriptor@7aea003d]) on dataset Dataset[label=null, name=DATE_DISTANCE_AGGREGATOR2 Test]
127.0.0.1 - - [17/Jan/2023:00:52:40 +0000] "GET /api/datasets/DATE_DISTANCE_AGGREGATOR2%20Test/result/DATE_DISTANCE_AGGREGATOR2$20Test.9474126d-ade3-4091-9312-3f5d09d0e505.csv?pretty=false HTTP/1.1" 200 314 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:52:40,485] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DATE_DISTANCE_AGGREGATOR2 Test on 11 rows
INFO  [2023-01-17 00:52:40,485] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-17 00:52:40,485] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-17 00:52:40,485] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DATE_DISTANCE_AGGREGATOR2 Test, name=DATE_DISTANCE_AGGREGATOR2 Test]
INFO  [2023-01-17 00:52:40,485] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR2 Test_32e0728f-1932-40d5-9cb1-32cf1fd97edf
INFO  [2023-01-17 00:52:40,485] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DATE_DISTANCE_AGGREGATOR2 Test_d964f11d-24b7-41db-b861-17c6f2e6291e
INFO  [2023-01-17 00:52:40,583] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-17 00:52:40,584] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR2 Test_32e0728f-1932-40d5-9cb1-32cf1fd97edf
INFO  [2023-01-17 00:52:40,584] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DATE_DISTANCE_AGGREGATOR2 Test_d964f11d-24b7-41db-b861-17c6f2e6291e
INFO  [2023-01-17 00:52:40,587] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DATE_DISTANCE_AGGREGATOR2$20Test
INFO  [2023-01-17 00:52:40,587] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,714] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DATE_DISTANCE_AGGREGATOR2 Test
INFO  [2023-01-17 00:52:40,715] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:40,715] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:40,716] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:40,717] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:40,717] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:40,717] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:40,717] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_a614ca07-1efe-40b2-ad9c-bbf048d23308 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_a614ca07-1efe-40b2-ad9c-bbf048d23308 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_8cdad9b8-402e-4116-9598-05808f00214b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_8cdad9b8-402e-4116-9598-05808f00214b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:40,718] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:40,822] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,829] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:40,829] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:40,829] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table DURATION_SUM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:40,942] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,051] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:41,051] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:41,051] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 166 B in total
INFO  [2023-01-17 00:52:41,051] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000363425sINFO  [2023-01-17 00:52:41,088] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=6, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:52:41,088] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=6, nullLines=0), minParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14610, maxValue=15927), dateReader=com.bakdata.conquery.util.DateReader@2668e13a), maxParser=DateParser(super=Parser(lines=6, nullLines=0), subType=IntegerParser(super=Parser(lines=6, nullLines=0), minValue=14640, maxValue=15928), dateReader=com.bakdata.conquery.util.DateReader@2a340ee1), dateReader=com.bakdata.conquery.util.DateReader@21e1e62f, onlyQuarters=false, maxValue=15928, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:52:41,090] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:41,090] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:41,090] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_DURATION_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:41,109] com.bakdata.conquery.models.jobs.ImportJob: Importing table into DURATION_SUM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:41,109] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:41 +0000] "POST /admin/datasets/DURATION_SUM_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_DURATION_SUM_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:41,111] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:41,111] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:41,111] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:41,113] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:41,113] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_AGGREGATOR$20Test.table.table], containing 6 entries.
INFO  [2023-01-17 00:52:41,113] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[DURATION_SUM_AGGREGATOR$20Test.table.table], containing 6 entries.
WARN  [2023-01-17 00:52:41,115] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:41,115] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:41,115] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received DURATION_SUM_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:41,220] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,226] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,235] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,235] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:41,235] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:41,341] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: DURATION_SUM_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:41,356] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[DURATION_SUM_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:41,356] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[985b1fe1-915e-4e69-a7fe-0959dc06a1bc] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:41,359] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc
INFO  [2023-01-17 00:52:41,359] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc
INFO  [2023-01-17 00:52:41,360] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc] with 1 results within PT0.000928S
127.0.0.1 - - [17/Jan/2023:00:52:41 +0000] "POST /api/datasets/DURATION_SUM_AGGREGATOR$20Test/queries HTTP/1.1" 201 1346 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:41,360] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc] with 3 results within PT0.001228S
INFO  [2023-01-17 00:52:41,361] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc, workerId=DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_8cdad9b8-402e-4116-9598-05808f00214b, startTime=2023-01-17T00:52:41.359542, finishTime=2023-01-17T00:52:41.360470) of size 1
INFO  [2023-01-17 00:52:41,361] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc, workerId=DURATION_SUM_AGGREGATOR$20Test.worker_DURATION_SUM_AGGREGATOR$20Test_a614ca07-1efe-40b2-ad9c-bbf048d23308, startTime=2023-01-17T00:52:41.359516, finishTime=2023-01-17T00:52:41.360744) of size 3
INFO  [2023-01-17 00:52:41,361] com.bakdata.conquery.models.execution.ManagedExecution: DONE 985b1fe1-915e-4e69-a7fe-0959dc06a1bc ManagedQuery within PT0.004702S
127.0.0.1 - - [17/Jan/2023:00:52:41 +0000] "GET /api/datasets/DURATION_SUM_AGGREGATOR$20Test/queries/DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc HTTP/1.1" 200 1657 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:41,389] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test], queryId=985b1fe1-915e-4e69-a7fe-0959dc06a1bc, label=concept	@§$, creationTime=2023-01-17T00:52:41.356558, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5a652e01[Count = 0], startTime=2023-01-17T00:52:41.356730, finishTime=2023-01-17T00:52:41.361432, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1731d0a6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@836f2c6, com.bakdata.conquery.models.query.ColumnDescriptor@197dfdc9, com.bakdata.conquery.models.query.ColumnDescriptor@6a9bcdc1]) download on dataset Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:41,389] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test], queryId=985b1fe1-915e-4e69-a7fe-0959dc06a1bc, label=concept	@§$, creationTime=2023-01-17T00:52:41.356558, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@5a652e01[Count = 0], startTime=2023-01-17T00:52:41.356730, finishTime=2023-01-17T00:52:41.361432, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@1731d0a6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_DURATION_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@836f2c6, com.bakdata.conquery.models.query.ColumnDescriptor@197dfdc9, com.bakdata.conquery.models.query.ColumnDescriptor@6a9bcdc1]) on dataset Dataset[label=null, name=DURATION_SUM_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:41 +0000] "GET /api/datasets/DURATION_SUM_AGGREGATOR%20Test/result/DURATION_SUM_AGGREGATOR$20Test.985b1fe1-915e-4e69-a7fe-0959dc06a1bc.csv?pretty=false HTTP/1.1" 200 142 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:52:41,407] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest DURATION_SUM_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:41,407] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:41,408] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:41,408] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=DURATION_SUM_AGGREGATOR Test, name=DURATION_SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:41,408] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_AGGREGATOR Test_8cdad9b8-402e-4116-9598-05808f00214b
INFO  [2023-01-17 00:52:41,408] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_DURATION_SUM_AGGREGATOR Test_a614ca07-1efe-40b2-ad9c-bbf048d23308
INFO  [2023-01-17 00:52:41,417] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:41,418] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_AGGREGATOR Test_a614ca07-1efe-40b2-ad9c-bbf048d23308
INFO  [2023-01-17 00:52:41,418] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_DURATION_SUM_AGGREGATOR Test_8cdad9b8-402e-4116-9598-05808f00214b
INFO  [2023-01-17 00:52:41,515] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of DURATION_SUM_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:41,515] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,541] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test DURATION_SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:41,541] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-17 00:52:41,541] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:41,541] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:41,542] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-17 00:52:41,543] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:41,542] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-17 00:52:41,543] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_c3e3de80-c9de-4eb8-ab6d-d11c9f9e982b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_c3e3de80-c9de-4eb8-ab6d-d11c9f9e982b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_b2166542-b305-4d99-9977-d476c1ad2ece are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_b2166542-b305-4d99-9977-d476c1ad2ece are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:41,544] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:41,649] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,655] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,656] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table
INFO  [2023-01-17 00:52:41,656] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table
INFO  [2023-01-17 00:52:41,774] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:41,885] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:41,885] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:41,885] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 388 B in total
INFO  [2023-01-17 00:52:41,885] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000315611sINFO  [2023-01-17 00:52:41,917] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:41,918] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@597a23cb)
INFO  [2023-01-17 00:52:41,918] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@7a4ee74a), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@5850d860), dateReader=com.bakdata.conquery.util.DateReader@6e30c7b5, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:52:41,919] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:41,920] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:41,920] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:41,930] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table
INFO  [2023-01-17 00:52:41,930] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:41 +0000] "POST /admin/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_EVENT_DATE_AGGREGATOR_NO_RESTRICTION+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:41,931] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:41,931] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:41,931] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:41,933] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:52:41,934] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:41,934] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:41,935] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.0
WARN  [2023-01-17 00:52:41,935] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:41,935] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.2
INFO  [2023-01-17 00:52:41,935] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.1
INFO  [2023-01-17 00:52:41,935] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.table.table.3
INFO  [2023-01-17 00:52:42,040] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,045] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,058] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,058] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:42,058] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:42,164] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test QUERY INIT
INFO  [2023-01-17 00:52:42,181] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:42,181] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b26959db-cda9-453b-ad4e-67268fa5be4d] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test))]]
INFO  [2023-01-17 00:52:42,185] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d
INFO  [2023-01-17 00:52:42,185] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d
INFO  [2023-01-17 00:52:42,186] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d] with 4 results within PT0.001428S
INFO  [2023-01-17 00:52:42,187] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d] with 6 results within PT0.00181S
127.0.0.1 - - [17/Jan/2023:00:52:42 +0000] "POST /api/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test/queries HTTP/1.1" 201 1675 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:42,187] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d, workerId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_b2166542-b305-4d99-9977-d476c1ad2ece, startTime=2023-01-17T00:52:42.185380, finishTime=2023-01-17T00:52:42.186808) of size 4
INFO  [2023-01-17 00:52:42,187] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d, workerId=EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test_c3e3de80-c9de-4eb8-ab6d-d11c9f9e982b, startTime=2023-01-17T00:52:42.185203, finishTime=2023-01-17T00:52:42.187013) of size 6
INFO  [2023-01-17 00:52:42,187] com.bakdata.conquery.models.execution.ManagedExecution: DONE b26959db-cda9-453b-ad4e-67268fa5be4d ManagedQuery within PT0.00594S
127.0.0.1 - - [17/Jan/2023:00:52:42 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test/queries/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d HTTP/1.1" 200 2038 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:42,214] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test], queryId=b26959db-cda9-453b-ad4e-67268fa5be4d, label=concept	@§$, creationTime=2023-01-17T00:52:42.181761, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7d4ed2f1[Count = 0], startTime=2023-01-17T00:52:42.181950, finishTime=2023-01-17T00:52:42.187890, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4805f78d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3fb7efd4, com.bakdata.conquery.models.query.ColumnDescriptor@33b6ed7b, com.bakdata.conquery.models.query.ColumnDescriptor@386478a6, com.bakdata.conquery.models.query.ColumnDescriptor@5fba49b8]) download on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:42,214] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test], queryId=b26959db-cda9-453b-ad4e-67268fa5be4d, label=concept	@§$, creationTime=2023-01-17T00:52:42.181761, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@7d4ed2f1[Count = 0], startTime=2023-01-17T00:52:42.181950, finishTime=2023-01-17T00:52:42.187890, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4805f78d), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3fb7efd4, com.bakdata.conquery.models.query.ColumnDescriptor@33b6ed7b, com.bakdata.conquery.models.query.ColumnDescriptor@386478a6, com.bakdata.conquery.models.query.ColumnDescriptor@5fba49b8]) on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
127.0.0.1 - - [17/Jan/2023:00:52:42 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_NO_RESTRICTION%20Test/result/EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test.b26959db-cda9-453b-ad4e-67268fa5be4d.csv?pretty=false HTTP/1.1" 200 784 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:42,217] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test on 11 rows
INFO  [2023-01-17 00:52:42,217] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-17 00:52:42,218] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-17 00:52:42,218] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test]
INFO  [2023-01-17 00:52:42,218] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_b2166542-b305-4d99-9977-d476c1ad2ece
INFO  [2023-01-17 00:52:42,218] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_c3e3de80-c9de-4eb8-ab6d-d11c9f9e982b
INFO  [2023-01-17 00:52:42,247] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-17 00:52:42,247] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_b2166542-b305-4d99-9977-d476c1ad2ece
INFO  [2023-01-17 00:52:42,247] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test_c3e3de80-c9de-4eb8-ab6d-d11c9f9e982b
INFO  [2023-01-17 00:52:42,347] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EVENT_DATE_AGGREGATOR_NO_RESTRICTION$20Test
INFO  [2023-01-17 00:52:42,348] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,369] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EVENT_DATE_AGGREGATOR_NO_RESTRICTION Test
INFO  [2023-01-17 00:52:42,369] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-17 00:52:42,369] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:42,369] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:42,370] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-17 00:52:42,370] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-17 00:52:42,370] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:42,371] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:42,372] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_e244188d-c139-4858-aa23-f273c739d4fb are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:42,372] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_e244188d-c139-4858-aa23-f273c739d4fb are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:42,372] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:42,372] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_fe27ca71-ebd6-4f7f-8da0-b00d276bfc4a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:42,372] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_fe27ca71-ebd6-4f7f-8da0-b00d276bfc4a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:42,372] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:42,377] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,476] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,483] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,483] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table
INFO  [2023-01-17 00:52:42,483] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table
INFO  [2023-01-17 00:52:42,600] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,712] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:42,712] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:42,712] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 388 B in total
INFO  [2023-01-17 00:52:42,712] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EVENT_DATE_AGGREGATOR_RESTRICTION Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000250332sINFO  [2023-01-17 00:52:42,738] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=10, sum=10, min=1, average=1.000000, max=1}
INFO  [2023-01-17 00:52:42,738] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[geburtsdatum] with DateParser(super=Parser(lines=10, nullLines=1), subType=IntegerParser(super=Parser(lines=10, nullLines=1), minValue=10226, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@48bf8ed0)
INFO  [2023-01-17 00:52:42,738] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[indexdatum] with DateRangeParser(super=Parser(lines=10, nullLines=0), minParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14610, maxValue=14610), dateReader=com.bakdata.conquery.util.DateReader@30a57a70), maxParser=DateParser(super=Parser(lines=10, nullLines=0), subType=IntegerParser(super=Parser(lines=10, nullLines=0), minValue=14699, maxValue=14699), dateReader=com.bakdata.conquery.util.DateReader@28f77267), dateReader=com.bakdata.conquery.util.DateReader@73ef5920, onlyQuarters=true, maxValue=14699, minValue=14610, anyOpen=false)
INFO  [2023-01-17 00:52:42,740] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EVENT_DATE_AGGREGATOR_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:42,740] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:42,740] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EVENT_DATE_AGGREGATOR_RESTRICTION Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:42,757] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table
127.0.0.1 - - [17/Jan/2023:00:52:42 +0000] "POST /admin/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_EVENT_DATE_AGGREGATOR_RESTRICTION+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:42,758] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,759] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:42,759] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:42,759] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:42,762] com.bakdata.conquery.models.jobs.ImportJob: Start sending 4 Buckets
INFO  [2023-01-17 00:52:42,762] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:42,762] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:42,763] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.0
WARN  [2023-01-17 00:52:42,764] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:42,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.1
INFO  [2023-01-17 00:52:42,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.2
INFO  [2023-01-17 00:52:42,764] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.table.table.3
INFO  [2023-01-17 00:52:42,869] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,874] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,883] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:42,884] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:42,884] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:42,990] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EVENT_DATE_AGGREGATOR_RESTRICTION Test QUERY INIT
INFO  [2023-01-17 00:52:43,005] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:43,005] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b03991f4-1575-4c1d-8fc1-7de9bb3a1931] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_RESTRICTION Test))]]
INFO  [2023-01-17 00:52:43,009] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931
INFO  [2023-01-17 00:52:43,009] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "POST /api/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test/queries HTTP/1.1" 201 1756 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:43,010] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931] with 4 results within PT0.001418S
INFO  [2023-01-17 00:52:43,011] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931] with 6 results within PT0.001814S
INFO  [2023-01-17 00:52:43,011] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931, workerId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_e244188d-c139-4858-aa23-f273c739d4fb, startTime=2023-01-17T00:52:43.009301, finishTime=2023-01-17T00:52:43.010719) of size 4
INFO  [2023-01-17 00:52:43,011] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931, workerId=EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.worker_EVENT_DATE_AGGREGATOR_RESTRICTION$20Test_fe27ca71-ebd6-4f7f-8da0-b00d276bfc4a, startTime=2023-01-17T00:52:43.009275, finishTime=2023-01-17T00:52:43.011089) of size 6
INFO  [2023-01-17 00:52:43,011] com.bakdata.conquery.models.execution.ManagedExecution: DONE b03991f4-1575-4c1d-8fc1-7de9bb3a1931 ManagedQuery within PT0.006062S
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test/queries/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931 HTTP/1.1" 200 2108 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:43,038] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test], queryId=b03991f4-1575-4c1d-8fc1-7de9bb3a1931, label=concept	@§$, creationTime=2023-01-17T00:52:43.005534, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@37291ea0[Count = 0], startTime=2023-01-17T00:52:43.005705, finishTime=2023-01-17T00:52:43.011767, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5a02c0c5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@64859556, com.bakdata.conquery.models.query.ColumnDescriptor@3202fa29, com.bakdata.conquery.models.query.ColumnDescriptor@4da13ed7, com.bakdata.conquery.models.query.ColumnDescriptor@169db4e]) download on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:43,038] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test], queryId=b03991f4-1575-4c1d-8fc1-7de9bb3a1931, label=concept	@§$, creationTime=2023-01-17T00:52:43.005534, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@37291ea0[Count = 0], startTime=2023-01-17T00:52:43.005705, finishTime=2023-01-17T00:52:43.011767, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@5a02c0c5), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EVENT_DATE_AGGREGATOR_RESTRICTION Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=10, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@64859556, com.bakdata.conquery.models.query.ColumnDescriptor@3202fa29, com.bakdata.conquery.models.query.ColumnDescriptor@4da13ed7, com.bakdata.conquery.models.query.ColumnDescriptor@169db4e]) on dataset Dataset[label=null, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "GET /api/datasets/EVENT_DATE_AGGREGATOR_RESTRICTION%20Test/result/EVENT_DATE_AGGREGATOR_RESTRICTION$20Test.b03991f4-1575-4c1d-8fc1-7de9bb3a1931.csv?pretty=false HTTP/1.1" 200 784 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:43,055] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EVENT_DATE_AGGREGATOR_RESTRICTION Test on 11 rows
INFO  [2023-01-17 00:52:43,055] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-17 00:52:43,056] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-17 00:52:43,056] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EVENT_DATE_AGGREGATOR_RESTRICTION Test, name=EVENT_DATE_AGGREGATOR_RESTRICTION Test]
INFO  [2023-01-17 00:52:43,056] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_e244188d-c139-4858-aa23-f273c739d4fb
INFO  [2023-01-17 00:52:43,056] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_fe27ca71-ebd6-4f7f-8da0-b00d276bfc4a
INFO  [2023-01-17 00:52:43,071] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-17 00:52:43,072] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_e244188d-c139-4858-aa23-f273c739d4fb
INFO  [2023-01-17 00:52:43,072] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EVENT_DATE_AGGREGATOR_RESTRICTION Test_fe27ca71-ebd6-4f7f-8da0-b00d276bfc4a
INFO  [2023-01-17 00:52:43,165] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EVENT_DATE_AGGREGATOR_RESTRICTION$20Test
INFO  [2023-01-17 00:52:43,165] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,190] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EVENT_DATE_AGGREGATOR_RESTRICTION Test
INFO  [2023-01-17 00:52:43,190] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:43,190] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:43,190] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:43,192] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-17 00:52:43,192] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-17 00:52:43,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:43,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:43,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_2e705c7b-ecba-4dfd-bd0c-d82ccde2d2e7 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:43,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_2e705c7b-ecba-4dfd-bd0c-d82ccde2d2e7 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:43,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:43,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_317e7913-14cf-4e7f-9ffc-cf46e81bcb74 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:43,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_317e7913-14cf-4e7f-9ffc-cf46e81bcb74 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:43,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:43,198] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,298] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,305] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,306] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table
INFO  [2023-01-17 00:52:43,306] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table
INFO  [2023-01-17 00:52:43,416] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,526] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:43,526] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:43,526] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 210 B in total
INFO  [2023-01-17 00:52:43,526] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00045319sINFO  [2023-01-17 00:52:43,572] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-17 00:52:43,572] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=7, nullLines=0), minParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16251, maxValue=16251), dateReader=com.bakdata.conquery.util.DateReader@50f1042b), maxParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@66d4a360), dateReader=com.bakdata.conquery.util.DateReader@61d626d, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-17 00:52:43,572] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=7, nullLines=2), requiredPrecision=4.9E-324, floatULP=2.384185791015625E-7)
INFO  [2023-01-17 00:52:43,574] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:43,575] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:43,575] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:43,585] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table
INFO  [2023-01-17 00:52:43,585] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "POST /admin/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_EXISTS_AGGREGATOR+%26+NUMBER+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:43,586] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:43,586] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:43,586] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:43,587] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:43,587] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table], containing 7 entries.
INFO  [2023-01-17 00:52:43,587] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table], containing 7 entries.
WARN  [2023-01-17 00:52:43,588] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:43,588] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table.0
INFO  [2023-01-17 00:52:43,588] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test.table.table.1
INFO  [2023-01-17 00:52:43,693] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,699] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,713] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:43,713] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:43,713] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:43,819] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EXISTS_AGGREGATOR & NUMBER Test QUERY INIT
INFO  [2023-01-17 00:52:43,832] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EXISTS_AGGREGATOR$20&$20NUMBER$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:43,832] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1efaeca5-8764-4071-b608-89f9daf6db2a] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test))]]
INFO  [2023-01-17 00:52:43,836] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a
INFO  [2023-01-17 00:52:43,836] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "POST /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test/queries HTTP/1.1" 201 1483 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:43,837] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a] with 1 results within PT0.001098S
INFO  [2023-01-17 00:52:43,837] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a] with 2 results within PT0.001304S
INFO  [2023-01-17 00:52:43,838] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_2e705c7b-ecba-4dfd-bd0c-d82ccde2d2e7, startTime=2023-01-17T00:52:43.836634, finishTime=2023-01-17T00:52:43.837732) of size 1
INFO  [2023-01-17 00:52:43,838] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test.worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test_317e7913-14cf-4e7f-9ffc-cf46e81bcb74, startTime=2023-01-17T00:52:43.836602, finishTime=2023-01-17T00:52:43.837906) of size 2
INFO  [2023-01-17 00:52:43,838] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1efaeca5-8764-4071-b608-89f9daf6db2a ManagedQuery within PT0.005806S
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "GET /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test/queries/EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a HTTP/1.1" 200 1822 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:43,879] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test], queryId=1efaeca5-8764-4071-b608-89f9daf6db2a, label=concept	@§$, creationTime=2023-01-17T00:52:43.832606, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4b52a790[Count = 0], startTime=2023-01-17T00:52:43.832786, finishTime=2023-01-17T00:52:43.838592, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@32487129), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@11dfd98, com.bakdata.conquery.models.query.ColumnDescriptor@555926af, com.bakdata.conquery.models.query.ColumnDescriptor@b6965b3]) download on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:43,879] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test], queryId=1efaeca5-8764-4071-b608-89f9daf6db2a, label=concept	@§$, creationTime=2023-01-17T00:52:43.832606, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4b52a790[Count = 0], startTime=2023-01-17T00:52:43.832786, finishTime=2023-01-17T00:52:43.838592, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@32487129), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@11dfd98, com.bakdata.conquery.models.query.ColumnDescriptor@555926af, com.bakdata.conquery.models.query.ColumnDescriptor@b6965b3]) on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test]
127.0.0.1 - - [17/Jan/2023:00:52:43 +0000] "GET /api/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test/result/EXISTS_AGGREGATOR$20&$20NUMBER$20Test.1efaeca5-8764-4071-b608-89f9daf6db2a.csv?pretty=false HTTP/1.1" 200 112 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:43,881] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EXISTS_AGGREGATOR & NUMBER Test on 4 rows
INFO  [2023-01-17 00:52:43,882] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:43,882] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-17 00:52:43,882] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test, name=EXISTS_AGGREGATOR & NUMBER Test]
INFO  [2023-01-17 00:52:43,882] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test_317e7913-14cf-4e7f-9ffc-cf46e81bcb74
INFO  [2023-01-17 00:52:43,882] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test_2e705c7b-ecba-4dfd-bd0c-d82ccde2d2e7
INFO  [2023-01-17 00:52:43,892] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:43,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test_2e705c7b-ecba-4dfd-bd0c-d82ccde2d2e7
INFO  [2023-01-17 00:52:43,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test_317e7913-14cf-4e7f-9ffc-cf46e81bcb74
INFO  [2023-01-17 00:52:43,988] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EXISTS_AGGREGATOR$20&$20NUMBER$20Test
INFO  [2023-01-17 00:52:43,988] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,022] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:44,023] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:44,023] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:44,023] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:44,024] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-17 00:52:44,024] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-17 00:52:44,024] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:44,024] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_967e4dda-857e-4f50-8a91-e6d7e5bf31b4 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_967e4dda-857e-4f50-8a91-e6d7e5bf31b4 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_7a909a6c-114f-46e4-ab0f-c1e708ee5659 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_7a909a6c-114f-46e4-ab0f-c1e708ee5659 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:44,025] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:44,130] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,136] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,137] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table
INFO  [2023-01-17 00:52:44,137] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table
INFO  [2023-01-17 00:52:44,252] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,368] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:44,369] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:44,369] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 78 B in total
INFO  [2023-01-17 00:52:44,369] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test[1]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000239505sINFO  [2023-01-17 00:52:44,393] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=4, min=1, average=1.333333, max=2}
INFO  [2023-01-17 00:52:44,393] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=0), subType=IntegerParser(super=Parser(lines=4, nullLines=0), minValue=18262, maxValue=18262), dateReader=com.bakdata.conquery.util.DateReader@539a01b2)
INFO  [2023-01-17 00:52:44,393] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=4, nullLines=0), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:44,396] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:44,396] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:44,396] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test[1]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:44,416] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table
127.0.0.1 - - [17/Jan/2023:00:52:44 +0000] "POST /admin/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B1%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_EXISTS_AGGREGATOR+%26+NUMBER+Test%5B1%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:52:44,418] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,419] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:44,420] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:44,420] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:44,423] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:44,423] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table.table], containing 4 entries.
INFO  [2023-01-17 00:52:44,423] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table.table], containing 4 entries.
WARN  [2023-01-17 00:52:44,424] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:44,424] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].table.table.0
INFO  [2023-01-17 00:52:44,529] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,534] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,549] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,550] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:44,668] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EXISTS_AGGREGATOR & NUMBER Test QUERY INIT
INFO  [2023-01-17 00:52:44,682] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:44,683] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[ccba4cad-d288-46b8-9e24-46459fd4155b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[1]))]]
INFO  [2023-01-17 00:52:44,688] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].ccba4cad-d288-46b8-9e24-46459fd4155b
INFO  [2023-01-17 00:52:44,688] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ArrayConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].ccba4cad-d288-46b8-9e24-46459fd4155b
WARN  [2023-01-17 00:52:44,688] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:44,688] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].ccba4cad-d288-46b8-9e24-46459fd4155b] with 0 results within PT0.000277S
INFO  [2023-01-17 00:52:44,689] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].ccba4cad-d288-46b8-9e24-46459fd4155b, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_7a909a6c-114f-46e4-ab0f-c1e708ee5659, startTime=2023-01-17T00:52:44.688624, finishTime=2023-01-17T00:52:44.688901) of size 0
INFO  [2023-01-17 00:52:44,689] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].ccba4cad-d288-46b8-9e24-46459fd4155b] with 3 results within PT0.001377S
127.0.0.1 - - [17/Jan/2023:00:52:44 +0000] "POST /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D/queries HTTP/1.1" 201 3469 "-" "Conquery (test client)" 10
INFO  [2023-01-17 00:52:44,690] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].ccba4cad-d288-46b8-9e24-46459fd4155b, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]_967e4dda-857e-4f50-8a91-e6d7e5bf31b4, startTime=2023-01-17T00:52:44.688420, finishTime=2023-01-17T00:52:44.689797) of size 3
INFO  [2023-01-17 00:52:44,690] com.bakdata.conquery.models.execution.ManagedExecution: DONE ccba4cad-d288-46b8-9e24-46459fd4155b ManagedQuery within PT0.006885S
127.0.0.1 - - [17/Jan/2023:00:52:44 +0000] "GET /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D/queries/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D.ccba4cad-d288-46b8-9e24-46459fd4155b HTTP/1.1" 200 4132 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:44,719] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]], queryId=ccba4cad-d288-46b8-9e24-46459fd4155b, label=concept-1 concept-2	@§$, creationTime=2023-01-17T00:52:44.683332, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@512ce8b3[Count = 0], startTime=2023-01-17T00:52:44.683576, finishTime=2023-01-17T00:52:44.690461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2b1114c4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[1])), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6772e81, com.bakdata.conquery.models.query.ColumnDescriptor@2e118e70, com.bakdata.conquery.models.query.ColumnDescriptor@18ce9271, com.bakdata.conquery.models.query.ColumnDescriptor@3bda0298, com.bakdata.conquery.models.query.ColumnDescriptor@4e62044, com.bakdata.conquery.models.query.ColumnDescriptor@26264ca6, com.bakdata.conquery.models.query.ColumnDescriptor@4a971f09, com.bakdata.conquery.models.query.ColumnDescriptor@21b21a01]) download on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:44,719] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]], queryId=ccba4cad-d288-46b8-9e24-46459fd4155b, label=concept-1 concept-2	@§$, creationTime=2023-01-17T00:52:44.683332, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@512ce8b3[Count = 0], startTime=2023-01-17T00:52:44.683576, finishTime=2023-01-17T00:52:44.690461, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@2b1114c4), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[1])), query=com.bakdata.conquery.apiv1.query.ArrayConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6772e81, com.bakdata.conquery.models.query.ColumnDescriptor@2e118e70, com.bakdata.conquery.models.query.ColumnDescriptor@18ce9271, com.bakdata.conquery.models.query.ColumnDescriptor@3bda0298, com.bakdata.conquery.models.query.ColumnDescriptor@4e62044, com.bakdata.conquery.models.query.ColumnDescriptor@26264ca6, com.bakdata.conquery.models.query.ColumnDescriptor@4a971f09, com.bakdata.conquery.models.query.ColumnDescriptor@21b21a01]) on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[1]]
127.0.0.1 - - [17/Jan/2023:00:52:44 +0000] "GET /api/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B1%5D/result/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B1%5D.ccba4cad-d288-46b8-9e24-46459fd4155b.csv?pretty=false HTTP/1.1" 200 206 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:44,722] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EXISTS_AGGREGATOR & NUMBER Test on 4 rows
INFO  [2023-01-17 00:52:44,722] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EXISTS_AGGREGATOR & NUMBER Test[1]
INFO  [2023-01-17 00:52:44,722] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-17 00:52:44,722] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[1], name=EXISTS_AGGREGATOR & NUMBER Test[1]]
INFO  [2023-01-17 00:52:44,722] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[1]_7a909a6c-114f-46e4-ab0f-c1e708ee5659
INFO  [2023-01-17 00:52:44,722] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[1]_967e4dda-857e-4f50-8a91-e6d7e5bf31b4
INFO  [2023-01-17 00:52:44,724] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EXISTS_AGGREGATOR & NUMBER Test[1]
INFO  [2023-01-17 00:52:44,724] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EXISTS_AGGREGATOR$20&$20NUMBER$20Test[1]
INFO  [2023-01-17 00:52:44,724] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,725] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[1]_967e4dda-857e-4f50-8a91-e6d7e5bf31b4
INFO  [2023-01-17 00:52:44,725] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[1]_7a909a6c-114f-46e4-ab0f-c1e708ee5659
INFO  [2023-01-17 00:52:44,862] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:44,863] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:44,863] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:44,863] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:44,864] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-17 00:52:44,864] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:44,864] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-17 00:52:44,864] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:44,866] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_114919c1-5b0a-4a6d-bb61-0ddb5f109aaf are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:44,866] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_114919c1-5b0a-4a6d-bb61-0ddb5f109aaf are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:44,866] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:44,866] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_19eaaaae-fe97-4973-9997-20976d3d7f31 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:44,866] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_19eaaaae-fe97-4973-9997-20976d3d7f31 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:44,866] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:44,870] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,970] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,976] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:44,977] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table
INFO  [2023-01-17 00:52:44,977] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table
INFO  [2023-01-17 00:52:45,102] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,212] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:45,213] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:45,213] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 210 B in total
INFO  [2023-01-17 00:52:45,213] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test[2]/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000325372sINFO  [2023-01-17 00:52:45,246] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-17 00:52:45,246] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateRangeParser(super=Parser(lines=7, nullLines=0), minParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16251, maxValue=16251), dateReader=com.bakdata.conquery.util.DateReader@1aaf20ff), maxParser=DateParser(super=Parser(lines=7, nullLines=0), subType=IntegerParser(super=Parser(lines=7, nullLines=0), minValue=16616, maxValue=16616), dateReader=com.bakdata.conquery.util.DateReader@2eef035f), dateReader=com.bakdata.conquery.util.DateReader@4f0ca202, onlyQuarters=false, maxValue=16616, minValue=16251, anyOpen=false)
INFO  [2023-01-17 00:52:45,246] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with RealParser(super=Parser(lines=7, nullLines=2), requiredPrecision=4.9E-324, floatULP=2.384185791015625E-7)
INFO  [2023-01-17 00:52:45,248] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:45,248] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:45,248] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_EXISTS_AGGREGATOR & NUMBER Test[2]/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:45,266] com.bakdata.conquery.models.jobs.ImportJob: Importing table into EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table
INFO  [2023-01-17 00:52:45,266] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:45 +0000] "POST /admin/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B2%5D/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_EXISTS_AGGREGATOR+%26+NUMBER+Test%5B2%5D%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:45,267] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:45,267] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:45,267] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:45,268] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:45,268] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table], containing 7 entries.
INFO  [2023-01-17 00:52:45,268] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table], containing 7 entries.
WARN  [2023-01-17 00:52:45,268] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:45,269] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table.0
INFO  [2023-01-17 00:52:45,269] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].table.table.1
INFO  [2023-01-17 00:52:45,374] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,379] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,394] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,394] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:45,394] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:45,508] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: EXISTS_AGGREGATOR & NUMBER Test QUERY INIT
INFO  [2023-01-17 00:52:45,524] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:45,524] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[a849ec2c-fc99-48df-a748-81b48e10dc94] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[2]))]]
INFO  [2023-01-17 00:52:45,529] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].a849ec2c-fc99-48df-a748-81b48e10dc94
INFO  [2023-01-17 00:52:45,530] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].a849ec2c-fc99-48df-a748-81b48e10dc94
INFO  [2023-01-17 00:52:45,530] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].a849ec2c-fc99-48df-a748-81b48e10dc94] with 2 results within PT0.000908S
INFO  [2023-01-17 00:52:45,530] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].a849ec2c-fc99-48df-a748-81b48e10dc94] with 1 results within PT0.00086S
127.0.0.1 - - [17/Jan/2023:00:52:45 +0000] "POST /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D/queries HTTP/1.1" 201 2204 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:45,531] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].a849ec2c-fc99-48df-a748-81b48e10dc94, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_114919c1-5b0a-4a6d-bb61-0ddb5f109aaf, startTime=2023-01-17T00:52:45.529793, finishTime=2023-01-17T00:52:45.530701) of size 2
INFO  [2023-01-17 00:52:45,531] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].a849ec2c-fc99-48df-a748-81b48e10dc94, workerId=EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2].worker_EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]_19eaaaae-fe97-4973-9997-20976d3d7f31, startTime=2023-01-17T00:52:45.530074, finishTime=2023-01-17T00:52:45.530934) of size 1
INFO  [2023-01-17 00:52:45,531] com.bakdata.conquery.models.execution.ManagedExecution: DONE a849ec2c-fc99-48df-a748-81b48e10dc94 ManagedQuery within PT0.00698S
127.0.0.1 - - [17/Jan/2023:00:52:45 +0000] "GET /api/datasets/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D/queries/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D.a849ec2c-fc99-48df-a748-81b48e10dc94 HTTP/1.1" 200 2867 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:45,561] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]], queryId=a849ec2c-fc99-48df-a748-81b48e10dc94, label=concept	@§$, creationTime=2023-01-17T00:52:45.524502, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4e475494[Count = 0], startTime=2023-01-17T00:52:45.524702, finishTime=2023-01-17T00:52:45.531682, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@35c34ee1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3858f35b, com.bakdata.conquery.models.query.ColumnDescriptor@586d4848, com.bakdata.conquery.models.query.ColumnDescriptor@2cfa4425, com.bakdata.conquery.models.query.ColumnDescriptor@361a31d]) download on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:45,561] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]], queryId=a849ec2c-fc99-48df-a748-81b48e10dc94, label=concept	@§$, creationTime=2023-01-17T00:52:45.524502, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4e475494[Count = 0], startTime=2023-01-17T00:52:45.524702, finishTime=2023-01-17T00:52:45.531682, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@35c34ee1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_EXISTS_AGGREGATOR & NUMBER Test[2])), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=3, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@3858f35b, com.bakdata.conquery.models.query.ColumnDescriptor@586d4848, com.bakdata.conquery.models.query.ColumnDescriptor@2cfa4425, com.bakdata.conquery.models.query.ColumnDescriptor@361a31d]) on dataset Dataset[label=null, name=EXISTS_AGGREGATOR & NUMBER Test[2]]
127.0.0.1 - - [17/Jan/2023:00:52:45 +0000] "GET /api/datasets/EXISTS_AGGREGATOR%20&%20NUMBER%20Test%5B2%5D/result/EXISTS_AGGREGATOR$20&$20NUMBER$20Test%5B2%5D.a849ec2c-fc99-48df-a748-81b48e10dc94.csv?pretty=false HTTP/1.1" 200 135 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:45,563] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest EXISTS_AGGREGATOR & NUMBER Test on 4 rows
INFO  [2023-01-17 00:52:45,564] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast EXISTS_AGGREGATOR & NUMBER Test[2]
INFO  [2023-01-17 00:52:45,564] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-17 00:52:45,564] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=EXISTS_AGGREGATOR & NUMBER Test[2], name=EXISTS_AGGREGATOR & NUMBER Test[2]]
INFO  [2023-01-17 00:52:45,564] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[2]_114919c1-5b0a-4a6d-bb61-0ddb5f109aaf
INFO  [2023-01-17 00:52:45,564] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_EXISTS_AGGREGATOR & NUMBER Test[2]_19eaaaae-fe97-4973-9997-20976d3d7f31
INFO  [2023-01-17 00:52:45,564] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow EXISTS_AGGREGATOR & NUMBER Test[2]
INFO  [2023-01-17 00:52:45,565] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[2]_114919c1-5b0a-4a6d-bb61-0ddb5f109aaf
INFO  [2023-01-17 00:52:45,566] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_EXISTS_AGGREGATOR & NUMBER Test[2]_19eaaaae-fe97-4973-9997-20976d3d7f31
INFO  [2023-01-17 00:52:45,569] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of EXISTS_AGGREGATOR$20&$20NUMBER$20Test[2]
INFO  [2023-01-17 00:52:45,569] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,713] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test EXISTS_AGGREGATOR & NUMBER Test
INFO  [2023-01-17 00:52:45,713] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test FIRST_AGGREGATOR Test
INFO  [2023-01-17 00:52:45,713] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:45,713] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:45,714] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:45,714] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:45,714] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:45,714] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:45,716] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_c90b9a72-a60b-4cb4-b613-37573f828c2c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:45,716] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_c90b9a72-a60b-4cb4-b613-37573f828c2c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:45,716] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:45,716] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_88fe7136-ef20-4dc9-858a-0bc13ce7021c are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:45,716] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_88fe7136-ef20-4dc9-858a-0bc13ce7021c are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:45,716] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:45,721] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,820] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,826] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:45,827] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FIRST_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:45,827] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table FIRST_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:45,945] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,056] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:46,057] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:46,057] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 161 B in total
INFO  [2023-01-17 00:52:46,057] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FIRST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000471367sINFO  [2023-01-17 00:52:46,105] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=2}
INFO  [2023-01-17 00:52:46,105] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=10, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:46,105] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=2), subType=IntegerParser(super=Parser(lines=10, nullLines=2), minValue=14805, maxValue=15341), dateReader=com.bakdata.conquery.util.DateReader@22cafd1a)
INFO  [2023-01-17 00:52:46,107] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FIRST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:46,107] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:46,107] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_FIRST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:46,128] com.bakdata.conquery.models.jobs.ImportJob: Importing table into FIRST_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:46,129] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:46 +0000] "POST /admin/datasets/FIRST_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_FIRST_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 15
INFO  [2023-01-17 00:52:46,130] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:46,131] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:46,131] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:46,133] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:46,133] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FIRST_AGGREGATOR$20Test.table.table], containing 10 entries.
WARN  [2023-01-17 00:52:46,134] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:46,135] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FIRST_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:46,172] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[FIRST_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:46,172] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received FIRST_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:46,280] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,285] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,298] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,299] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:46,299] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:46,405] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: FIRST_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:46,419] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[FIRST_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:46,420] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[b4c45b48-cdf2-4c7d-a088-fe14060bcb48] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_FIRST_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:46,422] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48
INFO  [2023-01-17 00:52:46,423] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48
127.0.0.1 - - [17/Jan/2023:00:52:46 +0000] "POST /api/datasets/FIRST_AGGREGATOR$20Test/queries HTTP/1.1" 201 1311 "-" "Conquery (test client)" 5
INFO  [2023-01-17 00:52:46,424] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48] with 3 results within PT0.001165S
INFO  [2023-01-17 00:52:46,424] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48] with 2 results within PT0.001177S
INFO  [2023-01-17 00:52:46,424] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48, workerId=FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_c90b9a72-a60b-4cb4-b613-37573f828c2c, startTime=2023-01-17T00:52:46.422957, finishTime=2023-01-17T00:52:46.424122) of size 3
INFO  [2023-01-17 00:52:46,424] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48, workerId=FIRST_AGGREGATOR$20Test.worker_FIRST_AGGREGATOR$20Test_88fe7136-ef20-4dc9-858a-0bc13ce7021c, startTime=2023-01-17T00:52:46.423128, finishTime=2023-01-17T00:52:46.424305) of size 2
INFO  [2023-01-17 00:52:46,425] com.bakdata.conquery.models.execution.ManagedExecution: DONE b4c45b48-cdf2-4c7d-a088-fe14060bcb48 ManagedQuery within PT0.004603S
127.0.0.1 - - [17/Jan/2023:00:52:46 +0000] "GET /api/datasets/FIRST_AGGREGATOR$20Test/queries/FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48 HTTP/1.1" 200 1594 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:46,448] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=FIRST_AGGREGATOR Test], queryId=b4c45b48-cdf2-4c7d-a088-fe14060bcb48, label=concept	@§$, creationTime=2023-01-17T00:52:46.420238, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@45f28786[Count = 0], startTime=2023-01-17T00:52:46.420418, finishTime=2023-01-17T00:52:46.425021, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@9347311), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_FIRST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5570ac0f, com.bakdata.conquery.models.query.ColumnDescriptor@267d69b5, com.bakdata.conquery.models.query.ColumnDescriptor@7762144b]) download on dataset Dataset[label=null, name=FIRST_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:46,448] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=FIRST_AGGREGATOR Test], queryId=b4c45b48-cdf2-4c7d-a088-fe14060bcb48, label=concept	@§$, creationTime=2023-01-17T00:52:46.420238, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@45f28786[Count = 0], startTime=2023-01-17T00:52:46.420418, finishTime=2023-01-17T00:52:46.425021, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@9347311), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_FIRST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5570ac0f, com.bakdata.conquery.models.query.ColumnDescriptor@267d69b5, com.bakdata.conquery.models.query.ColumnDescriptor@7762144b]) on dataset Dataset[label=null, name=FIRST_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:46 +0000] "GET /api/datasets/FIRST_AGGREGATOR%20Test/result/FIRST_AGGREGATOR$20Test.b4c45b48-cdf2-4c7d-a088-fe14060bcb48.csv?pretty=false HTTP/1.1" 200 167 "-" "Conquery (test client)" 23
INFO  [2023-01-17 00:52:46,470] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest FIRST_AGGREGATOR Test on 6 rows
INFO  [2023-01-17 00:52:46,470] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast FIRST_AGGREGATOR Test
INFO  [2023-01-17 00:52:46,470] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:46,470] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=FIRST_AGGREGATOR Test, name=FIRST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:46,470] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FIRST_AGGREGATOR Test_c90b9a72-a60b-4cb4-b613-37573f828c2c
INFO  [2023-01-17 00:52:46,470] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_FIRST_AGGREGATOR Test_88fe7136-ef20-4dc9-858a-0bc13ce7021c
INFO  [2023-01-17 00:52:46,515] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow FIRST_AGGREGATOR Test
INFO  [2023-01-17 00:52:46,516] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FIRST_AGGREGATOR Test_c90b9a72-a60b-4cb4-b613-37573f828c2c
INFO  [2023-01-17 00:52:46,516] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_FIRST_AGGREGATOR Test_88fe7136-ef20-4dc9-858a-0bc13ce7021c
INFO  [2023-01-17 00:52:46,535] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of FIRST_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:46,535] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,704] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test FIRST_AGGREGATOR Test
INFO  [2023-01-17 00:52:46,705] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test LAST_AGGREGATOR Test
INFO  [2023-01-17 00:52:46,705] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:46,705] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:46,706] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:46,706] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:46,706] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:46,706] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:46,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_eb14c6d7-9f6c-4703-ae52-3bd3c01b9500 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:46,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_eb14c6d7-9f6c-4703-ae52-3bd3c01b9500 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:46,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:46,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_f8cbd988-39d8-4ccd-a5f2-dc560d699ae0 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:46,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_f8cbd988-39d8-4ccd-a5f2-dc560d699ae0 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:46,708] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:46,712] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,811] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,817] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:46,818] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table LAST_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:46,818] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table LAST_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:46,939] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,052] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:47,052] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:47,052] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 161 B in total
INFO  [2023-01-17 00:52:47,052] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_LAST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000315829sINFO  [2023-01-17 00:52:47,084] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=10, min=1, average=1.666667, max=2}
INFO  [2023-01-17 00:52:47,084] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=10, nullLines=2), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:47,084] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=10, nullLines=2), subType=IntegerParser(super=Parser(lines=10, nullLines=2), minValue=14805, maxValue=15341), dateReader=com.bakdata.conquery.util.DateReader@2f332abe)
INFO  [2023-01-17 00:52:47,087] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_LAST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:47,087] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:47,087] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_LAST_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:47,098] com.bakdata.conquery.models.jobs.ImportJob: Importing table into LAST_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:47,099] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:47 +0000] "POST /admin/datasets/LAST_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_LAST_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:47,100] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:47,101] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:47,101] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:47,103] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:47,103] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[LAST_AGGREGATOR$20Test.table.table], containing 10 entries.
INFO  [2023-01-17 00:52:47,104] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[LAST_AGGREGATOR$20Test.table.table], containing 10 entries.
WARN  [2023-01-17 00:52:47,105] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:47,105] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received LAST_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:47,105] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received LAST_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:47,210] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,216] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,228] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,229] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:47,229] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:47,335] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: LAST_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:47,363] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[LAST_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:47,364] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[5891ee71-89c4-4359-9347-5e14a646d8cb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_LAST_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:47,367] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb
INFO  [2023-01-17 00:52:47,367] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb
INFO  [2023-01-17 00:52:47,368] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb] with 3 results within PT0.001403S
INFO  [2023-01-17 00:52:47,368] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb] with 2 results within PT0.001196S
127.0.0.1 - - [17/Jan/2023:00:52:47 +0000] "POST /api/datasets/LAST_AGGREGATOR$20Test/queries HTTP/1.1" 201 1306 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:47,369] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb, workerId=LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_eb14c6d7-9f6c-4703-ae52-3bd3c01b9500, startTime=2023-01-17T00:52:47.367703, finishTime=2023-01-17T00:52:47.368899) of size 2
INFO  [2023-01-17 00:52:47,369] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb, workerId=LAST_AGGREGATOR$20Test.worker_LAST_AGGREGATOR$20Test_f8cbd988-39d8-4ccd-a5f2-dc560d699ae0, startTime=2023-01-17T00:52:47.367444, finishTime=2023-01-17T00:52:47.368847) of size 3
INFO  [2023-01-17 00:52:47,369] com.bakdata.conquery.models.execution.ManagedExecution: DONE 5891ee71-89c4-4359-9347-5e14a646d8cb ManagedQuery within PT0.005336S
127.0.0.1 - - [17/Jan/2023:00:52:47 +0000] "GET /api/datasets/LAST_AGGREGATOR$20Test/queries/LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb HTTP/1.1" 200 1585 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:47,396] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=LAST_AGGREGATOR Test], queryId=5891ee71-89c4-4359-9347-5e14a646d8cb, label=concept	@§$, creationTime=2023-01-17T00:52:47.364256, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@495ac533[Count = 0], startTime=2023-01-17T00:52:47.364466, finishTime=2023-01-17T00:52:47.369802, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ab5e8e6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_LAST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@161f38d0, com.bakdata.conquery.models.query.ColumnDescriptor@77f347f1, com.bakdata.conquery.models.query.ColumnDescriptor@681f7761]) download on dataset Dataset[label=null, name=LAST_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:47,396] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=LAST_AGGREGATOR Test], queryId=5891ee71-89c4-4359-9347-5e14a646d8cb, label=concept	@§$, creationTime=2023-01-17T00:52:47.364256, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@495ac533[Count = 0], startTime=2023-01-17T00:52:47.364466, finishTime=2023-01-17T00:52:47.369802, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@4ab5e8e6), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_LAST_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@161f38d0, com.bakdata.conquery.models.query.ColumnDescriptor@77f347f1, com.bakdata.conquery.models.query.ColumnDescriptor@681f7761]) on dataset Dataset[label=null, name=LAST_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:47 +0000] "GET /api/datasets/LAST_AGGREGATOR%20Test/result/LAST_AGGREGATOR$20Test.5891ee71-89c4-4359-9347-5e14a646d8cb.csv?pretty=false HTTP/1.1" 200 167 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:47,398] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest LAST_AGGREGATOR Test on 6 rows
INFO  [2023-01-17 00:52:47,398] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast LAST_AGGREGATOR Test
INFO  [2023-01-17 00:52:47,398] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:47,398] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=LAST_AGGREGATOR Test, name=LAST_AGGREGATOR Test]
INFO  [2023-01-17 00:52:47,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_LAST_AGGREGATOR Test_f8cbd988-39d8-4ccd-a5f2-dc560d699ae0
INFO  [2023-01-17 00:52:47,399] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_LAST_AGGREGATOR Test_eb14c6d7-9f6c-4703-ae52-3bd3c01b9500
INFO  [2023-01-17 00:52:47,406] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow LAST_AGGREGATOR Test
INFO  [2023-01-17 00:52:47,407] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_LAST_AGGREGATOR Test_eb14c6d7-9f6c-4703-ae52-3bd3c01b9500
INFO  [2023-01-17 00:52:47,407] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_LAST_AGGREGATOR Test_f8cbd988-39d8-4ccd-a5f2-dc560d699ae0
INFO  [2023-01-17 00:52:47,505] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of LAST_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:47,505] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,534] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test LAST_AGGREGATOR Test
INFO  [2023-01-17 00:52:47,534] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:47,534] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:47,535] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:47,536] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:47,536] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:47,536] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:47,536] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_f5e2df05-9872-45cc-934f-c24b558c7a0a are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_f5e2df05-9872-45cc-934f-c24b558c7a0a are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_945cd8ed-60df-4ac4-88ce-95ab7b3f2f08 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_945cd8ed-60df-4ac4-88ce-95ab7b3f2f08 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:47,537] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:47,641] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,648] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,648] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:47,648] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table MULTI_SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:47,776] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:47,888] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:47,889] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:47,889] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 166 B in total
INFO  [2023-01-17 00:52:47,889] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000192022sINFO  [2023-01-17 00:52:47,908] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=6, sum=9, min=1, average=1.500000, max=2}
INFO  [2023-01-17 00:52:47,908] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=9, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:47,908] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=9, nullLines=0), subType=IntegerParser(super=Parser(lines=9, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@71fe64e0)
INFO  [2023-01-17 00:52:47,911] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:47,911] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:47,911] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_MULTI_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:47,928] com.bakdata.conquery.models.jobs.ImportJob: Importing table into MULTI_SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:47,928] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:47 +0000] "POST /admin/datasets/MULTI_SELECT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_MULTI_SELECT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:47,930] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:47,931] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:47,931] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:47,932] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:47,933] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_AGGREGATOR$20Test.table.table], containing 9 entries.
INFO  [2023-01-17 00:52:47,933] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[MULTI_SELECT_AGGREGATOR$20Test.table.table], containing 9 entries.
WARN  [2023-01-17 00:52:47,933] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:47,933] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:47,933] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received MULTI_SELECT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:48,039] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,044] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,057] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,057] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:48,057] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:48,163] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: MULTI_SELECT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:48,189] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[MULTI_SELECT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:48,190] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[23f33adf-1c4d-46e4-8688-b6fc086a4659] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:48,193] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659
INFO  [2023-01-17 00:52:48,193] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659
127.0.0.1 - - [17/Jan/2023:00:52:48 +0000] "POST /api/datasets/MULTI_SELECT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1346 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:48,194] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659] with 3 results within PT0.001372S
INFO  [2023-01-17 00:52:48,194] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659] with 3 results within PT0.001597S
INFO  [2023-01-17 00:52:48,197] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659, workerId=MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_945cd8ed-60df-4ac4-88ce-95ab7b3f2f08, startTime=2023-01-17T00:52:48.193222, finishTime=2023-01-17T00:52:48.194594) of size 3
INFO  [2023-01-17 00:52:48,197] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659, workerId=MULTI_SELECT_AGGREGATOR$20Test.worker_MULTI_SELECT_AGGREGATOR$20Test_f5e2df05-9872-45cc-934f-c24b558c7a0a, startTime=2023-01-17T00:52:48.193069, finishTime=2023-01-17T00:52:48.194666) of size 3
INFO  [2023-01-17 00:52:48,197] com.bakdata.conquery.models.execution.ManagedExecution: DONE 23f33adf-1c4d-46e4-8688-b6fc086a4659 ManagedQuery within PT0.007773S
127.0.0.1 - - [17/Jan/2023:00:52:48 +0000] "GET /api/datasets/MULTI_SELECT_AGGREGATOR$20Test/queries/MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659 HTTP/1.1" 200 1657 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:48,223] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test], queryId=23f33adf-1c4d-46e4-8688-b6fc086a4659, label=concept	@§$, creationTime=2023-01-17T00:52:48.189831, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4d6aa3cc[Count = 0], startTime=2023-01-17T00:52:48.190043, finishTime=2023-01-17T00:52:48.197816, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@cfe0bc1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e51cad, com.bakdata.conquery.models.query.ColumnDescriptor@401c9e33, com.bakdata.conquery.models.query.ColumnDescriptor@5870213f]) download on dataset Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:48,223] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test], queryId=23f33adf-1c4d-46e4-8688-b6fc086a4659, label=concept	@§$, creationTime=2023-01-17T00:52:48.189831, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4d6aa3cc[Count = 0], startTime=2023-01-17T00:52:48.190043, finishTime=2023-01-17T00:52:48.197816, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@cfe0bc1), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_MULTI_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=6, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6e51cad, com.bakdata.conquery.models.query.ColumnDescriptor@401c9e33, com.bakdata.conquery.models.query.ColumnDescriptor@5870213f]) on dataset Dataset[label=null, name=MULTI_SELECT_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:48 +0000] "GET /api/datasets/MULTI_SELECT_AGGREGATOR%20Test/result/MULTI_SELECT_AGGREGATOR$20Test.23f33adf-1c4d-46e4-8688-b6fc086a4659.csv?pretty=false HTTP/1.1" 200 222 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:48,225] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest MULTI_SELECT_AGGREGATOR Test on 7 rows
INFO  [2023-01-17 00:52:48,226] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:48,226] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:48,226] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=MULTI_SELECT_AGGREGATOR Test, name=MULTI_SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:48,226] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_AGGREGATOR Test_f5e2df05-9872-45cc-934f-c24b558c7a0a
INFO  [2023-01-17 00:52:48,226] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_MULTI_SELECT_AGGREGATOR Test_945cd8ed-60df-4ac4-88ce-95ab7b3f2f08
INFO  [2023-01-17 00:52:48,236] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:48,237] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_AGGREGATOR Test_f5e2df05-9872-45cc-934f-c24b558c7a0a
INFO  [2023-01-17 00:52:48,237] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_MULTI_SELECT_AGGREGATOR Test_945cd8ed-60df-4ac4-88ce-95ab7b3f2f08
INFO  [2023-01-17 00:52:48,334] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of MULTI_SELECT_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:48,334] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,362] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test MULTI_SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:48,363] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test PREFIX_AGGREGATOR Test
INFO  [2023-01-17 00:52:48,363] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:48,363] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:48,364] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-17 00:52:48,364] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:48,364] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-17 00:52:48,364] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:48,365] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_1d5b44e1-112d-4d5f-baf5-10b61f50323f are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:48,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_1d5b44e1-112d-4d5f-baf5-10b61f50323f are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:48,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:48,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_4e14c4a7-894b-410d-86ef-b76ed9f62463 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:48,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_4e14c4a7-894b-410d-86ef-b76ed9f62463 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:48,366] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:48,470] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,477] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,478] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table PREFIX_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:48,478] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table PREFIX_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:48,611] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,728] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:48,728] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:48,728] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 100 B in total
INFO  [2023-01-17 00:52:48,728] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_PREFIX_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000313419sINFO  [2023-01-17 00:52:48,760] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:52:48,760] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:48,760] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@191b0805)
INFO  [2023-01-17 00:52:48,762] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_PREFIX_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:48,762] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:48,762] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_PREFIX_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:48,782] com.bakdata.conquery.models.jobs.ImportJob: Importing table into PREFIX_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:48,783] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:48 +0000] "POST /admin/datasets/PREFIX_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_PREFIX_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 14
INFO  [2023-01-17 00:52:48,783] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:48,783] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:48,783] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:48,784] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:48,785] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[PREFIX_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:52:48,785] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[PREFIX_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:52:48,785] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:48,785] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received PREFIX_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:48,785] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received PREFIX_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:48,890] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,896] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,908] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:48,909] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:48,909] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:49,014] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: PREFIX_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:49,029] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[PREFIX_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:49,030] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[7ce3935e-ed69-43a0-9b2c-5554dc6c55fb] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_PREFIX_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:49,033] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb
INFO  [2023-01-17 00:52:49,033] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb
INFO  [2023-01-17 00:52:49,033] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb] with 1 results within PT0.00078S
INFO  [2023-01-17 00:52:49,034] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb] with 3 results within PT0.00106S
127.0.0.1 - - [17/Jan/2023:00:52:49 +0000] "POST /api/datasets/PREFIX_AGGREGATOR$20Test/queries HTTP/1.1" 201 1316 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:49,034] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb, workerId=PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_4e14c4a7-894b-410d-86ef-b76ed9f62463, startTime=2023-01-17T00:52:49.033185, finishTime=2023-01-17T00:52:49.033965) of size 1
INFO  [2023-01-17 00:52:49,035] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb, workerId=PREFIX_AGGREGATOR$20Test.worker_PREFIX_AGGREGATOR$20Test_1d5b44e1-112d-4d5f-baf5-10b61f50323f, startTime=2023-01-17T00:52:49.033039, finishTime=2023-01-17T00:52:49.034099) of size 3
INFO  [2023-01-17 00:52:49,035] com.bakdata.conquery.models.execution.ManagedExecution: DONE 7ce3935e-ed69-43a0-9b2c-5554dc6c55fb ManagedQuery within PT0.005349S
127.0.0.1 - - [17/Jan/2023:00:52:49 +0000] "GET /api/datasets/PREFIX_AGGREGATOR$20Test/queries/PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb HTTP/1.1" 200 1603 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:49,058] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=PREFIX_AGGREGATOR Test], queryId=7ce3935e-ed69-43a0-9b2c-5554dc6c55fb, label=concept	@§$, creationTime=2023-01-17T00:52:49.030083, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6797789c[Count = 0], startTime=2023-01-17T00:52:49.030264, finishTime=2023-01-17T00:52:49.035613, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@582e333f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_PREFIX_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7132f479, com.bakdata.conquery.models.query.ColumnDescriptor@281ff6e, com.bakdata.conquery.models.query.ColumnDescriptor@5e0e343a]) download on dataset Dataset[label=null, name=PREFIX_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:49,058] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=PREFIX_AGGREGATOR Test], queryId=7ce3935e-ed69-43a0-9b2c-5554dc6c55fb, label=concept	@§$, creationTime=2023-01-17T00:52:49.030083, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@6797789c[Count = 0], startTime=2023-01-17T00:52:49.030264, finishTime=2023-01-17T00:52:49.035613, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@582e333f), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_PREFIX_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@7132f479, com.bakdata.conquery.models.query.ColumnDescriptor@281ff6e, com.bakdata.conquery.models.query.ColumnDescriptor@5e0e343a]) on dataset Dataset[label=null, name=PREFIX_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:49 +0000] "GET /api/datasets/PREFIX_AGGREGATOR%20Test/result/PREFIX_AGGREGATOR$20Test.7ce3935e-ed69-43a0-9b2c-5554dc6c55fb.csv?pretty=false HTTP/1.1" 200 149 "-" "Conquery (test client)" 18
INFO  [2023-01-17 00:52:49,075] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest PREFIX_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:49,076] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast PREFIX_AGGREGATOR Test
INFO  [2023-01-17 00:52:49,076] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-17 00:52:49,076] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=PREFIX_AGGREGATOR Test, name=PREFIX_AGGREGATOR Test]
INFO  [2023-01-17 00:52:49,076] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PREFIX_AGGREGATOR Test_4e14c4a7-894b-410d-86ef-b76ed9f62463
INFO  [2023-01-17 00:52:49,076] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_PREFIX_AGGREGATOR Test_1d5b44e1-112d-4d5f-baf5-10b61f50323f
INFO  [2023-01-17 00:52:49,174] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PREFIX_AGGREGATOR Test_4e14c4a7-894b-410d-86ef-b76ed9f62463
INFO  [2023-01-17 00:52:49,175] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow PREFIX_AGGREGATOR Test
INFO  [2023-01-17 00:52:49,175] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_PREFIX_AGGREGATOR Test_1d5b44e1-112d-4d5f-baf5-10b61f50323f
INFO  [2023-01-17 00:52:49,192] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of PREFIX_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:49,192] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,323] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test PREFIX_AGGREGATOR Test
INFO  [2023-01-17 00:52:49,323] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test QUARTER_AGGREGATOR
INFO  [2023-01-17 00:52:49,323] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:49,323] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:49,347] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-17 00:52:49,347] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:49,348] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-17 00:52:49,348] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:49,357] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_0cb40bc4-e42f-4a63-a8bd-635a59cdcaa5 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:49,357] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_0cb40bc4-e42f-4a63-a8bd-635a59cdcaa5 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:49,357] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:49,419] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_7a781f44-f1a4-4dd7-983b-ff86eddeae90 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:49,419] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_7a781f44-f1a4-4dd7-983b-ff86eddeae90 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:49,419] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:49,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,522] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,529] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,529] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTER_AGGREGATOR.table
INFO  [2023-01-17 00:52:49,529] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table QUARTER_AGGREGATOR.table
INFO  [2023-01-17 00:52:49,653] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,763] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:49,764] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:49,764] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 51 B in total
INFO  [2023-01-17 00:52:49,764] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_QUARTER_AGGREGATOR/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000379195sINFO  [2023-01-17 00:52:49,802] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=3, sum=4, min=1, average=1.333333, max=2}
INFO  [2023-01-17 00:52:49,802] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=4, nullLines=1), subType=IntegerParser(super=Parser(lines=4, nullLines=1), minValue=16452, maxValue=16679), dateReader=com.bakdata.conquery.util.DateReader@75543815)
INFO  [2023-01-17 00:52:49,804] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_QUARTER_AGGREGATOR/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:49,804] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:49,804] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_QUARTER_AGGREGATOR/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:49,821] com.bakdata.conquery.models.jobs.ImportJob: Importing table into QUARTER_AGGREGATOR.table
127.0.0.1 - - [17/Jan/2023:00:52:49 +0000] "POST /admin/datasets/QUARTER_AGGREGATOR/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_QUARTER_AGGREGATOR%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:49,822] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,823] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:49,823] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:49,823] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:49,825] com.bakdata.conquery.models.jobs.ImportJob: Start sending 1 Buckets
INFO  [2023-01-17 00:52:49,825] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTER_AGGREGATOR.table.table], containing 4 entries.
INFO  [2023-01-17 00:52:49,826] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[QUARTER_AGGREGATOR.table.table], containing 4 entries.
WARN  [2023-01-17 00:52:49,826] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:49,827] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received QUARTER_AGGREGATOR.table.table.0
INFO  [2023-01-17 00:52:49,932] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,937] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,951] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:49,951] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:50,057] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: QUARTER_AGGREGATOR QUERY INIT
INFO  [2023-01-17 00:52:50,072] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[QUARTER_AGGREGATOR] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:50,073] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[c6c1baec-2796-4405-93ab-f4cc25013b9e] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_QUARTER_AGGREGATOR))]]
INFO  [2023-01-17 00:52:50,075] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e
INFO  [2023-01-17 00:52:50,075] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e
WARN  [2023-01-17 00:52:50,076] com.bakdata.conquery.models.query.QueryExecutor: Entities for query are empty
INFO  [2023-01-17 00:52:50,076] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e] with 0 results within PT0.000321S
INFO  [2023-01-17 00:52:50,076] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e, workerId=QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_7a781f44-f1a4-4dd7-983b-ff86eddeae90, startTime=2023-01-17T00:52:50.075867, finishTime=2023-01-17T00:52:50.076188) of size 0
127.0.0.1 - - [17/Jan/2023:00:52:50 +0000] "POST /api/datasets/QUARTER_AGGREGATOR/queries HTTP/1.1" 201 1285 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:50,085] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e] with 2 results within PT0.009511S
INFO  [2023-01-17 00:52:50,085] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e, workerId=QUARTER_AGGREGATOR.worker_QUARTER_AGGREGATOR_0cb40bc4-e42f-4a63-a8bd-635a59cdcaa5, startTime=2023-01-17T00:52:50.075868, finishTime=2023-01-17T00:52:50.085379) of size 2
INFO  [2023-01-17 00:52:50,086] com.bakdata.conquery.models.execution.ManagedExecution: DONE c6c1baec-2796-4405-93ab-f4cc25013b9e ManagedQuery within PT0.013011S
127.0.0.1 - - [17/Jan/2023:00:52:50 +0000] "GET /api/datasets/QUARTER_AGGREGATOR/queries/QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e HTTP/1.1" 200 1549 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:50,107] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTER_AGGREGATOR], queryId=c6c1baec-2796-4405-93ab-f4cc25013b9e, label=concept	@§$, creationTime=2023-01-17T00:52:50.072870, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4832c711[Count = 0], startTime=2023-01-17T00:52:50.073084, finishTime=2023-01-17T00:52:50.086095, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7ebb1d35), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTER_AGGREGATOR)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5822e20d, com.bakdata.conquery.models.query.ColumnDescriptor@635592e7, com.bakdata.conquery.models.query.ColumnDescriptor@3d81cbd3]) download on dataset Dataset[label=null, name=QUARTER_AGGREGATOR] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:50,108] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=QUARTER_AGGREGATOR], queryId=c6c1baec-2796-4405-93ab-f4cc25013b9e, label=concept	@§$, creationTime=2023-01-17T00:52:50.072870, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@4832c711[Count = 0], startTime=2023-01-17T00:52:50.073084, finishTime=2023-01-17T00:52:50.086095, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@7ebb1d35), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_QUARTER_AGGREGATOR)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=2, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@5822e20d, com.bakdata.conquery.models.query.ColumnDescriptor@635592e7, com.bakdata.conquery.models.query.ColumnDescriptor@3d81cbd3]) on dataset Dataset[label=null, name=QUARTER_AGGREGATOR]
127.0.0.1 - - [17/Jan/2023:00:52:50 +0000] "GET /api/datasets/QUARTER_AGGREGATOR/result/QUARTER_AGGREGATOR.c6c1baec-2796-4405-93ab-f4cc25013b9e.csv?pretty=false HTTP/1.1" 200 120 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:52:50,125] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest QUARTER_AGGREGATOR on 3 rows
INFO  [2023-01-17 00:52:50,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast QUARTER_AGGREGATOR
INFO  [2023-01-17 00:52:50,126] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-17 00:52:50,126] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=QUARTER_AGGREGATOR, name=QUARTER_AGGREGATOR]
INFO  [2023-01-17 00:52:50,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTER_AGGREGATOR_7a781f44-f1a4-4dd7-983b-ff86eddeae90
INFO  [2023-01-17 00:52:50,126] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_QUARTER_AGGREGATOR_0cb40bc4-e42f-4a63-a8bd-635a59cdcaa5
INFO  [2023-01-17 00:52:50,148] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow QUARTER_AGGREGATOR
INFO  [2023-01-17 00:52:50,158] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTER_AGGREGATOR_0cb40bc4-e42f-4a63-a8bd-635a59cdcaa5
INFO  [2023-01-17 00:52:50,224] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_QUARTER_AGGREGATOR_7a781f44-f1a4-4dd7-983b-ff86eddeae90
INFO  [2023-01-17 00:52:50,227] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of QUARTER_AGGREGATOR
INFO  [2023-01-17 00:52:50,227] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,357] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test QUARTER_AGGREGATOR
INFO  [2023-01-17 00:52:50,357] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test RANDOM_AGGREGATOR Test
INFO  [2023-01-17 00:52:50,357] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:50,357] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:50,358] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:50,358] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:50,358] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:50,358] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:50,359] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_e47bfe9a-4368-412c-908f-fa30f694ef92 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:50,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_e47bfe9a-4368-412c-908f-fa30f694ef92 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:50,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:50,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_2bfabf1c-d420-4e05-b763-4161dbc70739 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:50,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_2bfabf1c-d420-4e05-b763-4161dbc70739 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:50,360] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:50,464] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,471] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,471] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RANDOM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:50,471] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table RANDOM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:50,583] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,692] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:50,692] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:50,692] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 111 B in total
INFO  [2023-01-17 00:52:50,692] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_RANDOM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.00041535sINFO  [2023-01-17 00:52:50,734] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=7, min=1, average=1.400000, max=2}
INFO  [2023-01-17 00:52:50,734] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=7, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:50,734] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=7, nullLines=2), subType=IntegerParser(super=Parser(lines=7, nullLines=2), minValue=14805, maxValue=15340), dateReader=com.bakdata.conquery.util.DateReader@6ccc1411)
INFO  [2023-01-17 00:52:50,737] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_RANDOM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:50,737] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:50,737] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_RANDOM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:50,757] com.bakdata.conquery.models.jobs.ImportJob: Importing table into RANDOM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:50,757] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:50 +0000] "POST /admin/datasets/RANDOM_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_RANDOM_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:50,758] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:50,759] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:50,759] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:50,760] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:50,761] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RANDOM_AGGREGATOR$20Test.table.table], containing 7 entries.
INFO  [2023-01-17 00:52:50,761] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[RANDOM_AGGREGATOR$20Test.table.table], containing 7 entries.
WARN  [2023-01-17 00:52:50,761] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:50,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RANDOM_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:50,762] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received RANDOM_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:50,867] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,872] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,885] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:50,886] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:50,886] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:50,991] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: RANDOM_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:51,002] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[RANDOM_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:51,002] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[68364669-ae05-443d-b4d2-17ae7c922718] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_RANDOM_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:51,004] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718
INFO  [2023-01-17 00:52:51,004] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "POST /api/datasets/RANDOM_AGGREGATOR$20Test/queries HTTP/1.1" 201 1316 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:51,005] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718] with 2 results within PT0.001025S
INFO  [2023-01-17 00:52:51,005] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718] with 2 results within PT0.00124S
INFO  [2023-01-17 00:52:51,006] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718, workerId=RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_e47bfe9a-4368-412c-908f-fa30f694ef92, startTime=2023-01-17T00:52:51.004683, finishTime=2023-01-17T00:52:51.005708) of size 2
INFO  [2023-01-17 00:52:51,006] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718, workerId=RANDOM_AGGREGATOR$20Test.worker_RANDOM_AGGREGATOR$20Test_2bfabf1c-d420-4e05-b763-4161dbc70739, startTime=2023-01-17T00:52:51.004688, finishTime=2023-01-17T00:52:51.005928) of size 2
INFO  [2023-01-17 00:52:51,006] com.bakdata.conquery.models.execution.ManagedExecution: DONE 68364669-ae05-443d-b4d2-17ae7c922718 ManagedQuery within PT0.00371S
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "GET /api/datasets/RANDOM_AGGREGATOR$20Test/queries/RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718 HTTP/1.1" 200 1603 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:51,031] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RANDOM_AGGREGATOR Test], queryId=68364669-ae05-443d-b4d2-17ae7c922718, label=concept	@§$, creationTime=2023-01-17T00:52:51.002866, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@437a913b[Count = 0], startTime=2023-01-17T00:52:51.002985, finishTime=2023-01-17T00:52:51.006695, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@48c1cb55), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RANDOM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6eeaa087, com.bakdata.conquery.models.query.ColumnDescriptor@5385f8a0, com.bakdata.conquery.models.query.ColumnDescriptor@446148a2]) download on dataset Dataset[label=null, name=RANDOM_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:51,033] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=RANDOM_AGGREGATOR Test], queryId=68364669-ae05-443d-b4d2-17ae7c922718, label=concept	@§$, creationTime=2023-01-17T00:52:51.002866, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@437a913b[Count = 0], startTime=2023-01-17T00:52:51.002985, finishTime=2023-01-17T00:52:51.006695, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@48c1cb55), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_RANDOM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@6eeaa087, com.bakdata.conquery.models.query.ColumnDescriptor@5385f8a0, com.bakdata.conquery.models.query.ColumnDescriptor@446148a2]) on dataset Dataset[label=null, name=RANDOM_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "GET /api/datasets/RANDOM_AGGREGATOR%20Test/result/RANDOM_AGGREGATOR$20Test.68364669-ae05-443d-b4d2-17ae7c922718.csv?pretty=false HTTP/1.1" 200 139 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:52:51,049] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest RANDOM_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:51,049] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast RANDOM_AGGREGATOR Test
INFO  [2023-01-17 00:52:51,050] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:51,050] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=RANDOM_AGGREGATOR Test, name=RANDOM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:51,050] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RANDOM_AGGREGATOR Test_e47bfe9a-4368-412c-908f-fa30f694ef92
INFO  [2023-01-17 00:52:51,050] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_RANDOM_AGGREGATOR Test_2bfabf1c-d420-4e05-b763-4161dbc70739
INFO  [2023-01-17 00:52:51,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow RANDOM_AGGREGATOR Test
INFO  [2023-01-17 00:52:51,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RANDOM_AGGREGATOR Test_e47bfe9a-4368-412c-908f-fa30f694ef92
INFO  [2023-01-17 00:52:51,059] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_RANDOM_AGGREGATOR Test_2bfabf1c-d420-4e05-b763-4161dbc70739
INFO  [2023-01-17 00:52:51,062] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of RANDOM_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:51,062] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,191] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test RANDOM_AGGREGATOR Test
INFO  [2023-01-17 00:52:51,191] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:51,192] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:51,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:51,192] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:51,192] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:51,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:51,192] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:51,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_25c752b5-5359-4f43-8f81-e60a7fd46dd1 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:51,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_25c752b5-5359-4f43-8f81-e60a7fd46dd1 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:51,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:51,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_3b24bb7d-9712-45db-99d1-971b133d200e are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:51,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_3b24bb7d-9712-45db-99d1-971b133d200e are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:51,194] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:51,198] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,298] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,305] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,305] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:51,305] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:51,423] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,533] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:51,533] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:51,533] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 98 B in total
INFO  [2023-01-17 00:52:51,533] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000363234sINFO  [2023-01-17 00:52:51,570] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:52:51,570] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=5, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:51,570] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5a1ca5ec)
INFO  [2023-01-17 00:52:51,572] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:51,572] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:51,572] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SELECT_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:51,589] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SELECT_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:51,590] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "POST /admin/datasets/SELECT_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SELECT_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:51,590] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:51,591] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:51,591] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:51,592] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:51,592] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:52:51,592] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SELECT_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:52:51,593] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:51,593] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:51,593] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SELECT_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:51,698] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,703] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,716] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,717] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:51,717] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:51,823] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SELECT_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:51,838] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SELECT_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:51,838] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SELECT_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:51,841] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7
INFO  [2023-01-17 00:52:51,841] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "POST /api/datasets/SELECT_AGGREGATOR$20Test/queries HTTP/1.1" 201 1317 "-" "Conquery (test client)" 7
INFO  [2023-01-17 00:52:51,842] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7] with 1 results within PT0.001002S
INFO  [2023-01-17 00:52:51,842] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7] with 3 results within PT0.001362S
INFO  [2023-01-17 00:52:51,843] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7, workerId=SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_25c752b5-5359-4f43-8f81-e60a7fd46dd1, startTime=2023-01-17T00:52:51.841446, finishTime=2023-01-17T00:52:51.842448) of size 1
INFO  [2023-01-17 00:52:51,843] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7, workerId=SELECT_AGGREGATOR$20Test.worker_SELECT_AGGREGATOR$20Test_3b24bb7d-9712-45db-99d1-971b133d200e, startTime=2023-01-17T00:52:51.841342, finishTime=2023-01-17T00:52:51.842704) of size 3
INFO  [2023-01-17 00:52:51,843] com.bakdata.conquery.models.execution.ManagedExecution: DONE 54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7 ManagedQuery within PT0.004849S
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "GET /api/datasets/SELECT_AGGREGATOR$20Test/queries/SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7 HTTP/1.1" 200 1604 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:51,882] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT_AGGREGATOR Test], queryId=54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7, label=concept	@§$, creationTime=2023-01-17T00:52:51.838323, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3e93f801[Count = 0], startTime=2023-01-17T00:52:51.838498, finishTime=2023-01-17T00:52:51.843347, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@20812180), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@65134078, com.bakdata.conquery.models.query.ColumnDescriptor@7303ce85, com.bakdata.conquery.models.query.ColumnDescriptor@52f17314]) download on dataset Dataset[label=null, name=SELECT_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:51,882] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SELECT_AGGREGATOR Test], queryId=54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7, label=concept	@§$, creationTime=2023-01-17T00:52:51.838323, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@3e93f801[Count = 0], startTime=2023-01-17T00:52:51.838498, finishTime=2023-01-17T00:52:51.843347, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@20812180), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SELECT_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@65134078, com.bakdata.conquery.models.query.ColumnDescriptor@7303ce85, com.bakdata.conquery.models.query.ColumnDescriptor@52f17314]) on dataset Dataset[label=null, name=SELECT_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:51 +0000] "GET /api/datasets/SELECT_AGGREGATOR%20Test/result/SELECT_AGGREGATOR$20Test.54e19bdd-f1f6-4df2-99b9-7bafb83ae3c7.csv?pretty=false HTTP/1.1" 200 138 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:51,884] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SELECT_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:51,885] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:51,885] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:51,885] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SELECT_AGGREGATOR Test, name=SELECT_AGGREGATOR Test]
INFO  [2023-01-17 00:52:51,885] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT_AGGREGATOR Test_25c752b5-5359-4f43-8f81-e60a7fd46dd1
INFO  [2023-01-17 00:52:51,885] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SELECT_AGGREGATOR Test_3b24bb7d-9712-45db-99d1-971b133d200e
INFO  [2023-01-17 00:52:51,893] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:51,893] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SELECT_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:51,893] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:51,893] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT_AGGREGATOR Test_25c752b5-5359-4f43-8f81-e60a7fd46dd1
INFO  [2023-01-17 00:52:51,894] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SELECT_AGGREGATOR Test_3b24bb7d-9712-45db-99d1-971b133d200e
INFO  [2023-01-17 00:52:52,022] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SELECT_AGGREGATOR Test
INFO  [2023-01-17 00:52:52,023] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:52,023] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:52,023] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:52,025] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:52,025] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:52,025] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:52,025] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:52,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_bd40eebb-2ee2-4151-af7f-ecca21d77933 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:52,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_bd40eebb-2ee2-4151-af7f-ecca21d77933 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:52,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:52,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_6cc22f97-07b9-47b0-b027-5c60c4d0e429 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:52,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_6cc22f97-07b9-47b0-b027-5c60c4d0e429 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:52,027] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:52,028] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,131] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,139] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,140] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:52,140] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:52,265] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,388] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:52,388] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:52,388] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 91 B in total
INFO  [2023-01-17 00:52:52,388] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000377362sINFO  [2023-01-17 00:52:52,427] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:52:52,427] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with IntegerParser(super=Parser(lines=5, nullLines=1), minValue=-1, maxValue=1)
INFO  [2023-01-17 00:52:52,427] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@5847032b)
INFO  [2023-01-17 00:52:52,429] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:52,430] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:52,430] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:52,449] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:52,449] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:52 +0000] "POST /admin/datasets/SUM_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SUM_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 12
INFO  [2023-01-17 00:52:52,450] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:52,450] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:52,450] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:52,452] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:52,452] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:52:52,452] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:52:52,453] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:52,453] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:52,453] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:52,558] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,563] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,576] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,577] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:52,577] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:52,683] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:52,698] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:52,698] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[fa784286-2843-4013-98dc-750af233ce5b] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:52,701] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b
INFO  [2023-01-17 00:52:52,701] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b
127.0.0.1 - - [17/Jan/2023:00:52:52 +0000] "POST /api/datasets/SUM_AGGREGATOR$20Test/queries HTTP/1.1" 201 1302 "-" "Conquery (test client)" 8
INFO  [2023-01-17 00:52:52,703] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b] with 1 results within PT0.001615S
INFO  [2023-01-17 00:52:52,703] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b] with 3 results within PT0.001725S
INFO  [2023-01-17 00:52:52,704] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b, workerId=SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_bd40eebb-2ee2-4151-af7f-ecca21d77933, startTime=2023-01-17T00:52:52.701785, finishTime=2023-01-17T00:52:52.703400) of size 1
INFO  [2023-01-17 00:52:52,704] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b, workerId=SUM_AGGREGATOR$20Test.worker_SUM_AGGREGATOR$20Test_6cc22f97-07b9-47b0-b027-5c60c4d0e429, startTime=2023-01-17T00:52:52.701688, finishTime=2023-01-17T00:52:52.703413) of size 3
INFO  [2023-01-17 00:52:52,704] com.bakdata.conquery.models.execution.ManagedExecution: DONE fa784286-2843-4013-98dc-750af233ce5b ManagedQuery within PT0.005648S
127.0.0.1 - - [17/Jan/2023:00:52:52 +0000] "GET /api/datasets/SUM_AGGREGATOR$20Test/queries/SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b HTTP/1.1" 200 1577 "-" "Conquery (test client)" 3
INFO  [2023-01-17 00:52:52,742] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_AGGREGATOR Test], queryId=fa784286-2843-4013-98dc-750af233ce5b, label=concept	@§$, creationTime=2023-01-17T00:52:52.698456, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@462f43ca[Count = 0], startTime=2023-01-17T00:52:52.698656, finishTime=2023-01-17T00:52:52.704304, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@62a73039), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@788f2cf9, com.bakdata.conquery.models.query.ColumnDescriptor@3b54964f, com.bakdata.conquery.models.query.ColumnDescriptor@7d9a3fc]) download on dataset Dataset[label=null, name=SUM_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:52,742] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_AGGREGATOR Test], queryId=fa784286-2843-4013-98dc-750af233ce5b, label=concept	@§$, creationTime=2023-01-17T00:52:52.698456, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@462f43ca[Count = 0], startTime=2023-01-17T00:52:52.698656, finishTime=2023-01-17T00:52:52.704304, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@62a73039), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@788f2cf9, com.bakdata.conquery.models.query.ColumnDescriptor@3b54964f, com.bakdata.conquery.models.query.ColumnDescriptor@7d9a3fc]) on dataset Dataset[label=null, name=SUM_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:52 +0000] "GET /api/datasets/SUM_AGGREGATOR%20Test/result/SUM_AGGREGATOR$20Test.fa784286-2843-4013-98dc-750af233ce5b.csv?pretty=false HTTP/1.1" 200 140 "-" "Conquery (test client)" 4
INFO  [2023-01-17 00:52:52,744] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SUM_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:52,744] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:52,745] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:52,745] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_AGGREGATOR Test, name=SUM_AGGREGATOR Test]
INFO  [2023-01-17 00:52:52,745] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_AGGREGATOR Test_bd40eebb-2ee2-4151-af7f-ecca21d77933
INFO  [2023-01-17 00:52:52,745] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_AGGREGATOR Test_6cc22f97-07b9-47b0-b027-5c60c4d0e429
INFO  [2023-01-17 00:52:52,832] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_AGGREGATOR Test_bd40eebb-2ee2-4151-af7f-ecca21d77933
INFO  [2023-01-17 00:52:52,832] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:52,832] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_AGGREGATOR Test_6cc22f97-07b9-47b0-b027-5c60c4d0e429
INFO  [2023-01-17 00:52:52,853] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:52,853] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,982] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_AGGREGATOR Test
INFO  [2023-01-17 00:52:52,983] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-17 00:52:52,983] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:52,983] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:52,984] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-17 00:52:52,984] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:52,984] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-17 00:52:52,984] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:52,985] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:52,986] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_7a37e35d-75d4-473e-ab92-c9b1b49c4e3b are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:52,986] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_7a37e35d-75d4-473e-ab92-c9b1b49c4e3b are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:52,986] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:52,986] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_29e63e09-90ae-4a8d-9bda-8b0803282cc3 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:52,986] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_29e63e09-90ae-4a8d-9bda-8b0803282cc3 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:52,986] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:53,090] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,096] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,097] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DIFF_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:53,097] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table SUM_DIFF_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:53,217] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,328] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:53,329] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:53,329] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 103 B in total
INFO  [2023-01-17 00:52:53,329] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_DIFF_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000436427sINFO  [2023-01-17 00:52:53,373] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=4, sum=5, min=1, average=1.250000, max=2}
INFO  [2023-01-17 00:52:53,373] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sum] with IntegerParser(super=Parser(lines=5, nullLines=1), minValue=-1, maxValue=1)
INFO  [2023-01-17 00:52:53,373] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[sub] with IntegerParser(super=Parser(lines=5, nullLines=2), minValue=-1, maxValue=1)
INFO  [2023-01-17 00:52:53,373] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=5, nullLines=0), subType=IntegerParser(super=Parser(lines=5, nullLines=0), minValue=14805, maxValue=16019), dateReader=com.bakdata.conquery.util.DateReader@1c313fb3)
INFO  [2023-01-17 00:52:53,375] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_DIFF_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:53,375] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:53,375] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_SUM_DIFF_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:53,393] com.bakdata.conquery.models.jobs.ImportJob: Importing table into SUM_DIFF_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:53,394] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
127.0.0.1 - - [17/Jan/2023:00:52:53 +0000] "POST /admin/datasets/SUM_DIFF_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_SUM_DIFF_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 11
INFO  [2023-01-17 00:52:53,395] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:53,395] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:53,395] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:53,397] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:53,397] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DIFF_AGGREGATOR$20Test.table.table], containing 5 entries.
INFO  [2023-01-17 00:52:53,397] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[SUM_DIFF_AGGREGATOR$20Test.table.table], containing 5 entries.
WARN  [2023-01-17 00:52:53,398] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:53,398] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DIFF_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:53,399] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received SUM_DIFF_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:53,504] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,509] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,526] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,528] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:53,528] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:53,633] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: SUM_DIFF_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:53,650] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[SUM_DIFF_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:53,650] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[161993c2-40d6-4fe6-bd1d-573c9272156c] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DIFF_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:53,655] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c
INFO  [2023-01-17 00:52:53,655] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c
INFO  [2023-01-17 00:52:53,656] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c] with 1 results within PT0.001015S
INFO  [2023-01-17 00:52:53,656] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c] with 3 results within PT0.001053S
127.0.0.1 - - [17/Jan/2023:00:52:53 +0000] "POST /api/datasets/SUM_DIFF_AGGREGATOR$20Test/queries HTTP/1.1" 201 1327 "-" "Conquery (test client)" 9
INFO  [2023-01-17 00:52:53,657] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c, workerId=SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_7a37e35d-75d4-473e-ab92-c9b1b49c4e3b, startTime=2023-01-17T00:52:53.655457, finishTime=2023-01-17T00:52:53.656472) of size 1
INFO  [2023-01-17 00:52:53,657] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c, workerId=SUM_DIFF_AGGREGATOR$20Test.worker_SUM_DIFF_AGGREGATOR$20Test_29e63e09-90ae-4a8d-9bda-8b0803282cc3, startTime=2023-01-17T00:52:53.655455, finishTime=2023-01-17T00:52:53.656508) of size 3
INFO  [2023-01-17 00:52:53,657] com.bakdata.conquery.models.execution.ManagedExecution: DONE 161993c2-40d6-4fe6-bd1d-573c9272156c ManagedQuery within PT0.006929S
127.0.0.1 - - [17/Jan/2023:00:52:53 +0000] "GET /api/datasets/SUM_DIFF_AGGREGATOR$20Test/queries/SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c HTTP/1.1" 200 1622 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:53,684] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test], queryId=161993c2-40d6-4fe6-bd1d-573c9272156c, label=concept	@§$, creationTime=2023-01-17T00:52:53.650491, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1ca8e511[Count = 0], startTime=2023-01-17T00:52:53.650724, finishTime=2023-01-17T00:52:53.657653, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@53ed4c68), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DIFF_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@40c4f488, com.bakdata.conquery.models.query.ColumnDescriptor@365243b3, com.bakdata.conquery.models.query.ColumnDescriptor@6d09b805]) download on dataset Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:53,684] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test], queryId=161993c2-40d6-4fe6-bd1d-573c9272156c, label=concept	@§$, creationTime=2023-01-17T00:52:53.650491, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@1ca8e511[Count = 0], startTime=2023-01-17T00:52:53.650724, finishTime=2023-01-17T00:52:53.657653, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@53ed4c68), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_SUM_DIFF_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=4, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@40c4f488, com.bakdata.conquery.models.query.ColumnDescriptor@365243b3, com.bakdata.conquery.models.query.ColumnDescriptor@6d09b805]) on dataset Dataset[label=null, name=SUM_DIFF_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:53 +0000] "GET /api/datasets/SUM_DIFF_AGGREGATOR%20Test/result/SUM_DIFF_AGGREGATOR$20Test.161993c2-40d6-4fe6-bd1d-573c9272156c.csv?pretty=false HTTP/1.1" 200 140 "-" "Conquery (test client)" 20
INFO  [2023-01-17 00:52:53,702] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest SUM_DIFF_AGGREGATOR Test on 5 rows
INFO  [2023-01-17 00:52:53,703] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-17 00:52:53,703] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-17 00:52:53,703] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=SUM_DIFF_AGGREGATOR Test, name=SUM_DIFF_AGGREGATOR Test]
INFO  [2023-01-17 00:52:53,703] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DIFF_AGGREGATOR Test_7a37e35d-75d4-473e-ab92-c9b1b49c4e3b
INFO  [2023-01-17 00:52:53,703] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_SUM_DIFF_AGGREGATOR Test_29e63e09-90ae-4a8d-9bda-8b0803282cc3
INFO  [2023-01-17 00:52:53,784] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-17 00:52:53,801] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DIFF_AGGREGATOR Test_29e63e09-90ae-4a8d-9bda-8b0803282cc3
INFO  [2023-01-17 00:52:53,801] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of SUM_DIFF_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:53,801] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_SUM_DIFF_AGGREGATOR Test_7a37e35d-75d4-473e-ab92-c9b1b49c4e3b
INFO  [2023-01-17 00:52:53,801] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:53,934] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test SUM_DIFF_AGGREGATOR Test
INFO  [2023-01-17 00:52:53,934] com.bakdata.conquery.integration.IntegrationTest$Wrapper: STARTING integration test VALUES_AGGREGATOR Test
INFO  [2023-01-17 00:52:53,934] com.bakdata.conquery.util.support.TestConquery: Setting up dataset
INFO  [2023-01-17 00:52:53,934] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.NamespaceStorage
INFO  [2023-01-17 00:52:53,935] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-17 00:52:53,935] com.bakdata.conquery.models.messages.network.specific.AddWorker: creating a new worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-17 00:52:53,935] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:53,935] com.bakdata.conquery.io.storage.NamespacedStorage: Done reading null / com.bakdata.conquery.io.storage.WorkerStorage
INFO  [2023-01-17 00:52:53,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_9852b6ee-a0f2-49c8-b177-0eee95986330 are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:53,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_9852b6ee-a0f2-49c8-b177-0eee95986330 are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:53,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:53,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Imports of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_aa68e584-9e3e-476a-b565-3a66a74be0cd are consistent with the manager: 0 Imports
INFO  [2023-01-17 00:52:53,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Buckets of worker VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_aa68e584-9e3e-476a-b565-3a66a74be0cd are consistent with the manager: 0 Buckets
INFO  [2023-01-17 00:52:53,937] com.bakdata.conquery.models.messages.namespaces.specific.ReportConsistency: Consistency check was successful
INFO  [2023-01-17 00:52:53,942] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,041] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,048] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,048] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALUES_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:54,048] com.bakdata.conquery.models.messages.namespaces.specific.UpdateTable: Received update of Table VALUES_AGGREGATOR$20Test.table
INFO  [2023-01-17 00:52:54,162] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,275] com.bakdata.conquery.commands.PreprocessorCommand: Preprocessing from command line config.
INFO  [2023-01-17 00:52:54,276] com.bakdata.conquery.commands.PreprocessorCommand: DOES NOT EXIST
INFO  [2023-01-17 00:52:54,276] com.bakdata.conquery.commands.PreprocessorCommand: Required to preprocess 235 B in total
INFO  [2023-01-17 00:52:54,276] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING START in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VALUES_AGGREGATOR Test/table.import.json, tag=Optional.empty)
██████████████████████████████████████████████████▌ 100%	est. time remaining: 0.000250865sINFO  [2023-01-17 00:52:54,301] com.bakdata.conquery.models.preproc.Preprocessed: Statistics = IntSummaryStatistics{count=5, sum=13, min=1, average=2.600000, max=4}
INFO  [2023-01-17 00:52:54,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[value] with StringParser(super=Parser(lines=13, nullLines=1), encoding=null, prefix=, suffix=)
INFO  [2023-01-17 00:52:54,301] com.bakdata.conquery.models.preproc.PPColumn: Compute best Subtype for  Column[datum] with DateParser(super=Parser(lines=13, nullLines=0), subType=IntegerParser(super=Parser(lines=13, nullLines=0), minValue=14805, maxValue=15343), dateReader=com.bakdata.conquery.util.DateReader@64c19334)
INFO  [2023-01-17 00:52:54,304] com.bakdata.conquery.models.preproc.Preprocessor: PREPROCESSING DONE in PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VALUES_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:54,304] com.bakdata.conquery.commands.PreprocessorCommand: Successfully Preprocess 1 Jobs:
INFO  [2023-01-17 00:52:54,304] com.bakdata.conquery.commands.PreprocessorCommand: 	Succeeded Preprocessing for PreprocessingJob(descriptionFile=/tmp/conqueryIntegrationTest7483997657802395183/tmp_VALUES_AGGREGATOR Test/table.import.json, tag=Optional.empty)
INFO  [2023-01-17 00:52:54,324] com.bakdata.conquery.models.jobs.ImportJob: Importing table into VALUES_AGGREGATOR$20Test.table
127.0.0.1 - - [17/Jan/2023:00:52:54 +0000] "POST /admin/datasets/VALUES_AGGREGATOR%20Test/imports?file=%2Ftmp%2FconqueryIntegrationTest7483997657802395183%2Ftmp_VALUES_AGGREGATOR+Test%2Ftable.cqpp HTTP/1.1" 204 0 "-" "Conquery (test client)" 13
INFO  [2023-01-17 00:52:54,325] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,326] com.bakdata.conquery.models.jobs.ImportJob: Importing Dictionaries
INFO  [2023-01-17 00:52:54,327] com.bakdata.conquery.models.jobs.ImportJob: Remapping Dictionaries []
WARN  [2023-01-17 00:52:54,327] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: Max cannot be decreased.
INFO  [2023-01-17 00:52:54,329] com.bakdata.conquery.models.jobs.ImportJob: Start sending 2 Buckets
INFO  [2023-01-17 00:52:54,330] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALUES_AGGREGATOR$20Test.table.table], containing 13 entries.
INFO  [2023-01-17 00:52:54,330] com.bakdata.conquery.models.messages.namespaces.specific.AddImport: Received Import[VALUES_AGGREGATOR$20Test.table.table], containing 13 entries.
WARN  [2023-01-17 00:52:54,331] com.bakdata.conquery.util.progressreporter.ProgressReporterImpl: One or more Children are not done yet
INFO  [2023-01-17 00:52:54,331] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALUES_AGGREGATOR$20Test.table.table.0
INFO  [2023-01-17 00:52:54,331] com.bakdata.conquery.models.messages.namespaces.specific.ImportBucket: Received VALUES_AGGREGATOR$20Test.table.table.1
INFO  [2023-01-17 00:52:54,436] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,441] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,451] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,451] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:54,451] com.bakdata.conquery.models.messages.namespaces.specific.UpdateMatchingStatsMessage: BEGIN update Matching stats for 1 Concepts
INFO  [2023-01-17 00:52:54,557] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: VALUES_AGGREGATOR Test QUERY INIT
INFO  [2023-01-17 00:52:54,571] com.bakdata.conquery.apiv1.QueryProcessor: Query posted on Dataset[VALUES_AGGREGATOR$20Test] by User[{user.SUPERUSER@SUPERUSER].
INFO  [2023-01-17 00:52:54,572] com.bakdata.conquery.models.query.ExecutionManager: Executing Query[1ff8cf0f-6556-4487-a5e4-63782b507f90] in Datasets[[Namespace(storage=NamespacedStorage(pathName=dataset_VALUES_AGGREGATOR Test))]]
INFO  [2023-01-17 00:52:54,574] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90
INFO  [2023-01-17 00:52:54,574] com.bakdata.conquery.models.messages.namespaces.specific.ExecuteQuery: Started ConceptQuery VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90
127.0.0.1 - - [17/Jan/2023:00:52:54 +0000] "POST /api/datasets/VALUES_AGGREGATOR$20Test/queries HTTP/1.1" 201 1322 "-" "Conquery (test client)" 6
INFO  [2023-01-17 00:52:54,576] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90] with 2 results within PT0.001101S
INFO  [2023-01-17 00:52:54,577] com.bakdata.conquery.models.query.results.ShardResult: FINISHED Query[VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90] with 3 results within PT0.002425S
INFO  [2023-01-17 00:52:54,578] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90, workerId=VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_aa68e584-9e3e-476a-b565-3a66a74be0cd, startTime=2023-01-17T00:52:54.574832, finishTime=2023-01-17T00:52:54.577257) of size 3
INFO  [2023-01-17 00:52:54,578] com.bakdata.conquery.models.messages.namespaces.specific.CollectQueryResult: Received ShardResult(queryId=VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90, workerId=VALUES_AGGREGATOR$20Test.worker_VALUES_AGGREGATOR$20Test_9852b6ee-a0f2-49c8-b177-0eee95986330, startTime=2023-01-17T00:52:54.574927, finishTime=2023-01-17T00:52:54.576028) of size 2
INFO  [2023-01-17 00:52:54,578] com.bakdata.conquery.models.execution.ManagedExecution: DONE 1ff8cf0f-6556-4487-a5e4-63782b507f90 ManagedQuery within PT0.006085S
127.0.0.1 - - [17/Jan/2023:00:52:54 +0000] "GET /api/datasets/VALUES_AGGREGATOR$20Test/queries/VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90 HTTP/1.1" 200 1609 "-" "Conquery (test client)" 2
INFO  [2023-01-17 00:52:54,600] com.bakdata.conquery.resources.api.ResultCsvResource: Result for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALUES_AGGREGATOR Test], queryId=1ff8cf0f-6556-4487-a5e4-63782b507f90, label=concept	@§$, creationTime=2023-01-17T00:52:54.571902, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@776bb6f5[Count = 0], startTime=2023-01-17T00:52:54.572076, finishTime=2023-01-17T00:52:54.578161, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@37c8cafa), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALUES_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2a1a823d, com.bakdata.conquery.models.query.ColumnDescriptor@5c3911e7, com.bakdata.conquery.models.query.ColumnDescriptor@76cfd88b]) download on dataset Dataset[label=null, name=VALUES_AGGREGATOR Test] by subject user.SUPERUSER@SUPERUSER (SUPERUSER@SUPERUSER).
INFO  [2023-01-17 00:52:54,600] com.bakdata.conquery.io.result.csv.ResultCsvProcessor: Downloading results for ManagedQuery(super=ManagedExecution(dataset=Dataset[label=null, name=VALUES_AGGREGATOR Test], queryId=1ff8cf0f-6556-4487-a5e4-63782b507f90, label=concept	@§$, creationTime=2023-01-17T00:52:54.571902, owner=User[user.SUPERUSER@SUPERUSER], tags=[], shared=false, machineGenerated=false, state=DONE, execution=java.util.concurrent.CountDownLatch@776bb6f5[Count = 0], startTime=2023-01-17T00:52:54.572076, finishTime=2023-01-17T00:52:54.578161, error=null, progress=null, initialized=true, executionManager=com.bakdata.conquery.models.query.ExecutionManager@37c8cafa), namespace=Namespace(storage=NamespacedStorage(pathName=dataset_VALUES_AGGREGATOR Test)), query=com.bakdata.conquery.apiv1.query.ConceptQuery@1, lastResultCount=5, involvedWorkers=2, executingThreads=0, config=Configuration{server=DefaultServerFactory{applicationConnectors=[io.dropwizard.jetty.HttpConnectorFactory@60f14d7f], adminConnectors=[io.dropwizard.jetty.HttpConnectorFactory@32483c45], adminMaxThreads=64, adminMinThreads=1, applicationContextPath='/', adminContextPath='/'}, logging=DefaultLoggingFactory{level=INFO, loggers={}, appenders=[io.dropwizard.logging.ConsoleAppenderFactory@1351979]}, metrics=MetricsFactory{frequency=1 minute, reporters=[], reportOnStop=false}, admin=AdminFactory[healthChecks=HealthCheckConfiguration[minThreads=1, maxThreads=4, workQueueSize=1], tasks=TaskConfiguration[printStackTraceOnError=false]]}, columnDescriptions=[com.bakdata.conquery.models.query.ColumnDescriptor@2a1a823d, com.bakdata.conquery.models.query.ColumnDescriptor@5c3911e7, com.bakdata.conquery.models.query.ColumnDescriptor@76cfd88b]) on dataset Dataset[label=null, name=VALUES_AGGREGATOR Test]
127.0.0.1 - - [17/Jan/2023:00:52:54 +0000] "GET /api/datasets/VALUES_AGGREGATOR%20Test/result/VALUES_AGGREGATOR$20Test.1ff8cf0f-6556-4487-a5e4-63782b507f90.csv?pretty=false HTTP/1.1" 200 193 "-" "Conquery (test client)" 19
INFO  [2023-01-17 00:52:54,618] com.bakdata.conquery.integration.json.AbstractQueryEngineTest: INTEGRATION TEST SUCCESSFUL QueryTest VALUES_AGGREGATOR Test on 6 rows
INFO  [2023-01-17 00:52:54,618] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast VALUES_AGGREGATOR Test
INFO  [2023-01-17 00:52:54,619] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-17 00:52:54,619] com.bakdata.conquery.models.messages.network.specific.RemoveWorker: removing worker for Dataset[label=VALUES_AGGREGATOR Test, name=VALUES_AGGREGATOR Test]
INFO  [2023-01-17 00:52:54,619] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALUES_AGGREGATOR Test_9852b6ee-a0f2-49c8-b177-0eee95986330
INFO  [2023-01-17 00:52:54,619] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast worker_VALUES_AGGREGATOR Test_aa68e584-9e3e-476a-b565-3a66a74be0cd
INFO  [2023-01-17 00:52:54,635] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow VALUES_AGGREGATOR Test
INFO  [2023-01-17 00:52:54,637] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALUES_AGGREGATOR Test_aa68e584-9e3e-476a-b565-3a66a74be0cd
INFO  [2023-01-17 00:52:54,637] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow worker_VALUES_AGGREGATOR Test_9852b6ee-a0f2-49c8-b177-0eee95986330
INFO  [2023-01-17 00:52:54,731] com.bakdata.conquery.models.worker.Namespace: Removing namespace storage of VALUES_AGGREGATOR$20Test
INFO  [2023-01-17 00:52:54,732] com.bakdata.conquery.util.support.TestConquery: Waiting for jobs to finish
INFO  [2023-01-17 00:52:54,757] com.bakdata.conquery.integration.IntegrationTest$Wrapper: SUCCESS integration test VALUES_AGGREGATOR Test
[INFO] Tests run: 172, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 175.122 s - in com.bakdata.conquery.integration.ConqueryIntegrationTests
[INFO] Running com.bakdata.conquery.tasks.PermissionCleanupTaskTest
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.tasks.PermissionCleanupTaskTest
[INFO] Running com.bakdata.conquery.tasks.IsUUIDTestTest
INFO  [2023-01-17 00:52:54,765] com.bakdata.conquery.tasks.IsUUIDTestTest: ec7c3deb-39d2-4d3f-9c54-7af1f5c02fca
INFO  [2023-01-17 00:52:54,766] com.bakdata.conquery.tasks.IsUUIDTestTest: f0e60729-c565-4687-8c0d-c989b30a4421
INFO  [2023-01-17 00:52:54,766] com.bakdata.conquery.tasks.IsUUIDTestTest: 85a4a183-f6f0-41a5-9cac-0cc9c649dca5
INFO  [2023-01-17 00:52:54,767] com.bakdata.conquery.tasks.IsUUIDTestTest: 8c620ce2-3353-4247-b6c2-701aff2e2722
INFO  [2023-01-17 00:52:54,767] com.bakdata.conquery.tasks.IsUUIDTestTest: 624ea994-39b5-4707-b6c8-97cf25689bce
INFO  [2023-01-17 00:52:54,768] com.bakdata.conquery.tasks.IsUUIDTestTest: 04aff740-c63e-4d65-a464-c5fba5f349ac
INFO  [2023-01-17 00:52:54,768] com.bakdata.conquery.tasks.IsUUIDTestTest: e9a04c9e-dfbb-4286-a170-9982dc48e90d
INFO  [2023-01-17 00:52:54,769] com.bakdata.conquery.tasks.IsUUIDTestTest: 8d63c84e-c1e5-4265-9855-6d02e4107469
INFO  [2023-01-17 00:52:54,769] com.bakdata.conquery.tasks.IsUUIDTestTest: dcbd320d-4b0e-4eff-b711-14b689782df7
INFO  [2023-01-17 00:52:54,769] com.bakdata.conquery.tasks.IsUUIDTestTest: 61b22bb6-dc01-4055-a92b-11331d7ff367
INFO  [2023-01-17 00:52:54,770] com.bakdata.conquery.tasks.IsUUIDTestTest: 9da1c4c9-c453-48f2-93dd-455eca0b04a3
INFO  [2023-01-17 00:52:54,770] com.bakdata.conquery.tasks.IsUUIDTestTest: 9af20cc5-a5bf-42a1-a8b1-a6cece081313
INFO  [2023-01-17 00:52:54,771] com.bakdata.conquery.tasks.IsUUIDTestTest: 30583ed5-21ee-4ebe-a33f-89b32b25232d
INFO  [2023-01-17 00:52:54,771] com.bakdata.conquery.tasks.IsUUIDTestTest: 3a2e7b5e-431c-423d-a5a7-0a18b220b206
INFO  [2023-01-17 00:52:54,771] com.bakdata.conquery.tasks.IsUUIDTestTest: 73c53fb5-758e-4d6c-85df-b8404394e704
INFO  [2023-01-17 00:52:54,772] com.bakdata.conquery.tasks.IsUUIDTestTest: 07175358-ebbd-45fa-aab8-a6047646cd04
INFO  [2023-01-17 00:52:54,773] com.bakdata.conquery.tasks.IsUUIDTestTest: 84318139-e04b-4442-b04d-90c20807fb15
INFO  [2023-01-17 00:52:54,775] com.bakdata.conquery.tasks.IsUUIDTestTest: 1b4e75ec-26d4-4da8-adc9-ce8aa74abeb5
INFO  [2023-01-17 00:52:54,775] com.bakdata.conquery.tasks.IsUUIDTestTest: 2fdc08ae-dba2-43e9-a779-9de20309f6ae
INFO  [2023-01-17 00:52:54,775] com.bakdata.conquery.tasks.IsUUIDTestTest: bff9ff39-26ab-44ca-9993-0e9fd6e51672
INFO  [2023-01-17 00:52:54,775] com.bakdata.conquery.tasks.IsUUIDTestTest: 78696e5d-36ec-4087-848f-4f3ac34559be
INFO  [2023-01-17 00:52:54,776] com.bakdata.conquery.tasks.IsUUIDTestTest: e9380ddd-b0f8-433e-a327-170318cca32d
INFO  [2023-01-17 00:52:54,776] com.bakdata.conquery.tasks.IsUUIDTestTest: ad83b5ab-f427-4164-83fe-1340a5ab1543
INFO  [2023-01-17 00:52:54,776] com.bakdata.conquery.tasks.IsUUIDTestTest: 503962e6-db51-4a5d-b6b4-e04db4df0190
INFO  [2023-01-17 00:52:54,777] com.bakdata.conquery.tasks.IsUUIDTestTest: 28389d79-ee66-48b6-be4e-66d4a94b1816
INFO  [2023-01-17 00:52:54,777] com.bakdata.conquery.tasks.IsUUIDTestTest: 5ec9abee-64c3-48ad-b78e-ee6da651b186
INFO  [2023-01-17 00:52:54,777] com.bakdata.conquery.tasks.IsUUIDTestTest: 55a1527b-c5e1-4cdd-95fb-a6d617a2dc71
INFO  [2023-01-17 00:52:54,777] com.bakdata.conquery.tasks.IsUUIDTestTest: f6e2cd1a-bc5f-481a-a050-368145056029
INFO  [2023-01-17 00:52:54,778] com.bakdata.conquery.tasks.IsUUIDTestTest: 8b3f4e54-64ac-4d6c-b1ae-62729dbd100e
INFO  [2023-01-17 00:52:54,778] com.bakdata.conquery.tasks.IsUUIDTestTest: 99b1b5ae-85fc-4c8a-af04-e6efd45b75af
INFO  [2023-01-17 00:52:54,778] com.bakdata.conquery.tasks.IsUUIDTestTest: a268df66-92e9-4ffc-ade6-3f325dce763d
INFO  [2023-01-17 00:52:54,778] com.bakdata.conquery.tasks.IsUUIDTestTest: c312edc2-425c-4119-8432-b863f5809319
INFO  [2023-01-17 00:52:54,779] com.bakdata.conquery.tasks.IsUUIDTestTest: 71a3d0de-34bc-4dc8-9e1f-0181be14f166
INFO  [2023-01-17 00:52:54,779] com.bakdata.conquery.tasks.IsUUIDTestTest: d5a2cf4a-5226-4937-be13-36bd70f88323
INFO  [2023-01-17 00:52:54,779] com.bakdata.conquery.tasks.IsUUIDTestTest: d62bc036-0632-4bbc-86e8-52b4a17f2ee6
INFO  [2023-01-17 00:52:54,780] com.bakdata.conquery.tasks.IsUUIDTestTest: 84509da5-3b10-481f-80f0-d3c5afd120bd
INFO  [2023-01-17 00:52:54,780] com.bakdata.conquery.tasks.IsUUIDTestTest: 2c678c88-1962-420d-8910-5c6e515e34d8
INFO  [2023-01-17 00:52:54,780] com.bakdata.conquery.tasks.IsUUIDTestTest: 1b412bc1-9397-453b-b638-a795a36bbef5
INFO  [2023-01-17 00:52:54,780] com.bakdata.conquery.tasks.IsUUIDTestTest: b58b9113-bd8c-4f53-a93a-16110a91cc83
INFO  [2023-01-17 00:52:54,781] com.bakdata.conquery.tasks.IsUUIDTestTest: 5ffa3b8e-18dd-49f7-b5df-9e22f5377578
INFO  [2023-01-17 00:52:54,781] com.bakdata.conquery.tasks.IsUUIDTestTest: 8db81d0d-8def-40e1-ba2b-a4f8e032672a
INFO  [2023-01-17 00:52:54,781] com.bakdata.conquery.tasks.IsUUIDTestTest: 474ad015-c9f7-44b4-bb5f-8730f1a4accc
INFO  [2023-01-17 00:52:54,781] com.bakdata.conquery.tasks.IsUUIDTestTest: 415b8272-f110-4953-9740-4223f897ed15
INFO  [2023-01-17 00:52:54,782] com.bakdata.conquery.tasks.IsUUIDTestTest: 5fc8084b-86f7-416f-a658-90dab87cf47b
INFO  [2023-01-17 00:52:54,782] com.bakdata.conquery.tasks.IsUUIDTestTest: 6670dd30-2773-46bf-bc21-c25128f3b28e
INFO  [2023-01-17 00:52:54,782] com.bakdata.conquery.tasks.IsUUIDTestTest: ab6e2587-cb24-421a-92a7-f7a790453bb0
INFO  [2023-01-17 00:52:54,782] com.bakdata.conquery.tasks.IsUUIDTestTest: 418e7a60-ec3c-43da-9e2b-531a35834c52
INFO  [2023-01-17 00:52:54,783] com.bakdata.conquery.tasks.IsUUIDTestTest: a5a2c172-0c48-4d0e-8aec-2bb7178e29dd
INFO  [2023-01-17 00:52:54,783] com.bakdata.conquery.tasks.IsUUIDTestTest: 1549ff47-dc0f-48e0-9f8c-2467bc2a09e4
INFO  [2023-01-17 00:52:54,783] com.bakdata.conquery.tasks.IsUUIDTestTest: 6e35ae86-ad66-4efd-8f09-1cdf7b584648
[INFO] Tests run: 51, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 s - in com.bakdata.conquery.tasks.IsUUIDTestTest
[INFO] Running com.bakdata.conquery.tasks.QueryCleanupTaskTest
INFO  [2023-01-17 00:52:54,785] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 1
INFO  [2023-01-17 00:52:54,785] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-17 00:52:54,786] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 2
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT720H of 1
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,787] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
INFO  [2023-01-17 00:52:54,788] com.bakdata.conquery.tasks.QueryCleanupTask: Starting deletion of queries older than PT719H of 1
INFO  [2023-01-17 00:52:54,788] com.bakdata.conquery.tasks.QueryCleanupTask: Deleting 1 Executions
INFO  [2023-01-17 00:52:54,788] com.bakdata.conquery.tasks.QueryCleanupTask: No queries to delete
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.tasks.QueryCleanupTaskTest
[INFO] Running com.bakdata.conquery.io.storage.xodus.stores.SerializingStoreDumpTest
INFO  [2023-01-17 00:52:54,815] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/1673916774794-0
INFO  [2023-01-17 00:52:54,815] com.bakdata.conquery.io.storage.xodus.stores.SerializingStoreDumpTest: This test will throw some warnings from the SerializingStore.
WARN  [2023-01-17 00:52:54,829] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse value for key [user.testU2]
WARN  [2023-01-17 00:52:54,830] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse key [:)
Vnot a valid conquery Id]
WARN  [2023-01-17 00:52:54,830] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Removing 2 unreadable elements from the store AUTH_USER.
WARN  [2023-01-17 00:52:54,832] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Removing 0 unreadable elements from the store AUTH_USER.
INFO  [2023-01-17 00:52:54,861] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/1673916774838-0
INFO  [2023-01-17 00:52:54,880] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Dumping value of key :)
Vnot a valid conquery Id to /tmp/1673916774838-0/20230117-AUTH_USER-____Vnot a valid conquery Id.json (because it cannot be deserialized anymore).
WARN  [2023-01-17 00:52:54,880] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse key [:)
Vnot a valid conquery Id]
INFO  [2023-01-17 00:52:54,908] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/1673916774889-0
INFO  [2023-01-17 00:52:54,923] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Dumping value of key user.testU2 to /tmp/1673916774889-0/20230117-AUTH_USER-user.testU2.json (because it cannot be deserialized anymore).
WARN  [2023-01-17 00:52:54,923] com.bakdata.conquery.io.storage.xodus.stores.SerializingStore: Could not parse value for key [user.testU2]
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.137 s - in com.bakdata.conquery.io.storage.xodus.stores.SerializingStoreDumpTest
[INFO] Running com.bakdata.conquery.io.storage.xodus.stores.BigStoreTest
INFO  [2023-01-17 00:52:54,951] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/BigStoreTest2380889876695957018
INFO  [2023-01-17 00:52:55,001] jetbrains.exodus.env.EnvironmentImpl: Exodus environment created: /tmp/BigStoreTest1322805703992098002
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.018 s - in com.bakdata.conquery.io.storage.xodus.stores.BigStoreTest
[INFO] Running com.bakdata.conquery.io.jackson.SerializationBlockTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.io.jackson.SerializationBlockTest
[INFO] Running com.bakdata.conquery.io.jackson.JacksonTest
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 s - in com.bakdata.conquery.io.jackson.JacksonTest
[INFO] Running com.bakdata.conquery.io.jackson.serializer.RangeSerializerTest
[INFO] Tests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.266 s - in com.bakdata.conquery.io.jackson.serializer.RangeSerializerTest
[INFO] Running com.bakdata.conquery.io.jackson.serializer.ClassToInstanceMapDeserializerTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in com.bakdata.conquery.io.jackson.serializer.ClassToInstanceMapDeserializerTest
[INFO] Running com.bakdata.conquery.io.jackson.serializer.IdRefrenceTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.041 s - in com.bakdata.conquery.io.jackson.serializer.IdRefrenceTest
[INFO] Running com.bakdata.conquery.io.cps.CPSBaseTest
[INFO] Tests run: 188, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.bakdata.conquery.io.cps.CPSBaseTest
[INFO] Running com.bakdata.conquery.io.result.csv.CsvResultGenerationTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.004 s - in com.bakdata.conquery.io.result.csv.CsvResultGenerationTest
INFO  [2023-01-17 00:52:58,325] com.bakdata.conquery.io.result.csv.CsvResultGenerationTest: Wrote and than read this csv data: id1,id2,BOOLEAN,INTEGER,NUMERIC,CATEGORICAL,RESOLUTION,DATE,DATE_RANGE,STRING,MONEY,LIST[BOOLEAN],LIST[DATE_RANGE],LIST[STRING]
1,1,Ja,2.345.634,"123.423,34",CAT1,Tag,17.06.1985,12.12.1970 - 19.06.1971,test_string,"45,21","Ja, Nein","12.12.1970 - 19.06.1971, 02.01.1970 - 03.01.1970","fizz, buzz"
2,2,Nein,,,,,,,,,,19.05.1973 - +∞,
2,2,Ja,,,,,,,,,"Nein, Nein",,
3,3,Nein,,,,,,,,,Nein,,
3,3,Ja,,,,,,,,,,,
3,3,Ja,,,,,,,,"0,04","Ja, Nein, Ja, Nein",,

[INFO] Running com.bakdata.conquery.io.result.arrow.ArrowResultGenerationTest
INFO  [2023-01-17 00:52:58,332] org.apache.arrow.memory.BaseAllocator: Debug mode enabled.
INFO  [2023-01-17 00:52:58,334] org.apache.arrow.memory.DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
INFO  [2023-01-17 00:52:58,335] org.apache.arrow.memory.CheckAllocator: Using DefaultAllocationManager at memory-netty/6.0.1/arrow-memory-netty-6.0.1.jar!/org/apache/arrow/memory/DefaultAllocationManagerFactory.class
INFO  [2023-01-17 00:52:58,504] com.bakdata.conquery.io.result.arrow.ArrowResultGenerationTest: Reading the produced arrow data.
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.239 s - in com.bakdata.conquery.io.result.arrow.ArrowResultGenerationTest
[INFO] Running com.bakdata.conquery.io.result.excel.ExcelResultRenderTest
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.507 s <<< FAILURE! - in com.bakdata.conquery.io.result.excel.ExcelResultRenderTest
[ERROR] writeAndRead  Time elapsed: 0.507 s  <<< ERROR!
java.lang.AbstractMethodError: Receiver class org.apache.poi.xssf.streaming.SXSSFRow does not define or inherit an implementation of the resolved method 'java.util.Iterator iterator()' of interface org.apache.poi.ss.usermodel.Row.
	at org.apache.poi.xssf.streaming.AutoSizeColumnTracker.updateColumnWidths(AutoSizeColumnTracker.java:323)
	at org.apache.poi.xssf.streaming.SXSSFSheet.flushOneRow(SXSSFSheet.java:1806)
	at org.apache.poi.xssf.streaming.SXSSFSheet.flushRows(SXSSFSheet.java:1775)
	at org.apache.poi.xssf.streaming.SXSSFSheet.flushRows(SXSSFSheet.java:1788)
	at org.apache.poi.xssf.streaming.SXSSFSheet.dispose(SXSSFSheet.java:1831)
	at org.apache.poi.xssf.streaming.SXSSFWorkbook.dispose(SXSSFWorkbook.java:1031)
	at com.bakdata.conquery.io.result.excel.ExcelRenderer.renderToStream(ExcelRenderer.java:94)
	at com.bakdata.conquery.io.result.excel.ExcelResultRenderTest.writeAndRead(ExcelResultRenderTest.java:86)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[INFO] Running com.bakdata.conquery.io.result.ResultNameTest
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in com.bakdata.conquery.io.result.ResultNameTest
INFO  [2023-01-17 00:52:59,110] org.eclipse.jetty.server.AbstractConnector: Stopped application@710ee5b1{HTTP/1.1, (http/1.1)}{0.0.0.0:42273}
INFO  [2023-01-17 00:52:59,110] org.eclipse.jetty.server.AbstractConnector: Stopped admin@23968461{HTTP/1.1, (http/1.1)}{0.0.0.0:36369}
INFO  [2023-01-17 00:52:59,133] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@4d8f7bfe{/,null,STOPPED}
INFO  [2023-01-17 00:52:59,136] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@60786bcf{/,null,STOPPED}
INFO  [2023-01-17 00:52:59,137] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-17 00:52:59,178] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-17 00:52:59,179] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:52:59,179] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:52:59,179] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:52:59,195] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-17 00:52:59,278] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-17 00:52:59,279] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:52:59,279] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:52:59,279] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:52:59,286] com.bakdata.conquery.models.auth.apitoken.TokenStorage: Closing the environment.
INFO  [2023-01-17 00:52:59,297] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-17 00:52:59,349] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-17 00:52:59,379] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/executions:EXECUTIONS}
INFO  [2023-01-17 00:52:59,379] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/executions
INFO  [2023-01-17 00:52:59,390] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/formConfigs:FORM_CONFIG}
INFO  [2023-01-17 00:52:59,390] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/formConfigs
INFO  [2023-01-17 00:52:59,406] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/users:AUTH_USER}
INFO  [2023-01-17 00:52:59,406] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/users
INFO  [2023-01-17 00:52:59,416] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/roles:AUTH_ROLE}
INFO  [2023-01-17 00:52:59,417] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/roles
INFO  [2023-01-17 00:52:59,427] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/groups:AUTH_GROUP}
INFO  [2023-01-17 00:52:59,427] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/ApiTokenRealmTest/manager/meta/groups
INFO  [2023-01-17 00:52:59,447] org.eclipse.jetty.server.AbstractConnector: Stopped application@2a1da4ae{HTTP/1.1, (http/1.1)}{0.0.0.0:38705}
INFO  [2023-01-17 00:52:59,448] org.eclipse.jetty.server.AbstractConnector: Stopped admin@16c690dc{HTTP/1.1, (http/1.1)}{0.0.0.0:44953}
INFO  [2023-01-17 00:52:59,455] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@28a33c23{/,null,STOPPED}
INFO  [2023-01-17 00:52:59,459] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@2c099b35{/,null,STOPPED}
INFO  [2023-01-17 00:52:59,460] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-17 00:52:59,479] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-17 00:52:59,579] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:52:59,579] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:52:59,579] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:52:59,580] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-17 00:52:59,679] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-17 00:52:59,780] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:52:59,780] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:52:59,780] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:52:59,780] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-17 00:52:59,813] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-17 00:52:59,883] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions:EXECUTIONS}
INFO  [2023-01-17 00:52:59,883] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/executions
INFO  [2023-01-17 00:52:59,896] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs:FORM_CONFIG}
INFO  [2023-01-17 00:52:59,896] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/formConfigs
INFO  [2023-01-17 00:52:59,907] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users:AUTH_USER}
INFO  [2023-01-17 00:52:59,907] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/users
INFO  [2023-01-17 00:52:59,918] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles:AUTH_ROLE}
INFO  [2023-01-17 00:52:59,918] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/roles
INFO  [2023-01-17 00:52:59,928] com.bakdata.conquery.models.config.XodusStoreFactory: Closed XodusStore: XodusStore[/tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups:AUTH_GROUP}
INFO  [2023-01-17 00:52:59,928] com.bakdata.conquery.models.config.XodusStoreFactory: Closed last XodusStore in Environment. Closing Environment as well: /tmp/conqueryIntegrationTest17490523365462791536/manager/meta/groups
INFO  [2023-01-17 00:52:59,952] org.eclipse.jetty.server.AbstractConnector: Stopped application@2fd2b877{HTTP/1.1, (http/1.1)}{0.0.0.0:34263}
INFO  [2023-01-17 00:52:59,952] org.eclipse.jetty.server.AbstractConnector: Stopped admin@71cf415f{HTTP/1.1, (http/1.1)}{0.0.0.0:40167}
INFO  [2023-01-17 00:52:59,959] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@1449d690{/,null,STOPPED}
INFO  [2023-01-17 00:52:59,963] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@52836725{/,null,STOPPED}
INFO  [2023-01-17 00:52:59,963] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-17 00:53:00,001] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-17 00:53:00,080] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:53:00,080] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:53:00,080] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:53:00,081] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-17 00:53:00,101] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-17 00:53:00,180] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:53:00,180] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:53:00,180] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:53:00,181] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-17 00:53:00,264] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
INFO  [2023-01-17 00:53:00,306] org.eclipse.jetty.server.AbstractConnector: Stopped application@3239a0b6{HTTP/1.1, (http/1.1)}{0.0.0.0:45089}
INFO  [2023-01-17 00:53:00,307] org.eclipse.jetty.server.AbstractConnector: Stopped admin@1c699f7{HTTP/1.1, (http/1.1)}{0.0.0.0:46479}
INFO  [2023-01-17 00:53:00,312] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@ac8b0e5{/,null,STOPPED}
INFO  [2023-01-17 00:53:00,316] org.eclipse.jetty.server.handler.ContextHandler: Stopped i.d.j.MutableServletContextHandler@536093ae{/,null,STOPPED}
INFO  [2023-01-17 00:53:00,316] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node1
INFO  [2023-01-17 00:53:00,379] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node1
INFO  [2023-01-17 00:53:00,380] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:53:00,380] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:53:00,380] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:53:00,381] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast shard-node0
INFO  [2023-01-17 00:53:00,480] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow shard-node0
INFO  [2023-01-17 00:53:00,480] com.bakdata.conquery.commands.ShardNode: Connection was closed by ManagerNode
INFO  [2023-01-17 00:53:00,480] com.bakdata.conquery.commands.ShardNode: Disconnected from ManagerNode
INFO  [2023-01-17 00:53:00,480] com.bakdata.conquery.commands.ManagerNode: Client 'null' disconnected 
INFO  [2023-01-17 00:53:00,481] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager fast ManagerNode
INFO  [2023-01-17 00:53:00,495] com.bakdata.conquery.models.jobs.JobExecutor: Closing Job Manager slow ManagerNode
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   ExcelResultRenderTest.writeAndRead:86 » AbstractMethod Receiver class org.apac...
[INFO] 
[ERROR] Tests run: 1666, Failures: 0, Errors: 1, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Conquery Parent 0.0.0-SNAPSHOT:
[INFO] 
[INFO] Conquery Parent .................................... SUCCESS [  0.137 s]
[INFO] backend ............................................ FAILURE [03:31 min]
[INFO] executable ......................................... SKIPPED
[INFO] autodoc ............................................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  03:31 min
[INFO] Finished at: 2023-01-17T00:53:01Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.2:test (default-test) on project backend: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/gabsko/breaking-updates/backend/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :backend
